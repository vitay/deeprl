@unpublished{Abdulsamad2020,
  title = {Hierarchical {{Decomposition}} of {{Nonlinear Dynamics}} and {{Control}} for {{System Identification}} and {{Policy Distillation}}},
  author = {Abdulsamad, Hany and Peters, Jan},
  date = {2020-05-04},
  eprint = {2005.01432},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2005.01432},
  urldate = {2020-05-10},
  abstract = {The control of nonlinear dynamical systems remains a major challenge for autonomous agents. Current trends in reinforcement learning (RL) focus on complex representations of dynamics and policies, which have yielded impressive results in solving a variety of hard control tasks. However, this new sophistication and extremely over-parameterized models have come with the cost of an overall reduction in our ability to interpret the resulting policies. In this paper, we take inspiration from the control community and apply the principles of hybrid switching systems in order to break down complex dynamics into simpler components. We exploit the rich representational power of probabilistic graphical models and derive an expectation-maximization (EM) algorithm for learning a sequence model to capture the temporal structure of the data and automatically decompose nonlinear dynamics into stochastic switching linear dynamical systems. Moreover, we show how this framework of switching models enables extracting hierarchies of Markovian and auto-regressive locally linear controllers from nonlinear experts in an imitation learning scenario.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/XBG9X5CY/Abdulsamad_Peters_2020_Hierarchical_Decomposition_of_Nonlinear_Dynamics_and_Control_for_System.pdf;/Users/vitay/Documents/Zotero/storage/29K5EL6Q/2005.html}
}

@online{Abramson2022,
  title = {Creating {{Multimodal Interactive Agents}} with {{Imitation}} and {{Self-Supervised Learning}}},
  author = {Abramson, Josh and Ahuja, Arun and Brussee, Arthur and Carnevale, Federico and Cassin, Mary and Fischer, Felix and Georgiev, Petko and Goldin, Alex and Gupta, Mansi and Harley, Tim and Hill, Felix and Humphreys, Peter C. and Hung, Alden and Landon, Jessica and Lillicrap, Timothy and Merzic, Hamza and Muldal, Alistair and Santoro, Adam and Scully, Guy and family=Glehn, given=Tamara, prefix=von, useprefix=true and Wayne, Greg and Wong, Nathaniel and Yan, Chen and Zhu, Rui},
  date = {2022-02-02},
  eprint = {2112.03763},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.03763},
  url = {http://arxiv.org/abs/2112.03763},
  urldate = {2022-09-13},
  abstract = {A common vision from science fiction is that robots will one day inhabit our physical spaces, sense the world as we do, assist our physical labours, and communicate with us through natural language. Here we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment. We show that imitation learning of human-human interactions in a simulated world, in conjunction with self-supervised learning, is sufficient to produce a multimodal interactive agent, which we call MIA, that successfully interacts with non-adversarial humans 75\% of the time. We further identify architectural and algorithmic techniques that improve performance, such as hierarchical action selection. Altogether, our results demonstrate that imitation of multi-modal, real-time human behaviour may provide a straightforward and surprisingly effective means of imbuing agents with a rich behavioural prior from which agents might then be fine-tuned for specific purposes, thus laying a foundation for training capable agents for interactive robots or digital assistants. A video of MIA's behaviour may be found at https://youtu.be/ZFgRhviF7mY},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/T9ZK2YM2/Abramson_et_al_2022_Creating_Multimodal_Interactive_Agents_with_Imitation_and_Self-Supervised.pdf;/Users/vitay/Documents/Zotero/storage/87GE8QKN/2112.html}
}

@article{Aburaya2024,
  title = {Review of Vision-Based Reinforcement Learning for Drone Navigation},
  author = {Aburaya, Anas and Selamat, Hazlina and Muslim, Mohd Taufiq},
  date = {2024-06-28},
  journaltitle = {International Journal of Intelligent Robotics and Applications},
  shortjournal = {Int J Intell Robot Appl},
  issn = {2366-598X},
  doi = {10.1007/s41315-024-00356-9},
  url = {https://doi.org/10.1007/s41315-024-00356-9},
  urldate = {2024-10-11},
  abstract = {In recent years, Unmanned aerial vehicles (UAVs) have witnessed a surge in popularity and implementation for both civilian and military usage. UAVs can be utilized for a wide range of applications, including mapping, surveillance, and inspection. For many of these applications, a high level of autonomy is required. Autonomy refers to the ability to complete missions or tasks without human intervention. Autonomous navigation is an essential element of autonomy, especially in GPS-denied environments where GNSS-based navigation is not reliable. Due to size and weight limitations, many UAVs employ vision-based localization and navigation techniques for GPS-denied environments. Reinforcement Learning (RL) is also increasingly being implemented for robotic applications, including obstacle avoidance, battery management, and navigation. Existing reviews typically focus on either vision-based autonomous navigation of drones or RL navigation for drones in general, but none specifically concentrate on the use of vision-based methods and RL for drone navigation. Moreover, previous reviews have highlighted the use of reinforcement learning based on tasks such as takeoff, landing, and navigation, whereas this review categorizes the use of RL based on the navigation problem and image input types for the RL models as these define the needed hardware and processing capabilities of the system. We define the current challenges and limitations for vision based RL navigation to provide direction for future works. Finally we provide an analysis of the favorable conditions for each category and the possibility of combining multiple categories to overcome the disadvantages of each.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/YFCRB4V6/Aburaya et al. - 2024 - Review of vision-based reinforcement learning for drone navigation.pdf}
}

@inproceedings{Agarwal2014,
  title = {Taming the {{Monster}}: {{A Fast}} and {{Simple Algorithm}} for {{Contextual Bandits}}},
  booktitle = {Proceedings of the 31 St {{International Conference}} on {{Machine Learning}}},
  author = {Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert E},
  date = {2014},
  pages = {9},
  location = {Beijing, China},
  url = {https://arxiv.org/abs/1402.0555},
  abstract = {We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes one of K actions in response to the observed context, and observes the reward only for that action. Our method assumes access to an oracle for solving fully supervised costsensitive classification problems and achieves the sO˜ta(t√isKticTal)lyoroapctliemcaalllrsegarcertosgsuaarlal nTteerowunitdhs.onBlyy doing so, we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes. We conduct a proof-of-concept experiment which demonstrates the excellent computational and statistical performance of (an online variant of) our algorithm relative to several strong baselines.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/2B82Q2GJ/Agarwal et al. - Taming the Monster A Fast and Simple Algorithm fo.pdf}
}

@inproceedings{Agarwal2021,
  title = {Deep {{Reinforcement Learning}} at the {{Edge}} of the {{Statistical Precipice}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  date = {2021},
  volume = {34},
  pages = {29304--29320},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/f514cec81cb148559cf475e7426eed5e-Abstract.html},
  urldate = {2023-08-14},
  abstract = {Deep reinforcement learning (RL) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep RL benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a finite number of training runs. Beginning with the Arcade Learning Environment (ALE), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates. In this paper, we argue that reliable evaluation in the few run deep RL regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the field. We illustrate this point using a case study on the Atari 100k benchmark, where we find substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. With the aim of increasing the field's confidence in reported results with a handful of runs, we advocate for reporting interval estimates of aggregate performance and propose performance profiles to account for the variability in results, as well as present more robust and efficient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results. Using such statistical tools, we scrutinize performance evaluations of existing algorithms on other widely used RL benchmarks including the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies in prior comparisons. Our findings call for a change in how we evaluate performance in deep RL, for which we present a more rigorous evaluation methodology, accompanied with an open-source library rliable, to prevent unreliable results from stagnating the field. This work received an outstanding paper award at NeurIPS 2021.},
  file = {/Users/vitay/Documents/Zotero/storage/WDRV4VIK/Agarwal_et_al_2021_Deep_Reinforcement_Learning_at_the_Edge_of_the_Statistical_Precipice.pdf}
}

@article{Agrawal2016,
  title = {Learning to {{Poke}} by {{Poking}}: {{Experiential Learning}} of {{Intuitive Physics}}},
  author = {Agrawal, Pulkit and Nair, Ashvin and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey},
  date = {2016-06},
  url = {http://arxiv.org/abs/1606.07419},
  abstract = {We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 100K pokes on different objects. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels. Our experiments show that this joint modeling approach outperforms alternative methods.},
  file = {/Users/vitay/Documents/Zotero/storage/Q8DNZ9FL/Agrawal et al_2016_Learning to Poke by Poking.pdf}
}

@unpublished{Ahn2019,
  title = {{{ROBEL}}: {{Robotics Benchmarks}} for {{Learning}} with {{Low-Cost Robots}}},
  shorttitle = {{{ROBEL}}},
  author = {Ahn, Michael and Zhu, Henry and Hartikainen, Kristian and Ponte, Hugo and Gupta, Abhishek and Levine, Sergey and Kumar, Vikash},
  date = {2019-09-25},
  eprint = {1909.11639},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1909.11639},
  urldate = {2019-10-01},
  abstract = {ROBEL is an open-source platform of cost-effective robots designed for reinforcement learning in the real world. ROBEL introduces two robots, each aimed to accelerate reinforcement learning research in different task domains: D'Claw is a three-fingered hand robot that facilitates learning dexterous manipulation tasks, and D'Kitty is a four-legged robot that facilitates learning agile legged locomotion tasks. These low-cost, modular robots are easy to maintain and are robust enough to sustain on-hardware reinforcement learning from scratch with over 14000 training hours registered on them to date. To leverage this platform, we propose an extensible set of continuous control benchmark tasks for each robot. These tasks feature dense and sparse task objectives, and additionally introduce score metrics as hardware-safety. We provide benchmark scores on an initial set of tasks using a variety of learning-based methods. Furthermore, we show that these results can be replicated across copies of the robots located in different institutions. Code, documentation, design files, detailed assembly instructions, final policies, baseline details, task videos, and all supplementary materials required to reproduce the results are available at www.roboticsbenchmarks.org.},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/5XA4Y2GQ/Ahn et al_2019_ROBEL.pdf;/Users/vitay/Documents/Zotero/storage/59HJFSNX/1909.html}
}

@inproceedings{Ajay2022,
  title = {Distributionally {{Adaptive Meta Reinforcement Learning}}},
  booktitle = {Decision {{Awareness}} in {{Reinforcement Learning Workshop}} at the 39th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Ajay, Anurag and Ghosh, Dibya and Levine, Sergey and Agrawal, Pulkit and Gupta, Abhishek},
  date = {2022},
  pages = {21},
  location = {Baltimore, Maryland, USA},
  abstract = {Meta-reinforcement learning algorithms provide a data-driven way to acquire policies that quickly adapt to many tasks with varying rewards or dynamics functions. However, learned meta-policies are often effective only on the exact task distribution on which they were trained and struggle in the presence of distribution shift of test-time rewards or transition dynamics. In this work, we develop a framework for meta-RL algorithms that are able to behave appropriately under test-time distribution shifts in the space of tasks. Our framework centers on an adaptive approach to distributional robustness that trains a population of meta-policies to be robust to varying levels of distribution shift. When evaluated on a potentially shifted test-time distribution of tasks, this allows us to choose the meta-policy with the most appropriate level of robustness, and use it to perform fast adaptation. We formally show how our framework allows for improved regret under distribution shift, and empirically show its efficacy on simulated robotics problems under a wide range of distribution shifts.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/446CJABD/Ajay_et_al_2022_Distributionally_Adaptive_Meta_Reinforcement_Learning.pdf}
}

@unpublished{Akrour2020,
  title = {Reinforcement {{Learning}} from a {{Mixture}} of {{Interpretable Experts}}},
  author = {Akrour, Riad and Tateo, Davide and Peters, Jan},
  date = {2020-06-10},
  eprint = {2006.05911},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.05911},
  urldate = {2020-06-16},
  abstract = {Reinforcement learning (RL) has demonstrated its ability to solve high dimensional tasks by leveraging non-linear function approximators. These successes however are mostly achieved by 'black-box' policies in simulated domains. When deploying RL to the real world, several concerns regarding the use of a 'black-box' policy might be raised. In an effort to make the policies learned by RL more transparent, we propose in this paper a policy iteration scheme that retains a complex function approximator for its internal value predictions but constrains the policy to have a concise, hierarchical, and human-readable structure, based on a mixture of interpretable experts. We show that our proposed algorithm can learn compelling policies on continuous action deep RL benchmarks, matching the performance of neural network based policies, but returns policies that are more amenable to human inspection than neural network or linear-in-feature policies.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/5QWK8HKY/Akrour_et_al_2020_Reinforcement_Learning_from_a_Mixture_of_Interpretable_Experts.pdf;/Users/vitay/Documents/Zotero/storage/DQG5DRVD/2006.html}
}

@book{Albrecht2024,
  title = {Multi-{{Agent Reinforcement Learning}}: {{Foundations}} and {{Modern Approaches}}},
  author = {Albrecht, Stefano V and Christianos, Filippos and Schäfer, Lukas},
  date = {2024},
  publisher = {MIT Press},
  url = {https://www.marl-book.com/},
  isbn = {978-0-262-38051-5},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/9Y3M92JS/Albrecht et al. - 2024 - Multi-Agent Reinforcement Learning Foundations and Modern Approaches.pdf}
}

@article{Amari1998,
  title = {Natural Gradient Works Efficiently in Learning},
  author = {Amari, S.-I.},
  date = {1998},
  journaltitle = {Neural Computation},
  volume = {10},
  number = {2},
  pages = {251--276},
  file = {/Users/vitay/Documents/Zotero/storage/7DMDHMH4/Amari_1998_Natural gradient works efficiently in learning.pdf}
}

@article{Amarjyoti2017,
  title = {Deep {{Reinforcement Learning}} for {{Robotic Manipulation-The}} State of the Art},
  author = {Amarjyoti, Smruti},
  date = {2017-01},
  url = {http://arxiv.org/abs/1701.08878},
  abstract = {The focus of this work is to enumerate the various approaches and algorithms that center around application of reinforcement learning in robotic ma- ]]nipulation tasks. Earlier methods utilized specialized policy representations and human demonstrations to constrict the policy. Such methods worked well with continuous state and policy space of robots but failed to come up with generalized policies. Subsequently, high dimensional non-linear function approximators like neural networks have been used to learn policies from scratch. Several novel and recent approaches have also embedded control policy with efficient perceptual representation using deep learning. This has led to the emergence of a new branch of dynamic robot control system called deep r inforcement learning(DRL). This work embodies a survey of the most recent algorithms, architectures and their implementations in simulations and real world robotic platforms. The gamut of DRL architectures are partitioned into two different branches namely, discrete action space algorithms(DAS) and continuous action space algorithms(CAS). Further, the CAS algorithms are divided into stochastic continuous action space(SCAS) and deterministic continuous action space(DCAS) algorithms. Along with elucidating an organ- isation of the DRL algorithms this work also manifests some of the state of the art applications of these approaches in robotic manipulation tasks.},
  file = {/Users/vitay/Documents/Zotero/storage/HNZ836AR/Amarjyoti_2017_Deep Reinforcement Learning for Robotic Manipulation-The state of the art.pdf}
}

@unpublished{Andrychowicz2017,
  title = {Hindsight {{Experience Replay}}},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  date = {2017-07},
  eprint = {1707.01495},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.01495},
  abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
  file = {/Users/vitay/Documents/Zotero/storage/UNAGQWEP/Andrychowicz et al_2017_Hindsight Experience Replay.pdf}
}

@article{Anschel2016,
  title = {Averaged-{{DQN}}: {{Variance Reduction}} and {{Stabilization}} for {{Deep Reinforcement Learning}}},
  author = {Anschel, Oron and Baram, Nir and Shimkin, Nahum},
  date = {2016-11},
  url = {http://arxiv.org/abs/1611.01929},
  abstract = {Instability and variability of Deep Reinforcement Learning (DRL) algorithms tend to adversely affect their performance. Averaged-DQN is a simple extension to the DQN algorithm, based on averaging previously learned Q-values estimates, which leads to a more stable training procedure and improved performance by reducing approximation error variance in the target values. To understand the effect of the algorithm, we examine the source of value function estimation errors and provide an analytical comparison within a simplified model. We further present experiments on the Arcade Learning Environment benchmark that demonstrate significantly improved stability and performance due to the proposed extension.},
  file = {/Users/vitay/Documents/Zotero/storage/KXQMDXR3/Anschel et al_2016_Averaged-DQN.pdf}
}

@unpublished{Arora2019,
  title = {A {{Survey}} of {{Inverse Reinforcement Learning}}: {{Challenges}}, {{Methods}} and {{Progress}}},
  shorttitle = {A {{Survey}} of {{Inverse Reinforcement Learning}}},
  author = {Arora, Saurabh and Doshi, Prashant},
  date = {2019-08-01},
  eprint = {1806.06877},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.06877},
  urldate = {2020-01-31},
  abstract = {Inverse reinforcement learning is the problem of inferring the reward function of an observed agent, given its policy or behavior. Researchers perceive IRL both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners in machine learning to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges which include accurate inference, generalizability, correctness of prior knowledge, and growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions of traditional IRL methods: (i) inaccurate and incomplete perception, (ii) incomplete model, (iii) multiple rewards, and (iv) non-linear reward functions. This discussion concludes with some broad advances in the research area and currently open research questions.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/847GY5KA/Arora_Doshi_2019_A_Survey_of_Inverse_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/N5SKFKTC/1806.html}
}

@online{Arora2020,
  title = {A {{Survey}} of {{Inverse Reinforcement Learning}}: {{Challenges}}, {{Methods}} and {{Progress}}},
  shorttitle = {A {{Survey}} of {{Inverse Reinforcement Learning}}},
  author = {Arora, Saurabh and Doshi, Prashant},
  date = {2020-11-18},
  eprint = {1806.06877},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1806.06877},
  url = {http://arxiv.org/abs/1806.06877},
  urldate = {2022-08-01},
  abstract = {Inverse reinforcement learning (IRL) is the problem of inferring the reward function of an agent, given its policy or observed behavior. Analogous to RL, IRL is perceived both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners of machine learning and beyond to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges such as the difficulty in performing accurate inference and its generalizability, its sensitivity to prior knowledge, and the disproportionate growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions to traditional IRL methods for handling: inaccurate and incomplete perception, an incomplete model, multiple reward functions, and nonlinear reward functions. This survey concludes the discussion with some broad advances in the research area and currently open research questions.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/KATQC4KB/Arora_Doshi_2020_A_Survey_of_Inverse_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/F3QPIRR3/1806.html}
}

@article{Arulkumaran2017,
  title = {A {{Brief Survey}} of {{Deep Reinforcement Learning}}},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  date = {2017},
  url = {https://arxiv.org/pdf/1708.05866.pdf},
  abstract = {Deep reinforcement learning is poised to revolu-tionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep Q-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  file = {/Users/vitay/Documents/Zotero/storage/CEAEHS6B/Arulkumaran et al_2017_A Brief Survey of Deep Reinforcement Learning.pdf}
}

@unpublished{Aubret2019,
  title = {A Survey on Intrinsic Motivation in Reinforcement Learning},
  author = {Aubret, Arthur and Matignon, Laetitia and Hassas, Salima},
  date = {2019-11-19},
  eprint = {1908.06976},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1908.06976},
  urldate = {2021-01-17},
  abstract = {The reinforcement learning (RL) research area is very active, with an important number of new contributions; especially considering the emergent field of deep RL (DRL). However a number of scientific and technical challenges still need to be addressed, amongst which we can mention the ability to abstract actions or the difficulty to explore the environment which can be addressed by intrinsic motivation (IM). In this article, we provide a survey on the role of intrinsic motivation in DRL. We categorize the different kinds of intrinsic motivations and detail for each category, its advantages and limitations with respect to the mentioned challenges. Additionnally, we conduct an in-depth investigation of substantial current research questions, that are currently under study or not addressed at all in the considered research area of DRL. We choose to survey these research works, from the perspective of learning how to achieve tasks. We suggest then, that solving current challenges could lead to a larger developmental architecture which may tackle most of the tasks. We describe this developmental architecture on the basis of several building blocks composed of a RL algorithm and an IM module compressing information.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/AWRM9UEJ/Aubret_et_al_2019_A_survey_on_intrinsic_motivation_in_reinforcement_learning.pdf;/Users/vitay/Documents/Zotero/storage/IM9CA4CK/1908.html}
}

@article{Ba2014,
  title = {Multiple {{Object Recognition}} with {{Visual Attention}}},
  author = {Ba, Jimmy and Mnih, Volodymyr and Kavukcuoglu, Koray},
  date = {2014-12},
  url = {http://arxiv.org/abs/1412.7755},
  abstract = {We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.},
  file = {/Users/vitay/Documents/Zotero/storage/FUYRENWJ/Ba et al_2014_Multiple Object Recognition with Visual Attention.pdf}
}

@unpublished{Bacon2016,
  title = {The {{Option-Critic Architecture}}},
  author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  date = {2016-12-02},
  eprint = {1609.05140},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1609.05140},
  urldate = {2020-01-31},
  abstract = {Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup \& Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/vitay/Documents/Zotero/storage/YBDQRWR3/Bacon_et_al_2016_The_Option-Critic_Architecture.pdf;/Users/vitay/Documents/Zotero/storage/LIVLRZAA/1609.html}
}

@unpublished{Badia2020,
  title = {Agent57: {{Outperforming}} the {{Atari Human Benchmark}}},
  shorttitle = {Agent57},
  author = {Badia, Adrià Puigdomènech and Piot, Bilal and Kapturowski, Steven and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Blundell, Charles},
  date = {2020-03-30},
  eprint = {2003.13350},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2003.13350},
  urldate = {2022-01-17},
  abstract = {Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.},
  file = {/Users/vitay/Documents/Zotero/storage/K9WFMJVS/Badia_et_al_2020_Agent57.pdf;/Users/vitay/Documents/Zotero/storage/AV3GDYAE/2003.html}
}

@unpublished{Badia2020a,
  title = {Never {{Give Up}}: {{Learning Directed Exploration Strategies}}},
  shorttitle = {Never {{Give Up}}},
  author = {Badia, Adrià Puigdomènech and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Piot, Bilal and Kapturowski, Steven and Tieleman, Olivier and Arjovsky, Martín and Pritzel, Alexander and Bolt, Andew and Blundell, Charles},
  date = {2020-02-14},
  eprint = {2002.06038},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.06038},
  urldate = {2022-01-17},
  abstract = {We propose a reinforcement learning agent to solve hard exploration games by learning a range of directed exploratory policies. We construct an episodic memory-based intrinsic reward using k-nearest neighbors over the agent's recent experience to train the directed exploratory policies, thereby encouraging the agent to repeatedly revisit all states in its environment. A self-supervised inverse dynamics model is used to train the embeddings of the nearest neighbour lookup, biasing the novelty signal towards what the agent can control. We employ the framework of Universal Value Function Approximators (UVFA) to simultaneously learn many directed exploration policies with the same neural network, with different trade-offs between exploration and exploitation. By using the same neural network for different degrees of exploration/exploitation, transfer is demonstrated from predominantly exploratory policies yielding effective exploitative policies. The proposed method can be incorporated to run with modern distributed RL agents that collect large amounts of experience from many actors running in parallel on separate environment instances. Our method doubles the performance of the base agent in all hard exploration in the Atari-57 suite while maintaining a very high score across the remaining games, obtaining a median human normalised score of 1344.0\%. Notably, the proposed method is the first algorithm to achieve non-zero rewards (with a mean score of 8,400) in the game of Pitfall! without using demonstrations or hand-crafted features.},
  file = {/Users/vitay/Documents/Zotero/storage/2A27F2EX/Badia_et_al_2020_Never_Give_Up.pdf;/Users/vitay/Documents/Zotero/storage/62Q5EA33/2002.html}
}

@report{Baird1993,
  title = {Advantage Updating},
  author = {Baird, L.C.},
  date = {1993},
  number = {Technical Report WL-TR-93-1146},
  institution = {Wright-Patterson Air Force Base},
  url = {http://leemon.com/papers/1993b.pdf}
}

@inproceedings{Bakker2001,
  title = {Reinforcement {{Learning}} with {{Long Short-Term Memory}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 14 ({{NIPS}} 2001)},
  author = {Bakker, Bram},
  date = {2001},
  pages = {1475--1482},
  url = {https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory}
}

@online{Ball2023,
  title = {Efficient {{Online Reinforcement Learning}} with {{Offline Data}}},
  author = {Ball, Philip J. and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  date = {2023-02-06},
  eprint = {2302.02948},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.02948},
  urldate = {2023-02-09},
  abstract = {Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see that correct application of these simple recommendations can provide a \$\textbackslash mathbf\{2.5\textbackslash times\}\$ improvement over existing approaches across a diverse set of competitive benchmarks, with no additional computational overhead.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/BA3N6SSR/Ball_et_al_2023_Efficient_Online_Reinforcement_Learning_with_Offline_Data.pdf}
}

@unpublished{Banino2020,
  title = {{{MEMO}}: {{A Deep Network}} for {{Flexible Combination}} of {{Episodic Memories}}},
  shorttitle = {{{MEMO}}},
  author = {Banino, Andrea and Badia, Adrià Puigdomènech and Köster, Raphael and Chadwick, Martin J. and Zambaldi, Vinicius and Hassabis, Demis and Barry, Caswell and Botvinick, Matthew and Kumaran, Dharshan and Blundell, Charles},
  date = {2020-01-29},
  eprint = {2001.10913},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2001.10913},
  urldate = {2020-02-04},
  abstract = {Recent research developing neural network architectures with external memory have often used the benchmark bAbI question and answering dataset which provides a challenging number of tasks requiring reasoning. Here we employed a classic associative inference task from the memory-based reasoning neuroscience literature in order to more carefully probe the reasoning capacity of existing memory-augmented architectures. This task is thought to capture the essence of reasoning -- the appreciation of distant relationships among elements distributed across multiple facts or memories. Surprisingly, we found that current architectures struggle to reason over long distance associations. Similar results were obtained on a more complex task involving finding the shortest path between nodes in a path. We therefore developed MEMO, an architecture endowed with the capacity to reason over longer distances. This was accomplished with the addition of two novel components. First, it introduces a separation between memories (facts) stored in external memory and the items that comprise these facts in external memory. Second, it makes use of an adaptive retrieval mechanism, allowing a variable number of "memory hops" before the answer is produced. MEMO is capable of solving our novel reasoning tasks, as well as match state of the art results in bAbI.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.6},
  file = {/Users/vitay/Documents/Zotero/storage/LN7GZ3HU/Banino_et_al_2020_MEMO.pdf;/Users/vitay/Documents/Zotero/storage/QP9NSK6D/2001.html}
}

@unpublished{Barreto2016,
  title = {Successor {{Features}} for {{Transfer}} in {{Reinforcement Learning}}},
  author = {Barreto, André and Dabney, Will and Munos, Rémi and Hunt, Jonathan J. and Schaul, Tom and family=Hasselt, given=Hado, prefix=van, useprefix=true and Silver, David},
  date = {2016-06-16},
  eprint = {1606.05312},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1606.05312},
  urldate = {2019-03-11},
  abstract = {Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. We propose a transfer framework for the scenario where the reward function changes between tasks but the environment's dynamics remain the same. Our approach rests on two key ideas: "successor features", a value function representation that decouples the dynamics of the environment from the rewards, and "generalized policy improvement", a generalization of dynamic programming's policy improvement operation that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows the free exchange of information across tasks. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice, significantly outperforming alternative methods in a sequence of navigation tasks and in the control of a simulated robotic arm.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/vitay/Documents/Zotero/storage/NYHTKNG6/Barreto et al_2016_Successor Features for Transfer in Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/7BD9GIBJ/1606.html}
}

@incollection{Barreto2019,
  title = {The {{Option Keyboard}}: {{Combining Skills}} in {{Reinforcement Learning}}},
  shorttitle = {The {{Option Keyboard}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Barreto, Andre and Borsa, Diana and Hou, Shaobo and Comanici, Gheorghe and Aygün, Eser and Hamel, Philippe and Toyama, Daniel and {hunt}, Jonathan and Mourad, Shibl and Silver, David and Precup, Doina},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Alché-Buc, given=F., prefix=d\textbackslash textquotesingle, useprefix=false and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {13031--13041},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/9463-the-option-keyboard-combining-skills-in-reinforcement-learning.pdf},
  urldate = {2019-11-19},
  file = {/Users/vitay/Documents/Zotero/storage/Q5SIIA4Q/Barreto et al_2019_The Option Keyboard.pdf;/Users/vitay/Documents/Zotero/storage/KGAWLC2L/9463-the-option-keyboard-combining-skills-in-reinforcement-learning.html}
}

@article{Barreto2020,
  title = {Fast Reinforcement Learning with Generalized Policy Updates},
  author = {Barreto, André and Hou, Shaobo and Borsa, Diana and Silver, David and Precup, Doina},
  date = {2020-12-01},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {117},
  number = {48},
  eprint = {32817541},
  eprinttype = {pmid},
  pages = {30079--30087},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1907370117},
  url = {https://www.pnas.org/content/117/48/30079},
  urldate = {2021-03-10},
  abstract = {The combination of reinforcement learning with deep learning is a promising approach to tackle important sequential decision-making problems that are currently intractable. One obstacle to overcome is the amount of data needed by learning systems of this type. In this article, we propose to address this issue through a divide-and-conquer approach. We argue that complex decision problems can be naturally decomposed into multiple tasks that unfold in sequence or in parallel. By associating each task with a reward function, this problem decomposition can be seamlessly accommodated within the standard reinforcement-learning formalism. The specific way we do so is through a generalization of two fundamental operations in reinforcement learning: policy improvement and policy evaluation. The generalized version of these operations allow one to leverage the solution of some tasks to speed up the solution of others. If the reward function of a task can be well approximated as a linear combination of the reward functions of tasks previously solved, we can reduce a reinforcement-learning problem to a simpler linear regression. When this is not the case, the agent can still exploit the task solutions by using them to interact with and learn about the environment. Both strategies considerably reduce the amount of data needed to solve a reinforcement-learning problem.},
  langid = {english},
  keywords = {artificial intelligence,generalized policy evaluation,generalized policy improvement,reinforcement learning,successor features},
  file = {/Users/vitay/Documents/Zotero/storage/ITYTUALU/Barreto et al. - 2020 - Fast reinforcement learning with generalized polic.pdf;/Users/vitay/Documents/Zotero/storage/YHX9KMWZ/Barreto_et_al_2020_Fast_reinforcement_learning_with_generalized_policy_updates.pdf;/Users/vitay/Documents/Zotero/storage/5ZV855GW/30079.html}
}

@unpublished{Barth-Maron2018,
  title = {Distributed {{Distributional Deterministic Policy Gradients}}},
  author = {Barth-Maron, Gabriel and Hoffman, Matthew W. and Budden, David and Dabney, Will and Horgan, Dan and TB, Dhruva and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
  date = {2018-04},
  eprint = {1804.08617},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1804.08617},
  abstract = {This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of \$N\$-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.},
  file = {/Users/vitay/Documents/Zotero/storage/CZLUQP4W/Barth-Maron et al_2018_Distributed Distributional Deterministic Policy Gradients.pdf}
}

@incollection{Barto2013,
  title = {Intrinsic {{Motivation}} and {{Reinforcement Learning}}},
  booktitle = {Intrinsically {{Motivated Learning}} in {{Natural}} and {{Artificial Systems}}},
  author = {Barto, Andrew G.},
  editor = {Baldassarre, Gianluca and Mirolli, Marco},
  date = {2013},
  pages = {17--47},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-32375-1_2},
  url = {https://doi.org/10.1007/978-3-642-32375-1_2},
  urldate = {2021-02-06},
  abstract = {Psychologists distinguish between extrinsically motivated behavior, which is behavior undertaken to achieve some externally supplied reward, such as a prize, a high grade, or a high-paying job, and intrinsically motivated behavior, which is behavior done for its own sake. Is an analogous distinction meaningful for machine learning systems? Can we say of a machine learning system that it is motivated to learn, and if so, is it possible to provide it with an analog of intrinsic motivation? Despite the fact that a formal distinction between extrinsic and intrinsic motivation is elusive, this chapter argues that the answer to both questions is assuredly “yes” and that the machine learning framework of reinforcement learning is particularly appropriate for bringing learning together with what in animals one would call motivation. Despite the common perception that a reinforcement learning agent’s reward has to be extrinsic because the agent has a distinct input channel for reward signals, reinforcement learning provides a natural framework for incorporating principles of intrinsic motivation.},
  isbn = {978-3-642-32375-1},
  langid = {english},
  keywords = {Artificial Agent,Internal Environment,Intrinsic Motivation,Reinforcement Learning,Reward Function}
}

@unpublished{Bear2020,
  title = {Learning {{Physical Graph Representations}} from {{Visual Scenes}}},
  author = {Bear, Daniel M. and Fan, Chaofei and Mrowca, Damian and Li, Yunzhu and Alter, Seth and Nayebi, Aran and Schwartz, Jeremy and Fei-Fei, Li and Wu, Jiajun and Tenenbaum, Joshua B. and Yamins, Daniel L. K.},
  date = {2020-06-24},
  eprint = {2006.12373},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2006.12373},
  urldate = {2020-06-27},
  abstract = {Convolutional Neural Networks (CNNs) have proved exceptional at learning representations for visual object categorization. However, CNNs do not explicitly encode objects, parts, and their physical properties, which has limited CNNs' success on tasks that require structured understanding of visual scenes. To overcome these limitations, we introduce the idea of Physical Scene Graphs (PSGs), which represent scenes as hierarchical graphs, with nodes in the hierarchy corresponding intuitively to object parts at different scales, and edges to physical connections between parts. Bound to each node is a vector of latent attributes that intuitively represent object properties such as surface shape and texture. We also describe PSGNet, a network architecture that learns to extract PSGs by reconstructing scenes through a PSG-structured bottleneck. PSGNet augments standard CNNs by including: recurrent feedback connections to combine low and high-level image information; graph pooling and vectorization operations that convert spatially-uniform feature maps into object-centric graph structures; and perceptual grouping principles to encourage the identification of meaningful scene elements. We show that PSGNet outperforms alternative self-supervised scene representation algorithms at scene segmentation tasks, especially on complex real-world images, and generalizes well to unseen object types and scene arrangements. PSGNet is also able learn from physical motion, enhancing scene estimates even for static images. We present a series of ablation studies illustrating the importance of each component of the PSGNet architecture, analyses showing that learned latent attributes capture intuitive scene properties, and illustrate the use of PSGs for compositional scene inference.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.2.6,I.4.8},
  file = {/Users/vitay/Documents/Zotero/storage/A7NR98S3/Bear_et_al_2020_Learning_Physical_Graph_Representations_from_Visual_Scenes.pdf;/Users/vitay/Documents/Zotero/storage/B57W4BYU/2006.html}
}

@online{Beck2023,
  title = {A {{Survey}} of {{Meta-Reinforcement Learning}}},
  author = {Beck, Jacob and Vuorio, Risto and Liu, Evan Zheran and Xiong, Zheng and Zintgraf, Luisa and Finn, Chelsea and Whiteson, Shimon},
  date = {2023-01-19},
  eprint = {2301.08028},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.08028},
  url = {http://arxiv.org/abs/2301.08028},
  urldate = {2023-01-27},
  abstract = {While deep reinforcement learning (RL) has fueled multiple high-profile successes in machine learning, it is held back from more widespread adoption by its often poor data efficiency and the limited generality of the policies it produces. A promising approach for alleviating these limitations is to cast the development of better RL algorithms as a machine learning problem itself in a process called meta-RL. Meta-RL is most commonly studied in a problem setting where, given a distribution of tasks, the goal is to learn a policy that is capable of adapting to any new task from the task distribution with as little data as possible. In this survey, we describe the meta-RL problem setting in detail as well as its major variations. We discuss how, at a high level, meta-RL research can be clustered based on the presence of a task distribution and the learning budget available for each individual task. Using these clusters, we then survey meta-RL algorithms and applications. We conclude by presenting the open problems on the path to making meta-RL part of the standard toolbox for a deep RL practitioner.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/8KHJWTJN/Beck_et_al_2023_A_Survey_of_Meta-Reinforcement_Learning.pdf}
}

@unpublished{Becker-Ehmck2020,
  title = {Learning to {{Fly}} via {{Deep Model-Based Reinforcement Learning}}},
  author = {Becker-Ehmck, Philip and Karl, Maximilian and Peters, Jan and family=Smagt, given=Patrick, prefix=van der, useprefix=true},
  date = {2020-03-19},
  eprint = {2003.08876},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2003.08876},
  urldate = {2020-03-24},
  abstract = {Learning to control robots without requiring models has been a long-term goal, promising diverse and novel applications. Yet, reinforcement learning has only achieved limited impact on real-time robot control due to its high demand of real-world interactions. In this work, by leveraging a learnt probabilistic model of drone dynamics, we achieve human-like quadrotor control through model-based reinforcement learning. No prior knowledge of the flight dynamics is assumed; instead, a sequential latent variable model, used generatively and as an online filter, is learnt from raw sensory input. The controller and value function are optimised entirely by propagating stochastic analytic gradients through generated latent trajectories. We show that "learning to fly" can be achieved with less than 30 minutes of experience with a single drone, and can be deployed solely using onboard computational resources and sensors, on a self-built drone.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/2DL2MJIG/Becker-Ehmck_et_al_2020_Learning_to_Fly_via_Deep_Model-Based_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/JUWERV4Z/2003.html}
}

@article{Belkhale2021,
  title = {Model-{{Based Meta-Reinforcement Learning}} for {{Flight}} with {{Suspended Payloads}}},
  author = {Belkhale, Suneel and Li, Rachel and Kahn, Gregory and McAllister, Rowan and Calandra, Roberto and Levine, Sergey},
  date = {2021},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  eprint = {2004.11345},
  eprinttype = {arXiv},
  pages = {1--1},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2021.3057046},
  url = {http://arxiv.org/abs/2004.11345},
  urldate = {2021-02-06},
  abstract = {Transporting suspended payloads is challenging for autonomous aerial vehicles because the payload can cause significant and unpredictable changes to the robot's dynamics. These changes can lead to suboptimal flight performance or even catastrophic failure. Although adaptive control and learning-based methods can in principle adapt to changes in these hybrid robot-payload systems, rapid mid-flight adaptation to payloads that have a priori unknown physical properties remains an open problem. We propose a meta-learning approach that "learns how to learn" models of altered dynamics within seconds of post-connection flight data. Our experiments demonstrate that our online adaptation approach outperforms non-adaptive methods on a series of challenging suspended payload transportation tasks. Videos and other supplemental material are available on our website: https://sites.google.com/view/meta-rl-for-flight},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/BAHISQZ3/Belkhale_et_al_2021_Model-Based_Meta-Reinforcement_Learning_for_Flight_with_Suspended_Payloads.pdf;/Users/vitay/Documents/Zotero/storage/I8QTNHGQ/2004.html}
}

@unpublished{Bellemare2017,
  title = {A {{Distributional Perspective}} on {{Reinforcement Learning}}},
  author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
  date = {2017-07},
  eprint = {1707.06887},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.06887},
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  file = {/Users/vitay/Documents/Zotero/storage/MMVXRE9S/Bellemare et al_2017_A Distributional Perspective on Reinforcement Learning.pdf}
}

@unpublished{Berseth2019,
  title = {{{SMiRL}}: {{Surprise Minimizing RL}} in {{Dynamic Environments}}},
  shorttitle = {{{SMiRL}}},
  author = {Berseth, Glen and Geng, Daniel and Devin, Coline and Finn, Chelsea and Jayaraman, Dinesh and Levine, Sergey},
  date = {2019-12-11},
  eprint = {1912.05510},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.05510},
  urldate = {2019-12-17},
  abstract = {All living organisms struggle against the forces of nature to carve out niches where they can maintain homeostasis. We propose that such a search for order amidst chaos might offer a unifying principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing RL (SMiRL). SMiRL trains an agent with the objective of maximizing the probability of observed states under a model trained on previously seen states. The resulting agents can acquire proactive behaviors that seek out and maintain stable conditions, such as balancing and damage avoidance, that are closely tied to an environment's prevailing sources of entropy, such as wind, earthquakes, and other agents. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls and navigate to escape enemy agents, without any task-specific reward supervision. We further show that SMiRL can be used together with a standard task reward to accelerate reward-driven learning.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/ZDH9NHUZ/Berseth_et_al_2019_SMiRL.pdf;/Users/vitay/Documents/Zotero/storage/DQ2MEXT3/1912.html}
}

@article{Beyret2019,
  title = {Dot-to-{{Dot}}: {{Achieving Structured Robotic Manipulation}} through {{Hierarchical Reinforcement Learning}}},
  shorttitle = {Dot-to-{{Dot}}},
  author = {Beyret, Benjamin and Shafti, Ali and Faisal, A. Aldo},
  date = {2019-04-14},
  url = {https://arxiv.org/abs/1904.06703v1},
  urldate = {2019-04-20},
  abstract = {Robotic systems are ever more capable of automation and fulfilment of complex tasks, particularly with reliance on recent advances in intelligent systems, deep learning and artificial intelligence in general. However, as robots and humans come closer together in their interactions, the matter of interpretability, or explainability of robot decision-making processes for the human grows in importance. A successful interaction and collaboration would only be possible through mutual understanding of underlying representations of the environment and the task at hand. This is currently a challenge in deep learning systems. We present a hierarchical deep reinforcement learning system, consisting of a low-level agent handling the large actions/states space of a robotic system efficiently, by following the directives of a high-level agent which is learning the high-level dynamics of the environment and task. This high-level agent forms a representation of the world and task at hand that is interpretable for a human operator. The method, which we call Dot-to-Dot, is tested on a MuJoCo-based model of the Fetch Robotics Manipulator, as well as a Shadow Hand, to test its performance. Results show efficient learning of complex actions/states spaces by the low-level agent, and an interpretable representation of the task and decision-making process learned by the high-level agent.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/2FYY2792/Beyret et al_2019_Dot-to-Dot.pdf;/Users/vitay/Documents/Zotero/storage/T5Z6FJTE/1904.html}
}

@unpublished{Bietti2020,
  title = {A {{Contextual Bandit Bake-off}}},
  author = {Bietti, Alberto and Agarwal, Alekh and Langford, John},
  date = {2020-01-24},
  eprint = {1802.04064},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.04064},
  urldate = {2021-03-08},
  abstract = {Contextual bandit algorithms are essential for solving many real-world interactive machine learning problems. Despite multiple recent successes on statistically and computationally efficient methods, the practical behavior of these algorithms is still poorly understood. We leverage the availability of large numbers of supervised learning datasets to empirically evaluate contextual bandit algorithms, focusing on practical methods that learn by relying on optimization oracles from supervised learning. We find that a recent method (Foster et al., 2018) using optimism under uncertainty works the best overall. A surprisingly close second is a simple greedy baseline that only explores implicitly through the diversity of contexts, followed by a variant of Online Cover (Agarwal et al., 2014) which tends to be more conservative but robust to problem specification by design. Along the way, we also evaluate various components of contextual bandit algorithm design such as loss estimators. Overall, this is a thorough study and review of contextual bandit methodology.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/BN6GW6VX/Bietti_et_al_2020_A_Contextual_Bandit_Bake-off.pdf;/Users/vitay/Documents/Zotero/storage/3AY8UYXG/1802.html}
}

@article{Bietti2021,
  title = {A {{Contextual Bandit Bake-oﬀ}}},
  author = {Bietti, Alberto and Agarwal, Alekh and Langford, John},
  date = {2021},
  journaltitle = {Journal of Machine Learning Research},
  volume = {22},
  pages = {49},
  abstract = {Contextual bandit algorithms are essential for solving many real-world interactive machine learning problems. Despite multiple recent successes on statistically optimal and computationally efficient methods, the practical behavior of these algorithms is still poorly understood. We leverage the availability of large numbers of supervised learning datasets to empirically evaluate contextual bandit algorithms, focusing on practical methods that learn by relying on optimization oracles from supervised learning. We find that a recent method (Foster et al., 2018) using optimism under uncertainty works the best overall. A surprisingly close second is a simple greedy baseline that only explores implicitly through the diversity of contexts, followed by a variant of Online Cover (Agarwal et al., 2014) which tends to be more conservative but robust to problem specification by design. Along the way, we also evaluate various components of contextual bandit algorithm design such as loss estimators. Overall, this is a thorough study and review of contextual bandit methodology.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/K2TBYMBY/Bietti_et_al_2021_A_Contextual_Bandit_Bake-oﬀ.pdf}
}

@online{Black2023,
  title = {Training {{Diffusion Models}} with {{Reinforcement Learning}}},
  author = {Black, Kevin and Janner, Michael and Du, Yilun and Kostrikov, Ilya and Levine, Sergey},
  date = {2023-05-23},
  eprint = {2305.13301},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.13301},
  urldate = {2023-05-25},
  abstract = {Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decisionmaking problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation.},
  langid = {english},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/NNQY6ERG/Black et al. - 2023 - Training Diffusion Models with Reinforcement Learn.pdf}
}

@incollection{Blaes2019,
  title = {Control {{What You Can}}: {{Intrinsically Motivated Task-Planning Agent}}},
  shorttitle = {Control {{What You Can}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Blaes, Sebastian and Vlastelica Pogančić, Marin and Zhu, Jiajie and Martius, Georg},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Alché-Buc, given=F., prefix=d\textbackslash textquotesingle, useprefix=false and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {12541--12552},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/9418-control-what-you-can-intrinsically-motivated-task-planning-agent.pdf},
  urldate = {2020-04-19},
  file = {/Users/vitay/Documents/Zotero/storage/EMUQUY8M/Blaes_et_al_2019_Control_What_You_Can.pdf;/Users/vitay/Documents/Zotero/storage/YSPRHBLG/9418-control-what-you-can-intrinsically-motivated-task-planning-agent.html}
}

@unpublished{Blau2019,
  title = {Bayesian {{Curiosity}} for {{Efficient Exploration}} in {{Reinforcement Learning}}},
  author = {Blau, Tom and Ott, Lionel and Ramos, Fabio},
  date = {2019-11-19},
  eprint = {1911.08701},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1911.08701},
  urldate = {2019-11-26},
  abstract = {Balancing exploration and exploitation is a fundamental part of reinforcement learning, yet most state-of-the-art algorithms use a naive exploration protocol like \$\textbackslash epsilon\$-greedy. This contributes to the problem of high sample complexity, as the algorithm wastes effort by repeatedly visiting parts of the state space that have already been explored. We introduce a novel method based on Bayesian linear regression and latent space embedding to generate an intrinsic reward signal that encourages the learning agent to seek out unexplored parts of the state space. This method is computationally efficient, simple to implement, and can extend any state-of-the-art reinforcement learning algorithm. We evaluate the method on a range of algorithms and challenging control tasks, on both simulated and physical robots, demonstrating how the proposed method can significantly improve sample complexity.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/PF8SZ446/Blau et al_2019_Bayesian Curiosity for Efficient Exploration in Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/DUF3UQK7/1911.html}
}

@online{Blundell2016,
  title = {Model-{{Free Episodic Control}}},
  author = {Blundell, Charles and Uria, Benigno and Pritzel, Alexander and Li, Yazhe and Ruderman, Avraham and Leibo, Joel Z. and Rae, Jack and Wierstra, Daan and Hassabis, Demis},
  date = {2016-06-14},
  eprint = {1606.04460},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio, stat},
  doi = {10.48550/arXiv.1606.04460},
  url = {http://arxiv.org/abs/1606.04460},
  urldate = {2023-02-06},
  abstract = {State of the art deep reinforcement learning algorithms take many millions of interactions to attain human-level performance. Humans, on the other hand, can very quickly exploit highly rewarding nuances of an environment upon first discovery. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Here we investigate whether a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks. We demonstrate that it not only attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/EJCAWKQ4/Blundell_et_al_2016_Model-Free_Episodic_Control.pdf}
}

@article{Botvinick2014,
  title = {Model-Based Hierarchical Reinforcement Learning and Human Action Control},
  author = {Botvinick, Matthew and Weinstein, Ari},
  date = {2014-11-05},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {369},
  number = {1655},
  pages = {20130480},
  doi = {10.1098/rstb.2013.0480},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2013.0480},
  urldate = {2020-01-24},
  abstract = {Recent work has reawakened interest in goal-directed or ‘model-based’ choice, where decisions are based on prospective evaluation of potential action outcomes. Concurrently, there has been growing attention to the role of hierarchy in decision-making and action control. We focus here on the intersection between these two areas of interest, considering the topic of hierarchical model-based control. To characterize this form of action control, we draw on the computational framework of hierarchical reinforcement learning, using this to interpret recent empirical findings. The resulting picture reveals how hierarchical model-based mechanisms might play a special and pivotal role in human decision-making, dramatically extending the scope and complexity of human behaviour.},
  file = {/Users/vitay/Documents/Zotero/storage/J899WPRC/Botvinick_Weinstein_2014_Model-based_hierarchical_reinforcement_learning_and_human_action_control.pdf;/Users/vitay/Documents/Zotero/storage/6ZGX2JMK/rstb.2013.html}
}

@article{Botvinick2019,
  title = {Reinforcement {{Learning}}, {{Fast}} and {{Slow}}},
  author = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X. and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
  date = {2019-05-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {5},
  eprint = {31003893},
  eprinttype = {pmid},
  pages = {408--422},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2019.02.006},
  url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(19)30061-0},
  urldate = {2019-05-08},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/GDZGLEPT/Botvinick et al_2019_Reinforcement Learning, Fast and Slow.pdf;/Users/vitay/Documents/Zotero/storage/XBI9WQW8/S1364-6613(19)30061-0.html}
}

@unpublished{Botvinick2020,
  title = {Deep {{Reinforcement Learning}} and Its {{Neuroscientific Implications}}},
  author = {Botvinick, Matthew and Wang, Jane X. and Dabney, Will and Miller, Kevin J. and Kurth-Nelson, Zeb},
  date = {2020-07-07},
  eprint = {2007.03750},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2007.03750},
  urldate = {2020-07-14},
  abstract = {The emergence of powerful artificial intelligence is defining new research directions in neuroscience. To date, this research has focused largely on deep neural networks trained using supervised learning, in tasks such as image classification. However, there is another area of recent AI work which has so far received less attention from neuroscientists, but which may have profound neuroscientific implications: deep reinforcement learning. Deep RL offers a comprehensive framework for studying the interplay among learning, representation and decision-making, offering to the brain sciences a new set of research tools and a wide range of novel hypotheses. In the present review, we provide a high-level introduction to deep RL, discuss some of its initial applications to neuroscience, and survey its wider implications for research on brain and behavior, concluding with a list of opportunities for next-stage research.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/Users/vitay/Documents/Zotero/storage/WZCU88XJ/Botvinick et al. - 2020 - Deep Reinforcement Learning and its Neuroscientifi.pdf;/Users/vitay/Documents/Zotero/storage/8XYH4LGM/2007.html}
}

@unpublished{Bouneffouf2019,
  title = {A {{Survey}} on {{Practical Applications}} of {{Multi-Armed}} and {{Contextual Bandits}}},
  author = {Bouneffouf, Djallel and Rish, Irina},
  date = {2019-04-02},
  eprint = {1904.10040},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1904.10040},
  urldate = {2019-10-14},
  abstract = {In recent years, multi-armed bandit (MAB) framework has attracted a lot of attention in various applications, from recommender systems and information retrieval to healthcare and finance, due to its stellar performance combined with certain attractive properties, such as learning from less feedback. The multi-armed bandit field is currently flourishing, as novel problem settings and algorithms motivated by various practical applications are being introduced, building on top of the classical bandit problem. This article aims to provide a comprehensive review of top recent developments in multiple real-life applications of the multi-armed bandit. Specifically, we introduce a taxonomy of common MAB-based applications and summarize state-of-art for each of those domains. Furthermore, we identify important current trends and provide new perspectives pertaining to the future of this exciting and fast-growing field.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/HSB8WSBE/Bouneffouf_Rish_2019_A Survey on Practical Applications of Multi-Armed and Contextual Bandits.pdf;/Users/vitay/Documents/Zotero/storage/A2ZYR7ZT/1904.html}
}

@unpublished{Bram2019,
  title = {Attentive {{Multi-Task Deep Reinforcement Learning}}},
  author = {Bram, Timo and Brunner, Gino and Richter, Oliver and Wattenhofer, Roger},
  date = {2019-07-05},
  eprint = {1907.02874},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.02874},
  urldate = {2019-07-13},
  abstract = {Sharing knowledge between tasks is vital for efficient learning in a multi-task setting. However, most research so far has focused on the easier case where knowledge transfer is not harmful, i.e., where knowledge from one task cannot negatively impact the performance on another task. In contrast, we present an approach to multi-task deep reinforcement learning based on attention that does not require any a-priori assumptions about the relationships between tasks. Our attention network automatically groups task knowledge into sub-networks on a state level granularity. It thereby achieves positive knowledge transfer if possible, and avoids negative transfer in cases where tasks interfere. We test our algorithm against two state-of-the-art multi-task/transfer learning approaches and show comparable or superior performance while requiring fewer network parameters.},
  keywords = {93E35,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.6,I.2.8,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/D8QN82PJ/Bram et al_2019_Attentive Multi-Task Deep Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/QVHHFYU4/1907.html}
}

@article{Brandle2022,
  title = {Intrinsic {{Exploration}} as {{Empowerment}} in a {{Richly Structured Online Game}}},
  author = {Brändle, Franziska and Stocks, Lena J. and Tenenbaum, Joshua and Gershman, Samuel J. and Schulz, Eric},
  date = {2022-01-14T10:42:51},
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/ybs7g},
  url = {https://psyarxiv.com/ybs7g/},
  urldate = {2022-01-18},
  abstract = {Studies of human exploration frequently cast people as serendipitously stumbling upon good options. Yet these studies may not capture the richness of exploration strategies that people exhibit in more complex environments. We study human behavior in a large data set of 29,493 players of the richly-structured online game  "Little Alchemy 2''. In this game, players start with four elements, which they can combine to create up to 720 complex objects. We find that players are driven to create objects that empower them to create even more objects. We find that this drive for empowerment is eliminated when people play a version of the game that lacks recognizable semantics, indicating that they use their knowledge about the world to guide their exploration. Our results suggest that the drive for empowerment may be a potent source of intrinsic motivation in richly structured domains, particularly those that lack explicit reward signals.},
  langid = {american},
  file = {/Users/vitay/Documents/Zotero/storage/7CH6ZI4H/Brändle_et_al_2022_Intrinsic_Exploration_as_Empowerment_in_a_Richly_Structured_Online_Game.pdf}
}

@unpublished{Breyer2019,
  title = {Comparing {{Task Simplifications}} to {{Learn Closed-Loop Object Picking Using Deep Reinforcement Learning}}},
  author = {Breyer, Michel and Furrer, Fadri and Novkovic, Tonci and Siegwart, Roland and Nieto, Juan},
  date = {2019-01-31},
  eprint = {1803.04996},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1803.04996},
  urldate = {2020-03-04},
  abstract = {Enabling autonomous robots to interact in unstructured environments with dynamic objects requires manipulation capabilities that can deal with clutter, changes, and objects' variability. This paper presents a comparison of different reinforcement learning-based approaches for object picking with a robotic manipulator. We learn closed-loop policies mapping depth camera inputs to motion commands and compare different approaches to keep the problem tractable, including reward shaping, curriculum learning and using a policy pre-trained on a task with a reduced action set to warm-start the full problem. For efficient and more flexible data collection, we train in simulation and transfer the policies to a real robot. We show that using curriculum learning, policies learned with a sparse reward formulation can be trained at similar rates as with a shaped reward. These policies result in success rates comparable to the policy initialized on the simplified task. We could successfully transfer these policies to the real robot with only minor modifications of the depth image filtering. We found that using a heuristic to warm-start the training was useful to enforce desired behavior, while the policies trained from scratch using a curriculum learned better to cope with unseen scenarios where objects are removed.},
  keywords = {Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/9FLQDQ8H/Breyer_et_al_2019_Comparing_Task_Simplifications_to_Learn_Closed-Loop_Object_Picking_Using_Deep.pdf;/Users/vitay/Documents/Zotero/storage/IQF4VL62/1803.html}
}

@unpublished{Buchler2020,
  title = {Learning to {{Play Table Tennis From Scratch}} Using {{Muscular Robots}}},
  author = {Büchler, Dieter and Guist, Simon and Calandra, Roberto and Berenz, Vincent and Schölkopf, Bernhard and Peters, Jan},
  date = {2020-06-10},
  eprint = {2006.05935},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2006.05935},
  urldate = {2020-06-16},
  abstract = {Dynamic tasks like table tennis are relatively easy to learn for humans but pose significant challenges to robots. Such tasks require accurate control of fast movements and precise timing in the presence of imprecise state estimation of the flying ball and the robot. Reinforcement Learning (RL) has shown promise in learning of complex control tasks from data. However, applying step-based RL to dynamic tasks on real systems is safety-critical as RL requires exploring and failing safely for millions of time steps in high-speed regimes. In this paper, we demonstrate that safe learning of table tennis using model-free Reinforcement Learning can be achieved by using robot arms driven by pneumatic artificial muscles (PAMs). Softness and back-drivability properties of PAMs prevent the system from leaving the safe region of its state space. In this manner, RL empowers the robot to return and smash real balls with 5 m\textbackslash s and 12m\textbackslash s on average to a desired landing point. Our setup allows the agent to learn this safety-critical task (i) without safety constraints in the algorithm, (ii) while maximizing the speed of returned balls directly in the reward function (iii) using a stochastic policy that acts directly on the low-level controls of the real system and (iv) trains for thousands of trials (v) from scratch without any prior knowledge. Additionally, we present HYSR, a practical hybrid sim and real training that avoids playing real balls during training by randomly replaying recorded ball trajectories in simulation and applying actions to the real robot. This work is the first to (a) fail-safe learn of a safety-critical dynamic task using anthropomorphic robot arms, (b) learn a precision-demanding problem with a PAM-driven system despite the control challenges and (c) train robots to play table tennis without real balls. Videos and datasets are available at muscularTT.embodied.ml.},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/NP4RX2C6/Büchler_et_al_2020_Learning_to_Play_Table_Tennis_From_Scratch_using_Muscular_Robots.pdf;/Users/vitay/Documents/Zotero/storage/C4EPW2ZV/2006.html}
}

@unpublished{Burda2018,
  title = {Large-{{Scale Study}} of {{Curiosity-Driven Learning}}},
  author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
  date = {2018-08-13},
  eprint = {1808.04355},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1808.04355},
  urldate = {2021-02-06},
  abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/2PQYSKHY/Burda_et_al_2018_Large-Scale_Study_of_Curiosity-Driven_Learning.pdf;/Users/vitay/Documents/Zotero/storage/Q73XIHZZ/1808.html}
}

@online{Burda2018b,
  title = {Exploration by {{Random Network Distillation}}},
  author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  date = {2018-10-30},
  eprint = {1810.12894},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1810.12894},
  url = {http://arxiv.org/abs/1810.12894},
  urldate = {2024-06-24},
  abstract = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/DMX596BD/Burda et al. - 2018 - Exploration by Random Network Distillation.pdf}
}

@online{Cabi2017,
  title = {The {{Intentional Unintentional Agent}}: {{Learning}} to {{Solve Many Continuous Control Tasks Simultaneously}}},
  shorttitle = {The {{Intentional Unintentional Agent}}},
  author = {Cabi, Serkan and Colmenarejo, Sergio Gómez and Hoffman, Matthew W. and Denil, Misha and Wang, Ziyu and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2017-07-11},
  eprint = {1707.03300},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1707.03300},
  url = {http://arxiv.org/abs/1707.03300},
  urldate = {2023-06-20},
  abstract = {This paper introduces the Intentional Unintentional (IU) agent. This agent endows the deep deterministic policy gradients (DDPG) agent for continuous control with the ability to solve several tasks simultaneously. Learning to solve many tasks simultaneously has been a long-standing, core goal of artificial intelligence, inspired by infant development and motivated by the desire to build flexible robot manipulators capable of many diverse behaviours. We show that the IU agent not only learns to solve many tasks simultaneously but it also learns faster than agents that target a single task at-a-time. In some cases, where the single task DDPG method completely fails, the IU agent successfully solves the task. To demonstrate this, we build a playroom environment using the MuJoCo physics engine, and introduce a grounded formal language to automatically generate tasks.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/674TJ6H8/Cabi_et_al_2017_The_Intentional_Unintentional_Agent.pdf}
}

@unpublished{Campero2020,
  title = {Learning with {{AMIGo}}: {{Adversarially Motivated Intrinsic Goals}}},
  shorttitle = {Learning with {{AMIGo}}},
  author = {Campero, Andres and Raileanu, Roberta and Küttler, Heinrich and Tenenbaum, Joshua B. and Rocktäschel, Tim and Grefenstette, Edward},
  date = {2020-06-22},
  eprint = {2006.12122},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.12122},
  urldate = {2020-06-27},
  abstract = {A key challenge for reinforcement learning (RL) consists of learning in environments with sparse extrinsic rewards. In contrast to current RL methods, humans are able to learn new skills with little or no reward by using various forms of intrinsic motivation. We propose AMIGo, a novel agent incorporating a goal-generating teacher that proposes Adversarially Motivated Intrinsic Goals to train a goal-conditioned "student" policy in the absence of (or alongside) environment reward. Specifically, through a simple but effective "constructively adversarial" objective, the teacher learns to propose increasingly challenging---yet achievable---goals that allow the student to learn general skills for acting in a new environment, independent of the task to be solved. We show that our method generates a natural curriculum of self-proposed goals which ultimately allows the agent to solve challenging procedurally-generated tasks where other forms of intrinsic motivation and state-of-the-art RL methods fail.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/AYWPNMNT/Campero_et_al_2020_Learning_with_AMIGo.pdf;/Users/vitay/Documents/Zotero/storage/SJFSNH7J/2006.html}
}

@unpublished{Carroll2019,
  title = {On the {{Utility}} of {{Learning}} about {{Humans}} for {{Human-AI Coordination}}},
  author = {Carroll, Micah and Shah, Rohin and Ho, Mark K. and Griffiths, Thomas L. and Seshia, Sanjit A. and Abbeel, Pieter and Dragan, Anca},
  date = {2019-10-13},
  eprint = {1910.05789},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.05789},
  urldate = {2019-10-19},
  abstract = {While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human's gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them. Code is available at https://github.com/HumanCompatibleAI/overcooked\_ai.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/SXQFNEKR/Carroll et al_2019_On the Utility of Learning about Humans for Human-AI Coordination.pdf;/Users/vitay/Documents/Zotero/storage/3GMCEUZ3/1910.html}
}

@unpublished{Chang2019,
  title = {Convolutional {{Reservoir Computing}} for {{World Models}}},
  author = {Chang, Hanten and Futagami, Katsuya},
  date = {2019-07-18},
  eprint = {1907.08040},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.08040},
  urldate = {2020-06-19},
  abstract = {Recently, reinforcement learning models have achieved great success, completing complex tasks such as mastering Go and other games with higher scores than human players. Many of these models collect considerable data on the tasks and improve accuracy by extracting visual and time-series features using convolutional neural networks (CNNs) and recurrent neural networks, respectively. However, these networks have very high computational costs because they need to be trained by repeatedly using a large volume of past playing data. In this study, we propose a novel practical approach called reinforcement learning with convolutional reservoir computing (RCRC) model. The RCRC model has several desirable features: 1. it can extract visual and time-series features very fast because it uses random fixed-weight CNN and the reservoir computing model; 2. it does not require the training data to be stored because it extracts features without training and decides action with evolution strategy. Furthermore, the model achieves state of the art score in the popular reinforcement learning task. Incredibly, we find the random weight-fixed simple networks like only one dense layer network can also reach high score in the RL task.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/54UHN5BN/Chang_Futagami_2019_Convolutional_Reservoir_Computing_for_World_Models.pdf;/Users/vitay/Documents/Zotero/storage/E6TNTTR5/1907.html}
}

@unpublished{Chen2021,
  title = {Decision {{Transformer}}: {{Reinforcement Learning}} via {{Sequence Modeling}}},
  shorttitle = {Decision {{Transformer}}},
  author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  date = {2021-06-24},
  eprint = {2106.01345},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.01345},
  urldate = {2021-12-02},
  abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/YNV77INB/Chen_et_al_2021_Decision_Transformer.pdf;/Users/vitay/Documents/Zotero/storage/YGW5MEKJ/2106.html}
}

@unpublished{Chen2022,
  title = {Deep {{Reinforcement Learning}} with {{Spiking Q-learning}}},
  author = {Chen, Ding and Peng, Peixi and Huang, Tiejun and Tian, Yonghong},
  date = {2022-01-21},
  eprint = {2201.09754},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.09754},
  urldate = {2022-01-31},
  abstract = {With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combing SNNs and deep reinforcement learning (RL). There are only a few existing SNN-based RL methods at present. Most of them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training. The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in training. To develop a robust spike-based RL method, we draw inspiration from non-spiking interneurons found in insects and propose the deep spiking Q-network (DSQN), using the membrane voltage of non-spiking neurons as the representation of Q-value, which can directly learn robust policies from high-dimensional sensory inputs using end-to-end RL. Experiments conducted on 17 Atari games demonstrate the effectiveness of DSQN by outperforming the ANN-based deep Q-network (DQN) in most games. Moreover, the experimental results show superior learning stability and robustness to adversarial attacks of DSQN.},
  file = {/Users/vitay/Documents/Zotero/storage/WRLN539I/Chen_et_al_2022_Deep_Reinforcement_Learning_with_Spiking_Q-learning.pdf;/Users/vitay/Documents/Zotero/storage/NWUBY8QZ/2201.html}
}

@online{Chen2022a,
  title = {You {{Only Live Once}}: {{Single-Life Reinforcement Learning}}},
  shorttitle = {You {{Only Live Once}}},
  author = {Chen, Annie S. and Sharma, Archit and Levine, Sergey and Finn, Chelsea},
  date = {2022-10-17},
  eprint = {2210.08863},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.08863},
  urldate = {2022-10-20},
  abstract = {Reinforcement learning algorithms are typically designed to learn a performant policy that can repeatedly and autonomously complete a task, usually starting from scratch. However, in many real-world situations, the goal might not be to learn a policy that can do the task repeatedly, but simply to perform a new task successfully once in a single trial. For example, imagine a disaster relief robot tasked with retrieving an item from a fallen building, where it cannot get direct supervision from humans. It must retrieve this object within one test-time trial, and must do so while tackling unknown obstacles, though it may leverage knowledge it has of the building before the disaster. We formalize this problem setting, which we call single-life reinforcement learning (SLRL), where an agent must complete a task within a single episode without interventions, utilizing its prior experience while contending with some form of novelty. SLRL provides a natural setting to study the challenge of autonomously adapting to unfamiliar situations, and we find that algorithms designed for standard episodic reinforcement learning often struggle to recover from out-of-distribution states in this setting. Motivated by this observation, we propose an algorithm, \$Q\$-weighted adversarial learning (QWALE), which employs a distribution matching strategy that leverages the agent's prior experience as guidance in novel situations. Our experiments on several single-life continuous control problems indicate that methods based on our distribution matching formulation are 20-60\% more successful because they can more quickly recover from novel states.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/VVNLK6SD/Chen_et_al_2022_You_Only_Live_Once.pdf;/Users/vitay/Documents/Zotero/storage/FH3PTRXC/2210.html}
}

@online{Chenu2022,
  title = {Divide \& {{Conquer Imitation Learning}}},
  author = {Chenu, Alexandre and Perrin-Gilbert, Nicolas and Sigaud, Olivier},
  date = {2022-04-15},
  eprint = {2204.07404},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.07404},
  url = {http://arxiv.org/abs/2204.07404},
  urldate = {2022-12-19},
  abstract = {When cast into the Deep Reinforcement Learning framework, many robotics tasks require solving a long horizon and sparse reward problem, where learning algorithms struggle. In such context, Imitation Learning (IL) can be a powerful approach to bootstrap the learning process. However, most IL methods require several expert demonstrations which can be prohibitively difficult to acquire. Only a handful of IL algorithms have shown efficiency in the context of an extreme low expert data regime where a single expert demonstration is available. In this paper, we present a novel algorithm designed to imitate complex robotic tasks from the states of an expert trajectory. Based on a sequential inductive bias, our method divides the complex task into smaller skills. The skills are learned into a goal-conditioned policy that is able to solve each skill individually and chain skills to solve the entire task. We show that our method imitates a non-holonomic navigation task and scales to a complex simulated robotic manipulation task with very high sample efficiency.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/CAZ6MMIT/Chenu_et_al_2022_Divide_&_Conquer_Imitation_Learning.pdf;/Users/vitay/Documents/Zotero/storage/5EXF7PQU/2204.html}
}

@online{Chi2023,
  title = {Diffusion {{Policy}}: {{Visuomotor Policy Learning}} via {{Action Diffusion}}},
  shorttitle = {Diffusion {{Policy}}},
  author = {Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
  date = {2023-03-07},
  url = {https://arxiv.org/abs/2303.04137v5},
  urldate = {2024-10-09},
  abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu},
  langid = {english},
  organization = {arXiv.org},
  file = {/Users/vitay/Documents/Zotero/storage/KF2ZQTWU/Chi et al. - 2023 - Diffusion Policy Visuomotor Policy Learning via Action Diffusion.pdf}
}

@online{Chi2024,
  title = {Diffusion {{Policy}}: {{Visuomotor Policy Learning}} via {{Action Diffusion}}},
  shorttitle = {Diffusion {{Policy}}},
  author = {Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
  date = {2024-03-14},
  eprint = {2303.04137},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2303.04137},
  url = {http://arxiv.org/abs/2303.04137},
  urldate = {2024-11-02},
  abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/UCC747YK/Chi et al. - 2024 - Diffusion Policy Visuomotor Policy Learning via Action Diffusion.pdf}
}

@unpublished{Chitnis2020,
  title = {{{GLIB}}: {{Exploration}} via {{Goal-Literal Babbling}} for {{Lifted Operator Learning}}},
  shorttitle = {{{GLIB}}},
  author = {Chitnis, Rohan and Silver, Tom and Tenenbaum, Joshua and Kaelbling, Leslie Pack and Lozano-Perez, Tomas},
  date = {2020-01-22},
  eprint = {2001.08299},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2001.08299},
  urldate = {2020-01-28},
  abstract = {We address the problem of efficient exploration for learning lifted operators in sequential decision-making problems without extrinsic goals or rewards. Inspired by human curiosity, we propose goal-literal babbling (GLIB), a simple and general method for exploration in such problems. GLIB samples goals that are conjunctions of literals, which can be understood as specific, targeted effects that the agent would like to achieve in the world, and plans to achieve these goals using the operators being learned. We conduct a case study to elucidate two key benefits of GLIB: robustness to overly general preconditions and efficient exploration in domains with effects at long horizons. We also provide theoretical guarantees and further empirical results, finding GLIB to be effective on a range of benchmark planning tasks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/2HMNIZKG/Chitnis_et_al_2020_GLIB.pdf;/Users/vitay/Documents/Zotero/storage/BKA6BS4B/2001.html}
}

@inproceedings{Chou2017,
  title = {Improving {{Stochastic Policy Gradients}} in {{Continuous Control}} with {{Deep Reinforcement Learning}} Using the {{Beta Distribution}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Chou, Po-Wei and Maturana, Daniel and Scherer, Sebastian},
  date = {2017},
  url = {http://proceedings.mlr.press/v70/chou17a/chou17a.pdf},
  abstract = {Recently, reinforcement learning with deep neu-ral networks has achieved great success in chal-lenging continuous control problems such as 3D locomotion and robotic manipulation. However, in real-world control problems, the actions one can take are bounded by physical constraints, which introduces a bias when the standard Gaus-sian distribution is used as the stochastic policy. In this work, we propose to use the Beta dis-tribution as an alternative and analyze the bias and variance of the policy gradients of both poli-cies. We show that the Beta policy is bias-free and provides significantly faster convergence and higher scores over the Gaussian policy when both are used with trust region policy optimiza-tion (TRPO) and actor critic with experience re-play (ACER), the state-of-the-art on-and off-policy stochastic methods respectively, on Ope-nAI Gym's and MuJoCo's continuous control en-vironments.},
  file = {/Users/vitay/Documents/Zotero/storage/YSNDIMAZ/Chou_et_al_2017_Improving_Stochastic_Policy_Gradients_in_Continuous_Control_with_Deep.pdf}
}

@unpublished{Chow2020,
  title = {Variational {{Model-based Policy Optimization}}},
  author = {Chow, Yinlam and Cui, Brandon and Ryu, MoonKyung and Ghavamzadeh, Mohammad},
  date = {2020-06-09},
  eprint = {2006.05443},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.05443},
  urldate = {2020-06-16},
  abstract = {Model-based reinforcement learning (RL) algorithms allow us to combine model-generated data with those collected from interaction with the real system in order to alleviate the data efficiency problem in RL. However, designing such algorithms is often challenging because the bias in simulated data may overshadow the ease of data generation. A potential solution to this challenge is to jointly learn and improve model and policy using a universal objective function. In this paper, we leverage the connection between RL and probabilistic inference, and formulate such an objective function as a variational lower-bound of a log-likelihood. This allows us to use expectation maximization (EM) and iteratively fix a baseline policy and learn a variational distribution, consisting of a model and a policy (E-step), followed by improving the baseline policy given the learned variational distribution (M-step). We propose model-based and model-free policy iteration (actor-critic) style algorithms for the E-step and show how the variational distribution learned by them can be used to optimize the M-step in a fully model-based fashion. Our experiments on a number of continuous control tasks show that despite being more complex, our model-based (E-step) algorithm, called \{\textbackslash em variational model-based policy optimization\} (VMBPO), is more sample-efficient and robust to hyper-parameter tuning than its model-free (E-step) counterpart. Using the same control tasks, we also compare VMBPO with several state-of-the-art model-based and model-free RL algorithms and show its sample efficiency and performance.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/8UZMDMPU/Chow_et_al_2020_Variational_Model-based_Policy_Optimization.pdf;/Users/vitay/Documents/Zotero/storage/XAB2EJHU/2006.html}
}

@unpublished{Cini2020,
  title = {Deep {{Reinforcement Learning}} with {{Weighted Q-Learning}}},
  author = {Cini, Andrea and D'Eramo, Carlo and Peters, Jan and Alippi, Cesare},
  date = {2020-03-20},
  eprint = {2003.09280},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2003.09280},
  urldate = {2020-03-28},
  abstract = {Overestimation of the maximum action-value is a well-known problem that hinders Q-Learning performance, leading to suboptimal policies and unstable learning. Among several Q-Learning variants proposed to address this issue, Weighted Q-Learning (WQL) effectively reduces the bias and shows remarkable results in stochastic environments. WQL uses a weighted sum of the estimated action-values, where the weights correspond to the probability of each action-value being the maximum; however, the computation of these probabilities is only practical in the tabular settings. In this work, we provide the methodological advances to benefit from the WQL properties in Deep Reinforcement Learning (DRL), by using neural networks with Dropout Variational Inference as an effective approximation of deep Gaussian processes. In particular, we adopt the Concrete Dropout variant to obtain calibrated estimates of epistemic uncertainty in DRL. We show that model uncertainty in DRL can be useful not only for action selection, but also action evaluation. We analyze how the novel Weighted Deep Q-Learning algorithm reduces the bias w.r.t. relevant baselines and provide empirical evidence of its advantages on several representative benchmarks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/76DZXRRR/Cini_et_al_2020_Deep_Reinforcement_Learning_with_Weighted_Q-Learning.pdf;/Users/vitay/Documents/Zotero/storage/L5ENTKRW/2003.html}
}

@article{Clavera2018,
  title = {Learning to {{Adapt}}: {{Meta-Learning}} for {{Model-Based Control}}},
  author = {Clavera, Ignasi and Nagabandi, Anusha and Fearing, Ronald S. and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
  date = {2018-03},
  url = {http://arxiv.org/abs/1803.11347},
  abstract = {Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations can cause proficient but narrowly-learned policies to fail at test time. In this work, we propose to learn how to quickly and effectively adapt online to new situations as well as to perturbations. To enable sample-efficient meta-learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach trains a global model such that, when combined with recent data, the model can be be rapidly adapted to the local context. Our experiments demonstrate that our approach can enable simulated agents to adapt their behavior online to novel terrains, to a crippled leg, and in highly-dynamic environments.},
  file = {/Users/vitay/Documents/Zotero/storage/7L5SUXGY/Clavera et al_2018_Learning to Adapt.pdf}
}

@unpublished{Clavera2020,
  title = {Model-{{Augmented Actor-Critic}}: {{Backpropagating}} through {{Paths}}},
  shorttitle = {Model-{{Augmented Actor-Critic}}},
  author = {Clavera, Ignasi and Fu, Violet and Abbeel, Pieter},
  date = {2020-05-16},
  eprint = {2005.08068},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2005.08068},
  urldate = {2020-05-23},
  abstract = {Current model-based reinforcement learning approaches use the model simply as a learned black-box simulator to augment the data for policy optimization or value function learning. In this paper, we show how to make more effective use of the model by exploiting its differentiability. We construct a policy optimization algorithm that uses the pathwise derivative of the learned model and policy across future timesteps. Instabilities of learning across many timesteps are prevented by using a terminal value function, learning the policy in an actor-critic fashion. Furthermore, we present a derivation on the monotonic improvement of our objective in terms of the gradient error in the model and value function. We show that our approach (i) is consistently more sample efficient than existing state-of-the-art model-based algorithms, (ii) matches the asymptotic performance of model-free algorithms, and (iii) scales to long horizons, a regime where typically past model-based approaches have struggled.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/KAW2TGQV/Clavera_et_al_2020_Model-Augmented_Actor-Critic.pdf;/Users/vitay/Documents/Zotero/storage/MAZN5WNP/2005.html}
}

@online{Clements2020,
  title = {Estimating {{Risk}} and {{Uncertainty}} in {{Deep Reinforcement Learning}}},
  author = {Clements, William R. and Van Delft, Bastien and Robaglia, Benoît-Marie and Slaoui, Reda Bahi and Toth, Sébastien},
  date = {2020-09-09},
  eprint = {1905.09638},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1905.09638},
  url = {http://arxiv.org/abs/1905.09638},
  urldate = {2022-08-01},
  abstract = {Reinforcement learning agents are faced with two types of uncertainty. Epistemic uncertainty stems from limited data and is useful for exploration, whereas aleatoric uncertainty arises from stochastic environments and must be accounted for in risk-sensitive applications. We highlight the challenges involved in simultaneously estimating both of them, and propose a framework for disentangling and estimating these uncertainties on learned Q-values. We derive unbiased estimators of these uncertainties and introduce an uncertainty-aware DQN algorithm, which we show exhibits safe learning behavior and outperforms other DQN variants on the MinAtar testbed.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/89JY6YQG/Clements_et_al_2020_Estimating_Risk_and_Uncertainty_in_Deep_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/ZPYHIPPQ/1905.html}
}

@article{Co-Reyes2018,
  title = {Self-{{Consistent Trajectory Autoencoder}}: {{Hierarchical Reinforcement Learning}} with {{Trajectory Embeddings}}},
  author = {Co-Reyes, John D. and Liu, YuXuan and Gupta, Abhishek and Eysenbach, Benjamin and Abbeel, Pieter and Levine, Sergey},
  date = {2018},
  abstract = {In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.}
}

@article{Collette2017,
  title = {Neural Computations Underlying Inverse Reinforcement Learning in the Human Brain},
  author = {Collette, Sven and Pauli, Wolfgang M. and Bossaerts, Peter and O'Doherty, John},
  date = {2017-10-30},
  journaltitle = {eLife},
  shortjournal = {Elife},
  volume = {6},
  eprint = {29083301},
  eprinttype = {pmid},
  pages = {e29718},
  issn = {2050-084X},
  doi = {10.7554/eLife.29718},
  abstract = {In inverse reinforcement learning an observer infers the reward distribution available for actions in the environment solely through observing the actions implemented by another agent. To address whether this computational process is implemented in the human brain, participants underwent fMRI while learning about slot machines yielding hidden preferred and non-preferred food outcomes with varying probabilities, through observing the repeated slot choices of agents with similar and dissimilar food preferences. Using formal model comparison, we found that participants implemented inverse RL as opposed to a simple imitation strategy, in which the actions of the other agent are copied instead of inferring the underlying reward structure of the decision problem. Our computational fMRI analysis revealed that anterior dorsomedial prefrontal cortex encoded inferences about action-values within the value space of the agent as opposed to that of the observer, demonstrating that inverse RL is an abstract cognitive process divorceable from the values and concerns of the observer him/herself.},
  langid = {english},
  pmcid = {PMC5662289},
  file = {/Users/vitay/Documents/Zotero/storage/SZHTNMXI/Collette_et_al_2017_Neural_computations_underlying_inverse_reinforcement_learning_in_the_human_brain.pdf}
}

@article{Collins2020,
  title = {Beyond Dichotomies in Reinforcement Learning},
  author = {Collins, Anne G. E. and Cockburn, Jeffrey},
  date = {2020-10},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {21},
  number = {10},
  pages = {576--586},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/s41583-020-0355-6},
  url = {https://www.nature.com/articles/s41583-020-0355-6},
  urldate = {2022-01-08},
  abstract = {Reinforcement learning (RL) is a framework of particular importance to psychology, neuroscience and machine learning. Interactions between these fields, as promoted through the common hub of RL, has facilitated paradigm shifts that relate multiple levels of analysis in a singular framework (for example, relating dopamine function to a computationally defined RL signal). Recently, more sophisticated RL algorithms have been proposed to better account for human learning, and in particular its oft-documented reliance on two separable systems: a model-based (MB) system and a model-free (MF) system. However, along with many benefits, this dichotomous lens can distort questions, and may contribute to an unnecessarily narrow perspective on learning and decision-making. Here, we outline some of the consequences that come from overconfidently mapping algorithms, such as MB versus MF RL, with putative cognitive processes. We argue that the field is well positioned to move beyond simplistic dichotomies, and we propose a means of refocusing research questions towards the rich and complex components that comprise learning and decision-making.},
  issue = {10},
  langid = {english},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Reviews\\
Subject\_term: Cognitive neuroscience;Learning algorithms;Learning and memory;Psychology\\
Subject\_term\_id: cognitive-neuroscience;learning-algorithms;learning-and-memory;psychology},
  file = {/Users/vitay/Documents/Zotero/storage/67Y89TYM/Collins_Cockburn_2020_Beyond_dichotomies_in_reinforcement_learning.pdf;/Users/vitay/Documents/Zotero/storage/BKJJA7ZI/s41583-020-0355-6.html}
}

@inproceedings{Connect1992,
  title = {A {{Simple Weight Decay Can Improve Generalization}}},
  booktitle = {Proc. {{Advances}} in {{Neural Information Processing Systems}}},
  author = {Connect, a. K. and Krogh, A. and family=Hertz, given=J., prefix=a., useprefix=false},
  date = {1992},
  volume = {4},
  pages = {950--957},
  url = {http://0-citeseerx.ist.psu.edu.innopac.up.ac.za/viewdoc/summary?doi=10.1.1.41.2305},
  abstract = {It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk. 1 INTRODUCTION Many recent studies have shown that the generalization ability of a neural network (or any other `learning machine') depends on a balance between the information in the training examples and the complexity of the network, see for instance [1,2,3]. Bad generalization oc...},
  isbn = {1-55860-222-4}
}

@article{Corneil2018,
  title = {Efficient {{Model-Based Deep Reinforcement Learning}} with {{Variational State Tabulation}}},
  author = {Corneil, Dane and Gerstner, Wulfram and Brea, Johanni},
  date = {2018-02},
  url = {http://arxiv.org/abs/1802.04325},
  abstract = {Modern reinforcement learning algorithms reach super-human performance in many board and video games, but they are sample inefficient, i.e. they typically require significantly more playing experience than humans to reach an equal performance level. To improve sample efficiency, an agent may build a model of the environment and use planning methods to update its policy. In this article we introduce VaST (Variational State Tabulation), which maps an environment with a high-dimensional state space (e.g. the space of visual inputs) to an abstract tabular environment. Prioritized sweeping with small backups, a highly efficient planning method, can then be used to update state-action values. We show how VaST can rapidly learn to maximize reward in tasks like 3D navigation and efficiently adapt to sudden changes in rewards or transition probabilities.},
  file = {/Users/vitay/Documents/Zotero/storage/CM2NLAB2/Corneil et al_2018_Efficient Model-Based Deep Reinforcement Learning with Variational State.pdf}
}

@unpublished{Dabney2017,
  title = {Distributional {{Reinforcement Learning}} with {{Quantile Regression}}},
  author = {Dabney, Will and Rowland, Mark and Bellemare, Marc G. and Munos, Rémi},
  date = {2017-10-27},
  eprint = {1710.10044},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1710.10044},
  urldate = {2019-06-28},
  abstract = {In reinforcement learning an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/2LEAKQS2/Dabney et al_2017_Distributional Reinforcement Learning with Quantile Regression.pdf;/Users/vitay/Documents/Zotero/storage/JMNU7RJF/1710.html}
}

@unpublished{Dabney2018,
  title = {Implicit {{Quantile Networks}} for {{Distributional Reinforcement Learning}}},
  author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Rémi},
  date = {2018-06-14},
  eprint = {1806.06923},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.06923},
  urldate = {2019-11-26},
  abstract = {In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/RNLXM7RQ/Dabney et al_2018_Implicit Quantile Networks for Distributional Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/QZ8XUSGT/1806.html}
}

@unpublished{Dabney2020a,
  title = {The {{Value-Improvement Path}}: {{Towards Better Representations}} for {{Reinforcement Learning}}},
  shorttitle = {The {{Value-Improvement Path}}},
  author = {Dabney, Will and Barreto, André and Rowland, Mark and Dadashi, Robert and Quan, John and Bellemare, Marc G. and Silver, David},
  date = {2020-06-03},
  eprint = {2006.02243},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.02243},
  urldate = {2020-06-09},
  abstract = {In value-based reinforcement learning (RL), unlike in supervised learning, the agent faces not a single, stationary, approximation problem, but a sequence of value prediction problems. Each time the policy improves, the nature of the problem changes, shifting both the distribution of states and their values. In this paper we take a novel perspective, arguing that the value prediction problems faced by an RL agent should not be addressed in isolation, but rather as a single, holistic, prediction problem. An RL algorithm generates a sequence of policies that, at least approximately, improve towards the optimal policy. We explicitly characterize the associated sequence of value functions and call it the value-improvement path. Our main idea is to approximate the value-improvement path holistically, rather than to solely track the value function of the current policy. Specifically, we discuss the impact that this holistic view of RL has on representation learning. We demonstrate that a representation that spans the past value-improvement path will also provide an accurate value approximation for future policy improvements. We use this insight to better understand existing approaches to auxiliary tasks and to propose new ones. To test our hypothesis empirically, we augmented a standard deep RL agent with an auxiliary task of learning the value-improvement path. In a study of Atari 2600 games, the augmented agent achieved approximately double the mean and median performance of the baseline agent.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/VS6DCE7Z/Dabney_et_al_2020_The_Value-Improvement_Path.pdf;/Users/vitay/Documents/Zotero/storage/99DJ2Y5Q/2006.html}
}

@unpublished{Dasari2019,
  title = {{{RoboNet}}: {{Large-Scale Multi-Robot Learning}}},
  shorttitle = {{{RoboNet}}},
  author = {Dasari, Sudeep and Ebert, Frederik and Tian, Stephen and Nair, Suraj and Bucher, Bernadette and Schmeckpeper, Karl and Singh, Siddharth and Levine, Sergey and Finn, Chelsea},
  date = {2019-10-24},
  eprint = {1910.11215},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1910.11215},
  urldate = {2019-10-30},
  abstract = {Robot learning has emerged as a promising tool for taming the complexity and diversity of the real world. Methods based on high-capacity models, such as deep networks, hold the promise of providing effective generalization to a wide range of open-world environments. However, these same methods typically require large amounts of diverse training data to generalize effectively. In contrast, most robotic learning experiments are small-scale, single-domain, and single-robot. This leads to a frequent tension in robotic learning: how can we learn generalizable robotic controllers without having to collect impractically large amounts of data for each separate experiment? In this paper, we propose RoboNet, an open database for sharing robotic experience, which provides an initial pool of 15 million video frames, from 7 different robot platforms, and study how it can be used to learn generalizable models for vision-based robotic manipulation. We combine the dataset with two different learning algorithms: visual foresight, which uses forward video prediction models, and supervised inverse models. Our experiments test the learned algorithms' ability to work across new objects, new tasks, new scenes, new camera viewpoints, new grippers, or even entirely new robots. In our final experiment, we find that by pre-training on RoboNet and fine-tuning on data from a held-out Franka or Kuka robot, we can exceed the performance of a robot-specific training approach that uses 4x-20x more data. For videos and data, see the project webpage: https://www.robonet.wiki/},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/KX24JJNW/Dasari et al_2019_RoboNet.pdf;/Users/vitay/Documents/Zotero/storage/T8FVYGWN/1910.html}
}

@article{Dayan1993,
  title = {Improving {{Generalization}} for {{Temporal Difference Learning}}: {{The Successor Representation}}},
  shorttitle = {Improving {{Generalization}} for {{Temporal Difference Learning}}},
  author = {Dayan, Peter},
  date = {1993-07-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {5},
  number = {4},
  pages = {613--624},
  issn = {0899-7667},
  doi = {10.1162/neco.1993.5.4.613},
  url = {https://www.mitpressjournals.org/doi/10.1162/neco.1993.5.4.613},
  urldate = {2019-02-23},
  abstract = {Estimation of returns over time, the focus of temporal difference (TD) algorithms, imposes particular constraints on good function approximators or representations. Appropriate generalization between states is determined by how similar their successors are, and representations should follow suit. This paper shows how TD machinery can be used to learn such representations, and illustrates, using a navigation task, the appropriately distributed nature of the result.},
  file = {/Users/vitay/Documents/Zotero/storage/UDYRCIZQ/Dayan_1993_Improving Generalization for Temporal Difference Learning.pdf;/Users/vitay/Documents/Zotero/storage/KF59LCAQ/neco.1993.5.4.html}
}

@article{Dayan2008,
  ids = {Dayan2008a},
  title = {Reinforcement Learning: {{The Good}}, {{The Bad}} and {{The Ugly}}},
  shorttitle = {Reinforcement Learning},
  author = {Dayan, Peter and Niv, Yael},
  date = {2008-04},
  journaltitle = {Current Opinion in Neurobiology},
  volume = {18},
  number = {2},
  pages = {185--196},
  issn = {09594388},
  doi = {10.1016/j.conb.2008.08.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438808000767},
  urldate = {2019-03-03},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/JJ998SSN/Dayan_Niv_2008_Reinforcement learning.pdf;/Users/vitay/Documents/Zotero/storage/HATBATDZ/S0959438808000767.html}
}

@article{Degrave2022,
  title = {Magnetic Control of Tokamak Plasmas through Deep Reinforcement Learning},
  author = {Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and family=Casas, given=Diego, prefix=de las, useprefix=true and Donner, Craig and Fritz, Leslie and Galperti, Cristian and Huber, Andrea and Keeling, James and Tsimpoukelli, Maria and Kay, Jackie and Merle, Antoine and Moret, Jean-Marc and Noury, Seb and Pesamosca, Federico and Pfau, David and Sauter, Olivier and Sommariva, Cristian and Coda, Stefano and Duval, Basil and Fasoli, Ambrogio and Kohli, Pushmeet and Kavukcuoglu, Koray and Hassabis, Demis and Riedmiller, Martin},
  date = {2022-02},
  journaltitle = {Nature},
  volume = {602},
  number = {7897},
  pages = {414--419},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04301-9},
  url = {https://www.nature.com/articles/s41586-021-04301-9},
  urldate = {2023-02-03},
  abstract = {Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak à Configuration Variable1,2, including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and ‘snowflake’ configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained ‘droplets’ on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
  issue = {7897},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/E9DI9MT9/Degrave_et_al_2022_Magnetic_control_of_tokamak_plasmas_through_deep_reinforcement_learning.pdf}
}

@inproceedings{Degris2012,
  title = {Linear {{Off-Policy Actor-Critic}}},
  booktitle = {Proceedings of the 2012 {{International Conference}} on {{Machine Learning}}},
  author = {Degris, Thomas and White, Martha and Sutton, Richard S.},
  date = {2012-05},
  url = {http://arxiv.org/abs/1205.4839},
  abstract = {This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems.},
  file = {/Users/vitay/Documents/Zotero/storage/X636GFG6/Degris et al_2012_Linear Off-Policy Actor-Critic.pdf}
}

@incollection{Devin2019,
  title = {Compositional {{Plan Vectors}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Devin, Coline and Geng, Daniel and Abbeel, Pieter and Darrell, Trevor and Levine, Sergey},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Alché-Buc, given=F., prefix=d\textbackslash textquotesingle, useprefix=false and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {14963--14974},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/9636-compositional-plan-vectors.pdf},
  urldate = {2019-11-19},
  file = {/Users/vitay/Documents/Zotero/storage/R7EQH9AW/Devin et al_2019_Compositional Plan Vectors.pdf;/Users/vitay/Documents/Zotero/storage/IBZIPHBY/9636-compositional-plan-vectors.html}
}

@unpublished{Devin2019a,
  title = {Plan {{Arithmetic}}: {{Compositional Plan Vectors}} for {{Multi-Task Control}}},
  shorttitle = {Plan {{Arithmetic}}},
  author = {Devin, Coline and Geng, Daniel and Abbeel, Pieter and Darrell, Trevor and Levine, Sergey},
  date = {2019-10-30},
  eprint = {1910.14033},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.14033},
  urldate = {2019-11-05},
  abstract = {Autonomous agents situated in real-world environments must be able to master large repertoires of skills. While a single short skill can be learned quickly, it would be impractical to learn every task independently. Instead, the agent should share knowledge across behaviors such that each task can be learned efficiently, and such that the resulting model can generalize to new tasks, especially ones that are compositions or subsets of tasks seen previously. A policy conditioned on a goal or demonstration has the potential to share knowledge between tasks if it sees enough diversity of inputs. However, these methods may not generalize to a more complex task at test time. We introduce compositional plan vectors (CPVs) to enable a policy to perform compositions of tasks without additional supervision. CPVs represent trajectories as the sum of the subtasks within them. We show that CPVs can be learned within a one-shot imitation learning framework without any additional supervision or information about task hierarchy, and enable a demonstration-conditioned policy to generalize to tasks that sequence twice as many skills as the tasks seen during training. Analogously to embeddings such as word2vec in NLP, CPVs can also support simple arithmetic operations -- for example, we can add the CPVs for two different tasks to command an agent to compose both tasks, without any additional training.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/EU2XGNNL/Devin et al_2019_Plan Arithmetic.pdf;/Users/vitay/Documents/Zotero/storage/N79NHNHF/1910.html}
}

@inproceedings{Ding2019,
  title = {Goal-Conditioned {{Imitation Learning}}},
  author = {Ding, Yiming and Florensa, Carlos and Phielipp, Mariano and Abbeel, Pieter},
  date = {2019},
  volume = {97},
  pages = {8},
  publisher = {PMLR},
  location = {Long Beach, California},
  url = {https://openreview.net/pdf?id=HkglHcSj2N},
  abstract = {Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might take a very long time to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, our method can be used when only trajectories without expert actions are available, which can leverage kinestetic or third person demonstration.},
  eventtitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/C7T4LNLQ/Ding_et_al_2019_Goal-conditioned_Imitation_Learning.pdf}
}

@unpublished{Ding2020,
  title = {Mutual {{Information Maximization}} for {{Robust Plannable Representations}}},
  author = {Ding, Yiming and Clavera, Ignasi and Abbeel, Pieter},
  date = {2020-05-16},
  eprint = {2005.08114},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2005.08114},
  urldate = {2020-05-23},
  abstract = {Extending the capabilities of robotics to real-world complex, unstructured environments requires the need of developing better perception systems while maintaining low sample complexity. When dealing with high-dimensional state spaces, current methods are either model-free or model-based based on reconstruction objectives. The sample inefficiency of the former constitutes a major barrier for applying them to the real-world. The later, while they present low sample complexity, they learn latent spaces that need to reconstruct every single detail of the scene. In real environments, the task typically just represents a small fraction of the scene. Reconstruction objectives suffer in such scenarios as they capture all the unnecessary components. In this work, we present MIRO, an information theoretic representational learning algorithm for model-based reinforcement learning. We design a latent space that maximizes the mutual information with the future information while being able to capture all the information needed for planning. We show that our approach is more robust than reconstruction objectives in the presence of distractors and cluttered scenes},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/IJJAZF7L/Ding_et_al_2020_Mutual_Information_Maximization_for_Robust_Plannable_Representations.pdf;/Users/vitay/Documents/Zotero/storage/DBY3243Z/2005.html}
}

@article{Ding2022,
  title = {Dynamics Ensemble for Probabilistic Time-Series Forecasting via Deep Reinforcement Learning},
  author = {Ding, Yuhao and Park, Youngsuk and Gopalswamy, Karthick and Hasson, Hilaf and Wang, Bernie and Huan, Jun},
  date = {2022-09-29},
  url = {https://openreview.net/forum?id=a6NvoZ5DLoe},
  urldate = {2024-03-20},
  abstract = {Ensembles from given base learners are known to be indispensable in improving accuracy for most of the prediction tasks, leading to numerous methods. However, the only ensembling strategies that have been considered for time series forecasting in the past have been static methods, ones that have access to the predictions of the base learners but not to the base learners themselves. In this paper, we propose a novel \textbackslash textit\{dynamic ensemble policy\}, which, unlike static methods, uses the power of the ensemble to improve each of the base learners being ensembled by reducing the error accumulation of each base learner via consecutively feeding an ensembled sample to each base learner. To do so, we adopt a deep Reinforcement Learning (RL) framework with a Markov Decision Process (MDP) designed where the ensemble agent interacts with our environment (\textbackslash textit\{TS-GYM\}) from offline data. The output of our ensemble strategy is a single autoregressive forecaster that supports several desirable properties of uncertainty quantification and sample path, along with notable performance gain. The effectiveness of the proposed framework is demonstrated in multiple synthetic and real-world experiments.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/TLEADID2/Ding et al. - 2022 - DYNAMIC ENSEMBLE FOR PROBABILISTIC TIME- SERIES FORECASTING VIA DEEP REINFORCEMENT LEARNING.pdf}
}

@unpublished{Dornheim2018,
  title = {Model-{{Free Adaptive Optimal Control}} of {{Episodic Fixed-Horizon Manufacturing Processes}} Using {{Reinforcement Learning}}},
  author = {Dornheim, Johannes and Link, Norbert and Gumbsch, Peter},
  date = {2018-09-18},
  eprint = {1809.06646},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1809.06646},
  urldate = {2019-05-28},
  abstract = {A self-learning optimal control algorithm for episodic fixed-horizon manufacturing processes with time-discrete control actions is proposed and evaluated on a simulated deep drawing process. The control model is built during consecutive process executions under optimal control via reinforcement learning, using the measured product quality as reward after each process execution. Prior model formulation, which is required by state-of-the-art algorithms from model predictive control and approximate dynamic programming, is therefore obsolete. This avoids several difficulties namely in system identification, accurate modelling, and runtime complexity, that arise when dealing with processes subject to nonlinear dynamics and stochastic influences. Instead of using pre-created process and observation models, value function-based reinforcement learning algorithms build functions of expected future reward, which are used to derive optimal process control decisions. The expectation functions are learned online, by interacting with the process. The proposed algorithm takes stochastic variations of the process conditions into account and is able to cope with partial observability. A Q-learning-based method for adaptive optimal control of partially observable episodic fixed-horizon manufacturing processes is developed and studied. The resulting algorithm is instantiated and evaluated by applying it to a simulated stochastic optimal control problem in metal sheet deep drawing.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Systems and Control},
  file = {/Users/vitay/Documents/Zotero/storage/4CGTIL9M/Dornheim et al_2018_Model-Free Adaptive Optimal Control of Episodic Fixed-Horizon Manufacturing.pdf;/Users/vitay/Documents/Zotero/storage/FPS2F57D/1809.html}
}

@article{Dornheim2020,
  title = {Model-Free {{Adaptive Optimal Control}} of {{Episodic Fixed-horizon Manufacturing Processes Using Reinforcement Learning}}},
  author = {Dornheim, Johannes and Link, Norbert and Gumbsch, Peter},
  date = {2020-06-01},
  journaltitle = {International Journal of Control, Automation and Systems},
  shortjournal = {Int. J. Control Autom. Syst.},
  volume = {18},
  number = {6},
  pages = {1593--1604},
  issn = {2005-4092},
  doi = {10.1007/s12555-019-0120-7},
  url = {https://doi.org/10.1007/s12555-019-0120-7},
  urldate = {2021-05-17},
  abstract = {A self-learning optimal control algorithm for episodic fixed-horizon manufacturing processes with time-discrete control actions is proposed and evaluated on a simulated deep drawing process. The control model is built during consecutive process executions under optimal control via reinforcement learning, using the measured product quality as a reward after each process execution. Prior model formulation, which is required by algorithms from model predictive control and approximate dynamic programming, is therefore obsolete. This avoids several difficulties namely in system identification, accurate modeling, and runtime complexity, that arise when dealing with processes subject to nonlinear dynamics and stochastic influences. Instead of using the pre-created process and observation models, value-function-based reinforcement learning algorithms build functions of expected future reward, which are used to derive optimal process control decisions. The expectation functions are learned online, by interacting with the process. The proposed algorithm takes stochastic variations of the process conditions into account and is able to cope with partial observability. A Q-learning-based method for adaptive optimal control of partially observable episodic fixed-horizon manufacturing processes is developed and studied. The resulting algorithm is instantiated and evaluated by applying it to a simulated stochastic optimal control problem in metal sheet deep drawing.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/CAK3DSID/Dornheim_et_al_2020_Model-free_Adaptive_Optimal_Control_of_Episodic_Fixed-horizon_Manufacturing.pdf}
}

@article{Dosovitskiy2016,
  title = {Learning to {{Act}} by {{Predicting}} the {{Future}}},
  author = {Dosovitskiy, Alexey and Koltun, Vladlen},
  date = {2016-11},
  url = {http://arxiv.org/abs/1611.01779},
  abstract = {We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.},
  file = {/Users/vitay/Documents/Zotero/storage/FFGFLPZB/Dosovitskiy_Koltun_2016_Learning to Act by Predicting the Future.pdf}
}

@article{Duan2016,
  title = {Benchmarking {{Deep Reinforcement Learning}} for {{Continuous Control}}},
  author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  date = {2016-04},
  url = {http://arxiv.org/abs/1604.06778},
  abstract = {Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.},
  file = {/Users/vitay/Documents/Zotero/storage/D2XM9EU6/Duan et al_2016_Benchmarking Deep Reinforcement Learning for Continuous Control.pdf}
}

@unpublished{Duan2016a,
  title = {{{RL}}\$\textasciicircum 2\$: {{Fast Reinforcement Learning}} via {{Slow Reinforcement Learning}}},
  shorttitle = {{{RL}}\$\textasciicircum 2\$},
  author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
  date = {2016-11-09},
  eprint = {1611.02779},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.02779},
  urldate = {2021-02-05},
  abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL\$\textasciicircum 2\$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL\$\textasciicircum 2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL\$\textasciicircum 2\$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL\$\textasciicircum 2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/WLGIAAJR/Duan_et_al_2016_RL$^2$.pdf;/Users/vitay/Documents/Zotero/storage/789YVKQY/1611.html}
}

@unpublished{Dulac-Arnold2019,
  title = {Challenges of {{Real-World Reinforcement Learning}}},
  author = {Dulac-Arnold, Gabriel and Mankowitz, Daniel and Hester, Todd},
  date = {2019-04-29},
  eprint = {1904.12901},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1904.12901},
  urldate = {2020-11-11},
  abstract = {Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/X623R3XC/Dulac-Arnold_et_al_2019_Challenges_of_Real-World_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/756I6HJU/1904.html}
}

@report{Eckstein2019,
  type = {preprint},
  title = {Computational Evidence for Hierarchically-Structured Reinforcement Learning in Humans},
  author = {Eckstein, Maria K and Collins, Anne GE},
  date = {2019-08-10},
  institution = {Neuroscience},
  doi = {10.1101/731752},
  url = {http://biorxiv.org/lookup/doi/10.1101/731752},
  urldate = {2019-08-31},
  abstract = {Humans have the fascinating ability to achieve goals in a complex and constantly changing world, still surpassing modern machine learning algorithms in terms of flexibility and learning speed. It is generally accepted that a crucial factor for this ability is the use of abstract, hierarchical representations, which employ structure in the environment to guide learning and decision making. Nevertheless, it is poorly understood how we create and use these hierarchical representations. This study presents evidence that human behavior can be characterized as hierarchical reinforcement learning (RL). We designed an experiment to test specific predictions of hierarchical RL using a series of subtasks in the realm of context-based learning, and observed several behavioral markers of hierarchical RL, for instance asymmetric switch costs between changes in higher-level versus lower-level features, faster learning in higher-valued compared to lower-valued contexts, and preference for higher-valued compared to lower-valued contexts, which replicated across three samples. We simulated a classic flat and a hierarchical RL model and compared the behaviors of both to humans. As expected, only hierarchical RL reproduced all human behaviors, whereas flat RL was qualitatively unable to show some, and provided worse model fits for the rest. Importantly, individual hierarchical RL simulations showed human-like behaviors in all subtasks simultaneously, providing a unifying account for a diverse range of behaviors. This work shows that hierarchical RL, a biologically-inspired and computationally cheap algorithm, can reproduce human behavior in complex, hierarchical environments, and opens the avenue for future research in this field.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/C3MJEJAV/Eckstein_Collins_2019_Computational_evidence_for_hierarchically-structured_reinforcement_learning_in.pdf}
}

@unpublished{Edwards2020,
  title = {Estimating {{Q}}(s,s') with {{Deep Deterministic Dynamics Gradients}}},
  author = {Edwards, Ashley D. and Sahni, Himanshu and Liu, Rosanne and Hung, Jane and Jain, Ankit and Wang, Rui and Ecoffet, Adrien and Miconi, Thomas and Isbell, Charles and Yosinski, Jason},
  date = {2020-02-21},
  eprint = {2002.09505},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.09505},
  urldate = {2020-03-01},
  abstract = {In this paper, we introduce a novel form of value function, \$Q(s, s')\$, that expresses the utility of transitioning from a state \$s\$ to a neighboring state \$s'\$ and then acting optimally thereafter. In order to derive an optimal policy, we develop a forward dynamics model that learns to make next-state predictions that maximize this value. This formulation decouples actions from values while still learning off-policy. We highlight the benefits of this approach in terms of value function transfer, learning within redundant action spaces, and learning off-policy from state observations generated by sub-optimal or completely random policies. Code and videos are available at \textbackslash url\{sites.google.com/view/qss-paper\}.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/4CTHSM8B/Edwards_et_al_2020_Estimating_Q(s,s')_with_Deep_Deterministic_Dynamics_Gradients.pdf;/Users/vitay/Documents/Zotero/storage/7HHEKDC5/2002.html}
}

@book{Eibl2017,
  title = {Informatik 2017 : {{Bände I}} Bis {{III}}, {{Tagung}} Vom 25.- 29. {{September}} 2017 in {{Chemnitz}}},
  author = {Eibl, Maximilian and Gaedke, Martin and Gesellschaft für Informatik, Fred},
  date = {2017},
  publisher = {Gesellschaft für Informatik, Bonn},
  isbn = {978-3-88579-669-5},
  keywords = {actor critic,asynchronous learning,continuous action space,deep learning,policy gradient methods,reinforcement learning,robotics}
}

@online{Espeholt2018,
  title = {{{IMPALA}}: {{Scalable Distributed Deep-RL}} with {{Importance Weighted Actor-Learner Architectures}}},
  shorttitle = {{{IMPALA}}},
  author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  date = {2018-06-28},
  eprint = {1802.01561},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1802.01561},
  url = {http://arxiv.org/abs/1802.01561},
  urldate = {2022-09-28},
  abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/MHFVF447/Espeholt_et_al_2018_IMPALA.pdf;/Users/vitay/Documents/Zotero/storage/WRUEQ4LJ/1802.html}
}

@online{Eysenbach2018,
  title = {Diversity Is {{All You Need}}: {{Learning Skills}} without a {{Reward Function}}},
  shorttitle = {Diversity Is {{All You Need}}},
  author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  date = {2018-02-16},
  url = {https://arxiv.org/abs/1802.06070v6},
  urldate = {2023-06-19},
  abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
  langid = {english},
  organization = {arXiv.org},
  file = {/Users/vitay/Documents/Zotero/storage/HWDJR2M6/Eysenbach_et_al_2018_Diversity_is_All_You_Need.pdf}
}

@unpublished{Eysenbach2019,
  title = {If {{MaxEnt RL}} Is the {{Answer}}, {{What}} Is the {{Question}}?},
  author = {Eysenbach, Benjamin and Levine, Sergey},
  date = {2019-10-04},
  eprint = {1910.01913},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.01913},
  urldate = {2019-10-11},
  abstract = {Experimentally, it has been observed that humans and animals often make decisions that do not maximize their expected utility, but rather choose outcomes randomly, with probability proportional to expected utility. Probability matching, as this strategy is called, is equivalent to maximum entropy reinforcement learning (MaxEnt RL). However, MaxEnt RL does not optimize expected utility. In this paper, we formally show that MaxEnt RL does optimally solve certain classes of control problems with variability in the reward function. In particular, we show (1) that MaxEnt RL can be used to solve a certain class of POMDPs, and (2) that MaxEnt RL is equivalent to a two-player game where an adversary chooses the reward function. These results suggest a deeper connection between MaxEnt RL, robust control, and POMDPs, and provide insight for the types of problems for which we might expect MaxEnt RL to produce effective solutions. Specifically, our results suggest that domains with uncertainty in the task goal may be especially well-suited for MaxEnt RL methods.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/B7X9J624/Eysenbach_Levine_2019_If MaxEnt RL is the Answer, What is the Question.pdf;/Users/vitay/Documents/Zotero/storage/U3KMDFLC/1910.html}
}

@unpublished{Eysenbach2020,
  title = {Rewriting {{History}} with {{Inverse RL}}: {{Hindsight Inference}} for {{Policy Improvement}}},
  shorttitle = {Rewriting {{History}} with {{Inverse RL}}},
  author = {Eysenbach, Benjamin and Geng, Xinyang and Levine, Sergey and Salakhutdinov, Ruslan},
  date = {2020-02-25},
  eprint = {2002.11089},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.11089},
  urldate = {2020-03-01},
  abstract = {Multi-task reinforcement learning (RL) aims to simultaneously learn policies for solving many tasks. Several prior works have found that relabeling past experience with different reward functions can improve sample efficiency. Relabeling methods typically ask: if, in hindsight, we assume that our experience was optimal for some task, for what task was it optimal? In this paper, we show that hindsight relabeling is inverse RL, an observation that suggests that we can use inverse RL in tandem for RL algorithms to efficiently solve many tasks. We use this idea to generalize goal-relabeling techniques from prior work to arbitrary classes of tasks. Our experiments confirm that relabeling data using inverse RL accelerates learning in general multi-task settings, including goal-reaching, domains with discrete sets of rewards, and those with linear reward functions.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/C97UML3P/Eysenbach_et_al_2020_Rewriting_History_with_Inverse_RL.pdf;/Users/vitay/Documents/Zotero/storage/Z4K9JGC4/2002.html}
}

@unpublished{Eysenbach2020a,
  title = {Off-{{Dynamics Reinforcement Learning}}: {{Training}} for {{Transfer}} with {{Domain Classifiers}}},
  shorttitle = {Off-{{Dynamics Reinforcement Learning}}},
  author = {Eysenbach, Benjamin and Asawa, Swapnil and Chaudhari, Shreyas and Salakhutdinov, Ruslan and Levine, Sergey},
  date = {2020-06-24},
  eprint = {2006.13916},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.13916},
  urldate = {2020-06-30},
  abstract = {We propose a simple, practical, and intuitive approach for domain adaptation in reinforcement learning. Our approach stems from the idea that the agent's experience in the source domain should look similar to its experience in the target domain. Building off of a probabilistic view of RL, we formally show that we can achieve this goal by compensating for the difference in dynamics by modifying the reward function. This modified reward function is simple to estimate by learning auxiliary classifiers that distinguish source-domain transitions from target-domain transitions. Intuitively, the modified reward function penalizes the agent for visiting states and taking actions in the source domain which are not possible in the target domain. Said another way, the agent is penalized for transitions that would indicate that the agent is interacting with the source domain, rather than the target domain. Our approach is applicable to domains with continuous states and actions and does not require learning an explicit model of the dynamics. On discrete and continuous control tasks, we illustrate the mechanics of our approach and demonstrate its scalability to high-dimensional tasks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/GBBNYT2D/Eysenbach_et_al_2020_Off-Dynamics_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/MPIATIPD/2006.html}
}

@unpublished{Eysenbach2021,
  title = {C-{{Learning}}: {{Learning}} to {{Achieve Goals}} via {{Recursive Classification}}},
  shorttitle = {C-{{Learning}}},
  author = {Eysenbach, Benjamin and Salakhutdinov, Ruslan and Levine, Sergey},
  date = {2021-04-19},
  eprint = {2011.08909},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2011.08909},
  urldate = {2021-09-25},
  abstract = {We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/9VLVG6LQ/Eysenbach_et_al_2020_C-Learning.pdf;/Users/vitay/Documents/Zotero/storage/KQN28ZAP/Eysenbach_et_al_2021_C-Learning.pdf;/Users/vitay/Documents/Zotero/storage/HTIDKXRA/2011.html}
}

@online{Eysenbach2022,
  title = {Contrastive {{Learning}} as {{Goal-Conditioned Reinforcement Learning}}},
  author = {Eysenbach, Benjamin and Zhang, Tianjun and Salakhutdinov, Ruslan and Levine, Sergey},
  date = {2022-06-15},
  eprint = {2206.07568},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2206.07568},
  urldate = {2022-06-20},
  abstract = {In reinforcement learning (RL), it is easier to solve a task if given a good representation. While deep RL should automatically acquire such good representations, prior work often finds that learning representations in an end-to-end fashion is unstable and instead equip RL algorithms with additional representation learning parts (e.g., auxiliary losses, data augmentation). How can we design RL algorithms that directly acquire good representations? In this paper, instead of adding representation learning parts to an existing RL algorithm, we show (contrastive) representation learning methods can be cast as RL algorithms in their own right. To do this, we build upon prior work and apply contrastive representation learning to action-labeled trajectories, in such a way that the (inner product of) learned representations exactly corresponds to a goal-conditioned value function. We use this idea to reinterpret a prior RL method as performing contrastive learning, and then use the idea to propose a much simpler method that achieves similar performance. Across a range of goal-conditioned RL tasks, we demonstrate that contrastive RL methods achieve higher success rates than prior non-contrastive methods, including in the offline RL setting. We also show that contrastive RL outperforms prior methods on image-based tasks, without using data augmentation or auxiliary objectives.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/VVNIYD7M/Eysenbach_et_al_2022_Contrastive_Learning_as_Goal-Conditioned_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/NCBHF9MN/2206.html}
}

@inproceedings{Fedus2019,
  title = {Revisiting {{Fundamentals}} of {{Experience Replay}}},
  booktitle = {Proceedings of the 36 Th {{International Conference}} on {{Machine Learning}}},
  author = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
  date = {2019},
  pages = {17},
  abstract = {Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay - greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counter-intuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across three deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/PD7YVZ73/Fedus_et_al_2019_Revisiting_Fundamentals_of_Experience_Replay.pdf}
}

@unpublished{Feinberg2018,
  title = {Model-{{Based Value Estimation}} for {{Efficient Model-Free Reinforcement Learning}}},
  author = {Feinberg, Vladimir and Wan, Alvin and Stoica, Ion and Jordan, Michael I. and Gonzalez, Joseph E. and Levine, Sergey},
  date = {2018-02},
  eprint = {1803.00101},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1803.00101},
  abstract = {Recent model-free reinforcement learning algorithms have proposed incorporating learned dynamics models as a source of additional data with the intention of reducing sample complexity. Such methods hold the promise of incorporating imagined data coupled with a notion of model uncertainty to accelerate the learning of continuous control tasks. Unfortunately, they rely on heuristics that limit usage of the dynamics model. We present model-based value expansion, which controls for uncertainty in the model by only allowing imagination to fixed depth. By enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm, we improve value estimation, which, in turn, reduces the sample complexity of learning.},
  file = {/Users/vitay/Documents/Zotero/storage/IZEDVVPB/Feinberg et al_2018_Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning.pdf}
}

@unpublished{Ferret2019,
  title = {Self-{{Attentional Credit Assignment}} for {{Transfer}} in {{Reinforcement Learning}}},
  author = {Ferret, Johan and Marinier, Raphaël and Geist, Matthieu and Pietquin, Olivier},
  date = {2019-11-22},
  eprint = {1907.08027},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.24963/ijcai.2020/368},
  url = {http://arxiv.org/abs/1907.08027},
  urldate = {2021-01-13},
  abstract = {The ability to transfer knowledge to novel environments and tasks is a sensible desiderata for general learning agents. Despite the apparent promises, transfer in RL is still an open and little exploited research area. In this paper, we take a brand-new perspective about transfer: we suggest that the ability to assign credit unveils structural invariants in the tasks that can be transferred to make RL more sample-efficient. Our main contribution is SECRET, a novel approach to transfer learning for RL that uses a backward-view credit assignment mechanism based on a self-attentive architecture. Two aspects are key to its generality: it learns to assign credit as a separate offline supervised process and exclusively modifies the reward function. Consequently, it can be supplemented by transfer methods that do not modify the reward function and it can be plugged on top of any RL algorithm.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/S3FIGPCQ/Ferret_et_al_2019_Self-Attentional_Credit_Assignment_for_Transfer_in_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/WPH6CBNX/1907.html}
}

@unpublished{Finn2016,
  title = {Unsupervised {{Learning}} for {{Physical Interaction}} through {{Video Prediction}}},
  author = {Finn, Chelsea and Goodfellow, Ian and Levine, Sergey},
  date = {2016-10-17},
  eprint = {1605.07157},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1605.07157},
  urldate = {2020-01-04},
  abstract = {A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a "visual imagination" of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/JCGB5K89/Finn_et_al_2016_Unsupervised_Learning_for_Physical_Interaction_through_Video_Prediction.pdf;/Users/vitay/Documents/Zotero/storage/RSXTQWCK/1605.html}
}

@unpublished{Finn2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  date = {2017-07-18},
  eprint = {1703.03400},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.03400},
  urldate = {2021-02-06},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/vitay/Documents/Zotero/storage/SG7L5PGS/Finn_et_al_2017_Model-Agnostic_Meta-Learning_for_Fast_Adaptation_of_Deep_Networks.pdf;/Users/vitay/Documents/Zotero/storage/Q5ATMZA7/1703.html}
}

@unpublished{Flennerhag2021,
  title = {Bootstrapped {{Meta-Learning}}},
  author = {Flennerhag, Sebastian and Schroecker, Yannick and Zahavy, Tom and family=Hasselt, given=Hado, prefix=van, useprefix=true and Silver, David and Singh, Satinder},
  date = {2021-09-09},
  eprint = {2109.04504},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2109.04504},
  urldate = {2021-09-17},
  abstract = {Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem that often exhibits ill-conditioning, and myopic meta-objectives. We propose an algorithm that tackles these issues by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that the improvement is related to the target distance. Thus, by controlling curvature, the distance measure can be used to ease meta-optimization, for instance by reducing ill-conditioning. Further, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. The algorithm is versatile and easy to implement. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark, improve upon MAML in few-shot learning, and demonstrate how our approach opens up new possibilities by meta-learning efficient exploration in a Q-learning agent.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/64ZRDNF8/Flennerhag_et_al_2021_Bootstrapped_Meta-Learning.pdf;/Users/vitay/Documents/Zotero/storage/V2J5T6PC/2109.html}
}

@unpublished{Flet-Berliac2020,
  title = {Is {{Standard Deviation}} the {{New Standard}}? {{Revisiting}} the {{Critic}} in {{Deep Policy Gradients}}},
  shorttitle = {Is {{Standard Deviation}} the {{New Standard}}?},
  author = {Flet-Berliac, Yannis and Ouhamma, Reda and Maillard, Odalric-Ambrym and Preux, Philippe},
  date = {2020-10-09},
  eprint = {2010.04440},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2010.04440},
  urldate = {2020-10-18},
  abstract = {Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the relative value of the states (resp. state-action pairs) rather than their absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/H855EQDX/Flet-Berliac_et_al_2020_Is_Standard_Deviation_the_New_Standard.pdf;/Users/vitay/Documents/Zotero/storage/7KPC7QIN/2010.html}
}

@unpublished{Fortunato2017,
  title = {Noisy {{Networks}} for {{Exploration}}},
  author = {Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and Blundell, Charles and Legg, Shane},
  date = {2017-06-30},
  eprint = {1706.10295},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.10295},
  urldate = {2020-03-02},
  abstract = {We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and \$\textbackslash epsilon\$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/8XE3SQXF/Fortunato_et_al_2019_Noisy_Networks_for_Exploration.pdf;/Users/vitay/Documents/Zotero/storage/8KHB8E78/1706.html}
}

@unpublished{Fox2019,
  title = {Hierarchical {{Variational Imitation Learning}} of {{Control Programs}}},
  author = {Fox, Roy and Shin, Richard and Paul, William and Zou, Yitian and Song, Dawn and Goldberg, Ken and Abbeel, Pieter and Stoica, Ion},
  date = {2019-12-29},
  eprint = {1912.12612},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.12612},
  urldate = {2020-01-04},
  abstract = {Autonomous agents can learn by imitating teacher demonstrations of the intended behavior. Hierarchical control policies are ubiquitously useful for such learning, having the potential to break down structured tasks into simpler sub-tasks, thereby improving data efficiency and generalization. In this paper, we propose a variational inference method for imitation learning of a control policy represented by parametrized hierarchical procedures (PHP), a program-like structure in which procedures can invoke sub-procedures to perform sub-tasks. Our method discovers the hierarchical structure in a dataset of observation-action traces of teacher demonstrations, by learning an approximate posterior distribution over the latent sequence of procedure calls and terminations. Samples from this learned distribution then guide the training of the hierarchical control policy. We identify and demonstrate a novel benefit of variational inference in the context of hierarchical imitation learning: in decomposing the policy into simpler procedures, inference can leverage acausal information that is unused by other methods. Training PHP with variational inference outperforms LSTM baselines in terms of data efficiency and generalization, requiring less than half as much data to achieve a 24\% error rate in executing the bubble sort algorithm, and to achieve no error in executing Karel programs.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/GV8Q6TAQ/Fox_et_al_2019_Hierarchical_Variational_Imitation_Learning_of_Control_Programs.pdf;/Users/vitay/Documents/Zotero/storage/NNI8KSRW/1912.html}
}

@article{Francois-Lavet2018,
  title = {An {{Introduction}} to {{Deep Reinforcement Learning}}},
  author = {Francois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
  date = {2018},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {FNT in Machine Learning},
  volume = {11},
  number = {3-4},
  eprint = {1811.12560},
  eprinttype = {arXiv},
  pages = {219--354},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000071},
  url = {http://arxiv.org/abs/1811.12560},
  urldate = {2020-06-14},
  abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/5Y8H22CU/Francois-Lavet_et_al_2018_An_Introduction_to_Deep_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/Y7KUD9H4/1811.html}
}

@unpublished{Frans2017,
  title = {Meta {{Learning Shared Hierarchies}}},
  author = {Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
  date = {2017-10-26},
  eprint = {1710.09767},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1710.09767},
  urldate = {2020-01-31},
  abstract = {We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/IX28QAY9/Frans_et_al_2017_Meta_Learning_Shared_Hierarchies.pdf;/Users/vitay/Documents/Zotero/storage/IPZ5TDKD/1710.html}
}

@online{Freire2021,
  title = {Sequential {{Episodic Control}}},
  author = {Freire, Ismael T. and Amil, Adrián F. and Verschure, Paul F. M. J.},
  date = {2021-12-29},
  eprint = {2112.14734},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio},
  doi = {10.48550/arXiv.2112.14734},
  url = {http://arxiv.org/abs/2112.14734},
  urldate = {2023-02-06},
  abstract = {State of the art deep reinforcement learning algorithms are sample inefficient due to the large number of episodes they require to achieve asymptotic performance. Episodic Reinforcement Learning (ERL) algorithms, inspired by the mammalian hippocampus, typically use extended memory systems to bootstrap learning from past events to overcome this sample-inefficiency problem. However, such memory augmentations are often used as mere buffers, from which isolated past experiences are drawn to learn from in an offline fashion (e.g., replay). Here, we demonstrate that including a bias in the acquired memory content derived from the order of episodic sampling improves both the sample and memory efficiency of an episodic control algorithm. We test our Sequential Episodic Control (SEC) model in a foraging task to show that storing and using integrated episodes as event sequences leads to faster learning with fewer memory requirements as opposed to a standard ERL benchmark, Model-Free Episodic Control, that buffers isolated events only. We also study the effect of memory constraints and forgetting on the sequential and non-sequential version of the SEC algorithm. Furthermore, we discuss how a hippocampal-like fast memory system could bootstrap slow cortical and subcortical learning subserving habit formation in the mammalian brain.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/TBV2TRET/Freire_et_al_2021_Sequential_Episodic_Control.pdf}
}

@unpublished{Fu2020,
  title = {Datasets for {{Data-Driven Reinforcement Learning}}},
  author = {Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  date = {2020-04-15},
  eprint = {2004.07219},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2004.07219},
  urldate = {2020-04-19},
  abstract = {The offline reinforcement learning (RL) problem, also referred to as batch RL, refers to the setting where a policy must be learned from a dataset of previously collected data, without additional online data collection. In supervised learning, large datasets and complex deep neural networks have fueled impressive progress, but in contrast, conventional RL algorithms must collect large amounts of on-policy data and have had little success leveraging previously collected datasets. As a result, existing RL benchmarks are not well-suited for the offline setting, making progress in this area difficult to measure. To design a benchmark tailored to offline RL, we start by outlining key properties of datasets relevant to applications of offline RL. Based on these properties, we design a set of benchmark tasks and datasets that evaluate offline RL algorithms under these conditions. Examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multi-objective datasets, where an agent can perform different tasks in the same environment, and datasets consisting of a heterogeneous mix of high-quality and low-quality trajectories. By designing the benchmark tasks and datasets to reflect properties of real-world offline RL problems, our benchmark will focus research effort on methods that drive substantial improvements not just on simulated benchmarks, but ultimately on the kinds of real-world problems where offline RL will have the largest impact.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/3P87QDRG/Fu_et_al_2020_Datasets_for_Data-Driven_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/9FPYLGBN/2004.html}
}

@article{Fu2022,
  title = {{{MAML2}}: Meta Reinforcement Learning via Meta-Learning for Task Categories},
  shorttitle = {{{MAML2}}},
  author = {Fu, Qiming and Wang, Zhechao and Fang, Nengwei and Xing, Bin and Zhang, Xiao and Chen, Jianping},
  date = {2022-12-12},
  journaltitle = {Frontiers of Computer Science},
  shortjournal = {Front. Comput. Sci.},
  volume = {17},
  number = {4},
  pages = {174325},
  issn = {2095-2236},
  doi = {10.1007/s11704-022-2037-1},
  url = {https://doi.org/10.1007/s11704-022-2037-1},
  urldate = {2023-01-04},
  abstract = {Meta-learning has been widely applied to solving few-shot reinforcement learning problems, where we hope to obtain an agent that can learn quickly in a new task. However, these algorithms often ignore some isolated tasks in pursuit of the average performance, which may result in negative adaptation in these isolated tasks, and they usually need sufficient learning in a stationary task distribution. In this paper, our algorithm presents a hierarchical framework of double meta-learning, and the whole framework includes classification, meta-learning, and re-adaptation. Firstly, in the classification process, we classify tasks into several task subsets, considered as some categories of tasks, by learned parameters of each task, which can separate out some isolated tasks thereafter. Secondly, in the meta-learning process, we learn category parameters in all subsets via meta-learning. Simultaneously, based on the gradient of each category parameter in each subset, we use meta-learning again to learn a new meta-parameter related to the whole task set, which can be used as an initial parameter for the new task. Finally, in the re-adaption process, we adapt the parameter of the new task with two steps, by the meta-parameter and the appropriate category parameter successively. Experimentally, we demonstrate our algorithm prevents the agent from negative adaptation without losing the average performance for the whole task set. Additionally, our algorithm presents a more rapid adaptation process within re-adaptation. Moreover, we show the good performance of our algorithm with fewer samples as the agent is exposed to an online meta-learning setting.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/EN7ZS9YR/Fu_et_al_2022_MAML2.pdf}
}

@article{Fu2022a,
  title = {Reinforcement {{Learning Based Dynamic Model Combination}} for {{Time Series Forecasting}}},
  author = {Fu, Yuwei and Wu, Di and Boulet, Benoit},
  date = {2022-06-28},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {6},
  pages = {6639--6647},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i6.20618},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/20618},
  urldate = {2024-03-20},
  abstract = {Time series data appears in many real-world fields such as energy, transportation, communication systems. Accurate modelling and forecasting of time series data can be of significant importance to improve the efficiency of these systems. Extensive research efforts have been taken for time series problems. Different types of approaches, including both statistical-based methods and machine learning-based methods, have been investigated. Among these methods, ensemble learning has shown to be effective and robust. However, it is still an open question that how we should determine weights for base models in the ensemble. Sub-optimal weights may prevent the final model from reaching its full potential. To deal with this challenge, we propose a reinforcement learning (RL) based model combination (RLMC) framework for determining model weights in an ensemble for time series forecasting tasks. By formulating model selection as a sequential decision-making problem, RLMC learns a deterministic policy to output dynamic model weights for non-stationary time series data. RLMC further leverages deep learning to learn hidden features from raw time series data to adapt fast to the changing data distribution. Extensive experiments on multiple real-world datasets have been implemented to showcase the effectiveness of the proposed method.},
  issue = {6},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/G4RPIFBQ/Fu et al. - 2022 - Reinforcement Learning Based Dynamic Model Combination for Time Series Forecasting.pdf}
}

@unpublished{Fujimoto2018,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  author = {Fujimoto, Scott and family=Hoof, given=Herke, prefix=van, useprefix=true and Meger, David},
  date = {2018-10-22},
  eprint = {1802.09477},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.09477},
  urldate = {2020-03-01},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/JIDYFBGC/Fujimoto_et_al_2018_Addressing_Function_Approximation_Error_in_Actor-Critic_Methods.pdf;/Users/vitay/Documents/Zotero/storage/ZU8MAL6P/1802.html}
}

@unpublished{Fujimoto2018a,
  title = {Off-{{Policy Deep Reinforcement Learning}} without {{Exploration}}},
  author = {Fujimoto, Scott and Meger, David and Precup, Doina},
  date = {2018-12-06},
  eprint = {1812.02900},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.02900},
  urldate = {2019-08-09},
  abstract = {Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/C62RDXZX/Fujimoto et al_2018_Off-Policy Deep Reinforcement Learning without Exploration.pdf;/Users/vitay/Documents/Zotero/storage/M8U4VGEL/1812.html}
}

@inproceedings{Fujimoto2019,
  title = {Off-{{Policy Deep Reinforcement Learning}} without {{Exploration}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Fujimoto, Scott and Meger, David and Precup, Doina},
  date = {2019-05-24},
  pages = {2052--2062},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/fujimoto19a.html},
  urldate = {2021-10-01},
  abstract = {Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/J7QZYVX8/Fujimoto_et_al_2019_Off-Policy_Deep_Reinforcement_Learning_without_Exploration.pdf}
}

@article{Fujita2018,
  title = {Clipped {{Action Policy Gradient}}},
  author = {Fujita, Yasuhiro and Maeda, Shin-ichi},
  date = {2018-02},
  url = {http://arxiv.org/abs/1802.07564},
  abstract = {Many continuous control tasks have bounded action spaces. When policy gradient methods are applied to such tasks, out-of-bound actions need to be clipped before execution, while policies are usually optimized as if the actions are not clipped. We propose a policy gradient estimator that exploits the knowledge of actions being clipped to reduce the variance in estimation. We prove that our estimator, named clipped action policy gradient (CAPG), is unbiased and achieves lower variance than the conventional estimator that ignores action bounds. Experimental results demonstrate that CAPG generally outperforms the conventional estimator, indicating that it is a better policy gradient estimator for continuous control tasks. The source code is available at https://github.com/pfnet-research/capg.},
  file = {/Users/vitay/Documents/Zotero/storage/BJ6U4FH7/Fujita_Maeda_2018_Clipped Action Policy Gradient.pdf}
}

@article{Garcia2015,
  title = {A Comprehensive Survey on Safe Reinforcement Learning},
  author = {García, Javier and Fernández, Fernando},
  date = {2015-01-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {16},
  number = {1},
  pages = {1437--1480},
  issn = {1532-4435},
  abstract = {Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted finite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.},
  file = {/Users/vitay/Documents/Zotero/storage/L52KIIKV/García_Fernández_2015_A_comprehensive_survey_on_safe_reinforcement_learning.pdf}
}

@inproceedings{Gehring2015,
  title = {Approximate {{Linear Successor Representation}}},
  author = {Gehring, Clement A},
  date = {2015},
  pages = {5},
  url = {http://people.csail.mit.edu/gehring/publications/clement-gehring-rldm-2015.pdf},
  abstract = {The dependency of the value function on the dynamics and a fixed rewards function makes the reuse of information difficult when domains share dynamics but differ in their reward functions. This issue is present when attempting to reuse options to solve different tasks in the same domain. If instead of a value function, a successor representation is learned for some fixed dynamics, then any value function defined on any linear reward function can be computed efficiently. This setting can be particularly useful for reusing options in some hierarchical planning settings. Unfortunately, even linear parametrization of successor representation require a quadratic number of parameters with respect to the number of features and as many operations per temporal difference update step. We present a simple temporal difference-like algorithm for learning an approximate version of the successor representation with an amortized runtime O(nk) for n features and the maximum rank-k of the approximation. Preliminary results indicate that the rank parameter can be much smaller than the number of features.},
  eventtitle = {The Multi-Disciplinary Conference on {{Reinforcement Learning}} and {{Decision Making}} ({{RLDM}})},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/4JE9QYDB/Gehring_2015_Approximate_Linear_Successor_Representation.pdf}
}

@article{Gershman2012,
  title = {The {{Successor Representation}} and {{Temporal Context}}},
  author = {Gershman, Samuel J. and Moore, Christopher D. and Todd, Michael T. and Norman, Kenneth A. and Sederberg, Per B.},
  date = {2012-02-24},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {24},
  number = {6},
  pages = {1553--1568},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00282},
  url = {https://doi.org/10.1162/NECO_a_00282},
  urldate = {2019-02-26},
  abstract = {The successor representation was introduced into reinforcement learning by Dayan (1993) as a means of facilitating generalization between states with similar successors. Although reinforcement learning in general has been used extensively as a model of psychological and neural processes, the psychological validity of the successor representation has yet to be explored. An interesting possibility is that the successor representation can be used not only for reinforcement learning but for episodic learning as well. Our main contribution is to show that a variant of the temporal context model (TCM; Howard \& Kahana, 2002), an influential model of episodic memory, can be understood as directly estimating the successor representation using the temporal difference learning algorithm (Sutton \& Barto, 1998). This insight leads to a generalization of TCM and new experimental predictions. In addition to casting a new normative light on TCM, this equivalence suggests a previously unexplored point of contact between different learning systems.},
  file = {/Users/vitay/Documents/Zotero/storage/BDKR2F5G/Gershman et al_2012_The Successor Representation and Temporal Context.pdf;/Users/vitay/Documents/Zotero/storage/4XMZPQ53/NECO_a_00282.html}
}

@article{Gershman2018,
  title = {The {{Successor Representation}}: {{Its Computational Logic}} and {{Neural Substrates}}.},
  author = {Gershman, Samuel J},
  date = {2018-08},
  journaltitle = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
  volume = {38},
  number = {33},
  eprint = {30006364},
  eprinttype = {pubmed},
  pages = {7193--7200},
  doi = {10.1523/JNEUROSCI.0151-18.2018},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/30006364},
  abstract = {Reinforcement learning is the process by which an agent learns to predict long-term future reward. We now understand a great deal about the brain's reinforcement learning algorithms, but we know considerably less about the representations of states and actions over which these algorithms operate. A useful starting point is asking what kinds of representations we would want the brain to have, given the constraints on its computational architecture. Following this logic leads to the idea of the successor representation, which encodes states of the environment in terms of their predictive relationships with other states. Recent behavioral and neural studies have provided evidence for the successor representation, and computational studies have explored ways to extend the original idea. This paper reviews progress on these fronts, organizing them within a broader framework for understanding how the brain negotiates tradeoffs between efficiency and flexibility for reinforcement learning.},
  keywords = {cognitive map,dopamine,hippocampus,reinforcement learning,reward},
  file = {/Users/vitay/Documents/Zotero/storage/MGXMFEMF/Gershman_2018_The Successor Representation.pdf}
}

@article{Gershman2020a,
  title = {The Neurobiology of Deep Reinforcement Learning},
  author = {Gershman, Samuel J. and Ölveczky, Bence P.},
  date = {2020-06},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {30},
  number = {11},
  pages = {R629-R632},
  issn = {09609822},
  doi = {10.1016/j.cub.2020.04.021},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982220305017},
  urldate = {2020-06-13},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/C8M8CXFW/Gershman and Ölveczky - 2020 - The neurobiology of deep reinforcement learning.pdf}
}

@unpublished{Ghasemipour2021,
  title = {{{EMaQ}}: {{Expected-Max Q-Learning Operator}} for {{Simple Yet Effective Offline}} and {{Online RL}}},
  shorttitle = {{{EMaQ}}},
  author = {Ghasemipour, Seyed Kamyar Seyed and Schuurmans, Dale and Gu, Shixiang Shane},
  date = {2021-01-13},
  eprint = {2007.11091},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.11091},
  urldate = {2021-10-01},
  abstract = {Off-policy reinforcement learning holds the promise of sample-efficient learning of decision-making policies by leveraging past experience. However, in the offline RL setting -- where a fixed collection of interactions are provided and no further interactions are allowed -- it has been shown that standard off-policy RL methods can significantly underperform. Recently proposed methods often aim to address this shortcoming by constraining learned policies to remain close to the given dataset of interactions. In this work, we closely investigate an important simplification of BCQ -- a prior approach for offline RL -- which removes a heuristic design choice and naturally restricts extracted policies to remain exactly within the support of a given behavior policy. Importantly, in contrast to their original theoretical considerations, we derive this simplified algorithm through the introduction of a novel backup operator, Expected-Max Q-Learning (EMaQ), which is more closely related to the resulting practical algorithm. Specifically, in addition to the distribution support, EMaQ explicitly considers the number of samples and the proposal distribution, allowing us to derive new sub-optimality bounds which can serve as a novel measure of complexity for offline RL problems. In the offline RL setting -- the main focus of this work -- EMaQ matches and outperforms prior state-of-the-art in the D4RL benchmarks. In the online RL setting, we demonstrate that EMaQ is competitive with Soft Actor Critic. The key contributions of our empirical findings are demonstrating the importance of careful generative model design for estimating behavior policies, and an intuitive notion of complexity for offline RL problems. With its simple interpretation and fewer moving parts, such as no explicit function approximator representing the policy, EMaQ serves as a strong yet easy to implement baseline for future work.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/H8BT2KRU/Ghasemipour_et_al_2021_EMaQ.pdf;/Users/vitay/Documents/Zotero/storage/SPG3YA4T/2007.html}
}

@unpublished{Ghosh2019,
  title = {Learning {{To Reach Goals Without Reinforcement Learning}}},
  author = {Ghosh, Dibya and Gupta, Abhishek and Fu, Justin and Reddy, Ashwin and Devin, Coline and Eysenbach, Benjamin and Levine, Sergey},
  date = {2019-12-12},
  eprint = {1912.06088},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.06088},
  urldate = {2019-12-17},
  abstract = {Imitation learning algorithms provide a simple and straightforward approach for training control policies via supervised learning. By maximizing the likelihood of good actions provided by an expert demonstrator, supervised imitation learning can produce effective policies without the algorithmic complexities and optimization challenges of reinforcement learning, at the cost of requiring an expert demonstrator to provide the demonstrations. In this paper, we ask: can we take insights from imitation learning to design algorithms that can effectively acquire optimal policies from scratch without any expert demonstrations? The key observation that makes this possible is that, in the multi-task setting, trajectories that are generated by a suboptimal policy can still serve as optimal examples for other tasks. In particular, when tasks correspond to different goals, every trajectory is a successful demonstration for the goal state that it actually reaches. We propose a simple algorithm for learning goal-reaching behaviors without any demonstrations, complicated user-provided reward functions, or complex reinforcement learning methods. Our method simply maximizes the likelihood of actions the agent actually took in its own previous rollouts, conditioned on the goal being the state that it actually reached. Although related variants of this approach have been proposed previously in imitation learning with demonstrations, we show how this approach can effectively learn goal-reaching policies from scratch. We present a theoretical result linking self-supervised imitation learning and reinforcement learning, and empirical results showing that it performs competitively with more complex reinforcement learning methods on a range of challenging goal reaching problems, while yielding advantages in terms of stability and use of offline data.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/UXPFSRUG/Ghosh_et_al_2019_Learning_To_Reach_Goals_Without_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/ZE39KG7A/1912.html}
}

@online{Ghugare2022,
  title = {Simplifying {{Model-based RL}}: {{Learning Representations}}, {{Latent-space Models}}, and {{Policies}} with {{One Objective}}},
  shorttitle = {Simplifying {{Model-based RL}}},
  author = {Ghugare, Raj and Bharadhwaj, Homanga and Eysenbach, Benjamin and Levine, Sergey and Salakhutdinov, Ruslan},
  date = {2022-09-17},
  eprint = {2209.08466},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2209.08466},
  urldate = {2022-09-24},
  abstract = {While reinforcement learning (RL) methods that learn an internal model of the environment have the potential to be more sample efficient than their model-free counterparts, learning to model raw observations from high dimensional sensors can be challenging. Prior work has addressed this challenge by learning low-dimensional representation of observations through auxiliary objectives, such as reconstruction or value prediction. However, the alignment between these auxiliary objectives and the RL objective is often unclear. In this work, we propose a single objective which jointly optimizes a latent-space model and policy to achieve high returns while remaining self-consistent. This objective is a lower bound on expected returns. Unlike prior bounds for model-based RL on policy exploration or model guarantees, our bound is directly on the overall RL objective. We demonstrate that the resulting algorithm matches or improves the sample-efficiency of the best prior model-based and model-free RL methods. While such sample efficient methods typically are computationally demanding, our method attains the performance of SAC in about 50\textbackslash\% less wall-clock time.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/KVZZGULF/Ghugare_et_al_2022_Simplifying_Model-based_RL.pdf;/Users/vitay/Documents/Zotero/storage/23RVGSDY/2209.html}
}

@unpublished{Gleave2021,
  title = {Adversarial {{Policies}}: {{Attacking Deep Reinforcement Learning}}},
  shorttitle = {Adversarial {{Policies}}},
  author = {Gleave, Adam and Dennis, Michael and Wild, Cody and Kant, Neel and Levine, Sergey and Russell, Stuart},
  date = {2021-01-17},
  eprint = {1905.10615},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1905.10615},
  urldate = {2022-05-24},
  abstract = {Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.},
  file = {/Users/vitay/Documents/Zotero/storage/MLBIUQAT/Gleave_et_al_2021_Adversarial_Policies.pdf;/Users/vitay/Documents/Zotero/storage/TFGLN4JZ/1905.html}
}

@book{Goodfellow2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {MIT Press},
  url = {http://www.deeplearningbook.org}
}

@article{Goyal2018,
  title = {Recall {{Traces}}: {{Backtracking Models}} for {{Efficient Reinforcement Learning}}},
  author = {Goyal, Anirudh and Brakel, Philemon and Fedus, William and Lillicrap, Timothy and Levine, Sergey and Larochelle, Hugo and Bengio, Yoshua},
  date = {2018-04},
  url = {http://arxiv.org/abs/1804.00379},
  abstract = {In many environments only a tiny subset of all states yield high reward. In these cases, few of the interactions with the environment provide a relevant learning signal. Hence, we may want to preferentially train on those high-reward states and the probable trajectories leading to them. To this end, we advocate for the use of a backtracking model that predicts the preceding states that terminate at a given high-reward state. We can train a model which, starting from a high value state (or one that is estimated to have high value), predicts and sample for which the (state, action)-tuples may have led to that high value state. These traces of (state, action) pairs, which we refer to as Recall Traces, sampled from this backtracking model starting from a high value state, are informative as they terminate in good states, and hence we can use these traces to improve a policy. We provide a variational interpretation for this idea and a practical algorithm in which the backtracking model samples from an approximate posterior distribution over trajectories which lead to large rewards. Our method improves the sample efficiency of both on- and off-policy RL algorithms across several environments and tasks.},
  file = {/Users/vitay/Documents/Zotero/storage/4BDXE3J3/Goyal et al_2018_Recall Traces.pdf}
}

@unpublished{Goyal2019,
  title = {Reinforcement {{Learning}} with {{Competitive Ensembles}} of {{Information-Constrained Primitives}}},
  author = {Goyal, Anirudh and Sodhani, Shagun and Binas, Jonathan and Peng, Xue Bin and Levine, Sergey and Bengio, Yoshua},
  date = {2019-06-25},
  eprint = {1906.10667},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.10667},
  urldate = {2019-07-01},
  abstract = {Reinforcement learning agents that operate in diverse and complex environments can benefit from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a policy into lower-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. However, the meta-policy must still produce appropriate decisions in all states. In this work, we propose a policy design that decomposes into primitives, similarly to hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide for themselves whether they wish to act in the current state. We use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision and the primitive that requests the most information about the current state acts in the world. The primitives are regularized to use as little information as possible, which leads to natural competition and specialization. We experimentally demonstrate that this policy architecture improves over both flat and hierarchical policies in terms of generalization.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/JV6RHLMQ/Goyal et al_2019_Reinforcement Learning with Competitive Ensembles of Information-Constrained.pdf;/Users/vitay/Documents/Zotero/storage/R7E6VBC4/1906.html}
}

@article{Grondman2012,
  title = {A {{Survey}} of {{Actor-Critic Reinforcement Learning}}: {{Standard}} and {{Natural Policy Gradients}}},
  shorttitle = {A {{Survey}} of {{Actor-Critic Reinforcement Learning}}},
  author = {Grondman, I. and Busoniu, L. and Lopes, G. A. D. and Babuska, R.},
  date = {2012-11},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {42},
  number = {6},
  pages = {1291--1307},
  issn = {1094-6977},
  doi = {10.1109/TSMCC.2012.2218595},
  abstract = {Policy-gradient-based actor-critic algorithms are amongst the most popular algorithms in the reinforcement learning framework. Their advantage of being able to search for optimal policies using low-variance gradient estimates has made them useful in several real-life applications, such as robotics, power control, and finance. Although general surveys on reinforcement learning techniques already exist, no survey is specifically dedicated to actor-critic algorithms in particular. This paper, therefore, describes the state of the art of actor-critic algorithms, with a focus on methods that can work in an online setting and use function approximation in order to deal with continuous state and action spaces. After starting with a discussion on the concepts of reinforcement learning and the origins of actor-critic algorithms, this paper describes the workings of the natural gradient, which has made its way into many actor-critic algorithms over the past few years. A review of several standard and natural actor-critic algorithms is given, and the paper concludes with an overview of application areas and a discussion on open issues.},
  keywords = {Actor-critic,actor-critic reinforcement learning,Approximation algorithms,Approximation methods,Convergence,Equations,function approximation,gradient methods,learning (artificial intelligence),low-variance gradient estimation,natural gradient,natural policy gradients,optimal policies,Optimization,policy gradient,policy-gradient-based actor-critic algorithms,real-life applications,reinforcement learning (RL),RL,standard policy gradients,Standards},
  file = {/Users/vitay/Documents/Zotero/storage/E5SE29AR/Grondman et al_2012_A Survey of Actor-Critic Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/QJ88PU2L/6392457.html}
}

@unpublished{Gruslys2017,
  title = {The {{Reactor}}: {{A}} Fast and Sample-Efficient {{Actor-Critic}} Agent for {{Reinforcement Learning}}},
  author = {Gruslys, Audrunas and Dabney, Will and Azar, Mohammad Gheshlaghi and Piot, Bilal and Bellemare, Marc and Munos, Remi},
  date = {2017-04},
  eprint = {1704.04651},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1704.04651},
  abstract = {In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the \textbackslash b\{eta\}-leave-one-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.},
  file = {/Users/vitay/Documents/Zotero/storage/E3MFFSEU/Gruslys et al_2017_The Reactor.pdf}
}

@unpublished{Gu2016,
  title = {Continuous {{Deep Q-Learning}} with {{Model-based Acceleration}}},
  author = {Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
  date = {2016-03},
  eprint = {1603.00748},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1603.00748},
  abstract = {Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.},
  file = {/Users/vitay/Documents/Zotero/storage/MHC25F3Y/Gu et al_2016_Continuous Deep Q-Learning with Model-based Acceleration.pdf}
}

@article{Gu2016a,
  title = {Q-{{Prop}}: {{Sample-Efficient Policy Gradient}} with {{An Off-Policy Critic}}},
  author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
  date = {2016-11},
  url = {http://arxiv.org/abs/1611.02247},
  abstract = {Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.},
  file = {/Users/vitay/Documents/Zotero/storage/4RGM7AJ5/Gu et al_2016_Q-Prop.pdf}
}

@inproceedings{Gu2017,
  title = {Deep {{Reinforcement Learning}} for {{Robotic Manipulation}} with {{Asynchronous Off-Policy Updates}}},
  booktitle = {Proc. {{ICRA}}},
  author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  date = {2017},
  url = {http://arxiv.org/abs/1610.00633},
  abstract = {Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.},
  file = {/Users/vitay/Documents/Zotero/storage/5U8F879U/Gu_et_al_2017_Deep_Reinforcement_Learning_for_Robotic_Manipulation_with_Asynchronous.pdf}
}

@unpublished{Gupta2017,
  title = {Learning {{Invariant Feature Spaces}} to {{Transfer Skills}} with {{Reinforcement Learning}}},
  author = {Gupta, Abhishek and Devin, Coline and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
  date = {2017-03-08},
  eprint = {1703.02949},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.02949},
  urldate = {2019-03-03},
  abstract = {People can learn a wide range of tasks from their own experience, but can also learn from observing other creatures. This can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology. In this paper, we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents (e.g., different robots). We introduce a problem formulation where two agents are tasked with learning multiple skills by sharing information. Our method uses the skills that were learned by both agents to train invariant feature spaces that can then be used to transfer other skills from one agent to another. The process of learning these invariant feature spaces can be viewed as a kind of “analogy making,” or implicit learning of partial correspondences between two distinct domains. We evaluate our transfer learning algorithm in two simulated robotic manipulation skills, and illustrate that we can transfer knowledge between simulated robotic arms with different numbers of links, as well as simulated arms with different actuation mechanisms, where one robot is torque-driven while the other is tendon-driven.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/TBQD9HXW/Gupta et al_2017_Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning.pdf}
}

@unpublished{Ha2017,
  title = {A {{Neural Representation}} of {{Sketch Drawings}}},
  author = {Ha, David and Eck, Douglas},
  date = {2017-05-19},
  eprint = {1704.03477},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1704.03477},
  urldate = {2021-01-17},
  abstract = {We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on thousands of crude human-drawn images representing hundreds of classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/TSV3FA5T/Ha_Eck_2017_A_Neural_Representation_of_Sketch_Drawings.pdf;/Users/vitay/Documents/Zotero/storage/YN8FX3C3/Ha_Eck_2017_A_Neural_Representation_of_Sketch_Drawings.pdf;/Users/vitay/Documents/Zotero/storage/4SQZXAR7/1704.html;/Users/vitay/Documents/Zotero/storage/Y6UDXLR8/1704.html}
}

@unpublished{Ha2018,
  title = {World {{Models}}},
  author = {Ha, David and Schmidhuber, Jürgen},
  date = {2018-03},
  eprint = {1803.10122},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.5281/zenodo.1207631},
  url = {http://arxiv.org/abs/1803.10122},
  abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
  file = {/Users/vitay/Documents/Zotero/storage/GGFBBAF9/Ha_Schmidhuber_2018_World Models.pdf}
}

@unpublished{Ha2020,
  title = {Distilling a {{Hierarchical Policy}} for {{Planning}} and {{Control}} via {{Representation}} and {{Reinforcement Learning}}},
  author = {Ha, Jung-Su and Park, Young-Jin and Chae, Hyeok-Joo and Park, Soon-Seo and Choi, Han-Lim},
  date = {2020-11-16},
  eprint = {2011.08345},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2011.08345},
  urldate = {2020-11-22},
  abstract = {We present a hierarchical planning and control framework that enables an agent to perform various tasks and adapt to a new task flexibly. Rather than learning an individual policy for each particular task, the proposed framework, DISH, distills a hierarchical policy from a set of tasks by representation and reinforcement learning. The framework is based on the idea of latent variable models that represent high-dimensional observations using low-dimensional latent variables. The resulting policy consists of two levels of hierarchy: (i) a planning module that reasons a sequence of latent intentions that would lead to an optimistic future and (ii) a feedback control policy, shared across the tasks, that executes the inferred intention. Because the planning is performed in low-dimensional latent space, the learned policy can immediately be used to solve or adapt to new tasks without additional training. We demonstrate the proposed framework can learn compact representations (3- and 1-dimensional latent states and commands for a humanoid with 197- and 36-dimensional state features and actions) while solving a small number of imitation tasks, and the resulting policy is directly applicable to other types of tasks, i.e., navigation in cluttered environments.},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/77AD8LZJ/Ha_et_al_2020_Distilling_a_Hierarchical_Policy_for_Planning_and_Control_via_Representation.pdf;/Users/vitay/Documents/Zotero/storage/YZCZLKE2/2011.html}
}

@unpublished{Haarnoja2017,
  title = {Reinforcement {{Learning}} with {{Deep Energy-Based Policies}}},
  author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  date = {2017-02-27},
  eprint = {1702.08165},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1702.08165},
  urldate = {2019-02-13},
  abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/EB4N2MZ2/Haarnoja et al_2017_Reinforcement Learning with Deep Energy-Based Policies.pdf;/Users/vitay/Documents/Zotero/storage/PSBPNP8Y/1702.html}
}

@article{Haarnoja2018,
  title = {Latent {{Space Policies}} for {{Hierarchical Reinforcement Learning}}},
  author = {Haarnoja, Tuomas and Hartikainen, Kristian and Abbeel, Pieter and Levine, Sergey},
  date = {2018-04},
  url = {http://arxiv.org/abs/1804.02808},
  abstract = {We address the problem of learning hierarchical deep neural network policies for reinforcement learning. Our aim is to design a hierarchical reinforcement learning algorithm that can construct hierarchical representations in bottom-up layerwise fashion. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer's policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.},
  file = {/Users/vitay/Documents/Zotero/storage/69FJXRLE/Haarnoja et al_2018_Latent Space Policies for Hierarchical Reinforcement Learning.pdf}
}

@unpublished{Haarnoja2018a,
  title = {Soft {{Actor-Critic Algorithms}} and {{Applications}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  date = {2018-12-12},
  eprint = {1812.05905},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.05905},
  urldate = {2019-02-05},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/VHCCWTLW/Haarnoja et al_2018_Soft Actor-Critic Algorithms and Applications.pdf;/Users/vitay/Documents/Zotero/storage/TWUUVECD/1812.html}
}

@unpublished{Haarnoja2018b,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  date = {2018-01},
  eprint = {1801.01290},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1801.01290},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  file = {/Users/vitay/Documents/Zotero/storage/WQYRQVC7/Haarnoja et al_2018_Soft Actor-Critic.pdf}
}

@article{Hafner2011,
  title = {Reinforcement Learning in Feedback Control},
  author = {Hafner, Roland and Riedmiller, Martin},
  date = {2011-07},
  journaltitle = {Machine Learning},
  volume = {84},
  number = {1-2},
  pages = {137--169},
  doi = {10.1007/s10994-011-5235-x},
  url = {http://link.springer.com/10.1007/s10994-011-5235-x}
}

@unpublished{Hafner2019,
  title = {Learning {{Latent Dynamics}} for {{Planning}} from {{Pixels}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  date = {2019-06-04},
  eprint = {1811.04551},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.04551},
  urldate = {2020-01-24},
  abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/T3CC85YY/Hafner_et_al_2019_Learning_Latent_Dynamics_for_Planning_from_Pixels.pdf;/Users/vitay/Documents/Zotero/storage/WUZB3FN8/1811.html}
}

@unpublished{Hafner2020,
  title = {Dream to {{Control}}: {{Learning Behaviors}} by {{Latent Imagination}}},
  shorttitle = {Dream to {{Control}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  date = {2020-03-17},
  eprint = {1912.01603},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1912.01603},
  urldate = {2020-03-24},
  abstract = {Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/M6ZVB8BU/Hafner_et_al_2020_Dream_to_Control.pdf;/Users/vitay/Documents/Zotero/storage/KVSJMFCE/1912.html}
}

@online{Hafner2022,
  title = {Mastering {{Atari}} with {{Discrete World Models}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  date = {2022-02-12},
  eprint = {2010.02193},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2010.02193},
  urldate = {2022-07-28},
  abstract = {Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and surpasses the final performance of the top single-GPU agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous actions, where it learns an accurate world model of a complex humanoid robot and solves stand-up and walking from only pixel inputs.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/RI9GM7LD/Hafner_et_al_2022_Mastering_Atari_with_Discrete_World_Models.pdf;/Users/vitay/Documents/Zotero/storage/6EK8IG6Z/2010.html}
}

@unpublished{Han2020,
  title = {Diversity {{Actor-Critic}}: {{Sample-Aware Entropy Regularization}} for {{Sample-Efficient Exploration}}},
  shorttitle = {Diversity {{Actor-Critic}}},
  author = {Han, Seungyul and Sung, Youngchul},
  date = {2020-06-02},
  eprint = {2006.01419},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.01419},
  urldate = {2020-06-13},
  abstract = {Policy entropy regularization is commonly used for better exploration in deep reinforcement learning (RL). However, policy entropy regularization is sample-inefficient in off-policy learning since it does not take the distribution of previous samples stored in the replay buffer into account. In order to take advantage of the previous sample distribution from the replay buffer for sample-efficient exploration, we propose sample-aware entropy regularization which maximizes the entropy of weighted sum of the policy action distribution and the sample action distribution from the replay buffer. We formulate the problem of sample-aware entropy regularized policy iteration, prove its convergence, and provide a practical algorithm named diversity actor-critic (DAC) which is a generalization of soft actor-critic (SAC). Numerical results show that DAC outperforms SAC and other state-of-the-art RL algorithms.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/CIT56WU6/Han_Sung_2020_Diversity_Actor-Critic.pdf;/Users/vitay/Documents/Zotero/storage/6U7YJL2V/2006.html}
}

@article{Hansen2001,
  title = {Completely {{Derandomized Self-Adaptation}} in {{Evolution Strategies}}},
  author = {Hansen, Nikolaus and Ostermeier, Andreas},
  date = {2001-06-01},
  journaltitle = {Evolutionary Computation},
  shortjournal = {Evolutionary Computation},
  volume = {9},
  number = {2},
  pages = {159--195},
  issn = {1063-6560},
  doi = {10.1162/106365601750190398},
  url = {https://doi.org/10.1162/106365601750190398},
  urldate = {2024-11-14},
  abstract = {This paper puts forward two useful methods for self-adaptation of the mutation distribution - the concepts of derandomization and cumulation. Principle shortcomings of the concept of mutative strategy parameter control and two levels of derandomization are reviewed. Basic demands on the self-adaptation of arbitrary (normal) mutation distributions are developed. Applying arbitrary, normal mutation distributions is equiv-alent to applying a general, linear problem encoding.The underlying objective of mutative strategy parameter control is roughly to favor previously selected mutation steps in the future. If this objective is pursued rigor-ously, a completely derandomized self-adaptation scheme results, which adapts arbitrary normal mutation distributions. This scheme, called covariance matrix adaptation (CMA), meets the previously stated demands. It can still be considerably improved by cumulation - utilizing an evolution path rather than single search steps.Simulations on various test functions reveal local and global search properties of the evolution strategy with and without covariance matrix adaptation. Their performances are comparable only on perfectly scaled functions. On badly scaled, non-separable functions usually a speed up factor of several orders of magnitude is ob-served. On moderately mis-scaled functions a speed up factor of three to ten can be expected.}
}

@article{Harutyunyan2016,
  title = {Q(λ) with off-Policy Corrections.},
  author = {Harutyunyan, A. and Bellemare, M. G. and Stepleton, T. and Munos, R.},
  date = {2016},
  url = {http://arxiv.org/abs/1602.04951}
}

@unpublished{Hausknecht2015,
  title = {Deep {{Recurrent Q-Learning}} for {{Partially Observable MDPs}}},
  author = {Hausknecht, Matthew and Stone, Peter},
  date = {2015-07},
  eprint = {1507.06527},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1507.06527},
  abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting \textbackslash textit\{Deep Recurrent Q-Network\} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
  file = {/Users/vitay/Documents/Zotero/storage/AVNGURCU/Hausknecht_Stone_2015_Deep Recurrent Q-Learning for Partially Observable MDPs.pdf}
}

@article{He2016,
  title = {Learning to {{Play}} in a {{Day}}: {{Faster Deep Reinforcement Learning}} by {{Optimality Tightening}}},
  author = {He, Frank S. and Liu, Yang and Schwing, Alexander G. and Peng, Jian},
  date = {2016-11},
  url = {http://arxiv.org/abs/1611.01606},
  abstract = {We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time. We evaluate the performance of our approach on the 49 games of the challenging Arcade Learning Environment, and report significant improvements in both training time and accuracy.},
  file = {/Users/vitay/Documents/Zotero/storage/KX4VQVFD/He et al_2016_Learning to Play in a Day.pdf}
}

@article{Heess2015,
  title = {Learning Continuous Control Policies by Stochastic Value Gradients},
  author = {Heess, Nicolas and Wayne, Greg and Silver, David and Lillicrap, Timothy and Tassa, Yuval and Erez, Tom},
  date = {2015},
  journaltitle = {Proc. International Conference on Neural Information Processing Systems},
  pages = {2944--2952},
  url = {http://dl.acm.org/citation.cfm?id=2969569},
  file = {/Users/vitay/Documents/Zotero/storage/DSNQGDZW/Heess et al_2015_Learning continuous control policies by stochastic value gradients.pdf}
}

@unpublished{Heess2016,
  title = {Learning and {{Transfer}} of {{Modulated Locomotor Controllers}}},
  author = {Heess, Nicolas and Wayne, Greg and Tassa, Yuval and Lillicrap, Timothy and Riedmiller, Martin and Silver, David},
  date = {2016-10-17},
  eprint = {1610.05182},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1610.05182},
  urldate = {2020-01-31},
  abstract = {We study a novel architecture and training procedure for locomotion tasks. A high-frequency, low-level "spinal" network with access to proprioceptive sensors learns sensorimotor primitives by training on simple tasks. This pre-trained module is fixed and connected to a low-frequency, high-level "cortical" network, with access to all sensors, which drives behavior by modulating the inputs to the spinal network. Where a monolithic end-to-end architecture fails completely, learning with a pre-trained spinal module succeeds at multiple high-level tasks, and enables the effective exploration required to learn from sparse rewards. We test our proposed architecture on three simulated bodies: a 16-dimensional swimming snake, a 20-dimensional quadruped, and a 54-dimensional humanoid. Our results are illustrated in the accompanying video at https://youtu.be/sboPYvhpraQ},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/4BYCAS7D/Heess_et_al_2016_Learning_and_Transfer_of_Modulated_Locomotor_Controllers.pdf;/Users/vitay/Documents/Zotero/storage/X2ZID6SH/1610.html}
}

@unpublished{Heess2017,
  title = {Emergence of {{Locomotion Behaviours}} in {{Rich Environments}}},
  author = {Heess, Nicolas and TB, Dhruva and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, S. M. Ali and Riedmiller, Martin and Silver, David},
  date = {2017-07-10},
  eprint = {1707.02286},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.02286},
  urldate = {2019-12-18},
  abstract = {The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following https://youtu.be/hx\_bgoTF7bs .},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/vitay/Documents/Zotero/storage/UYUR9X5Y/Heess_et_al_2017_Emergence_of_Locomotion_Behaviours_in_Rich_Environments.pdf;/Users/vitay/Documents/Zotero/storage/6DDMH7LT/1707.html}
}

@article{Heinrich2015,
  title = {Fictitious {{Self-Play}} in {{Extensive-Form Games}}},
  author = {Heinrich, Johannes and Lanctot, Marc and Silver, David},
  date = {2015-06},
  pages = {805--813},
  url = {http://proceedings.mlr.press/v37/heinrich15.html},
  file = {/Users/vitay/Documents/Zotero/storage/C8YP9YT5/Heinrich et al_2015_Fictitious Self-Play in Extensive-Form Games.pdf}
}

@article{Heinrich2016,
  title = {Deep {{Reinforcement Learning}} from {{Self-Play}} in {{Imperfect-Information Games}}},
  author = {Heinrich, Johannes and Silver, David},
  date = {2016-03},
  url = {http://arxiv.org/abs/1603.01121},
  abstract = {Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on significant domain expertise.},
  file = {/Users/vitay/Documents/Zotero/storage/THMSAEL6/Heinrich_Silver_2016_Deep Reinforcement Learning from Self-Play in Imperfect-Information Games.pdf}
}

@article{Henaff2017,
  title = {Model-{{Based Planning}} with {{Discrete}} and {{Continuous Actions}}},
  author = {Henaff, Mikael and Whitney, William F. and LeCun, Yann},
  date = {2017-05},
  url = {http://arxiv.org/abs/1705.07177},
  abstract = {Action planning using learned and differentiable forward models of the world is a general approach which has a number of desirable properties, including improved sample complexity over model-free RL methods, reuse of learned models across different tasks, and the ability to perform efficient gradient-based optimization in continuous action spaces. However, this approach does not apply straightforwardly when the action space is discrete. In this work, we show that it is in fact possible to effectively perform planning via backprop in discrete action spaces, using a simple paramaterization of the actions vectors on the simplex combined with input noise when training the forward model. Our experiments show that this approach can match or outperform model-free RL and discrete planning methods on gridworld navigation tasks in terms of performance and/or planning time while using limited environment interactions, and can additionally be used to perform model-based control in a challenging new task where the action space combines discrete and continuous actions. We furthermore propose a policy distillation approach which yields a fast policy network which can be used at inference time, removing the need for an iterative planning procedure.},
  file = {/Users/vitay/Documents/Zotero/storage/AC3XTGYG/Henaff et al_2017_Model-Based Planning with Discrete and Continuous Actions.pdf}
}

@unpublished{Hessel2017,
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  author = {Hessel, Matteo and Modayil, Joseph and family=Hasselt, given=Hado, prefix=van, useprefix=true and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  date = {2017-10},
  eprint = {1707.06887},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1710.02298},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  file = {/Users/vitay/Documents/Zotero/storage/R9DWGPVU/Hessel et al_2017_Rainbow.pdf}
}

@unpublished{Hessel2021,
  title = {Muesli: {{Combining Improvements}} in {{Policy Optimization}}},
  shorttitle = {Muesli},
  author = {Hessel, Matteo and Danihelka, Ivo and Viola, Fabio and Guez, Arthur and Schmitt, Simon and Sifre, Laurent and Weber, Theophane and Silver, David and family=Hasselt, given=Hado, prefix=van, useprefix=true},
  date = {2021-04-13},
  eprint = {2104.06159},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.06159},
  urldate = {2021-04-17},
  abstract = {We propose a novel policy update that combines regularized policy optimization with model learning as an auxiliary loss. The update (henceforth Muesli) matches MuZero's state-of-the-art performance on Atari. Notably, Muesli does so without using deep search: it acts directly with a policy network and has computation speed comparable to model-free baselines. The Atari results are complemented by extensive ablations, and by additional results on continuous control and 9x9 Go.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/ERD3386C/Hessel_et_al_2021_Muesli.pdf;/Users/vitay/Documents/Zotero/storage/TB48AK6B/2104.html}
}

@unpublished{Horgan2018,
  title = {Distributed {{Prioritized Experience Replay}}},
  author = {Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and family=Hasselt, given=Hado, prefix=van, useprefix=true and Silver, David},
  date = {2018-03-02},
  eprint = {1803.00933},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1803.00933},
  urldate = {2019-12-14},
  abstract = {We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/9IVVHWI8/Horgan_et_al_2018_Distributed_Prioritized_Experience_Replay.pdf;/Users/vitay/Documents/Zotero/storage/SV86BKUN/1803.html}
}

@article{Hu2019,
  title = {Self-{{Attention-Based Temporary Curiosity}} in {{Reinforcement Learning Exploration}}},
  author = {Hu, Hangkai and Song, Shiji and Huang, Gao},
  date = {2019},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  pages = {1--12},
  issn = {2168-2232},
  doi = {10.1109/TSMC.2019.2957051},
  abstract = {In many real-world scenarios, extrinsic rewards provided by the environment are sparse. An agent trained with classic reinforcement learning algorithm fails to explore these environments in a sufficient and effective way. To address this problem, the exploration bonus which derives from environmental novelty serves as intrinsic motivation for the agent. In recent years, curiosity-driven exploration is a mainstream approach to describe environmental novelty through prediction errors of dynamics models. Due to the expressive ability limitations of curiosity-based environmental novelty and the difficulty of finding appropriate feature space, most curiosity-driven exploration methods have the problem of overprotection against repetition. This problem can reduce the efficiency of exploration and lead the agent into a trap with local optimality. In this article, we propose a combination of persisting curiosity and temporary curiosity framework to deal with the problem of overprotection against repetition. We introduce the self-attention mechanism from the field of computer vision and propose a sequence-based self-attention mechanism for temporary curiosity generation. We compare our framework with some previous exploration methods in hard-exploration environments, provide a series of comprehensive analysis of the proposed framework and investigate the effect of the individual components of our method. The experimental results indicate that the proposed framework delivers superior performance than existing methods.},
  keywords = {Attention mechanism,curiosity-driven,exploration,intrinsic reward,reinforcement learning},
  file = {/Users/vitay/Documents/Zotero/storage/ZNK247DG/8936518.html}
}

@article{Hu2020,
  title = {Attentive Multi-View Reinforcement Learning},
  author = {Hu, Yueyue and Sun, Shiliang and Xu, Xin and Zhao, Jing},
  date = {2020-05-04},
  journaltitle = {International Journal of Machine Learning and Cybernetics},
  shortjournal = {Int. J. Mach. Learn. \& Cyber.},
  issn = {1868-808X},
  doi = {10.1007/s13042-020-01130-6},
  url = {https://doi.org/10.1007/s13042-020-01130-6},
  urldate = {2020-05-10},
  abstract = {The reinforcement learning process usually takes millions of steps from scratch, due to the limited observation experience. More precisely, the representation approximated by a single deep network is usually limited for reinforcement learning agents. In this paper, we propose a novel multi-view deep attention network (MvDAN), which introduces multi-view representation learning into the reinforcement learning framework for the first time. Based on the multi-view scheme of function approximation, the proposed model approximates multiple view-specific policy or value functions in parallel by estimating the middle-level representation and integrates these functions based on attention mechanisms to generate a comprehensive strategy. Furthermore, we develop the multi-view generalized policy improvement to jointly optimize all policies instead of a single one. Compared with the single-view function approximation scheme in reinforcement learning methods, experimental results on eight Atari benchmarks show that MvDAN outperforms the state-of-the-art methods and has faster convergence and training stability.},
  langid = {english}
}

@article{Hu2020b,
  title = {Proximal Policy Optimization with an Integral Compensator for Quadrotor Control},
  author = {Hu, Huan and Wang, Qing-ling},
  date = {2020-05},
  journaltitle = {Frontiers of Information Technology \& Electronic Engineering},
  shortjournal = {Front Inform Technol Electron Eng},
  volume = {21},
  number = {5},
  pages = {777--795},
  issn = {2095-9184, 2095-9230},
  doi = {10.1631/FITEE.1900641},
  url = {http://link.springer.com/10.1631/FITEE.1900641},
  urldate = {2020-06-13},
  abstract = {We use the advanced proximal policy optimization (PPO) reinforcement learning algorithm to optimize the stochastic control strategy to achieve speed control of the “model-free” quadrotor. The model is controlled by four learned neural networks, which directly map the system states to control commands in an end-to-end style. By introducing an integral compensator into the actor-critic framework, the speed tracking accuracy and robustness have been greatly enhanced. In addition, a two-phase learning scheme which includes both offline- and online-learning is developed for practical use. A model with strong generalization ability is learned in the offline phase. Then, the flight policy of the model is continuously optimized in the online learning phase. Finally, the performances of our proposed algorithm are compared with those of the traditional PID algorithm.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/X6PGLLFT/Hu and Wang - 2020 - Proximal policy optimization with an integral comp.pdf}
}

@unpublished{Hubert2021,
  title = {Learning and {{Planning}} in {{Complex Action Spaces}}},
  author = {Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Barekatain, Mohammadamin and Schmitt, Simon and Silver, David},
  date = {2021-04-13},
  eprint = {2104.06303},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.06303},
  urldate = {2021-04-17},
  abstract = {Many important real-world problems have action spaces that are high-dimensional, continuous or both, making full enumeration of all possible actions infeasible. Instead, only small subsets of actions can be sampled for the purpose of policy evaluation and improvement. In this paper, we propose a general framework to reason in a principled way about policy evaluation and improvement over such sampled action subsets. This sample-based policy iteration framework can in principle be applied to any reinforcement learning algorithm based upon policy iteration. Concretely, we propose Sampled MuZero, an extension of the MuZero algorithm that is able to learn in domains with arbitrarily complex action spaces by planning over sampled actions. We demonstrate this approach on the classical board game of Go and on two continuous control benchmark domains: DeepMind Control Suite and Real-World RL Suite.},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/SV8PTQLY/Hubert_et_al_2021_Learning_and_Planning_in_Complex_Action_Spaces.pdf;/Users/vitay/Documents/Zotero/storage/36YKY5RI/2104.html}
}

@unpublished{Ibarz2018,
  title = {Reward Learning from Human Preferences and Demonstrations in {{Atari}}},
  author = {Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
  date = {2018-11-15},
  eprint = {1811.06521},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.06521},
  urldate = {2020-05-16},
  abstract = {To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/YNRCF45I/Ibarz_et_al_2018_Reward_learning_from_human_preferences_and_demonstrations_in_Atari.pdf;/Users/vitay/Documents/Zotero/storage/XPMPT32B/1811.html}
}

@article{Ibarz2021,
  title = {How to {{Train Your Robot}} with {{Deep Reinforcement Learning}}; {{Lessons We}}'ve {{Learned}}},
  author = {Ibarz, Julian and Tan, Jie and Finn, Chelsea and Kalakrishnan, Mrinal and Pastor, Peter and Levine, Sergey},
  date = {2021-04},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {40},
  number = {4-5},
  eprint = {2102.02915},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {698--721},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364920987859},
  url = {http://arxiv.org/abs/2102.02915},
  urldate = {2023-06-03},
  abstract = {Deep reinforcement learning (RL) has emerged as a promising approach for autonomously acquiring complex behaviors from low level sensor observations. Although a large portion of deep RL research has focused on applications in video games and simulated control, which does not connect with the constraints of learning in real environments, deep RL has also demonstrated promise in enabling physical robots to learn complex skills in the real world. At the same time,real world robotics provides an appealing domain for evaluating such algorithms, as it connects directly to how humans learn; as an embodied agent in the real world. Learning to perceive and move in the real world presents numerous challenges, some of which are easier to address than others, and some of which are often not considered in RL research that focuses only on simulated domains. In this review article, we present a number of case studies involving robotic deep RL. Building off of these case studies, we discuss commonly perceived challenges in deep RL and how they have been addressed in these works. We also provide an overview of other outstanding challenges, many of which are unique to the real-world robotics setting and are not often the focus of mainstream RL research. Our goal is to provide a resource both for roboticists and machine learning researchers who are interested in furthering the progress of deep RL in the real world.},
  file = {/Users/vitay/Documents/Zotero/storage/Z35WTPL9/Ibarz_et_al_2021_How_to_Train_Your_Robot_with_Deep_Reinforcement_Learning;_Lessons_We've_Learned.pdf}
}

@incollection{Jabri2019,
  title = {Unsupervised {{Curricula}} for {{Visual Meta-Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Jabri, Allan and Hsu, Kyle and Gupta, Abhishek and Eysenbach, Ben and Levine, Sergey and Finn, Chelsea},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Alché-Buc, given=F., prefix=d\textbackslash textquotesingle, useprefix=false and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {10519--10530},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/9238-unsupervised-curricula-for-visual-meta-reinforcement-learning.pdf},
  urldate = {2019-11-19},
  file = {/Users/vitay/Documents/Zotero/storage/23BDJVA8/Jabri et al_2019_Unsupervised Curricula for Visual Meta-Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/X74SY6Z6/9238-unsupervised-curricula-for-visual-meta-reinforcement-learning.html}
}

@unpublished{Jaderberg2016,
  title = {Reinforcement {{Learning}} with {{Unsupervised Auxiliary Tasks}}},
  author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z. and Silver, David and Kavukcuoglu, Koray},
  date = {2016-11-16},
  eprint = {1611.05397},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.05397},
  urldate = {2020-05-20},
  abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\textbackslash\% expert human performance, and a challenging suite of first-person, three-dimensional \textbackslash emph\{Labyrinth\} tasks leading to a mean speedup in learning of 10\$\textbackslash times\$ and averaging 87\textbackslash\% expert human performance on Labyrinth.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/vitay/Documents/Zotero/storage/KYMP62AS/Jaderberg_et_al_2016_Reinforcement_Learning_with_Unsupervised_Auxiliary_Tasks.pdf;/Users/vitay/Documents/Zotero/storage/YEQPT47V/1611.html}
}

@article{Jaderberg2019,
  title = {Human-Level Performance in {{3D}} Multiplayer Games with Population-Based Reinforcement Learning},
  author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castañeda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
  date = {2019-05-31},
  journaltitle = {Science},
  volume = {364},
  number = {6443},
  eprint = {31147514},
  eprinttype = {pmid},
  pages = {859--865},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aau6249},
  url = {https://science.sciencemag.org/content/364/6443/859},
  urldate = {2019-06-04},
  abstract = {Artificial teamwork Artificially intelligent agents are getting better and better at two-player games, but most real-world endeavors require teamwork. Jaderberg et al. designed a computer program that excels at playing the video game Quake III Arena in Capture the Flag mode, where two multiplayer teams compete in capturing the flags of the opposing team. The agents were trained by playing thousands of games, gradually learning successful strategies not unlike those favored by their human counterparts. Computer agents competed successfully against humans even when their reaction times were slowed to match those of humans. Science, this issue p. 859 Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research. Teams of artificial agents compete successfully against humans in the video game Quake III Arena in Capture the Flag mode. Teams of artificial agents compete successfully against humans in the video game Quake III Arena in Capture the Flag mode.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/2ZMKLBRQ/Jaderberg et al_2019_Human-level performance in 3D multiplayer games with population-based.pdf;/Users/vitay/Documents/Zotero/storage/P9ZT7MPH/859.html}
}

@unpublished{James2019,
  title = {Sim-to-{{Real}} via {{Sim-to-Sim}}: {{Data-efficient Robotic Grasping}} via {{Randomized-to-Canonical Adaptation Networks}}},
  shorttitle = {Sim-to-{{Real}} via {{Sim-to-Sim}}},
  author = {James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian and Levine, Sergey and Hadsell, Raia and Bousmalis, Konstantinos},
  date = {2019-07-21},
  eprint = {1812.07252},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1812.07252},
  urldate = {2021-05-01},
  abstract = {Real world data, especially in the domain of robotics, is notoriously costly to collect. One way to circumvent this can be to leverage the power of simulation to produce large amounts of labelled data. However, training models on simulated images does not readily transfer to real-world ones. Using domain adaptation methods to cross this "reality gap" requires a large amount of unlabelled real-world data, whilst domain randomization alone can waste modeling power. In this paper, we present Randomized-to-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no real-world data. Our method learns to translate randomized rendered images into their equivalent non-randomized, canonical versions. This in turn allows for real images to also be translated into canonical sim images. We demonstrate the effectiveness of this sim-to-real approach by training a vision-based closed-loop grasping reinforcement learning agent in simulation, and then transferring it to the real world to attain 70\% zero-shot grasp success on unseen objects, a result that almost doubles the success of learning the same task directly on domain randomization alone. Additionally, by joint finetuning in the real-world with only 5,000 real-world grasps, our method achieves 91\%, attaining comparable performance to a state-of-the-art system trained with 580,000 real-world grasps, resulting in a reduction of real-world data by more than 99\%.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/BMMZN2AG/James_et_al_2019_Sim-to-Real_via_Sim-to-Sim.pdf;/Users/vitay/Documents/Zotero/storage/KXGXK5LC/1812.html}
}

@unpublished{Janner2019,
  title = {When to {{Trust Your Model}}: {{Model-Based Policy Optimization}}},
  shorttitle = {When to {{Trust Your Model}}},
  author = {Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  date = {2019-06-19},
  eprint = {1906.08253},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.08253},
  urldate = {2019-06-25},
  abstract = {Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/9KXXAFZX/Janner et al_2019_When to Trust Your Model.pdf;/Users/vitay/Documents/Zotero/storage/RR3E8EDH/1906.html}
}

@unpublished{Janner2021,
  title = {Reinforcement {{Learning}} as {{One Big Sequence Modeling Problem}}},
  author = {Janner, Michael and Li, Qiyang and Levine, Sergey},
  date = {2021-06-03},
  eprint = {2106.02039},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.02039},
  urldate = {2021-06-28},
  abstract = {Reinforcement learning (RL) is typically concerned with estimating single-step policies or single-step models, leveraging the Markov property to factorize the problem in time. However, we can also view RL as a sequence modeling problem, with the goal being to predict a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether powerful, high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide simple and effective solutions to the RL problem. To this end, we explore how RL can be reframed as "one big sequence modeling" problem, using state-of-the-art Transformer architectures to model distributions over sequences of states, actions, and rewards. Addressing RL as a sequence modeling problem significantly simplifies a range of design decisions: we no longer require separate behavior policy constraints, as is common in prior work on offline model-free RL, and we no longer require ensembles or other epistemic uncertainty estimators, as is common in prior work on model-based RL. All of these roles are filled by the same Transformer sequence model. In our experiments, we demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/UCU2ZDMZ/Janner_et_al_2021_Reinforcement_Learning_as_One_Big_Sequence_Modeling_Problem.pdf;/Users/vitay/Documents/Zotero/storage/QNZ529XL/2106.html}
}

@online{Ji2022,
  title = {Hierarchical {{Reinforcement Learning}} for {{Precise Soccer Shooting Skills}} Using a {{Quadrupedal Robot}}},
  author = {Ji, Yandong and Li, Zhongyu and Sun, Yinan and Peng, Xue Bin and Levine, Sergey and Berseth, Glen and Sreenath, Koushil},
  date = {2022-08-01},
  eprint = {2208.01160},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2208.01160},
  urldate = {2022-08-06},
  abstract = {We address the problem of enabling quadrupedal robots to perform precise shooting skills in the real world using reinforcement learning. Developing algorithms to enable a legged robot to shoot a soccer ball to a given target is a challenging problem that combines robot motion control and planning into one task. To solve this problem, we need to consider the dynamics limitation and motion stability during the control of a dynamic legged robot. Moreover, we need to consider motion planning to shoot the hard-to-model deformable ball rolling on the ground with uncertain friction to a desired location. In this paper, we propose a hierarchical framework that leverages deep reinforcement learning to train (a) a robust motion control policy that can track arbitrary motions and (b) a planning policy to decide the desired kicking motion to shoot a soccer ball to a target. We deploy the proposed framework on an A1 quadrupedal robot and enable it to accurately shoot the ball to random targets in the real world.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/HXM2KCQ8/Ji_et_al_2022_Hierarchical_Reinforcement_Learning_for_Precise_Soccer_Shooting_Skills_using_a.pdf;/Users/vitay/Documents/Zotero/storage/VZ5HGI75/2208.html}
}

@inproceedings{Kakade2001,
  title = {A {{Natural Policy Gradient}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 14},
  author = {Kakade, Sham},
  date = {2001},
  url = {https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf},
  abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the param-eter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradi-ent is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sut-ton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.},
  file = {/Users/vitay/Documents/Zotero/storage/M4JBTABC/Kakade_2001_A Natural Policy Gradient.pdf}
}

@article{Kakade2002,
  title = {Approximately {{Optimal Approximate Reinforcement Learning}}},
  author = {Kakade, Sham and Langford, John},
  date = {2002},
  journaltitle = {Proc. 19th International Conference on Machine Learning},
  pages = {267--274},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601}
}

@unpublished{Kansky2017,
  title = {Schema {{Networks}}: {{Zero-shot Transfer}} with a {{Generative Causal Model}} of {{Intuitive Physics}}},
  shorttitle = {Schema {{Networks}}},
  author = {Kansky, Ken and Silver, Tom and Mély, David A. and Eldawy, Mohamed and Lázaro-Gredilla, Miguel and Lou, Xinghua and Dorfman, Nimrod and Sidor, Szymon and Phoenix, Scott and George, Dileep},
  date = {2017-06-14},
  eprint = {1706.04317},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.04317},
  urldate = {2019-01-10},
  abstract = {The recent adaptation of deep neural network-based methods to reinforcement learning and planning domains has yielded remarkable progress on individual tasks. Nonetheless, progress on task-to-task transfer remains limited. In pursuit of efficient and robust generalization, we introduce the Schema Network, an object-oriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We compare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reporting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationships are essential abilities on the path toward generally intelligent systems.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/vitay/Documents/Zotero/storage/BFYKIRKH/Kansky et al_2017_Schema Networks.pdf}
}

@inproceedings{Kapturowski2019,
  title = {Recurrent Experience Replay in Distributed Reinforcement Learning},
  author = {Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  date = {2019},
  pages = {19},
  url = {https://openreview.net/pdf?id=r1lyTjAqYX},
  abstract = {Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and fixed set of hyperparameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. It is the first agent to exceed human-level performance in 52 of the 57 Atari games.},
  eventtitle = {{{ICLR}}},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/DR8FH8II/Kapturowski et al_2019_Recurrent experience replay in distributed reinforcement learning.pdf}
}

@online{Kapturowski2022,
  title = {Human-Level {{Atari}} 200x Faster},
  author = {Kapturowski, Steven and Campos, Víctor and Jiang, Ray and Rakićević, Nemanja and family=Hasselt, given=Hado, prefix=van, useprefix=true and Blundell, Charles and Badia, Adrià Puigdomènech},
  date = {2022-09-15},
  eprint = {2209.07550},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.07550},
  url = {http://arxiv.org/abs/2209.07550},
  urldate = {2022-09-23},
  abstract = {The task of building general agents that perform well over a wide range of tasks has been an important goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 billion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to out perform the human baseline. We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. The four key components to our approach are (1) an approximate trust region method which enables stable bootstrapping from the online network, (2) a normalisation scheme for the loss and priorities which improves robustness when learning a set of value functions with a wide range of scales, (3) an improved architecture employing techniques from NFNets in order to leverage deeper networks without the need for normalization layers, and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy overtime.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/4SEFNXCY/Kapturowski_et_al_2022_Human-level_Atari_200x_faster.pdf;/Users/vitay/Documents/Zotero/storage/2YW429VB/2209.html}
}

@article{Kaufmann2023,
  title = {Champion-Level Drone Racing Using Deep Reinforcement Learning},
  author = {Kaufmann, Elia and Bauersfeld, Leonard and Loquercio, Antonio and Müller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
  date = {2023-08},
  journaltitle = {Nature},
  volume = {620},
  number = {7976},
  pages = {982--987},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06419-4},
  url = {https://www.nature.com/articles/s41586-023-06419-4},
  urldate = {2024-10-14},
  abstract = {First-person view (FPV) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors1. Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning (RL) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence2, which may inspire the deployment of hybrid learning-based solutions in other physical systems.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/LU72WIPE/Kaufmann et al. - 2023 - Champion-level drone racing using deep reinforcement learning.pdf}
}

@unpublished{Kendall2018,
  title = {Learning to {{Drive}} in a {{Day}}},
  author = {Kendall, Alex and Hawke, Jeffrey and Janz, David and Mazur, Przemyslaw and Reda, Daniele and Allen, John-Mark and Lam, Vinh-Dieu and Bewley, Alex and Shah, Amar},
  date = {2018-07-01},
  eprint = {1807.00412},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1807.00412},
  urldate = {2018-12-19},
  abstract = {We demonstrate the first application of deep reinforcement learning to autonomous driving. From randomly initialised parameters, our model is able to learn a policy for lane following in a handful of training episodes using a single monocular image as input. We provide a general and easy to obtain reward: the distance travelled by the vehicle without the safety driver taking control. We use a continuous, model-free deep reinforcement learning algorithm, with all exploration and optimisation performed on-vehicle. This demonstrates a new framework for autonomous driving which moves away from reliance on defined logical rules, mapping, and direct supervision. We discuss the challenges and opportunities to scale this approach to a broader range of autonomous driving tasks.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/L5FDQ7YN/Kendall et al_2018_Learning to Drive in a Day.pdf}
}

@unpublished{Kidambi2021,
  title = {{{MOReL}} : {{Model-Based Offline Reinforcement Learning}}},
  shorttitle = {{{MOReL}}},
  author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  date = {2021-03-01},
  eprint = {2005.05951},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2005.05951},
  urldate = {2021-10-05},
  abstract = {In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. The ability to train RL policies offline can greatly expand the applicability of RL, its data efficiency, and its experimental velocity. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP (P-MDP) using the offline dataset; and (b) learning a near-optimal policy in this P-MDP. The learned P-MDP has the property that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the P-MDP. This enables it to serve as a good surrogate for purposes of policy evaluation and learning, and overcome common pitfalls of model-based RL like model exploitation. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Through experiments, we show that MOReL matches or exceeds state-of-the-art results in widely studied offline RL benchmarks. Moreover, the modular design of MOReL enables future advances in its components (e.g. generative modeling, uncertainty estimation, planning etc.) to directly translate into advances for offline RL.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/S8VHUAEW/Kidambi_et_al_2021_MOReL.pdf;/Users/vitay/Documents/Zotero/storage/2SAS3RRA/2005.html}
}

@unpublished{Kirsch2020,
  title = {Improving {{Generalization}} in {{Meta Reinforcement Learning}} Using {{Learned Objectives}}},
  author = {Kirsch, Louis and family=Steenkiste, given=Sjoerd, prefix=van, useprefix=true and Schmidhuber, Jürgen},
  date = {2020-02-14},
  eprint = {1910.04098},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.04098},
  urldate = {2021-02-06},
  abstract = {Biological evolution has distilled the experiences of many learners into the general learning algorithms of humans. Our novel meta reinforcement learning algorithm MetaGenRL is inspired by this process. MetaGenRL distills the experiences of many complex agents to meta-learn a low-complexity neural objective function that decides how future individuals will learn. Unlike recent meta-RL algorithms, MetaGenRL can generalize to new environments that are entirely different from those used for meta-training. In some cases, it even outperforms human-engineered RL algorithms. MetaGenRL uses off-policy second-order gradients during meta-training that greatly increase its sample efficiency.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/3PUSDH2V/Kirsch_et_al_2020_Improving_Generalization_in_Meta_Reinforcement_Learning_using_Learned_Objectives.pdf;/Users/vitay/Documents/Zotero/storage/BGW83WCU/1910.html}
}

@unpublished{Klink2020,
  title = {Self-{{Paced Deep Reinforcement Learning}}},
  author = {Klink, Pascal and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
  date = {2020-04-24},
  eprint = {2004.11812},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2004.11812},
  urldate = {2020-04-30},
  abstract = {Generalization and reuse of agent behaviour across a variety of learning tasks promises to carry the next wave of breakthroughs in Reinforcement Learning (RL). The field of Curriculum Learning proposes strategies that aim to support a learning agent by exposing it to a tailored series of tasks throughout learning, e.g. by progressively increasing their complexity. In this paper, we consider recently established results in Curriculum Learning for episodic RL, proposing an extension that is easily integrated with well-known RL algorithms and providing a theoretical formulation from an RL-as-Inference perspective. We evaluate the proposed scheme with different Deep RL algorithms on representative tasks, demonstrating that it is capable of significantly improving learning performance.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/V7AIHJA5/Klink_et_al_2020_Self-Paced_Deep_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/DFJGJJXU/2004.html}
}

@article{Knight2018,
  title = {Natural {{Gradient Deep Q-learning}}},
  author = {Knight, Ethan and Lerner, Osher},
  date = {2018-03},
  url = {http://arxiv.org/abs/1803.07482},
  abstract = {This paper presents findings for training a Q-learning reinforcement learning agent using natural gradient techniques. We compare the original deep Q-network (DQN) algorithm to its natural gradient counterpart (NGDQN), measuring NGDQN and DQN performance on classic controls environments without target networks. We find that NGDQN performs favorably relative to DQN, converging to significantly better policies faster and more frequently. These results indicate that natural gradient could be used for value function optimization in reinforcement learning to accelerate and stabilize training.},
  file = {/Users/vitay/Documents/Zotero/storage/Y4TALWVH/Knight_Lerner_2018_Natural Gradient Deep Q-learning.pdf}
}

@article{Kober2013,
  title = {Reinforcement Learning in Robotics: {{A}} Survey},
  shorttitle = {Reinforcement Learning in Robotics},
  author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
  date = {2013-09-01},
  journaltitle = {The International Journal of Robotics Research},
  volume = {32},
  number = {11},
  pages = {1238--1274},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/0278364913495721},
  url = {https://doi.org/10.1177/0278364913495721},
  urldate = {2023-01-25},
  abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/ELDAR7CJ/Kober_et_al_2013_Reinforcement_learning_in_robotics.pdf}
}

@unpublished{Kulkarni2016,
  title = {Deep {{Successor Reinforcement Learning}}},
  author = {Kulkarni, Tejas D. and Saeedi, Ardavan and Gautam, Simanta and Gershman, Samuel J.},
  date = {2016-06-08},
  eprint = {1606.02396},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1606.02396},
  urldate = {2019-02-20},
  abstract = {Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/6XVTNIFG/Kulkarni et al_2016_Deep Successor Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/MR6M34YV/1606.html}
}

@unpublished{Kumar2020,
  title = {Conservative {{Q-Learning}} for {{Offline Reinforcement Learning}}},
  author = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  date = {2020-06-08},
  eprint = {2006.04779},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.04779},
  urldate = {2020-06-13},
  abstract = {Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a principled policy improvement procedure. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/GSVJWZ4V/Kumar_et_al_2020_Conservative_Q-Learning_for_Offline_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/9GG4DITA/2006.html}
}

@unpublished{Kumar2022,
  title = {When {{Should We Prefer Offline Reinforcement Learning Over Behavioral Cloning}}?},
  author = {Kumar, Aviral and Hong, Joey and Singh, Anikait and Levine, Sergey},
  date = {2022-04-12},
  eprint = {2204.05618},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2204.05618},
  urldate = {2022-04-16},
  abstract = {Offline reinforcement learning (RL) algorithms can acquire effective policies by utilizing previously collected experience, without any online interaction. It is widely understood that offline RL is able to extract good policies even from highly suboptimal data, a scenario where imitation learning finds suboptimal solutions that do not improve over the demonstrator that generated the dataset. However, another common use case for practitioners is to learn from data that resembles demonstrations. In this case, one can choose to apply offline RL, but can also use behavioral cloning (BC) algorithms, which mimic a subset of the dataset via supervised learning. Therefore, it seems natural to ask: when can an offline RL method outperform BC with an equal amount of expert data, even when BC is a natural choice? To answer this question, we characterize the properties of environments that allow offline RL methods to perform better than BC methods, even when only provided with expert data. Additionally, we show that policies trained on sufficiently noisy suboptimal data can attain better performance than even BC algorithms with expert data, especially on long-horizon problems. We validate our theoretical results via extensive experiments on both diagnostic and high-dimensional domains including robotic manipulation, maze navigation, and Atari games, with a variety of data distributions. We observe that, under specific but common conditions such as sparse rewards or noisy data sources, modern offline RL methods can significantly outperform BC.},
  file = {/Users/vitay/Documents/Zotero/storage/4FI33UVW/Kumar_et_al_2022_When_Should_We_Prefer_Offline_Reinforcement_Learning_Over_Behavioral_Cloning.pdf;/Users/vitay/Documents/Zotero/storage/PWICBIXP/2204.html}
}

@unpublished{Kurutach2018,
  title = {Model-{{Ensemble Trust-Region Policy Optimization}}},
  author = {Kurutach, Thanard and Clavera, Ignasi and Duan, Yan and Tamar, Aviv and Abbeel, Pieter},
  date = {2018-10-05},
  eprint = {1802.10592},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.10592},
  urldate = {2020-01-26},
  abstract = {Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity, which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/L5X3J38K/Kurutach_et_al_2018_Model-Ensemble_Trust-Region_Policy_Optimization.pdf;/Users/vitay/Documents/Zotero/storage/DNC6DP43/1802.html}
}

@unpublished{Lai2020,
  title = {Dual {{Policy Distillation}}},
  author = {Lai, Kwei-Herng and Zha, Daochen and Li, Yuening and Hu, Xia},
  date = {2020-06-07},
  eprint = {2006.04061},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2006.04061},
  urldate = {2020-06-16},
  abstract = {Policy distillation, which transfers a teacher policy to a student policy has achieved great success in challenging tasks of deep reinforcement learning. This teacher-student framework requires a well-trained teacher model which is computationally expensive. Moreover, the performance of the student model could be limited by the teacher model if the teacher model is not optimal. In the light of collaborative learning, we study the feasibility of involving joint intellectual efforts from diverse perspectives of student models. In this work, we introduce dual policy distillation(DPD), a student-student framework in which two learners operate on the same environment to explore different perspectives of the environment and extract knowledge from each other to enhance their learning. The key challenge in developing this dual learning framework is to identify the beneficial knowledge from the peer learner for contemporary learning-based reinforcement learning algorithms, since it is unclear whether the knowledge distilled from an imperfect and noisy peer learner would be helpful. To address the challenge, we theoretically justify that distilling knowledge from a peer learner will lead to policy improvement and propose a disadvantageous distillation strategy based on the theoretical results. The conducted experiments on several continuous control tasks show that the proposed framework achieves superior performance with a learning-based agent and function approximation without the use of expensive teacher models.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/7SDP6BKW/Lai_et_al_2020_Dual_Policy_Distillation.pdf;/Users/vitay/Documents/Zotero/storage/3YKHVVG4/2006.html}
}

@unpublished{Lambert2020,
  title = {Stein {{Variational Model Predictive Control}}},
  author = {Lambert, Alexander and Fishman, Adam and Fox, Dieter and Boots, Byron and Ramos, Fabio},
  date = {2020-11-15},
  eprint = {2011.07641},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2011.07641},
  urldate = {2020-11-22},
  abstract = {Decision making under uncertainty is critical to real-world, autonomous systems. Model Predictive Control (MPC) methods have demonstrated favorable performance in practice, but remain limited when dealing with complex probability distributions. In this paper, we propose a generalization of MPC that represents a multitude of solutions as posterior distributions. By casting MPC as a Bayesian inference problem, we employ variational methods for posterior computation, naturally encoding the complexity and multi-modality of the decision making problem. We propose a Stein variational gradient descent method to estimate the posterior directly over control parameters, given a cost function and observed state trajectories. We show that this framework leads to successful planning in challenging, non-convex optimal control problems.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/ZFYSSYSG/Lambert_et_al_2020_Stein_Variational_Model_Predictive_Control.pdf;/Users/vitay/Documents/Zotero/storage/SLPZDIXB/2011.html}
}

@unpublished{Laskin2020,
  title = {Reinforcement {{Learning}} with {{Augmented Data}}},
  author = {Laskin, Michael and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
  date = {2020-05-04},
  eprint = {2004.14990},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2004.14990},
  urldate = {2020-05-05},
  abstract = {Learning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/HCGKF3XP/Laskin_et_al_2020_Reinforcement_Learning_with_Augmented_Data.pdf;/Users/vitay/Documents/Zotero/storage/N89MCDJ3/2004.html}
}

@unpublished{Laux2020,
  title = {Deep {{Adversarial Reinforcement Learning}} for {{Object Disentangling}}},
  author = {Laux, Melvin and Arenz, Oleg and Peters, Jan and Pajarinen, Joni},
  date = {2020-03-08},
  eprint = {2003.03779},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2003.03779},
  urldate = {2020-03-15},
  abstract = {Deep learning in combination with improved training techniques and high computational power has led to recent advances in the field of reinforcement learning (RL) and to successful robotic RL applications such as in-hand manipulation. However, most robotic RL relies on a well known initial state distribution. In real-world tasks, this information is however often not available. For example, when disentangling waste objects the actual position of the robot w.r.t.\textbackslash{} the objects may not match the positions the RL policy was trained for. To solve this problem, we present a novel adversarial reinforcement learning (ARL) framework. The ARL framework utilizes an adversary, which is trained to steer the original agent, the protagonist, to challenging states. We train the protagonist and the adversary jointly to allow them to adapt to the changing policy of their opponent. We show that our method can generalize from training to test scenarios by training an end-to-end system for robot control to solve a challenging object disentangling task. Experiments with a KUKA LBR+ 7-DOF robot arm show that our approach outperforms the baseline method in disentangling when starting from different initial states than provided during training.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/2HSIYTT2/Laux_et_al_2020_Deep_Adversarial_Reinforcement_Learning_for_Object_Disentangling.pdf;/Users/vitay/Documents/Zotero/storage/3K3EKJ5L/2003.html}
}

@unpublished{Lee2019,
  title = {Efficient {{Exploration}} via {{State Marginal Matching}}},
  author = {Lee, Lisa and Eysenbach, Benjamin and Parisotto, Emilio and Xing, Eric and Levine, Sergey and Salakhutdinov, Ruslan},
  date = {2019-06-12},
  eprint = {1906.05274},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.05274},
  urldate = {2019-06-18},
  abstract = {To solve tasks with sparse rewards, reinforcement learning algorithms must be equipped with suitable exploration techniques. However, it is unclear what underlying objective is being optimized by existing exploration algorithms, or how they can be altered to incorporate prior knowledge about the task. Most importantly, it is difficult to use exploration experience from one task to acquire exploration strategies for another task. We address these shortcomings by learning a single exploration policy that can quickly solve a suite of downstream tasks in a multi-task setting, amortizing the cost of learning to explore. We recast exploration as a problem of State Marginal Matching (SMM): we learn a mixture of policies for which the state marginal distribution matches a given target state distribution, which can incorporate prior knowledge about the task. Without any prior knowledge, the SMM objective reduces to maximizing the marginal state entropy. We optimize the objective by reducing it to a two-player, zero-sum game, where we iteratively fit a state density model and then update the policy to visit states with low density under this model. While many previous algorithms for exploration employ a similar procedure, they omit a crucial historical averaging step, without which the iterative procedure does not converge to a Nash equilibria. To parallelize exploration, we extend our algorithm to use mixtures of policies, wherein we discover connections between SMM and previously-proposed skill learning methods based on mutual information. On complex navigation and manipulation tasks, we demonstrate that our algorithm explores faster and adapts more quickly to new tasks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/FBJIZR2P/Lee et al_2019_Efficient Exploration via State Marginal Matching.pdf;/Users/vitay/Documents/Zotero/storage/X6BFVRQK/1906.html}
}

@unpublished{Lee2019a,
  title = {{{IKEA Furniture Assembly Environment}} for {{Long-Horizon Complex Manipulation Tasks}}},
  author = {Lee, Youngwoon and Hu, Edward S. and Yang, Zhengyu and Yin, Alex and Lim, Joseph J.},
  date = {2019-11-17},
  eprint = {1911.07246},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1911.07246},
  urldate = {2019-11-24},
  abstract = {The IKEA Furniture Assembly Environment is one of the first benchmarks for testing and accelerating the automation of complex manipulation tasks. The environment is designed to advance reinforcement learning from simple toy tasks to complex tasks requiring both long-term planning and sophisticated low-level control. Our environment supports over 80 different furniture models, Sawyer and Baxter robot simulation, and domain randomization. The IKEA Furniture Assembly Environment is a testbed for methods aiming to solve complex manipulation tasks. The environment is publicly available at https://clvrai.com/furniture},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/TQIN4IKE/Lee et al_2019_IKEA Furniture Assembly Environment for Long-Horizon Complex Manipulation Tasks.pdf;/Users/vitay/Documents/Zotero/storage/XWHRES34/1911.html}
}

@unpublished{Lee2019b,
  title = {Stochastic {{Latent Actor-Critic}}: {{Deep Reinforcement Learning}} with a {{Latent Variable Model}}},
  shorttitle = {Stochastic {{Latent Actor-Critic}}},
  author = {Lee, Alex X. and Nagabandi, Anusha and Abbeel, Pieter and Levine, Sergey},
  date = {2019-07-01},
  eprint = {1907.00953},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.00953},
  urldate = {2019-07-07},
  abstract = {Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these kinds of observation spaces present a number of challenges in practice, since the policy must now solve two problems: a representation learning problem, and a task learning problem. In this paper, we aim to explicitly learn representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC learns a compact latent representation space using a stochastic sequential latent variable model, and then learns a critic model within this latent space. By learning a critic within a compact state space, SLAC can learn much more efficiently than standard RL methods. The proposed model improves performance substantially over alternative representations as well, such as variational autoencoders. In fact, our experimental evaluation demonstrates that the sample efficiency of our resulting method is comparable to that of model-based RL methods that directly use a similar type of model for control. Furthermore, our method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/95GUUX8P/Lee et al_2019_Stochastic Latent Actor-Critic.pdf;/Users/vitay/Documents/Zotero/storage/NNRYXVUL/1907.html}
}

@online{Lee2024,
  title = {{{SimBa}}: {{Simplicity Bias}} for {{Scaling Up Parameters}} in {{Deep Reinforcement Learning}}},
  shorttitle = {{{SimBa}}},
  author = {Lee, Hojoon and Hwang, Dongyoon and Kim, Donghu and Kim, Hyunseung and Tai, Jun Jet and Subramanian, Kaushik and Wurman, Peter R. and Choo, Jaegul and Stone, Peter and Seno, Takuma},
  date = {2024-10-13},
  eprint = {2410.09754},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.09754},
  url = {http://arxiv.org/abs/2410.09754},
  urldate = {2024-10-15},
  abstract = {Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms-including off-policy, on-policy, and unsupervised methods-is consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/PKEVZC9A/Lee et al. - 2024 - SimBa Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning.pdf}
}

@online{Lehnert2022,
  title = {Reward-{{Predictive Clustering}}},
  author = {Lehnert, Lucas and Frank, Michael J. and Littman, Michael L.},
  date = {2022-11-06},
  eprint = {2211.03281},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.03281},
  urldate = {2022-11-10},
  abstract = {Recent advances in reinforcement-learning research have demonstrated impressive results in building algorithms that can out-perform humans in complex tasks. Nevertheless, creating reinforcement-learning systems that can build abstractions of their experience to accelerate learning in new contexts still remains an active area of research. Previous work showed that reward-predictive state abstractions fulfill this goal, but have only be applied to tabular settings. Here, we provide a clustering algorithm that enables the application of such state abstractions to deep learning settings, providing compressed representations of an agent's inputs that preserve the ability to predict sequences of reward. A convergence theorem and simulations show that the resulting reward-predictive deep network maximally compresses the agent's inputs, significantly speeding up learning in high dimensional visual control tasks. Furthermore, we present different generalization experiments and analyze under which conditions a pre-trained reward-predictive representation network can be re-used without re-training to accelerate learning -- a form of systematic out-of-distribution transfer.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/WMWGTP7B/Lehnert_et_al_2022_Reward-Predictive_Clustering.pdf;/Users/vitay/Documents/Zotero/storage/YUMLCNEF/2211.html}
}

@inproceedings{Levine2013,
  title = {Guided {{Policy Search}}},
  booktitle = {Proceedings of {{Machine Learning Research}}},
  author = {Levine, Sergey and Koltun, Vladlen},
  date = {2013-02},
  pages = {1--9},
  url = {http://proceedings.mlr.press/v28/levine13.html},
  file = {/Users/vitay/Documents/Zotero/storage/SBFLRIEP/Levine_Koltun_2013_Guided Policy Search.pdf}
}

@inproceedings{Levine2016,
  title = {Learning {{Hand-Eye Coordination}} for {{Robotic Grasping}} with {{Deep Learning}} and {{Large-Scale Data Collection}}},
  booktitle = {Proc. {{ISER}}},
  author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
  date = {2016},
  url = {http://arxiv.org/abs/1603.02199},
  abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
  file = {/Users/vitay/Documents/Zotero/storage/7Y4WWFKK/Levine_et_al_2016_Learning_Hand-Eye_Coordination_for_Robotic_Grasping_with_Deep_Learning_and.pdf}
}

@article{Levine2016a,
  title = {End-to-{{End Training}} of {{Deep Visuomotor Policies}}},
  author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  date = {2016},
  journaltitle = {Journal of Machine Learning Research},
  volume = {17},
  number = {39},
  pages = {1--40},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v17/15-522.html},
  urldate = {2022-09-13},
  abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
  file = {/Users/vitay/Documents/Zotero/storage/3BRELQ4V/Levine_et_al_2016_End-to-End_Training_of_Deep_Visuomotor_Policies2.pdf;/Users/vitay/Documents/Zotero/storage/E6L8U8C5/Levine et al. - End-to-End Training of Deep Visuomotor Policies.pdf}
}

@unpublished{Levine2018,
  title = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}: {{Tutorial}} and {{Review}}},
  shorttitle = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}},
  author = {Levine, Sergey},
  date = {2018-05-20},
  eprint = {1805.00909},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.00909},
  urldate = {2020-12-18},
  abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/IGC9UGJI/Levine_2018_Reinforcement_Learning_and_Control_as_Probabilistic_Inference.pdf;/Users/vitay/Documents/Zotero/storage/YZUJYU25/1805.html}
}

@unpublished{Levine2020,
  title = {Offline {{Reinforcement Learning}}: {{Tutorial}}, {{Review}}, and {{Perspectives}} on {{Open Problems}}},
  shorttitle = {Offline {{Reinforcement Learning}}},
  author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  date = {2020-05-04},
  eprint = {2005.01643},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2005.01643},
  urldate = {2020-05-10},
  abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/4X3M8T4Y/Levine_et_al_2020_Offline_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/LFZP76H7/2005.html}
}

@unpublished{Levy2019,
  title = {Learning {{Multi-Level Hierarchies}} with {{Hindsight}}},
  author = {Levy, Andrew and Konidaris, George and Platt, Robert and Saenko, Kate},
  date = {2019-09-03},
  eprint = {1712.00948},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1712.00948},
  urldate = {2020-01-31},
  abstract = {Hierarchical agents have the potential to solve sequential decision making tasks with greater sample efficiency than their non-hierarchical counterparts because hierarchical agents can break down tasks into sets of subtasks that only require short sequences of decisions. In order to realize this potential of faster learning, hierarchical agents need to be able to learn their multiple levels of policies in parallel so these simpler subproblems can be solved simultaneously. Yet, learning multiple levels of policies in parallel is hard because it is inherently unstable: changes in a policy at one level of the hierarchy may cause changes in the transition and reward functions at higher levels in the hierarchy, making it difficult to jointly learn multiple levels of policies. In this paper, we introduce a new Hierarchical Reinforcement Learning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome the instability issues that arise when agents try to jointly learn multiple levels of policies. The main idea behind HAC is to train each level of the hierarchy independently of the lower levels by training each level as if the lower level policies are already optimal. We demonstrate experimentally in both grid world and simulated robotics domains that our approach can significantly accelerate learning relative to other non-hierarchical and hierarchical methods. Indeed, our framework is the first to successfully learn 3-level hierarchies in parallel in tasks with continuous state and action spaces.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/P5Z2HQ3S/Levy_et_al_2019_Learning_Multi-Level_Hierarchies_with_Hindsight.pdf;/Users/vitay/Documents/Zotero/storage/G4BQ6VP4/1712.html}
}

@article{Li2017,
  title = {Deep {{Reinforcement Learning}}: {{An Overview}}},
  author = {Li, Yuxi},
  date = {2017-01},
  url = {http://arxiv.org/abs/1701.07274},
  abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
  file = {/Users/vitay/Documents/Zotero/storage/GKC9ERWM/Li_2017_Deep Reinforcement Learning.pdf}
}

@unpublished{Li2019a,
  title = {Sub-Policy {{Adaptation}} for {{Hierarchical Reinforcement Learning}}},
  author = {Li, Alexander C. and Florensa, Carlos and Clavera, Ignasi and Abbeel, Pieter},
  date = {2019-06-13},
  eprint = {1906.05862},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.05862},
  urldate = {2019-11-01},
  abstract = {Hierarchical Reinforcement Learning is a promising approach to long-horizon decision-making problems with sparse rewards. Unfortunately, most methods still decouple the lower-level skill acquisition process and the training of a higher level that controls the skills in a new task. Treating the skills as fixed can lead to significant sub-optimality in the transfer setting. In this work, we propose a novel algorithm to discover a set of skills, and continuously adapt them along with the higher level even when training on a new task. Our main contributions are two-fold. First, we derive a new hierarchical policy gradient, as well as an unbiased latent-dependent baseline. We introduce Hierarchical Proximal Policy Optimization (HiPPO), an on-policy method to efficiently train all levels of the hierarchy simultaneously. Second, we propose a method of training time-abstractions that improves the robustness of the obtained skills to environment changes. Code and results are available at sites.google.com/view/hippo-rl .},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/DBSZTZ33/Li et al_2019_Sub-policy Adaptation for Hierarchical Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/E2I3UGZL/1906.html}
}

@unpublished{Li2020,
  title = {Generalized {{Hindsight}} for {{Reinforcement Learning}}},
  author = {Li, Alexander C. and Pinto, Lerrel and Abbeel, Pieter},
  date = {2020-02-26},
  eprint = {2002.11708},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.11708},
  urldate = {2020-03-03},
  abstract = {One of the key reasons for the high sample complexity in reinforcement learning (RL) is the inability to transfer knowledge from one task to another. In standard multi-task RL settings, low-reward data collected while trying to solve one task provides little to no signal for solving that particular task and is hence effectively wasted. However, we argue that this data, which is uninformative for one task, is likely a rich source of information for other tasks. To leverage this insight and efficiently reuse data, we present Generalized Hindsight: an approximate inverse reinforcement learning technique for relabeling behaviors with the right tasks. Intuitively, given a behavior generated under one task, Generalized Hindsight returns a different task that the behavior is better suited for. Then, the behavior is relabeled with this new task before being used by an off-policy RL optimizer. Compared to standard relabeling techniques, Generalized Hindsight provides a substantially more efficient reuse of samples, which we empirically demonstrate on a suite of multi-task navigation and manipulation tasks. Videos and code can be accessed here: https://sites.google.com/view/generalized-hindsight.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/N6IU7QEX/Li_et_al_2020_Generalized_Hindsight_for_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/2TCICVQI/2002.html}
}

@unpublished{Li2020a,
  title = {Visual {{Grounding}} of {{Learned Physical Models}}},
  author = {Li, Yunzhu and Lin, Toru and Yi, Kexin and Bear, Daniel and Yamins, Daniel L. K. and Wu, Jiajun and Tenenbaum, Joshua B. and Torralba, Antonio},
  date = {2020-04-28},
  eprint = {2004.13664},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2004.13664},
  urldate = {2020-05-02},
  abstract = {Humans intuitively recognize objects' physical properties and predict their motion, even when the objects are engaged in complicated interactions. The abilities to perform physical reasoning and to adapt to new environments, while intrinsic to humans, remain challenging to state-of-the-art computational models. In this work, we present a neural model that simultaneously reasons about physics and make future predictions based on visual and dynamics priors. The visual prior predicts a particle-based representation of the system from visual observations. An inference module operates on those particles, predicting and refining estimates of particle locations, object states, and physical parameters, subject to the constraints imposed by the dynamics prior, which we refer to as visual grounding. We demonstrate the effectiveness of our method in environments involving rigid objects, deformable materials, and fluids. Experiments show that our model can infer the physical properties within a few observations, which allows the model to quickly adapt to unseen scenarios and make accurate predictions into the future.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/SLE33IZZ/Li_et_al_2020_Visual_Grounding_of_Learned_Physical_Models.pdf;/Users/vitay/Documents/Zotero/storage/TISK7CAW/2004.html}
}

@article{Li2022a,
  title = {Missile Guidance with Assisted Deep Reinforcement Learning for Head-on Interception of Maneuvering Target},
  author = {Li, Weifan and Zhu, Yuanheng and Zhao, Dongbin},
  date = {2022-04-01},
  journaltitle = {Complex \& Intelligent Systems},
  shortjournal = {Complex Intell. Syst.},
  volume = {8},
  number = {2},
  pages = {1205--1216},
  issn = {2198-6053},
  doi = {10.1007/s40747-021-00577-6},
  url = {https://doi.org/10.1007/s40747-021-00577-6},
  urldate = {2024-10-14},
  abstract = {In missile guidance, pursuit performance is seriously degraded due to the uncertainty and randomness in target maneuverability, detection delay, and environmental noise. In many methods, accurately estimating the acceleration of the target or the time-to-go is needed to intercept the maneuvering target, which is hard in an environment with uncertainty. In this paper, we propose an assisted deep reinforcement learning (ARL) algorithm to optimize the neural network-based missile guidance controller for head-on interception. Based on the relative velocity, distance, and angle, ARL can control the missile to intercept the maneuvering target and achieve large terminal intercept angle. To reduce the influence of environmental uncertainty, ARL predicts the target’s acceleration as an auxiliary supervised task. The supervised learning task improves the ability of the agent to extract information from observations. To exploit the agent’s good trajectories, ARL presents the Gaussian self-imitation learning to make the mean of action distribution approach the agent’s good actions. Compared with vanilla self-imitation learning, Gaussian self-imitation learning improves the exploration in continuous control. Simulation results validate that ARL outperforms traditional methods and proximal policy optimization algorithm with higher hit rate and larger terminal intercept angle in the simulation environment with noise, delay, and maneuverable target.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/C3TUQFGN/Li et al. - 2022 - Missile guidance with assisted deep reinforcement learning for head-on interception of maneuvering t.pdf}
}

@online{Li2023,
  title = {A {{Survey}} on {{Transformers}} in {{Reinforcement Learning}}},
  author = {Li, Wenzhe and Luo, Hao and Lin, Zichuan and Zhang, Chongjie and Lu, Zongqing and Ye, Deheng},
  date = {2023-09-20},
  eprint = {2301.03044},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2301.03044},
  url = {http://arxiv.org/abs/2301.03044},
  urldate = {2024-11-11},
  abstract = {Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/MH48RNMY/Li et al. - 2023 - A Survey on Transformers in Reinforcement Learning.pdf}
}

@unpublished{Liang2019,
  title = {Federated {{Transfer Reinforcement Learning}} for {{Autonomous Driving}}},
  author = {Liang, Xinle and Liu, Yang and Chen, Tianjian and Liu, Ming and Yang, Qiang},
  date = {2019-10-14},
  eprint = {1910.06001},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1910.06001},
  urldate = {2019-10-19},
  abstract = {Reinforcement learning (RL) is widely used in autonomous driving tasks and training RL models typically involves in a multi-step process: pre-training RL models on simulators, uploading the pre-trained model to real-life robots, and fine-tuning the weight parameters on robot vehicles. This sequential process is extremely time-consuming and more importantly, knowledge from the fine-tuned model stays local and can not be re-used or leveraged collaboratively. To tackle this problem, we present an online federated RL transfer process for real-time knowledge extraction where all the participant agents make corresponding actions with the knowledge learned by others, even when they are acting in very different environments. To validate the effectiveness of the proposed approach, we constructed a real-life collision avoidance system with Microsoft Airsim simulator and NVIDIA JetsonTX2 car agents, which cooperatively learn from scratch to avoid collisions in indoor environment with obstacle objects. We demonstrate that with the proposed framework, the simulator car agents can transfer knowledge to the RC cars in real-time, with 27\% increase in the average distance with obstacles and 42\% decrease in the collision counts.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/ZEGCFRPR/Liang et al_2019_Federated Transfer Reinforcement Learning for Autonomous Driving.pdf;/Users/vitay/Documents/Zotero/storage/WQ9YIVWX/1910.html}
}

@unpublished{Libardi2020,
  title = {Guided {{Exploration}} with {{Proximal Policy Optimization}} Using a {{Single Demonstration}}},
  author = {Libardi, Gabriele and De Fabritiis, Gianni},
  date = {2020-07-07},
  eprint = {2007.03328},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2007.03328},
  urldate = {2020-07-11},
  abstract = {Solving sparse reward tasks through exploration is one of the major challenges in deep reinforcement learning, especially in three-dimensional, partially-observable environments. Critically, the algorithm proposed in this article uses a single human demonstration to solve hard-exploration problems. We train an agent on a combination of demonstrations and own experience to solve problems with variable initial conditions. We adapt this idea and integrate it with the proximal policy optimization (PPO). The agent is able to increase its performance and to tackle harder problems by replaying its own past trajectories prioritizing them based on the obtained reward and the maximum value of the trajectory. We compare different variations of this algorithm to behavioral cloning on a set of hard-exploration tasks in the Animal-AI Olympics environment. To the best of our knowledge, learning a task in a three-dimensional environment with comparable difficulty has never been considered before using only one human demonstration.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/vitay/Documents/Zotero/storage/Y8WH42C9/Libardi_De_Fabritiis_2020_Guided_Exploration_with_Proximal_Policy_Optimization_using_a_Single.pdf;/Users/vitay/Documents/Zotero/storage/6W37B8SS/2007.html}
}

@article{Lillicrap2015,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  date = {2015},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1509.02971},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  file = {/Users/vitay/Documents/Zotero/storage/SNGR3SWB/Lillicrap et al_2015_Continuous control with deep reinforcement learning.pdf}
}

@online{Lin2023,
  title = {Learning to {{Model}} the {{World}} with {{Language}}},
  author = {Lin, Jessy and Du, Yuqing and Watkins, Olivia and Hafner, Danijar and Abbeel, Pieter and Klein, Dan and Dragan, Anca},
  date = {2023-07-31},
  eprint = {2308.01399},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2308.01399},
  urldate = {2023-08-08},
  abstract = {To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model to predict future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to predict future language, video, and rewards. In addition to learning from online interaction in an environment, Dynalang can be pretrained on datasets of text, video, or both without actions or rewards. From using language hints in grid worlds to navigating photorealistic scans of homes, Dynalang utilizes diverse types of language to improve task performance, including environment descriptions, game rules, and instructions.},
  langid = {english},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/NHCNSZDV/Lin et al. - 2023 - Learning to Model the World with Language.pdf}
}

@unpublished{Lindner2021,
  title = {Learning {{What To Do}} by {{Simulating}} the {{Past}}},
  author = {Lindner, David and Shah, Rohin and Abbeel, Pieter and Dragan, Anca},
  date = {2021-04-08},
  eprint = {2104.03946},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2104.03946},
  urldate = {2021-05-01},
  abstract = {Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/QZQS4G4M/Lindner_et_al_2021_Learning_What_To_Do_by_Simulating_the_Past.pdf;/Users/vitay/Documents/Zotero/storage/SU9A92N8/2104.html}
}

@article{Liu2018,
  title = {Control with {{Distributed Deep Reinforcement Learning}}: {{Learn}} a {{Better Policy}}},
  author = {Liu, Qihao and Liu, Xiaofeng and Cai, Guoping},
  date = {2018},
  pages = {17},
  abstract = {Distributed approach is a very effective method to improve training efficiency of reinforcement learning. In this paper, we propose a new heuristic distributed architecture for deep reinforcement learning (DRL) algorithm, in which a PSO based network update mechanism is adopted to speed up learning an optimal policy besides using multiple agents for parallel training. In this mechanism, the update of neural network of each agent is not only according to the training result of itself, but also affected by the optimal neural network of all agents. In order to verify the effectiveness of the proposed method, the proposed architecture is implemented on the Deep Q-Network algorithm (DQN) and the Deep Deterministic Policy Gradient algorithm (DDPG) to train several typical control problems. The training results show that the proposed method is effective.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/L49BSBIV/Liu et al_2018_Control with Distributed Deep Reinforcement Learning.pdf}
}

@article{Liu2019a,
  title = {{{ReinforcementDriving}}: {{Exploring Trajectories}} and {{Navigation}} for {{Autonomous Vehicles}}},
  shorttitle = {{{ReinforcementDriving}}},
  author = {Liu, Meng and Zhao, Fei and Niu, Jianwei and Liu, Yu},
  date = {2019},
  journaltitle = {IEEE Transactions on Intelligent Transportation Systems},
  pages = {1--13},
  issn = {1558-0016},
  doi = {10.1109/TITS.2019.2960872},
  abstract = {Autonomous vehicles need to solve the road keeping problem and the existing solutions based on reinforcement learning are mainly implemented in the simulators. The key of transferring the well-trained models to the real world is bridging the gaps between the simulator scenarios and the real scenarios. In this paper, we propose a method called ReinforcementDriving which explores navigation skills and trajectories from simulator for full-sized road keeping. Based on the real scenario, a driving simulator is firstly established to train an intelligent driving agent. The well-trained ReinforcementDriving agent is evaluated in a real-world scenario. We compare our work with human driving, optimal control-based tracking methods and other reinforcement learning-based lane following methods. The results demonstrate that the ReinforcementDriving system can effectively achieve lane keeping in a realistic scenario with satisfactory running time and lateral accuracy.},
  keywords = {autonomous driving.,DDPG,lane keeping,navigation,Reinforcement learning,trajectory exploration},
  file = {/Users/vitay/Documents/Zotero/storage/WIQLKLYS/Liu_et_al_2019_ReinforcementDriving.pdf;/Users/vitay/Documents/Zotero/storage/QQE5MI7P/8946763.html}
}

@inproceedings{Liu2022a,
  title = {Safe {{Offline Reinforcement Learning Through Hierarchical Policies}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Liu, Shaofan and Sun, Shiliang},
  editor = {Gama, João and Li, Tianrui and Yu, Yang and Chen, Enhong and Zheng, Yu and Teng, Fei},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {380--391},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-05936-0_30},
  abstract = {Recently, offline reinforcement learning has gained increasing attention. However, the safety of offline reinforcement learning has been ignored. It poses a significant challenge to learn a safe and high-performance policy from a fixed dataset that contains unsafe or unexpected state-action pairs without interacting with the environment. Since the unsafe state-action pairs are usually sparse in the behavior data collected by humans, it is difficult to effectively model information about unsafe behaviors. This paper utilized the hierarchical reinforcement learning framework to alleviate the sparsity issue by modeling unsafe behaviors with hierarchical policies. Specifically, a high-level policy determines a prospective state, and a low-level policy takes action to reach the specified goal state. The training objective of the high-level policy is to improve the expected reward that the low-level policy collects when it moves toward the goal state and reduce the number of unsafe actions. We further develop data processing methods to provide training data for the high-level policy and the low-level policy. Evaluation experiments about performance and safety are conducted in simulation environments that return the rewards and unsafe costs obtained by agents during the interaction. Experimental results demonstrate that the proposed algorithm can choose safe actions while maintaining high performance.},
  isbn = {978-3-031-05936-0},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/8QCQ2K35/Liu_Sun_2022_Safe_Offline_Reinforcement_Learning_Through_Hierarchical_Policies.pdf}
}

@online{Liu2022b,
  title = {Hierarchical Clustering Optimizes the Tradeoff between Compositionality and Expressivity of Task Structures for Flexible Reinforcement Learning},
  author = {Liu, Rex G. and Frank, Michael J.},
  date = {2022-05-03},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2021.07.20.453122},
  doi = {10.1101/2021.07.20.453122},
  url = {https://www.biorxiv.org/content/10.1101/2021.07.20.453122v3},
  urldate = {2022-08-09},
  abstract = {A hallmark of human intelligence, but challenging for reinforcement learning (RL) agents, is the ability to compositionally generalise, that is, to recompose familiar knowledge components in novel ways to solve new problems. For instance, when navigating in a city, one needs to know the location of the destination and how to operate a vehicle to get there, whether it be pedalling a bike or operating a car. In RL, these correspond to the reward function and transition function, respectively. To compositionally generalize, these two components need to be transferable independently of each other: multiple modes of transport can reach the same goal, and any given mode can be used to reach multiple destinations. Yet there are also instances where it can be helpful to learn and transfer entire structures, jointly representing goals and transitions, particularly whenever these recur in natural tasks (e.g., given a suggestion to get ice cream, one might prefer to bike, even in new towns). Prior theoretical work has explored how, in model-based RL, agents can learn and generalize task components (transition and reward functions). But a satisfactory account for how a single agent can simultaneously satisfy the two competing demands is still lacking. Here, we propose a hierarchical RL agent that learns and transfers individual task components as well as entire structures (particular compositions of components) by inferring both through a non-parametric Bayesian model of the task. It maintains a factorised representation of task components through a hierarchical Dirichlet process, but it also represents different possible covariances between these components through a standard Dirichlet process. We validate our approach on a variety of navigation tasks covering a wide range of statistical correlations between task components and show that it can also improve generalisation and transfer in more complex, hierarchical tasks with goal/subgoal structures. Finally, we end with a discussion of our work including how this clustering algorithm could conceivably be implemented by cortico-striatal gating circuits in the brain.},
  langid = {english},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/6BVKW7BI/Liu_Frank_2022_Hierarchical_clustering_optimizes_the_tradeoff_between_compositionality_and.pdf;/Users/vitay/Documents/Zotero/storage/W2M5GFMH/2021.07.20.html}
}

@unpublished{Lockel2020,
  title = {A {{Probabilistic Framework}} for {{Imitating Human Race Driver Behavior}}},
  author = {Löckel, Stefan and Peters, Jan and family=Vliet, given=Peter, prefix=van, useprefix=true},
  date = {2020-01-22},
  eprint = {2001.08255},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2001.08255},
  urldate = {2020-01-28},
  abstract = {Understanding and modeling human driver behavior is crucial for advanced vehicle development. However, unique driving styles, inconsistent behavior, and complex decision processes render it a challenging task, and existing approaches often lack variability or robustness. To approach this problem, we propose Probabilistic Modeling of Driver behavior (ProMoD), a modular framework which splits the task of driver behavior modeling into multiple modules. A global target trajectory distribution is learned with Probabilistic Movement Primitives, clothoids are utilized for local path generation, and the corresponding choice of actions is performed by a neural network. Experiments in a simulated car racing setting show considerable advantages in imitation accuracy and robustness compared to other imitation learning algorithms. The modular architecture of the proposed framework facilitates straightforward extensibility in driving line adaptation and sequencing of multiple movement primitives for future research.},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,I.2.1,I.2.9,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/NZDJ3CNX/Löckel_et_al_2020_A_Probabilistic_Framework_for_Imitating_Human_Race_Driver_Behavior.pdf;/Users/vitay/Documents/Zotero/storage/6YK94HDP/2001.html}
}

@inproceedings{Lotzsch2017,
  title = {Training a Deep Policy Gradient-Based Neural Network with Asynchronous Learners on a Simulated Robotic Problem},
  booktitle = {{{INFORMATIK}} 2017. {{Gesellschaft}} Für {{Informatik}}},
  author = {Lötzsch, Winfried and Vitay, Julien and Hamker, Fred H.},
  editor = {Eibl, Maximilian and Gaedke, Martin},
  date = {2017},
  pages = {2143--2154},
  publisher = {Gesellschaft für Informatik, Bonn},
  url = {https://dl.gi.de/handle/20.500.12116/3986},
  abstract = {Recent advances in deep reinforcement learning methods have attracted a lot of attention, because of their ability to use raw signals such as video streams as inputs, instead of pre-processed state variables. However, the most popular methods (value-based methods, e.g. deep Q-networks) focus on discrete action spaces (e.g. the left/right buttons), while realistic robotic applications usually require a continuous action space (for example the joint space). Policy gradient methods, such as stochastic policy gradient or deep deterministic policy gradient, propose to overcome this problem by allowing continuous action spaces. Despite their promises, they suffer from long training times as they need huge numbers of interactions to converge. In this paper, we investigate in how far a recent asynchronously parallel actor-critic approach, initially proposed to speed up discrete RL algorithms, could be used for the continuous control of robotic arms. We demonstrate the capabilities of this end-to-end learning algorithm on a simulated 2 degrees-of-freedom robotic arm and discuss its applications to more realistic scenarios.},
  isbn = {978-3-88579-669-5}
}

@unpublished{Lu2019,
  title = {Adaptive {{Online Planning}} for {{Continual Lifelong Learning}}},
  author = {Lu, Kevin and Mordatch, Igor and Abbeel, Pieter},
  date = {2019-12-02},
  eprint = {1912.01188},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.01188},
  urldate = {2019-12-08},
  abstract = {We study learning control in an online lifelong learning scenario, where mistakes can compound catastrophically into the future and the underlying dynamics of the environment may change. Traditional model-free policy learning methods have achieved successes in difficult tasks due to their broad flexibility, and capably condense broad experiences into compact networks, but struggle in this setting, as they can activate failure modes early in their lifetimes which are difficult to recover from and face performance degradation as dynamics change. On the other hand, model-based planning methods learn and adapt quickly, but require prohibitive levels of computational resources. Under constrained computation limits, the agent must allocate its resources wisely, which requires the agent to understand both its own performance and the current state of the environment: knowing that its mastery over control in the current dynamics is poor, the agent should dedicate more time to planning. We present a new algorithm, Adaptive Online Planning (AOP), that achieves strong performance in this setting by combining model-based planning with model-free learning. By measuring the performance of the planner and the uncertainty of the model-free components, AOP is able to call upon more extensive planning only when necessary, leading to reduced computation times. We show that AOP gracefully deals with novel situations, adapting behaviors and policies effectively in the face of unpredictable changes in the world -- challenges that a continual learning agent naturally faces over an extended lifetime -- even when traditional reinforcement learning methods fail.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/6AEFKUQN/Lu_et_al_2019_Adaptive_Online_Planning_for_Continual_Lifelong_Learning.pdf;/Users/vitay/Documents/Zotero/storage/5W3TV3L8/1912.html}
}

@unpublished{Lu2019a,
  title = {Predictive {{Coding}} for {{Boosting Deep Reinforcement Learning}} with {{Sparse Rewards}}},
  author = {Lu, Xingyu and Tiomkin, Stas and Abbeel, Pieter},
  date = {2019-12-20},
  eprint = {1912.13414},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.13414},
  urldate = {2020-01-04},
  abstract = {While recent progress in deep reinforcement learning has enabled robots to learn complex behaviors, tasks with long horizons and sparse rewards remain an ongoing challenge. In this work, we propose an effective reward shaping method through predictive coding to tackle sparse reward problems. By learning predictive representations offline and using these representations for reward shaping, we gain access to reward signals that understand the structure and dynamics of the environment. In particular, our method achieves better learning by providing reward signals that 1) understand environment dynamics 2) emphasize on features most useful for learning 3) resist noise in learned representations through reward accumulation. We demonstrate the usefulness of this approach in different domains ranging from robotic manipulation to navigation, and we show that reward signals produced through predictive coding are as effective for learning as hand-crafted rewards.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/IXJDGMZN/Lu_et_al_2019_Predictive_Coding_for_Boosting_Deep_Reinforcement_Learning_with_Sparse_Rewards.pdf;/Users/vitay/Documents/Zotero/storage/GXFQBVP5/1912.html}
}

@online{Luo2022,
  title = {Controlling {{Commercial Cooling Systems Using Reinforcement Learning}}},
  author = {Luo, Jerry and Paduraru, Cosmin and Voicu, Octavian and Chervonyi, Yuri and Munns, Scott and Li, Jerry and Qian, Crystal and Dutta, Praneet and Davis, Jared Quincy and Wu, Ningjia and Yang, Xingwei and Chang, Chu-Ming and Li, Ted and Rose, Rob and Fan, Mingyan and Nakhost, Hootan and Liu, Tinglin and Kirkman, Brian and Altamura, Frank and Cline, Lee and Tonker, Patrick and Gouker, Joel and Uden, Dave and Bryan, Warren Buddy and Law, Jason and Fatiha, Deeni and Satra, Neil and Rothenberg, Juliet and Waraich, Mandeep and Carlin, Molly and Tallapaka, Satish and Witherspoon, Sims and Parish, David and Dolan, Peter and Zhao, Chenyu and Mankowitz, Daniel J.},
  date = {2022-12-14},
  eprint = {2211.07357},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2211.07357},
  url = {http://arxiv.org/abs/2211.07357},
  urldate = {2024-10-14},
  abstract = {This paper is a technical overview of DeepMind and Google's recent work on reinforcement learning for controlling commercial cooling systems. Building on expertise that began with cooling Google's data centers more efficiently, we recently conducted live experiments on two real-world facilities in partnership with Trane Technologies, a building management system provider. These live experiments had a variety of challenges in areas such as evaluation, learning from offline data, and constraint satisfaction. Our paper describes these challenges in the hope that awareness of them will benefit future applied RL work. We also describe the way we adapted our RL system to deal with these challenges, resulting in energy savings of approximately 9\% and 13\% respectively at the two live experiment sites.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/G7NGY65L/Luo et al. - 2022 - Controlling Commercial Cooling Systems Using Reinforcement Learning.pdf}
}

@article{Luong2019,
  title = {Applications of {{Deep Reinforcement Learning}} in {{Communications}} and {{Networking}}: {{A Survey}}},
  shorttitle = {Applications of {{Deep Reinforcement Learning}} in {{Communications}} and {{Networking}}},
  author = {Luong, Nguyen Cong and Hoang, Dinh Thai and Gong, Shimin and Niyato, Dusit and Wang, Ping and Liang, Ying-Chang and Kim, Dong In},
  date = {2019},
  journaltitle = {IEEE Communications Surveys \& Tutorials},
  volume = {21},
  number = {4},
  pages = {3133--3174},
  issn = {1553-877X},
  doi = {10.1109/COMST.2019.2916583},
  abstract = {This paper presents a comprehensive literature review on applications of deep reinforcement learning (DRL) in communications and networking. Modern networks, e.g., Internet of Things (IoT) and unmanned aerial vehicle (UAV) networks, become more decentralized and autonomous. In such networks, network entities need to make decisions locally to maximize the network performance under uncertainty of network environment. Reinforcement learning has been efficiently used to enable the network entities to obtain the optimal policy including, e.g., decisions or actions, given their states when the state and action spaces are small. However, in complex and large-scale networks, the state and action spaces are usually large, and the reinforcement learning may not be able to find the optimal policy in reasonable time. Therefore, DRL, a combination of reinforcement learning with deep learning, has been developed to overcome the shortcomings. In this survey, we first give a tutorial of DRL from fundamental concepts to advanced models. Then, we review DRL approaches proposed to address emerging issues in communications and networking. The issues include dynamic network access, data rate control, wireless caching, data offloading, network security, and connectivity preservation which are all important to next generation networks, such as 5G and beyond. Furthermore, we present applications of DRL for traffic routing, resource sharing, and data collection. Finally, we highlight important challenges, open issues, and future research directions of applying DRL.},
  eventtitle = {{{IEEE Communications Surveys}} \& {{Tutorials}}},
  file = {/Users/vitay/Documents/Zotero/storage/6629QNHM/Luong_et_al_2019_Applications_of_Deep_Reinforcement_Learning_in_Communications_and_Networking.pdf}
}

@inproceedings{Lynch2019,
  title = {Learning Latent Plans from Play Data},
  booktitle = {E {{Task-Agnostic Reinforcement Learning Workshop}}},
  author = {Lynch, Corey and Khansari, Mohi and Xiao, Ted and Kumar, Vikash and Tompson, Jonathan and Levine, Sergey and Sermanet, Pierre},
  date = {2019},
  pages = {12},
  url = {https://tarl2019.github.io/assets/papers/lynch2019learning.pdf},
  eventtitle = {{{ICLR}} 2019},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/YGR5FSUH/Lynch_et_al_2019_Learning_latent_plans_from_play_data.pdf}
}

@article{Ma2019,
  title = {Continuous Control of a Polymerization System with Deep Reinforcement Learning},
  author = {Ma, Yan and Zhu, Wenbo and Benton, Michael G. and Romagnoli, José},
  date = {2019-03-01},
  journaltitle = {Journal of Process Control},
  shortjournal = {Journal of Process Control},
  volume = {75},
  pages = {40--47},
  issn = {0959-1524},
  doi = {10.1016/j.jprocont.2018.11.004},
  url = {http://www.sciencedirect.com/science/article/pii/S0959152418304876},
  urldate = {2019-01-22},
  abstract = {Reinforcement learning is a branch of machine learning, where the machines gradually learn control behaviors via self-exploration of the environment. In this paper, we present a controller using deep reinforcement learning (DRL) with Deep Deterministic Policy Gradient (DDPG) for a non-linear semi-batch polymerization reaction. Several adaptations to apply DRL for chemical process control are addressed in this paper including the Markov state assumption, action boundaries and reward definition. This work illustrates that a DRL controller is capable of handling complicated control tasks for chemical processes with multiple inputs, non-linearity, large time delay and noise tolerance. The application of this AI-based framework, using DRL, is a promising direction in the field of chemical process control towards the goal of smart manufacturing.},
  keywords = {Deep reinforcement learning,Polymerization,Process control},
  file = {/Users/vitay/Documents/Zotero/storage/QW5C6WIB/Ma et al_2019_Continuous control of a polymerization system with deep reinforcement learning.pdf;/Users/vitay/Documents/Zotero/storage/WTMD6EF4/S0959152418304876.html}
}

@unpublished{Machado2018,
  title = {Count-{{Based Exploration}} with the {{Successor Representation}}},
  author = {Machado, Marlos C. and Bellemare, Marc G. and Bowling, Michael},
  date = {2018-07-30},
  eprint = {1807.11622},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1807.11622},
  urldate = {2019-02-23},
  abstract = {In this paper we introduce a simple approach for exploration in reinforcement learning (RL) that allows us to develop theoretically justified algorithms in the tabular case but that is also extendable to settings where function approximation is required. Our approach is based on the successor representation (SR), which was originally introduced as a representation defining state generalization by the similarity of successor states. Here we show that the norm of the SR, while it is being learned, can be used as a reward bonus to incentivize exploration. In order to better understand this transient behavior of the norm of the SR we introduce the substochastic successor representation (SSR) and we show that it implicitly counts the number of times each state (or feature) has been observed. We use this result to introduce an algorithm that performs as well as some theoretically sample-efficient approaches. Finally, we extend these ideas to a deep RL algorithm and show that it achieves state-of-the-art performance in Atari 2600 games.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/8GFT2U2B/Machado et al_2018_Count-Based Exploration with the Successor Representation.pdf;/Users/vitay/Documents/Zotero/storage/Z7K3NBPN/1807.html}
}

@online{Madeka2022,
  title = {Deep {{Inventory Management}}},
  author = {Madeka, Dhruv and Torkkola, Kari and Eisenach, Carson and Luo, Anna and Foster, Dean P. and Kakade, Sham M.},
  date = {2022-11-28},
  eprint = {2210.03137},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2210.03137},
  url = {http://arxiv.org/abs/2210.03137},
  urldate = {2024-10-14},
  abstract = {This work provides a Deep Reinforcement Learning approach to solving a periodic review inventory control system with stochastic vendor lead times, lost sales, correlated demand, and price matching. While this dynamic program has historically been considered intractable, our results show that several policy learning approaches are competitive with or outperform classical methods. In order to train these algorithms, we develop novel techniques to convert historical data into a simulator. On the theoretical side, we present learnability results on a subclass of inventory control problems, where we provide a provable reduction of the reinforcement learning problem to that of supervised learning. On the algorithmic side, we present a model-based reinforcement learning procedure (Direct Backprop) to solve the periodic review inventory control problem by constructing a differentiable simulator. Under a variety of metrics Direct Backprop outperforms model-free RL and newsvendor baselines, in both simulations and real-world deployments.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/AJA86K8M/Madeka et al. - 2022 - Deep Inventory Management.pdf}
}

@online{Malibari2023,
  title = {Systematic {{Review}} on {{Reinforcement Learning}} in the {{Field}} of {{Fintech}}},
  author = {Malibari, Nadeem and Katib, Iyad and Mehmood, Rashid},
  date = {2023-04-29},
  eprint = {2305.07466},
  eprinttype = {arXiv},
  eprintclass = {cs, q-fin},
  doi = {10.48550/arXiv.2305.07466},
  url = {http://arxiv.org/abs/2305.07466},
  urldate = {2024-10-14},
  abstract = {Applications of Reinforcement Learning in the Finance Technology (Fintech) have acquired a lot of admiration lately. Undoubtedly Reinforcement Learning, through its vast competence and proficiency, has aided remarkable results in the field of Fintech. The objective of this systematic survey is to perform an exploratory study on a correlation between reinforcement learning and Fintech to highlight the prediction accuracy, complexity, scalability, risks, profitability and performance. Major uses of reinforcement learning in finance or Fintech include portfolio optimization, credit risk reduction, investment capital management, profit maximization, effective recommendation systems, and better price setting strategies. Several studies have addressed the actual contribution of reinforcement learning to the performance of financial institutions. The latest studies included in this survey are publications from 2018 onward. The survey is conducted using PRISMA technique which focuses on the reporting of reviews and is based on a checklist and four-phase flow diagram. The conducted survey indicates that the performance of RL-based strategies in Fintech fields proves to perform considerably better than other state-of-the-art algorithms. The present work discusses the use of reinforcement learning algorithms in diverse decision-making challenges in Fintech and concludes that the organizations dealing with finance can benefit greatly from Robo-advising, smart order channelling, market making, hedging and options pricing, portfolio optimization, and optimal execution.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/X3FB3JQM/Malibari et al. - 2023 - Systematic Review on Reinforcement Learning in the Field of Fintech.pdf}
}

@inproceedings{Mao2016,
  title = {Resource {{Management}} with {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 15th {{ACM Workshop}} on {{Hot Topics}} in {{Networks}}  - {{HotNets}} '16},
  author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
  date = {2016},
  pages = {50--56},
  publisher = {ACM Press},
  location = {Atlanta, GA, USA},
  doi = {10.1145/3005745.3005750},
  url = {http://dl.acm.org/citation.cfm?doid=3005745.3005750},
  urldate = {2019-12-11},
  abstract = {Resource management problems in systems and networking often manifest as difficult online decision making tasks where appropriate solutions depend on understanding the workload and environment. Inspired by recent advances in deep reinforcement learning for AI problems, we consider building systems that learn to manage resources directly from experience. We present DeepRM, an example solution that translates the problem of packing tasks with multiple resource demands into a learning problem. Our initial results show that DeepRM performs comparably to state-ofthe-art heuristics, adapts to different conditions, converges quickly, and learns strategies that are sensible in hindsight.},
  eventtitle = {The 15th {{ACM Workshop}}},
  isbn = {978-1-4503-4661-0},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/N3HRJYJ6/Mao_et_al_2016_Resource_Management_with_Deep_Reinforcement_Learning.pdf}
}

@inproceedings{Mathieu2021,
  title = {{{StarCraft II Unplugged}}: {{Large Scale Offline Reinforcement Learning}}},
  shorttitle = {{{StarCraft II Unplugged}}},
  author = {Mathieu, Michael and Ozair, Sherjil and Srinivasan, Srivatsan and Gulcehre, Caglar and Zhang, Shangtong and Jiang, Ray and Paine, Tom Le and Zolna, Konrad and Powell, Richard and Schrittwieser, Julian and Choi, David and Georgiev, Petko and Toyama, Daniel Kenji and Huang, Aja and Ring, Roman and Babuschkin, Igor and Ewalds, Timo and Bordbar, Mahyar and Henderson, Sarah and Colmenarejo, Sergio Gómez and family=Oord, given=Aaron, prefix=van den, useprefix=false and Czarnecki, Wojciech M. and family=Freitas, given=Nando, prefix=de, useprefix=false and Vinyals, Oriol},
  date = {2021-10-12},
  url = {https://openreview.net/forum?id=Np8Pumfoty},
  urldate = {2022-03-03},
  abstract = {We introduce a benchmark for offline RL on StarCraft II, and propose baselines and evaluation protocols.},
  eventtitle = {Deep {{RL Workshop NeurIPS}} 2021},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/TQJPALX5/Mathieu_et_al_2021_StarCraft_II_Unplugged.pdf;/Users/vitay/Documents/Zotero/storage/NSHD9WKP/forum.html}
}

@inproceedings{McNeill2019,
  title = {A {{Comparison}} of {{Contextual Bandit Approaches}} to {{Human-in-the-Loop Robot Task Completion}} with {{Infrequent Feedback}}},
  booktitle = {2019 {{IEEE}} 31st {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  author = {McNeill, M. and Lyons, D.},
  date = {2019-11},
  pages = {117--124},
  issn = {2375-0197},
  doi = {10.1109/ICTAI.2019.00025},
  abstract = {Artificially intelligent assistive agents are playing an increased role in our work and homes. In contrast with currently predominant conversational agents, whose intelligence derives from dialogue trees and external modules, a fully autonomous domestic or workplace robot must carry out more complex reasoning. Such a robot must make good decisions as soon as possible, learn from experience, respond to feedback, and rely on feedback only as much as necessary. In this research, we narrow the focus of a hypothetical robot assistant to a room-tidying task in a simulated domestic environment. Given an item, the robot chooses where to put it among many destinations, then optionally receives feedback from a human operator. We frame the problem as a contextual bandit, a reinforcement learning approach frequently used in Web recommendation systems. We evaluate epsilon-greedy and LinUCB action selection methods under a variety of infrequent feedback scenarios, with several methods for managing the lack of feedback. Our empirical results show that, while early-episode performance and overall accuracy of epsilon-greedy action selection can be improved through learning from no-response feedback and careful management of remembered training episodes, a baseline LinUCB approach outperforms epsilon-greedy action selection in early-episode performance, overall accuracy, and simplicity.},
  eventtitle = {2019 {{IEEE}} 31st {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  keywords = {artificially intelligent assistive agents,baseline LinUCB approach,contextual bandit approaches,contextual bandits,conversational agents,dialogue trees,early-episode performance,epsilon-greedy action selection,external modules,fully autonomous domestic robot,human in the loop reinforcement learning,human operator,human-in-the-loop robot task completion,hypothetical robot assistant,inference mechanisms,learning (artificial intelligence),LinUCB action selection methods,mobile robots,no-response feedback,recommender systems,reinforcement learning approach,robotics,room-tidying task,service robots,sparse feedback,Web recommendation systems,workplace robot},
  file = {/Users/vitay/Documents/Zotero/storage/FAYL9QWE/8995186.html}
}

@unpublished{Merel2018,
  title = {Hierarchical Visuomotor Control of Humanoids},
  author = {Merel, Josh and Ahuja, Arun and Pham, Vu and Tunyasuvunakool, Saran and Liu, Siqi and Tirumala, Dhruva and Heess, Nicolas and Wayne, Greg},
  date = {2018-11-23},
  eprint = {1811.09656},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1811.09656},
  urldate = {2019-01-25},
  abstract = {We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. For a supplementary video link, see https://youtu.be/7GISvfbykLE .},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/J3KKADQK/Merel et al_2018_Hierarchical visuomotor control of humanoids.pdf;/Users/vitay/Documents/Zotero/storage/DSRM9L4B/1811.html}
}

@article{Merel2019,
  title = {Hierarchical Motor Control in Mammals and Machines},
  author = {Merel, Josh and Botvinick, Matthew and Wayne, Greg},
  date = {2019-12-02},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {10},
  number = {1},
  pages = {5489},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13239-6},
  url = {https://www.nature.com/articles/s41467-019-13239-6},
  urldate = {2023-01-26},
  abstract = {Advances in artificial intelligence are stimulating interest in neuroscience. However, most attention is given to discrete tasks with simple action spaces, such as board games and classic video games. Less discussed in neuroscience are parallel advances in “synthetic motor control”. While motor neuroscience has recently focused on optimization of single, simple movements, AI has progressed to the generation of rich, diverse motor behaviors across multiple tasks, at humanoid scale. It is becoming clear that specific, well-motivated hierarchical design elements repeatedly arise when engineering these flexible control systems. We review these core principles of hierarchical control, relate them to hierarchy in the nervous system, and highlight research themes that we anticipate will be critical in solving challenges at this disciplinary intersection.},
  issue = {1},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/QPP9FN9E/Merel_et_al_2019_Hierarchical_motor_control_in_mammals_and_machines.pdf;/Users/vitay/Documents/Zotero/storage/FB24NE5F/s41467-019-13239-6.html}
}

@report{Meuleau2000,
  title = {Off-{{Policy Policy Search}}},
  author = {Meuleau, Nicolas and Peshkin, Leonid and Kaelbling, Leslie P. and Kim, Kee-eung},
  date = {2000},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894}
}

@online{Micheli2022,
  title = {Transformers Are {{Sample Efficient World Models}}},
  author = {Micheli, Vincent and Alonso, Eloi and Fleuret, François},
  date = {2022-09-01},
  eprint = {2209.00588},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.00588},
  url = {http://arxiv.org/abs/2209.00588},
  urldate = {2022-10-20},
  abstract = {Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games. Our approach sets a new state of the art for methods without lookahead search, and even surpasses MuZero. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our codebase at https://github.com/eloialonso/iris.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/X9W96VYB/Micheli_et_al_2022_Transformers_are_Sample_Efficient_World_Models.pdf;/Users/vitay/Documents/Zotero/storage/23LUJ64W/2209.html}
}

@article{Mirowski2016,
  title = {Learning to {{Navigate}} in {{Complex Environments}}},
  author = {Mirowski, Piotr and Pascanu, Razvan and Viola, Fabio and Soyer, Hubert and Ballard, Andrew J. and Banino, Andrea and Denil, Misha and Goroshin, Ross and Sifre, Laurent and Kavukcuoglu, Koray and Kumaran, Dharshan and Hadsell, Raia},
  date = {2016-11},
  url = {http://arxiv.org/abs/1611.03673},
  abstract = {Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.},
  file = {/Users/vitay/Documents/Zotero/storage/I3R5EIYP/Mirowski_et_al_2016_Learning_to_Navigate_in_Complex_Environments.pdf}
}

@unpublished{Mnih2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  date = {2013-12},
  eprint = {1312.5602},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1312.5602},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  file = {/Users/vitay/Documents/Zotero/storage/9G2GJUYU/Mnih et al_2013_Playing Atari with Deep Reinforcement Learning.pdf}
}

@article{Mnih2014,
  title = {Recurrent {{Models}} of {{Visual Attention}}},
  author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
  date = {2014-06},
  url = {http://arxiv.org/abs/1406.6247},
  abstract = {Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.}
}

@article{Mnih2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  date = {2015-02},
  journaltitle = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  doi = {10.1038/nature14236},
  url = {http://www.nature.com/doifinder/10.1038/nature14236},
  file = {/Users/vitay/Documents/Zotero/storage/ILGLD768/Mnih et al_2015_Human-level control through deep reinforcement learning.pdf}
}

@inproceedings{Mnih2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  booktitle = {Proc. {{ICML}}},
  author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  date = {2016-02},
  url = {http://arxiv.org/abs/1602.01783},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task involving finding rewards in random 3D mazes using a visual input.},
  file = {/Users/vitay/Documents/Zotero/storage/SNST323A/Mnih et al_2016_Asynchronous Methods for Deep Reinforcement Learning.pdf}
}

@unpublished{Moerland2020,
  title = {Model-Based {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Model-Based {{Reinforcement Learning}}},
  author = {Moerland, Thomas M. and Broekens, Joost and Jonker, Catholijn M.},
  date = {2020-07-23},
  eprint = {2006.16712},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.16712},
  urldate = {2021-01-16},
  abstract = {Sequential decision making, commonly formalized as Markov Decision Process (MDP) optimization, is a key challenge in artificial intelligence. Two key approaches to this problem are reinforcement learning (RL) and planning. This paper presents a survey of the integration of both fields, better known as model-based reinforcement learning. Model-based RL has two main steps. First, we systematically cover approaches to dynamics model learning, including challenges like dealing with stochasticity, uncertainty, partial observability, and temporal abstraction. Second, we present a systematic categorization of planning-learning integration, including aspects like: where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning in the learning and acting loop. After these two key sections, we also discuss the potential benefits of model-based RL, like enhanced data efficiency, targeted exploration, and improved stability. Along the survey, we also draw connections to several related RL fields, like hierarchical RL and transfer, and other research disciplines, like behavioural psychology. Altogether, the survey presents a broad conceptual overview of planning-learning combinations for MDP optimization.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/N7LP8B2H/Moerland_et_al_2020_Model-based_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/JEBWP9GX/2006.html}
}

@article{Momennejad2017,
  title = {The Successor Representation in Human Reinforcement Learning},
  author = {Momennejad, I. and Russek, E. M. and Cheong, J. H. and Botvinick, M. M. and Daw, N. D. and Gershman, S. J.},
  date = {2017-09},
  journaltitle = {Nature Human Behaviour},
  volume = {1},
  number = {9},
  pages = {680--692},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0180-8},
  url = {http://www.nature.com/articles/s41562-017-0180-8},
  urldate = {2019-03-02},
  langid = {english},
  keywords = {Extinction,Human behaviour},
  file = {/Users/vitay/Documents/Zotero/storage/F2XEEEG4/Momennejad et al_2017_The successor representation in human reinforcement learning.pdf;/Users/vitay/Documents/Zotero/storage/F4QKFA5R/Momennejad et al_2017_The successor representation in human reinforcement learning.pdf}
}

@article{Moore1993,
  title = {Prioritized Sweeping: {{Reinforcement}} Learning with Less Data and Less Time},
  shorttitle = {Prioritized Sweeping},
  author = {Moore, Andrew W. and Atkeson, Christopher G.},
  date = {1993-10-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {13},
  number = {1},
  pages = {103--130},
  issn = {1573-0565},
  doi = {10.1007/BF00993104},
  url = {https://doi.org/10.1007/BF00993104},
  urldate = {2020-11-18},
  abstract = {We present a new algorithm,prioritized sweeping, for efficient prediction and control of stochastic Markov systems. Incremental learning methods such as temporal differencing and Q-learning have real-time performance. Classical methods are slower, but more accurate, because they make full use of the observations. Prioritized sweeping aims for the best of both worlds. It uses all previous experiences both to prioritize important dynamic programming sweeps and to guide the exploration of state-space. We compare prioritized sweeping with other reinforcement learning schemes for a number of different stochastic optimal control problems. It successfully solves large state-space real-time problems with which other methods have difficulty.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/F83K5THA/Moore_Atkeson_1993_Prioritized_sweeping.pdf}
}

@incollection{Mousavi2018,
  title = {Deep {{Reinforcement Learning}}: {{An Overview}}},
  author = {Mousavi, Seyed Sajad and Schukat, Michael and Howley, Enda},
  date = {2018-09},
  pages = {426--440},
  publisher = {Springer, Cham},
  doi = {10.1007/978-3-319-56991-8_32},
  url = {http://link.springer.com/10.1007/978-3-319-56991-8_32},
  file = {/Users/vitay/Documents/Zotero/storage/AHJ4XKLM/Mousavi et al_2018_Deep Reinforcement Learning.pdf}
}

@article{Munos2016,
  title = {Safe and {{Efficient Off-Policy Reinforcement Learning}}},
  author = {Munos, Rémi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc G.},
  date = {2016-06},
  url = {http://arxiv.org/abs/1606.02647},
  abstract = {In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace(\$\textbackslash lambda\$), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of "off-policyness"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to \$Q\textasciicircum *\$ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q(\$\textbackslash lambda\$), which was an open problem since 1989. We illustrate the benefits of Retrace(\$\textbackslash lambda\$) on a standard suite of Atari 2600 games.},
  file = {/Users/vitay/Documents/Zotero/storage/9W5PMA6L/Munos et al_2016_Safe and Efficient Off-Policy Reinforcement Learning.pdf}
}

@unpublished{Nachum2017,
  title = {Bridging the {{Gap Between Value}} and {{Policy Based Reinforcement Learning}}},
  author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  date = {2017-02-28},
  eprint = {1702.08892},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1702.08892},
  urldate = {2019-06-12},
  abstract = {We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values correspond to optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on- and off-policy traces. We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor-critic and Q-learning algorithms. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and the corresponding softmax state values, eliminating the need for a separate critic. The experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmarks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/NSMPJSML/Nachum et al_2017_Bridging the Gap Between Value and Policy Based Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/N5IW7I7A/1702.html}
}

@article{Nachum2018,
  title = {Data-{{Efficient Hierarchical Reinforcement Learning}}},
  author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  date = {2018-05},
  url = {http://arxiv.org/abs/1805.08296},
  abstract = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.},
  file = {/Users/vitay/Documents/Zotero/storage/39L5QGCY/Nachum et al_2018_Data-Efficient Hierarchical Reinforcement Learning.pdf}
}

@unpublished{Nachum2019,
  title = {Why {{Does Hierarchy}} ({{Sometimes}}) {{Work So Well}} in {{Reinforcement Learning}}?},
  author = {Nachum, Ofir and Tang, Haoran and Lu, Xingyu and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  date = {2019-09-23},
  eprint = {1909.10618},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1909.10618},
  urldate = {2019-09-28},
  abstract = {Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. However, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard "shallow" RL architectures. In this work, we isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation. Surprisingly, we find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, we present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/NVA964RK/Nachum et al_2019_Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/UICQPH6M/1909.html}
}

@unpublished{Nagabandi2017,
  title = {Neural {{Network Dynamics}} for {{Model-Based Deep Reinforcement Learning}} with {{Model-Free Fine-Tuning}}},
  author = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S. and Levine, Sergey},
  date = {2017-08-08},
  eprint = {1708.02596},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1708.02596},
  urldate = {2019-03-03},
  abstract = {Model-free deep reinforcement learning methods have successfully learned complex behavioral strategies for a wide range of tasks, but typically require many samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We perform this pre-initialization by using rollouts from the trained model-based controller as supervision to pre-train a policy, and then fine-tune the policy using a model-free method. We empirically demonstrate that this resulting hybrid algorithm can drastically accelerate model-free learning and outperform purely model-free learners on several MuJoCo locomotion benchmark tasks, achieving sample efficiency gains over a purely model-free learner of 330× on swimmer, 26× on hopper, 4× on half-cheetah, and 3× on ant. Videos can be found at https://sites.google.com/view/mbmf.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/JDX2QVHP/Nagabandi et al_2017_Neural Network Dynamics for Model-Based Deep Reinforcement Learning with.pdf}
}

@unpublished{Nagabandi2019,
  title = {Deep {{Dynamics Models}} for {{Learning Dexterous Manipulation}}},
  author = {Nagabandi, Anusha and Konoglie, Kurt and Levine, Sergey and Kumar, Vikash},
  date = {2019-09-25},
  eprint = {1909.11652},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1909.11652},
  urldate = {2019-10-01},
  abstract = {Dexterous multi-fingered hands can provide robots with the ability to flexibly perform a wide range of manipulation skills. However, many of the more complex behaviors are also notoriously difficult to control: Performing in-hand object manipulation, executing finger gaits to move objects, and exhibiting precise fine motor skills such as writing, all require finely balancing contact forces, breaking and reestablishing contacts repeatedly, and maintaining control of unactuated objects. Learning-based techniques provide the appealing possibility of acquiring these skills directly from data, but current learning approaches either require large amounts of data and produce task-specific policies, or they have not yet been shown to scale up to more complex and realistic tasks requiring fine motor skills. In this work, we demonstrate that our method of online planning with deep dynamics models (PDDM) addresses both of these limitations; we show that improvements in learned dynamics models, together with improvements in online model-predictive control, can indeed enable efficient and effective learning of flexible contact-rich dexterous manipulation skills -- and that too, on a 24-DoF anthropomorphic hand in the real world, using just 4 hours of purely real-world data to learn to simultaneously coordinate multiple free-floating objects. Videos can be found at https://sites.google.com/view/pddm/},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/4R7ZHKLX/Nagabandi et al_2019_Deep Dynamics Models for Learning Dexterous Manipulation.pdf;/Users/vitay/Documents/Zotero/storage/YF57JNK3/1909.html}
}

@unpublished{Nair2015,
  title = {Massively {{Parallel Methods}} for {{Deep Reinforcement Learning}}},
  author = {Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and De Maria, Alessandro and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and Legg, Shane and Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David},
  date = {2015},
  eprint = {1507.04296},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {https://arxiv.org/pdf/1507.04296.pdf},
  abstract = {We present the first massively distributed archi-tecture for deep reinforcement learning. This architecture uses four main components: paral-lel actors that generate new behaviour; paral-lel learners that are trained from stored experi-ence; a distributed neural network to represent the value function or behaviour policy; and a dis-tributed store of experience. We used our archi-tecture to implement the Deep Q-Network algo-rithm (DQN) (Mnih et al., 2013). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environ-ment, using identical hyperparameters. Our per-formance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.},
  file = {/Users/vitay/Documents/Zotero/storage/9RFG8JAK/Nair et al_2015_Massively Parallel Methods for Deep Reinforcement Learning.pdf}
}

@unpublished{Narvekar2020,
  title = {Curriculum {{Learning}} for {{Reinforcement Learning Domains}}: {{A Framework}} and {{Survey}}},
  shorttitle = {Curriculum {{Learning}} for {{Reinforcement Learning Domains}}},
  author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter},
  date = {2020-03-10},
  eprint = {2003.04960},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2003.04960},
  urldate = {2020-03-13},
  abstract = {Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/95RMN5UE/Narvekar_et_al_2020_Curriculum_Learning_for_Reinforcement_Learning_Domains.pdf;/Users/vitay/Documents/Zotero/storage/A8S2CCJ8/2003.html}
}

@unpublished{Nasiriany2019,
  title = {Planning with {{Goal-Conditioned Policies}}},
  author = {Nasiriany, Soroush and Pong, Vitchyr H. and Lin, Steven and Levine, Sergey},
  date = {2019-11-19},
  eprint = {1911.08453},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1911.08453},
  urldate = {2021-01-17},
  abstract = {Planning methods can solve temporally extended sequential decision making problems by composing simple behaviors. However, planning requires suitable abstractions for the states and transitions, which typically need to be designed by hand. In contrast, model-free reinforcement learning (RL) can acquire behaviors from low-level inputs directly, but often struggles with temporally extended tasks. Can we utilize reinforcement learning to automatically form the abstractions needed for planning, thus obtaining the best of both approaches? We show that goal-conditioned policies learned with RL can be incorporated into planning, so that a planner can focus on which states to reach, rather than how those states are reached. However, with complex state observations such as images, not all inputs represent valid states. We therefore also propose using a latent variable model to compactly represent the set of valid states for the planner, so that the policies provide an abstraction of actions, and the latent variable model provides an abstraction of states. We compare our method with planning-based and model-free methods and find that our method significantly outperforms prior work when evaluated on image-based robot navigation and manipulation tasks that require non-greedy, multi-staged behavior.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/SB9TYW7I/Nasiriany_et_al_2019_Planning_with_Goal-Conditioned_Policies.pdf;/Users/vitay/Documents/Zotero/storage/X6J349SJ/1911.html}
}

@article{Neftci2019,
  title = {Reinforcement Learning in Artificial and Biological Systems},
  author = {Neftci, Emre O. and Averbeck, Bruno B.},
  date = {2019-03},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {1},
  number = {3},
  pages = {133--143},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0025-4},
  url = {https://www.nature.com/articles/s42256-019-0025-4},
  urldate = {2023-01-26},
  abstract = {There is and has been a fruitful flow of concepts and ideas between studies of learning in biological and artificial systems. Much early work that led to the development of reinforcement learning (RL) algorithms for artificial systems was inspired by learning rules first developed in biology by Bush and Mosteller, and Rescorla and Wagner. More recently, temporal-difference RL, developed for learning in artificial agents, has provided a foundational framework for interpreting the activity of dopamine neurons. In this Review, we describe state-of-the-art work on RL in biological and artificial agents. We focus on points of contact between these disciplines and identify areas where future research can benefit from information flow between these fields. Most work in biological systems has focused on simple learning problems, often embedded in dynamic environments where flexibility and ongoing learning are important, similar to real-world learning problems faced by biological systems. In contrast, most work in artificial agents has focused on learning a single complex problem in a static environment. Moving forward, work in each field will benefit from a flow of ideas that represent the strengths within each discipline.},
  issue = {3},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/KVBVDJCT/Neftci_Averbeck_2019_Reinforcement_learning_in_artificial_and_biological_systems.pdf}
}

@inproceedings{Ng2000,
  title = {Algorithms for {{Inverse Reinforcement Learning}}},
  booktitle = {Proceedings of the {{Seventeenth International Conference}} on {{Machine Learning}}},
  author = {Ng, Andrew Y. and Russell, Stuart J.},
  date = {2000-06-29},
  series = {{{ICML}} '00},
  pages = {663--670},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  isbn = {978-1-55860-707-1}
}

@inproceedings{Nguyen2019,
  title = {Review of {{Deep Reinforcement Learning}} for {{Robot Manipulation}}},
  booktitle = {2019 {{Third IEEE International Conference}} on {{Robotic Computing}} ({{IRC}})},
  author = {Nguyen, Hai and La, Hung},
  date = {2019-02},
  pages = {590--595},
  doi = {10.1109/IRC.2019.00120},
  abstract = {Reinforcement learning combined with neural networks has recently led to a wide range of successes in learning policies in different domains. For robot manipulation, reinforcement learning algorithms bring the hope for machines to have the human-like abilities by directly learning dexterous manipulation from raw pixels. In this review paper, we address the current status of reinforcement learning algorithms used in the field. We also cover essential theoretical background and main issues with current algorithms, which are limiting their applications of reinforcement learning algorithms in solving practical problems in robotics. We also share our thoughts on a number of future directions for reinforcement learning research.},
  eventtitle = {2019 {{Third IEEE International Conference}} on {{Robotic Computing}} ({{IRC}})},
  keywords = {Deep reinforcement learning,Optimization,Reinforcement learning,Robot manipulation,Robot sensing systems,Task analysis,Taxonomy,Training},
  file = {/Users/vitay/Documents/Zotero/storage/RR7AFNT9/8675643.html}
}

@inproceedings{Niu2011,
  title = {{{HOGWILD}}!: {{A Lock-Free Approach}} to {{Parallelizing Stochastic Gradient Descent}}},
  booktitle = {Proc. {{Advances}} in {{Neural Information Processing Systems}}},
  author = {Niu, Feng and Recht, Benjamin and Re, Christopher and Wright, Stephen J.},
  date = {2011},
  pages = {21--21},
  url = {http://arxiv.org/abs/1106.5730},
  abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.},
  isbn = {978-1-61839-599-3},
  keywords = {incremental gradient methods,machine learning,multicore,parallel computing}
}

@online{Nottingham2023,
  title = {Do {{Embodied Agents Dream}} of {{Pixelated Sheep}}: {{Embodied Decision Making}} Using {{Language Guided World Modelling}}},
  shorttitle = {Do {{Embodied Agents Dream}} of {{Pixelated Sheep}}},
  author = {Nottingham, Kolby and Ammanabrolu, Prithviraj and Suhr, Alane and Choi, Yejin and Hajishirzi, Hannaneh and Singh, Sameer and Fox, Roy},
  date = {2023-04-27},
  eprint = {2301.12050},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.12050},
  url = {http://arxiv.org/abs/2301.12050},
  urldate = {2023-05-27},
  abstract = {Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/Y5KEDYVK/Nottingham_et_al_2023_Do_Embodied_Agents_Dream_of_Pixelated_Sheep.pdf}
}

@unpublished{ODonoghue2016,
  title = {Combining Policy Gradient and {{Q-learning}}},
  author = {O'Donoghue, Brendan and Munos, Remi and Kavukcuoglu, Koray and Mnih, Volodymyr},
  date = {2016-11-05},
  eprint = {1611.01626},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1611.01626},
  urldate = {2019-02-13},
  abstract = {Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as 'PGQL', for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/DNIDU5U6/O'Donoghue et al_2016_Combining policy gradient and Q-learning.pdf;/Users/vitay/Documents/Zotero/storage/F4ZVMSC8/1611.html}
}

@article{Oh2018,
  title = {Self-{{Imitation Learning}}},
  author = {Oh, Junhyuk and Guo, Yijie and Singh, Satinder and Lee, Honglak},
  date = {2018-06},
  url = {http://arxiv.org/abs/1806.05635},
  abstract = {This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.},
  file = {/Users/vitay/Documents/Zotero/storage/SZCW74H8/Oh et al_2018_Self-Imitation Learning.pdf}
}

@online{Ororbia2022,
  title = {Active {{Predicting Coding}}: {{Brain-Inspired Reinforcement Learning}} for {{Sparse Reward Robotic Control Problems}}},
  shorttitle = {Active {{Predicting Coding}}},
  author = {Ororbia, Alexander and Mali, Ankur},
  date = {2022-09-19},
  eprint = {2209.09174},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2209.09174},
  urldate = {2022-09-29},
  abstract = {In this article, we propose a backpropagation-free approach to robotic control through the neuro-cognitive computational framework of neural generative coding (NGC), designing an agent built completely from powerful predictive coding/processing circuits that facilitate dynamic, online learning from sparse rewards, embodying the principles of planning-as-inference. Concretely, we craft an adaptive agent system, which we call active predictive coding (ActPC), that balances an internally-generated epistemic signal (meant to encourage intelligent exploration) with an internally-generated instrumental signal (meant to encourage goal-seeking behavior) to ultimately learn how to control various simulated robotic systems as well as a complex robotic arm using a realistic robotics simulator, i.e., the Surreal Robotics Suite, for the block lifting task and can pick-and-place problems. Notably, our experimental results demonstrate that our proposed ActPC agent performs well in the face of sparse (extrinsic) reward signals and is competitive with or outperforms several powerful backprop-based RL approaches.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/37NFVAT3/Ororbia_Mali_2022_Active_Predicting_Coding.pdf;/Users/vitay/Documents/Zotero/storage/MH77BMNN/2209.html}
}

@unpublished{Ortega2019,
  title = {Meta-Learning of {{Sequential Strategies}}},
  author = {Ortega, Pedro A. and Wang, Jane X. and Rowland, Mark and Genewein, Tim and Kurth-Nelson, Zeb and Pascanu, Razvan and Heess, Nicolas and Veness, Joel and Pritzel, Alex and Sprechmann, Pablo and Jayakumar, Siddhant M. and McGrath, Tom and Miller, Kevin and Azar, Mohammad and Osband, Ian and Rabinowitz, Neil and György, András and Chiappa, Silvia and Osindero, Simon and Teh, Yee Whye and family=Hasselt, given=Hado, prefix=van, useprefix=true and family=Freitas, given=Nando, prefix=de, useprefix=true and Botvinick, Matthew and Legg, Shane},
  date = {2019-05-08},
  eprint = {1905.03030},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.03030},
  urldate = {2019-05-14},
  abstract = {In this report we review memory-based meta-learning as a tool for building sample-efficient strategies that learn from past experience to adapt to any task within a target class. Our goal is to equip the reader with the conceptual foundations of this tool for building new, scalable agents that operate on broad domains. To do so, we present basic algorithmic templates for building near-optimal predictors and reinforcement learners which behave as if they had a probabilistic model that allowed them to efficiently exploit task structure. Furthermore, we recast memory-based meta-learning within a Bayesian framework, showing that the meta-learned strategies are near-optimal because they amortize Bayes-filtered data, where the adaptation is implemented in the memory dynamics as a state-machine of sufficient statistics. Essentially, memory-based meta-learning translates the hard problem of probabilistic sequential inference into a regression problem.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/826M84YL/Ortega et al_2019_Meta-learning of Sequential Strategies.pdf;/Users/vitay/Documents/Zotero/storage/YFRXN4JU/1905.html}
}

@article{Osa2018,
  title = {An {{Algorithmic Perspective}} on {{Imitation Learning}}},
  author = {Osa, Takayuki and Pajarinen, Joni and Neumann, Gerhard and Bagnell, J. Andrew and Abbeel, Pieter and Peters, Jan},
  date = {2018},
  journaltitle = {Foundations and Trends in Robotics},
  shortjournal = {FNT in Robotics},
  volume = {7},
  number = {1-2},
  eprint = {1811.06711},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--179},
  issn = {1935-8253, 1935-8261},
  doi = {10.1561/2300000053},
  url = {http://arxiv.org/abs/1811.06711},
  urldate = {2023-03-12},
  abstract = {As robots and other intelligent agents move from simple environments and problems to more complex, unstructured settings, manually programming their behavior has become increasingly challenging and expensive. Often, it is easier for a teacher to demonstrate a desired behavior rather than attempt to manually engineer it. This process of learning from demonstrations, and the study of algorithms to do so, is called imitation learning. This work provides an introduction to imitation learning. It covers the underlying assumptions, approaches, and how they relate; the rich set of algorithms developed to tackle the problem; and advice on effective tools and implementation. We intend this paper to serve two audiences. First, we want to familiarize machine learning experts with the challenges of imitation learning, particularly those arising in robotics, and the interesting theoretical and practical distinctions between it and more familiar frameworks like statistical supervised learning theory and reinforcement learning. Second, we want to give roboticists and experts in applied artificial intelligence a broader appreciation for the frameworks and tools available for imitation learning.},
  file = {/Users/vitay/Documents/Zotero/storage/W5RKH8XC/Osa_et_al_2018_An_Algorithmic_Perspective_on_Imitation_Learning.pdf}
}

@article{Oudeyer2007,
  title = {Intrinsic {{Motivation Systems}} for {{Autonomous Mental Development}}},
  author = {Oudeyer, P. and Kaplan, F. and Hafner, V. V.},
  date = {2007-04},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  volume = {11},
  number = {2},
  pages = {265--286},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2006.890271},
  abstract = {Exploratory activities seem to be intrinsically rewarding for children and crucial for their cognitive development. Can a machine be endowed with such an intrinsic motivation system? This is the question we study in this paper, presenting a number of computational systems that try to capture this drive towards novel or curious situations. After discussing related research coming from developmental psychology, neuroscience, developmental robotics, and active learning, this paper presents the mechanism of Intelligent Adaptive Curiosity, an intrinsic motivation system which pushes a robot towards situations in which it maximizes its learning progress. This drive makes the robot focus on situations which are neither too predictable nor too unpredictable, thus permitting autonomous mental development. The complexity of the robot's activities autonomously increases and complex developmental sequences self-organize without being constructed in a supervised manner. Two experiments are presented illustrating the stage-like organization emerging with this mechanism. In one of them, a physical robot is placed on a baby play mat with objects that it can learn to manipulate. Experimental results show that the robot first spends time in situations which are easy to learn, then shifts its attention progressively to situations of increasing difficulty, avoiding situations in which nothing can be learned. Finally, these various results are discussed in relation to more complex forms of behavioral organization and data coming from developmental psychology},
  eventtitle = {{{IEEE Transactions}} on {{Evolutionary Computation}}},
  keywords = {active learning,Active learning,autonomous mental development,Autonomous mental development,autonomy,behavior,behavioral organization,cognitive development,Cognitive robotics,complexity,Computational intelligence,Computer science,curiosity,development,developmental psychology,developmental robotics,developmental trajectory,epigenetic robotics,Humans,intelligent adaptive curiosity,Intelligent robots,intrinsic motivation,intrinsic motivation systems,Laboratories,learning,learning (artificial intelligence),Neuroscience,Pediatrics,Psychology,reinforcement learning,robots,values},
  file = {/Users/vitay/Documents/Zotero/storage/7QHMFQXX/Oudeyer_et_al_2007_Intrinsic_Motivation_Systems_for_Autonomous_Mental_Development.pdf;/Users/vitay/Documents/Zotero/storage/A8VB8LIP/4141061.html}
}

@incollection{Oudeyer2016,
  title = {Chapter 11 - {{Intrinsic}} Motivation, Curiosity, and Learning: {{Theory}} and Applications in Educational Technologies},
  shorttitle = {Chapter 11 - {{Intrinsic}} Motivation, Curiosity, and Learning},
  booktitle = {Progress in {{Brain Research}}},
  author = {Oudeyer, P. -Y. and Gottlieb, J. and Lopes, M.},
  editor = {Studer, Bettina and Knecht, Stefan},
  date = {2016-01-01},
  series = {Motivation},
  volume = {229},
  pages = {257--284},
  publisher = {Elsevier},
  doi = {10.1016/bs.pbr.2016.05.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0079612316300589},
  urldate = {2021-02-06},
  abstract = {This chapter studies the bidirectional causal interactions between curiosity and learning and discusses how understanding these interactions can be leveraged in educational technology applications. First, we review recent results showing how state curiosity, and more generally the experience of novelty and surprise, can enhance learning and memory retention. Then, we discuss how psychology and neuroscience have conceptualized curiosity and intrinsic motivation, studying how the brain can be intrinsically rewarded by novelty, complexity, or other measures of information. We explain how the framework of computational reinforcement learning can be used to model such mechanisms of curiosity. Then, we discuss the learning progress (LP) hypothesis, which posits a positive feedback loop between curiosity and learning. We outline experiments with robots that show how LP-driven attention and exploration can self-organize a developmental learning curriculum scaffolding efficient acquisition of multiple skills/tasks. Finally, we discuss recent work exploiting these conceptual and computational models in educational technologies, showing in particular how intelligent tutoring systems can be designed to foster curiosity and learning.},
  langid = {english},
  keywords = {Active learning,Active teaching,Artificial intelligence,Computational modeling,Curiosity,Education,Educational technology,Intrinsic motivation,Learning,Neuroscience},
  file = {/Users/vitay/Documents/Zotero/storage/YK48UJDM/Oudeyer_et_al_2016_Chapter_11_-_Intrinsic_motivation,_curiosity,_and_learning.pdf;/Users/vitay/Documents/Zotero/storage/Q4IP2FWY/S0079612316300589.html}
}

@online{Ouyang2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  date = {2022-03-04},
  eprint = {2203.02155},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.02155},
  url = {http://arxiv.org/abs/2203.02155},
  urldate = {2023-04-28},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/TD27V5XK/Ouyang_et_al_2022_Training_language_models_to_follow_instructions_with_human_feedback.pdf}
}

@unpublished{Paine2019,
  title = {Making {{Efficient Use}} of {{Demonstrations}} to {{Solve Hard Exploration Problems}}},
  author = {Paine, Tom Le and Gulcehre, Caglar and Shahriari, Bobak and Denil, Misha and Hoffman, Matt and Soyer, Hubert and Tanburn, Richard and Kapturowski, Steven and Rabinowitz, Neil and Williams, Duncan and Barth-Maron, Gabriel and Wang, Ziyu and family=Freitas, given=Nando, prefix=de, useprefix=true and Team, Worlds},
  date = {2019-09-03},
  eprint = {1909.01387},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1909.01387},
  urldate = {2019-09-15},
  abstract = {This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/FI4ZDM2H/Paine et al_2019_Making Efficient Use of Demonstrations to Solve Hard Exploration Problems.pdf;/Users/vitay/Documents/Zotero/storage/VCFYT7N8/1909.html}
}

@unpublished{Pajarinen2019,
  title = {Compatible {{Natural Gradient Policy Search}}},
  author = {Pajarinen, Joni and Thai, Hong Linh and Akrour, Riad and Peters, Jan and Neumann, Gerhard},
  date = {2019-02-07},
  eprint = {1902.02823},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.02823},
  urldate = {2019-02-17},
  abstract = {Trust-region methods have yielded state-of-the-art results in policy search. A common approach is to use KL-divergence to bound the region of trust resulting in a natural gradient policy update. We show that the natural gradient and trust region optimization are equivalent if we use the natural parameterization of a standard exponential policy distribution in combination with compatible value function approximation. Moreover, we show that standard natural gradient updates may reduce the entropy of the policy according to a wrong schedule leading to premature convergence. To control entropy reduction we introduce a new policy search method called compatible policy search (COPOS) which bounds entropy loss. The experimental results show that COPOS yields state-of-the-art results in challenging continuous control tasks and in discrete partially observable tasks.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/6LQ8GLP2/Pajarinen et al_2019_Compatible Natural Gradient Policy Search.pdf;/Users/vitay/Documents/Zotero/storage/JAZ7NZDL/1902.html}
}

@article{Pardo2018,
  title = {Q-Map: A {{Convolutional Approach}} for {{Goal-Oriented Reinforcement Learning}}},
  author = {Pardo, Fabio and Levdik, Vitaly and Kormushev, Petar},
  date = {2018-10},
  url = {http://arxiv.org/abs/1810.02927},
  abstract = {Goal-oriented learning has become a core concept in reinforcement learning (RL), extending the reward signal as a sole way to define tasks. However, as parameterizing value functions with goals increases the learning complexity, efficiently reusing past experience to update estimates towards several goals at once becomes desirable but usually requires independent updates per goal. Considering that a significant number of RL environments can support spatial coordinates as goals, such as on-screen location of the character in ATARI or SNES games, we propose a novel goal-oriented agent called Q-map that utilizes an autoencoder-like neural network to predict the minimum number of steps towards each coordinate in a single forward pass. This architecture is similar to Horde with parameter sharing and allows the agent to discover correlations between visual patterns and navigation. For example learning how to use a ladder in a game could be transferred to other ladders later. We show how this network can be efficiently trained with a 3D variant of Q-learning to update the estimates towards all goals at once. While the Q-map agent could be used for a wide range of applications, we propose a novel exploration mechanism in place of epsilon-greedy that relies on goal selection at a desired distance followed by several steps taken towards it, allowing long and coherent exploratory steps in the environment. We demonstrate the accuracy and generalization qualities of the Q-map agent on a grid-world environment and then demonstrate the efficiency of the proposed exploration mechanism on the notoriously difficult Montezuma's Revenge and Super Mario All-Stars games.},
  file = {/Users/vitay/Documents/Zotero/storage/KZMXINYX/Pardo et al_2018_Q-map.pdf}
}

@unpublished{Parisi2019a,
  title = {Long-{{Term Visitation Value}} for {{Deep Exploration}} in {{Sparse Reward Reinforcement Learning}}},
  author = {Parisi, Simone and Tateo, Davide and Hensel, Maximilian and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
  date = {2019-12-31},
  eprint = {2001.00119},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2001.00119},
  urldate = {2020-01-07},
  abstract = {Reinforcement learning with sparse rewards is still an open challenge. Classic methods rely on getting feedback via extrinsic rewards to train the agent, and in situations where this occurs very rarely the agent learns slowly or cannot learn at all. Similarly, if the agent receives also rewards that create suboptimal modes of the objective function, it will likely prematurely stop exploring. More recent methods add auxiliary intrinsic rewards to encourage exploration. However, auxiliary rewards lead to a non-stationary target for the Q-function. In this paper, we present a novel approach that (1) plans exploration actions far into the future by using a long-term visitation count, and (2) decouples exploration and exploitation by learning a separate function assessing the exploration value of the actions. Contrary to existing methods which use models of reward and dynamics, our approach is off-policy and model-free. We further propose new tabular environments for benchmarking exploration in reinforcement learning. Empirical results on classic and novel benchmarks show that the proposed approach outperforms existing methods in environments with sparse rewards, especially in the presence of rewards that create suboptimal modes of the objective function. Results also suggest that our approach scales gracefully with the size of the environment. Source code is available at https://github.com/sparisi/visit-value-explore},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/EWDKJC6V/Parisi_et_al_2019_Long-Term_Visitation_Value_for_Deep_Exploration_in_Sparse_Reward_Reinforcement.pdf;/Users/vitay/Documents/Zotero/storage/M5EMXZAL/2001.html}
}

@unpublished{Parisotto2019,
  title = {Stabilizing {{Transformers}} for {{Reinforcement Learning}}},
  author = {Parisotto, Emilio and Song, H. Francis and Rae, Jack W. and Pascanu, Razvan and Gulcehre, Caglar and Jayakumar, Siddhant M. and Jaderberg, Max and Kaufman, Raphael Lopez and Clark, Aidan and Noury, Seb and Botvinick, Matthew M. and Heess, Nicolas and Hadsell, Raia},
  date = {2019-10-13},
  eprint = {1910.06764},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.06764},
  urldate = {2019-10-19},
  abstract = {Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially observable environments.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/S22GXA6N/Parisotto et al_2019_Stabilizing Transformers for Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/IJ7EDUUE/1910.html}
}

@article{Pateria2021,
  title = {Hierarchical {{Reinforcement Learning}}: {{A Comprehensive Survey}}},
  shorttitle = {Hierarchical {{Reinforcement Learning}}},
  author = {Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-hwee and Quek, Chai},
  date = {2021-06-05},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {54},
  number = {5},
  pages = {109:1--109:35},
  issn = {0360-0300},
  doi = {10.1145/3453160},
  url = {https://doi.org/10.1145/3453160},
  urldate = {2023-01-26},
  abstract = {Hierarchical Reinforcement Learning (HRL) enables autonomous decomposition of challenging long-horizon decision-making tasks into simpler subtasks. During the past years, the landscape of HRL research has grown profoundly, resulting in copious approaches. A comprehensive overview of this vast landscape is necessary to study HRL in an organized manner. We provide a survey of the diverse HRL approaches concerning the challenges of learning hierarchical policies, subtask discovery, transfer learning, and multi-agent learning using HRL. The survey is presented according to a novel taxonomy of the approaches. Based on the survey, a set of important open problems is proposed to motivate the future research in HRL. Furthermore, we outline a few suitable task domains for evaluating the HRL approaches and a few interesting examples of the practical applications of HRL in the Supplementary Material.}
}

@unpublished{Pathak2017,
  title = {Curiosity-Driven {{Exploration}} by {{Self-supervised Prediction}}},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  date = {2017-05-15},
  eprint = {1705.05363},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1705.05363},
  urldate = {2021-02-06},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/RW2P8SGF/Pathak_et_al_2017_Curiosity-driven_Exploration_by_Self-supervised_Prediction.pdf;/Users/vitay/Documents/Zotero/storage/U3HN4SKU/1705.html}
}

@article{Peng2018,
  title = {Deep {{Dyna-Q}}: {{Integrating Planning}} for {{Task-Completion Dialogue Policy Learning}}},
  author = {Peng, Baolin and Li, Xiujun and Gao, Jianfeng and Liu, Jingjing and Wong, Kam-Fai and Su, Shang-Yu},
  date = {2018-01},
  url = {http://arxiv.org/abs/1801.06176},
  abstract = {Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment, referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings.},
  file = {/Users/vitay/Documents/Zotero/storage/UR3FJK3F/Peng et al_2018_Deep Dyna-Q.pdf}
}

@article{Peng2018a,
  title = {{{DeepMimic}}: {{Example-Guided Deep Reinforcement Learning}} of {{Physics-Based Character Skills}}},
  shorttitle = {{{DeepMimic}}},
  author = {Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and family=Panne, given=Michiel, prefix=van de, useprefix=true},
  date = {2018-07-30},
  journaltitle = {ACM Transactions on Graphics},
  volume = {37},
  number = {4},
  eprint = {1804.02717},
  eprinttype = {arXiv},
  pages = {1--14},
  issn = {07300301},
  doi = {10.1145/3197517.3201311},
  url = {http://arxiv.org/abs/1804.02717},
  urldate = {2019-01-25},
  abstract = {A longstanding goal in character animation is to combine data-driven specification of behavior with a system that can execute a similar behavior in a physical simulation, thus enabling realistic responses to perturbations and environmental variation. We show that well-known reinforcement learning (RL) methods can be adapted to learn robust control policies capable of imitating a broad range of example motion clips, while also learning complex recoveries, adapting to changes in morphology, and accomplishing user-specified goals. Our method handles keyframed motions, highly-dynamic actions such as motion-captured flips and spins, and retargeted motions. By combining a motion-imitation objective with a task objective, we can train characters that react intelligently in interactive settings, e.g., by walking in a desired direction or throwing a ball at a user-specified target. This approach thus combines the convenience and motion quality of using motion clips to define the desired style and appearance, with the flexibility and generality afforded by RL methods and physics-based animation. We further explore a number of methods for integrating multiple clips into the learning process to develop multi-skilled agents capable of performing a rich repertoire of diverse skills. We demonstrate results using multiple characters (human, Atlas robot, bipedal dinosaur, dragon) and a large variety of skills, including locomotion, acrobatics, and martial arts.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/V3TW77M4/Peng et al_2018_DeepMimic.pdf;/Users/vitay/Documents/Zotero/storage/LIQUSZP7/1804.html}
}

@unpublished{Pertsch2020,
  title = {Long-{{Horizon Visual Planning}} with {{Goal-Conditioned Hierarchical Predictors}}},
  author = {Pertsch, Karl and Rybkin, Oleh and Ebert, Frederik and Finn, Chelsea and Jayaraman, Dinesh and Levine, Sergey},
  date = {2020-06-23},
  eprint = {2006.13205},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.13205},
  urldate = {2020-06-27},
  abstract = {The ability to predict and plan into the future is fundamental for agents acting in the world. To reach a faraway goal, we predict trajectories at multiple timescales, first devising a coarse plan towards the goal and then gradually filling in details. In contrast, current learning approaches for visual prediction and planning fail on long-horizon tasks as they generate predictions (1) without considering goal information, and (2) at the finest temporal resolution, one step at a time. In this work we propose a framework for visual prediction and planning that is able to overcome both of these limitations. First, we formulate the problem of predicting towards a goal and propose the corresponding class of latent space goal-conditioned predictors (GCPs). GCPs significantly improve planning efficiency by constraining the search space to only those trajectories that reach the goal. Further, we show how GCPs can be naturally formulated as hierarchical models that, given two observations, predict an observation between them, and by recursively subdividing each part of the trajectory generate complete sequences. This divide-and-conquer strategy is effective at long-term prediction, and enables us to design an effective hierarchical planning algorithm that optimizes trajectories in a coarse-to-fine manner. We show that by using both goal-conditioning and hierarchical prediction, GCPs enable us to solve visual planning tasks with much longer horizon than previously possible.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/Z9JAUBSZ/Pertsch_et_al_2020_Long-Horizon_Visual_Planning_with_Goal-Conditioned_Hierarchical_Predictors.pdf;/Users/vitay/Documents/Zotero/storage/NED499X9/2006.html}
}

@article{Peshkin2002,
  title = {Learning from {{Scarce Experience}}},
  author = {Peshkin, Leonid and Shelton, Christian R.},
  date = {2002-04},
  url = {http://arxiv.org/abs/cs/0204043},
  abstract = {Searching the space of policies directly for the optimal policy has been one popular method for solving partially observable reinforcement learning problems. Typically, with each change of the target policy, its value is estimated from the results of following that very policy. This requires a large number of interactions with the environment as different polices are considered. We present a family of algorithms based on likelihood ratio estimation that use data gathered when executing one policy (or collection of policies) to estimate the value of a different policy. The algorithms combine estimation and optimization stages. The former utilizes experience to build a non-parametric representation of an optimized function. The latter performs optimization on this estimate. We show positive empirical results and provide the sample complexity bound.},
  file = {/Users/vitay/Documents/Zotero/storage/PSH3KKIS/Peshkin_Shelton_2002_Learning from Scarce Experience.pdf}
}

@inproceedings{Peters2005,
  title = {Natural {{Actor-Critic}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2005},
  author = {Peters, Jan and Vijayakumar, Sethu and Schaal, Stefan},
  editor = {Gama, João and Camacho, Rui and Brazdil, Pavel B. and Jorge, Alípio Mário and Torgo, Luís},
  date = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {280--291},
  publisher = {Springer Berlin Heidelberg},
  abstract = {This paper investigates a novel model-free reinforcement learning architecture, the Natural Actor-Critic. The actor updates are based on stochastic policy gradients employing Amari’s natural gradient approach, while the critic obtains both the natural policy gradient and additional parameters of a value function simultaneously by linear regression. We show that actor improvements with natural policy gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation, and can be estimated more efficiently than regular policy gradients. The critic makes use of a special basis function parameterization motivated by the policy-gradient compatible function approximation. We show that several well-known reinforcement learning methods such as the original Actor-Critic and Bradtke’s Linear Quadratic Q-Learning are in fact Natural Actor-Critic algorithms. Empirical evaluations illustrate the effectiveness of our techniques in comparison to previous methods, and also demonstrate their applicability for learning control on an anthropomorphic robot arm.},
  isbn = {978-3-540-31692-3},
  langid = {english},
  keywords = {Fisher Information Matrix,Imitation Learning,Motor Primitive,Natural Gradient,Reinforcement Learning},
  file = {/Users/vitay/Documents/Zotero/storage/FPIZ3UKP/Peters et al_2005_Natural Actor-Critic.pdf}
}

@article{Peters2008,
  title = {Reinforcement Learning of Motor Skills with Policy Gradients},
  author = {Peters, Jan and Schaal, Stefan},
  date = {2008-05},
  journaltitle = {Neural Networks},
  volume = {21},
  number = {4},
  eprint = {18482830},
  eprinttype = {pubmed},
  pages = {682--697},
  doi = {10.1016/j.neunet.2008.02.003},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/18482830},
  abstract = {Autonomous learning is one of the hallmarks of human and animal behavior, and understanding the principles of learning will be crucial in order to achieve true autonomy in advanced machines like humanoid robots. In this paper, we examine learning of complex motor skills with human-like limbs. While supervised learning can offer useful tools for bootstrapping behavior, e.g., by learning from demonstration, it is only reinforcement learning that offers a general approach to the final trial-and-error improvement that is needed by each individual acquiring a skill. Neither neurobiological nor machine learning studies have, so far, offered compelling results on how reinforcement learning can be scaled to the high-dimensional continuous state and action spaces of humans or humanoids. Here, we combine two recent research developments on learning motor control in order to achieve this scaling. First, we interpret the idea of modular motor control by means of motor primitives as a suitable way to generate parameterized control policies for reinforcement learning. Second, we combine motor primitives with the theory of stochastic policy gradient learning, which currently seems to be the only feasible framework for reinforcement learning for humanoids. We evaluate different policy gradient methods with a focus on their applicability to parameterized motor primitives. We compare these algorithms in the context of motor primitive learning, and show that our most modern algorithm, the Episodic Natural Actor-Critic outperforms previous algorithms by at least an order of magnitude. We demonstrate the efficiency of this reinforcement learning method in the application of learning to hit a baseball with an anthropomorphic robot arm.}
}

@article{Piloto2022,
  title = {Intuitive Physics Learning in a Deep-Learning Model Inspired by Developmental Psychology},
  author = {Piloto, Luis S. and Weinstein, Ari and Battaglia, Peter and Botvinick, Matthew},
  date = {2022-07-11},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  pages = {1--11},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01394-8},
  url = {https://www.nature.com/articles/s41562-022-01394-8},
  urldate = {2022-07-17},
  abstract = {‘Intuitive physics’ enables our pragmatic engagement with the physical world and forms a key component of ‘common sense’ aspects of thought. Current artificial intelligence systems pale in their understanding of intuitive physics, in comparison to even very young children. Here we address this gap between humans and machines by drawing on the field of developmental psychology. First, we introduce and open-source a machine-learning dataset designed to evaluate conceptual understanding of intuitive physics, adopting the violation-of-expectation (VoE) paradigm from developmental psychology. Second, we build a deep-learning system that learns intuitive physics directly from visual data, inspired by studies of visual cognition in children. We demonstrate that our model can learn a diverse set of physical concepts, which depends critically on object-level representations, consistent with findings from developmental psychology. We consider the implications of these results both for AI and for research on human cognition.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/VSPAXNAI/Piloto_et_al_2022_Intuitive_physics_learning_in_a_deep-learning_model_inspired_by_developmental.pdf;/Users/vitay/Documents/Zotero/storage/FGH7U2GC/s41562-022-01394-8.html}
}

@report{Piray2019a,
  type = {preprint},
  title = {Linear Reinforcement Learning: {{Flexible}} Reuse of Computation in Planning, Grid Fields, and Cognitive Control},
  shorttitle = {Linear Reinforcement Learning},
  author = {Piray, Payam and Daw, Nathaniel D.},
  date = {2019-11-27},
  institution = {Neuroscience},
  doi = {10.1101/856849},
  url = {http://biorxiv.org/lookup/doi/10.1101/856849},
  urldate = {2020-05-10},
  abstract = {It is thought that the brain’s judicious allocation and reuse of computation underlies our ability to plan flexibly, but also failures to do so as in habits and compulsion. Yet we lack a complete, realistic account of either. Building on control engineering, we introduce a new model for decision making in the brain that reuses a temporally abstracted map of future events to enable biologically-realistic, flexible choice at the expense of specific, quantifiable biases. It replaces the classic nonlinear, model-based optimization with a linear approximation that softly maximizes around (and is weakly biased toward) a learned default policy. This solution exposes connections between seemingly disparate phenomena across behavioral neuroscience, notably flexible replanning with biases and cognitive control. It also gives new insight into how the brain can represent maps of long-distance contingencies stably and componentially, as in entorhinal response fields, and exploit them to guide choice even under changing goals.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/TRHY9MYX/Piray_Daw_2019_Linear_reinforcement_learning.pdf}
}

@book{Plaat2022,
  title = {Deep {{Reinforcement Learning}}, a Textbook},
  author = {Plaat, Aske},
  date = {2022},
  eprint = {2201.02135},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.02135},
  urldate = {2023-02-25},
  abstract = {Deep reinforcement learning has gathered much attention recently. Impressive results were achieved in activities as diverse as autonomous driving, game playing, molecular recombination, and robotics. In all these fields, computer programs have taught themselves to solve difficult problems. They have learned to fly model helicopters and perform aerobatic manoeuvers such as loops and rolls. In some applications they have even become better than the best humans, such as in Atari, Go, poker and StarCraft. The way in which deep reinforcement learning explores complex environments reminds us of how children learn, by playfully trying out things, getting feedback, and trying again. The computer seems to truly possess aspects of human learning; this goes to the heart of the dream of artificial intelligence. The successes in research have not gone unnoticed by educators, and universities have started to offer courses on the subject. The aim of this book is to provide a comprehensive overview of the field of deep reinforcement learning. The book is written for graduate students of artificial intelligence, and for researchers and practitioners who wish to better understand deep reinforcement learning methods and their challenges. We assume an undergraduate-level of understanding of computer science and artificial intelligence; the programming language of this book is Python. We describe the foundations, the algorithms and the applications of deep reinforcement learning. We cover the established model-free and model-based methods that form the basis of the field. Developments go quickly, and we also cover advanced topics: deep multi-agent reinforcement learning, deep hierarchical reinforcement learning, and deep meta learning.},
  file = {/Users/vitay/Documents/Zotero/storage/3WR399R9/Plaat - 2022 - Deep Reinforcement Learning, a textbook.pdf}
}

@unpublished{Plappert2018,
  title = {Parameter {{Space Noise}} for {{Exploration}}},
  author = {Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y. and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
  date = {2018-01-31},
  eprint = {1706.01905},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.01905},
  urldate = {2019-12-18},
  abstract = {Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/PKNRNMFH/Plappert_et_al_2018_Parameter_Space_Noise_for_Exploration.pdf;/Users/vitay/Documents/Zotero/storage/XUCDZHEV/1706.html}
}

@unpublished{Pong2018,
  title = {Temporal {{Difference Models}}: {{Model-Free Deep RL}} for {{Model-Based Control}}},
  author = {Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey},
  date = {2018-02},
  eprint = {1802.09081},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.09081},
  abstract = {Model-free reinforcement learning (RL) is a powerful, general tool for learning complex behaviors. However, its sample efficiency is often impractically large for solving challenging real-world problems, even with off-policy algorithms such as Q-learning. A limiting factor in classic model-free RL is that the learning signal consists only of scalar rewards, ignoring much of the rich information contained in state transition tuples. Model-based RL uses this information, by training a predictive model, but often does not achieve the same asymptotic performance as model-free RL due to model bias. We introduce temporal difference models (TDMs), a family of goal-conditioned value functions that can be trained with model-free learning and used for model-based control. TDMs combine the benefits of model-free and model-based RL: they leverage the rich information in state transitions to learn very efficiently, while still attaining asymptotic performance that exceeds that of direct model-based RL methods. Our experimental results show that, on a range of continuous control tasks, TDMs provide a substantial improvement in efficiency compared to state-of-the-art model-based and model-free methods.},
  file = {/Users/vitay/Documents/Zotero/storage/ATB4UNDG/Pong et al_2018_Temporal Difference Models.pdf}
}

@unpublished{Popov2017,
  title = {Data-Efficient {{Deep Reinforcement Learning}} for {{Dexterous Manipulation}}},
  author = {Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-Maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Riedmiller, Martin},
  date = {2017-04},
  eprint = {1704.03073},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1704.03073},
  abstract = {Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.}
}

@unpublished{Power2022,
  title = {Variational {{Inference MPC}} Using {{Normalizing Flows}} and {{Out-of-Distribution Projection}}},
  author = {Power, Thomas and Berenson, Dmitry},
  date = {2022-05-10},
  eprint = {2205.04667},
  eprinttype = {arXiv},
  eprintclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2205.04667},
  urldate = {2022-05-24},
  abstract = {We propose a Model Predictive Control (MPC) method for collision-free navigation that uses amortized variational inference to approximate the distribution of optimal control sequences by training a normalizing flow conditioned on the start, goal and environment. This representation allows us to learn a distribution that accounts for both the dynamics of the robot and complex obstacle geometries. We can then sample from this distribution to produce control sequences which are likely to be both goal-directed and collision-free as part of our proposed FlowMPPI sampling-based MPC method. However, when deploying this method, the robot may encounter an out-of-distribution (OOD) environment, i.e. one which is radically different from those used in training. In such cases, the learned flow cannot be trusted to produce low-cost control sequences. To generalize our method to OOD environments we also present an approach that performs projection on the representation of the environment as part of the MPC process. This projection changes the environment representation to be more in-distribution while also optimizing trajectory quality in the true environment. Our simulation results on a 2D double-integrator and a 3D 12DoF underactuated quadrotor suggest that FlowMPPI with projection outperforms state-of-the-art MPC baselines on both in-distribution and OOD environments, including OOD environments generated from real-world data.},
  file = {/Users/vitay/Documents/Zotero/storage/KMVHB4SP/Power_Berenson_2022_Variational_Inference_MPC_using_Normalizing_Flows_and_Out-of-Distribution.pdf;/Users/vitay/Documents/Zotero/storage/E3ZEUKUL/2205.html}
}

@inproceedings{Precup2000,
  title = {Eligibility Traces for Off-Policy Policy Evaluation},
  booktitle = {Proceedings of the {{Seventeenth International Conference}} on {{Machine Learning}}.},
  author = {Precup, D. and Sutton, R. S and Singh, S.},
  date = {2000}
}

@unpublished{Rajeswaran2019,
  title = {Meta-{{Learning}} with {{Implicit Gradients}}},
  author = {Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham and Levine, Sergey},
  date = {2019-09-10},
  eprint = {1909.04630},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1909.04630},
  urldate = {2019-09-14},
  abstract = {A core capability of intelligent systems is the ability to quickly learn new tasks by drawing on prior experience. Gradient (or optimization) based meta-learning has recently emerged as an effective approach for few-shot learning. In this formulation, meta-parameters are learned in the outer loop, while task-specific models are learned in the inner-loop, by using only a small amount of data from the current task. A key challenge in scaling these approaches is the need to differentiate through the inner loop learning process, which can impose considerable computational and memory burdens. By drawing upon implicit differentiation, we develop the implicit MAML algorithm, which depends only on the solution to the inner level optimization and not the path taken by the inner loop optimizer. This effectively decouples the meta-gradient computation from the choice of inner loop optimizer. As a result, our approach is agnostic to the choice of inner loop optimizer and can gracefully handle many gradient steps without vanishing gradients or memory constraints. Theoretically, we prove that implicit MAML can compute accurate meta-gradients with a memory footprint that is, up to small constant factors, no more than that which is required to compute a single inner loop gradient and at no overall increase in the total computational cost. Experimentally, we show that these benefits of implicit MAML translate into empirical gains on few-shot image recognition benchmarks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/B6N7Q455/Rajeswaran et al_2019_Meta-Learning with Implicit Gradients.pdf;/Users/vitay/Documents/Zotero/storage/F9HXRY29/1909.html}
}

@unpublished{Rakelly2019,
  title = {Efficient {{Off-Policy Meta-Reinforcement Learning}} via {{Probabilistic Context Variables}}},
  author = {Rakelly, Kate and Zhou, Aurick and Quillen, Deirdre and Finn, Chelsea and Levine, Sergey},
  date = {2019-03-19},
  eprint = {1903.08254},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1903.08254},
  urldate = {2021-02-05},
  abstract = {Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While in principle meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. The also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness in sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/RUFGUSJT/Rakelly_et_al_2019_Efficient_Off-Policy_Meta-Reinforcement_Learning_via_Probabilistic_Context.pdf;/Users/vitay/Documents/Zotero/storage/IJU3BHT6/1903.html}
}

@unpublished{Reddy2019,
  title = {Learning {{Human Objectives}} by {{Evaluating Hypothetical Behavior}}},
  author = {Reddy, Siddharth and Dragan, Anca D. and Levine, Sergey and Legg, Shane and Leike, Jan},
  date = {2019-12-05},
  eprint = {1912.05652},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.05652},
  urldate = {2019-12-17},
  abstract = {We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. To address this challenge, we propose an algorithm that safely and interactively learns a model of the user's reward function. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/AF2934J7/Reddy_et_al_2019_Learning_Human_Objectives_by_Evaluating_Hypothetical_Behavior.pdf;/Users/vitay/Documents/Zotero/storage/B7QD6ZC6/1912.html}
}

@online{Ren2024,
  title = {Diffusion {{Policy Policy Optimization}}},
  author = {Ren, Allen Z. and Lidard, Justin and Ankile, Lars L. and Simeonov, Anthony and Agrawal, Pulkit and Majumdar, Anirudha and Burchfiel, Benjamin and Dai, Hongkai and Simchowitz, Max},
  date = {2024-09-01},
  url = {https://arxiv.org/abs/2409.00588v1},
  urldate = {2024-10-10},
  abstract = {We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task. Website with code: diffusion-ppo.github.io},
  langid = {english},
  organization = {arXiv.org},
  file = {/Users/vitay/Documents/Zotero/storage/8PMBWKPF/Ren et al. - 2024 - Diffusion Policy Policy Optimization.pdf}
}

@unpublished{Rohde2018,
  title = {{{RecoGym}}: {{A Reinforcement Learning Environment}} for the Problem of {{Product Recommendation}} in {{Online Advertising}}},
  shorttitle = {{{RecoGym}}},
  author = {Rohde, David and Bonner, Stephen and Dunlop, Travis and Vasile, Flavian and Karatzoglou, Alexandros},
  date = {2018-09-14},
  eprint = {1808.00720},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1808.00720},
  urldate = {2019-11-24},
  abstract = {Recommender Systems are becoming ubiquitous in many settings and take many forms, from product recommendation in e-commerce stores, to query suggestions in search engines, to friend recommendation in social networks. Current research directions which are largely based upon supervised learning from historical data appear to be showing diminishing returns with a lot of practitioners report a discrepancy between improvements in offline metrics for supervised learning and the online performance of the newly proposed models. One possible reason is that we are using the wrong paradigm: when looking at the long-term cycle of collecting historical performance data, creating a new version of the recommendation model, A/B testing it and then rolling it out. We see that there a lot of commonalities with the reinforcement learning (RL) setup, where the agent observes the environment and acts upon it in order to change its state towards better states (states with higher rewards). To this end we introduce RecoGym, an RL environment for recommendation, which is defined by a model of user traffic patterns on e-commerce and the users response to recommendations on the publisher websites. We believe that this is an important step forward for the field of recommendation systems research, that could open up an avenue of collaboration between the recommender systems and reinforcement learning communities and lead to better alignment between offline and online performance metrics.},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/SCV934QP/Rohde et al_2018_RecoGym.pdf;/Users/vitay/Documents/Zotero/storage/E9TTM574/1808.html}
}

@unpublished{Roy2020,
  title = {{{OPAC}}: {{Opportunistic Actor-Critic}}},
  shorttitle = {{{OPAC}}},
  author = {Roy, Srinjoy and Bakshi, Saptam and Maharaj, Tamal},
  date = {2020-12-11},
  eprint = {2012.06555},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2012.06555},
  urldate = {2020-12-20},
  abstract = {Actor-critic methods, a type of model-free reinforcement learning (RL), have achieved state-of-the-art performances in many real-world domains in continuous control. Despite their success, the wide-scale deployment of these models is still a far cry. The main problems in these actor-critic methods are inefficient exploration and sub-optimal policies. Soft Actor-Critic (SAC) and Twin Delayed Deep Deterministic Policy Gradient (TD3), two cutting edge such algorithms, suffer from these issues. SAC effectively addressed the problems of sample complexity and convergence brittleness to hyper-parameters and thus outperformed all state-of-the-art algorithms including TD3 in harder tasks, whereas TD3 produced moderate results in all environments. SAC suffers from inefficient exploration owing to the Gaussian nature of its policy which causes borderline performance in simpler tasks. In this paper, we introduce Opportunistic Actor-Critic (OPAC), a novel model-free deep RL algorithm that employs better exploration policy and lesser variance. OPAC combines some of the most powerful features of TD3 and SAC and aims to optimize a stochastic policy in an off-policy way. For calculating the target Q-values, instead of two critics, OPAC uses three critics and based on the environment complexity, opportunistically chooses how the target Q-value is computed from the critics' evaluation. We have systematically evaluated the algorithm on MuJoCo environments where it achieves state-of-the-art performance and outperforms or at least equals the performance of TD3 and SAC.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/ZQYFREN8/Roy_et_al_2020_OPAC.pdf;/Users/vitay/Documents/Zotero/storage/WHQF3CZJ/2012.html}
}

@online{Roy2022,
  title = {{{PrefixRL}}: {{Optimization}} of {{Parallel Prefix Circuits}} Using {{Deep Reinforcement Learning}}},
  shorttitle = {{{PrefixRL}}},
  author = {Roy, Rajarshi and Raiman, Jonathan and Kant, Neel and Elkin, Ilyas and Kirby, Robert and Siu, Michael and Oberman, Stuart and Godil, Saad and Catanzaro, Bryan},
  date = {2022-05-14},
  eprint = {2205.07000},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.1109/DAC18074.2021.9586094},
  url = {http://arxiv.org/abs/2205.07000},
  urldate = {2023-02-05},
  abstract = {In this work, we present a reinforcement learning (RL) based approach to designing parallel prefix circuits such as adders or priority encoders that are fundamental to high-performance digital design. Unlike prior methods, our approach designs solutions tabula rasa purely through learning with synthesis in the loop. We design a grid-based state-action representation and an RL environment for constructing legal prefix circuits. Deep Convolutional RL agents trained on this environment produce prefix adder circuits that Pareto-dominate existing baselines with up to 16.0\% and 30.2\% lower area for the same delay in the 32b and 64b settings respectively. We observe that agents trained with open-source synthesis tools and cell library can design adder circuits that achieve lower area and delay than commercial tool adders in an industrial cell library.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/3ALU4CIM/Roy_et_al_2022_PrefixRL.pdf}
}

@article{Rudner2020,
  title = {Outcome-{{Driven Reinforcement Learning}} via {{Variational Inference}}},
  author = {Rudner, Tim G J and Pong, Vitchyr H and McAllister, Rowan and Gal, Yarin and Levine, Sergey},
  date = {2020},
  pages = {26},
  abstract = {While reinforcement learning algorithms provide automated acquisition of optimal policies, practical application of such methods requires a number of design decisions, such as manually designing reward functions that not only define the task, but also provide sufficient shaping to accomplish it. In this paper, we discuss a new perspective on reinforcement learning, recasting it as the problem of inferring actions that achieve desired outcomes, rather than a problem of maximizing rewards. To solve the resulting outcome-directed inference problem, we establish a novel variational inference formulation that allows us to derive a well-shaped reward function which can be learned directly from environment interactions. From the corresponding variational objective, we also derive a new probabilistic Bellman backup operator reminiscent of the standard Bellman backup operator and use it to develop an off-policy algorithm to solve goal-directed tasks. We empirically demonstrate that this method eliminates the need to design reward functions and leads to effective goal-directed behaviors.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/VLFSREAG/Rudner et al. - Outcome-Driven Reinforcement Learning via Variatio.pdf}
}

@article{Russek2017,
  title = {Predictive Representations Can Link Model-Based Reinforcement Learning to Model-Free Mechanisms},
  author = {Russek, Evan M. and Momennejad, Ida and Botvinick, Matthew M. and Gershman, Samuel J. and Daw, Nathaniel D.},
  editor = {Daunizeau, Jean},
  date = {2017-09-25},
  journaltitle = {PLOS Computational Biology},
  volume = {13},
  number = {9},
  pages = {e1005768},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005768},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1005768},
  urldate = {2019-03-25},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/4UDPNQ9T/Russek et al_2017_Predictive representations can link model-based reinforcement learning to.pdf}
}

@unpublished{Rusu2016,
  title = {Policy {{Distillation}}},
  author = {Rusu, Andrei A. and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  date = {2016-01-07},
  eprint = {1511.06295},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.06295},
  urldate = {2020-01-26},
  abstract = {Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/87PAKPFB/Rusu_et_al_2016_Policy_Distillation.pdf;/Users/vitay/Documents/Zotero/storage/AG534KB2/1511.html}
}

@unpublished{Rusu2016a,
  title = {Progressive {{Neural Networks}}},
  author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  date = {2016-09-07},
  eprint = {1606.04671},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1606.04671},
  urldate = {2022-04-26},
  abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
  file = {/Users/vitay/Documents/Zotero/storage/5ILYRXCH/Rusu_et_al_2016_Progressive_Neural_Networks.pdf;/Users/vitay/Documents/Zotero/storage/IDZBBUV5/1606.html}
}

@article{Salimans2017,
  title = {Evolution {{Strategies}} as a {{Scalable Alternative}} to {{Reinforcement Learning}}},
  author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  date = {2017-03},
  url = {http://arxiv.org/abs/1703.03864},
  abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.}
}

@unpublished{Sanchez-Gonzalez2018,
  title = {Graph Networks as Learnable Physics Engines for Inference and Control},
  author = {Sanchez-Gonzalez, Alvaro and Heess, Nicolas and Springenberg, Jost Tobias and Merel, Josh and Riedmiller, Martin and Hadsell, Raia and Battaglia, Peter},
  date = {2018-06-04},
  eprint = {1806.01242},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.01242},
  urldate = {2019-03-18},
  abstract = {Understanding and interacting with everyday physical scenes requires rich knowledge about the structure of the world, represented either implicitly in a value or policy function, or explicitly in a transition model. Here we introduce a new class of learnable models--based on graph networks--which implement an inductive bias for object- and relation-centric representations of complex, dynamical systems. Our results show that as a forward model, our approach supports accurate predictions from real and simulated data, and surprisingly strong and efficient generalization, across eight distinct physical systems which we varied parametrically and structurally. We also found that our inference model can perform system identification. Our models are also differentiable, and support online planning via gradient-based trajectory optimization, as well as offline policy optimization. Our framework offers new opportunities for harnessing and exploiting rich knowledge about the world, and takes a key step toward building machines with more human-like representations of the world.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/IUJ378PS/Sanchez-Gonzalez et al_2018_Graph networks as learnable physics engines for inference and control.pdf;/Users/vitay/Documents/Zotero/storage/X89VF4G4/1806.html}
}

@unpublished{Schaul2015,
  title = {Prioritized {{Experience Replay}}},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  date = {2015-11},
  eprint = {1511.05952},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.05952},
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  file = {/Users/vitay/Documents/Zotero/storage/PSL8LU4J/Schaul et al_2015_Prioritized Experience Replay.pdf}
}

@unpublished{Schmeckpeper2019,
  title = {Learning {{Predictive Models From Observation}} and {{Interaction}}},
  author = {Schmeckpeper, Karl and Xie, Annie and Rybkin, Oleh and Tian, Stephen and Daniilidis, Kostas and Levine, Sergey and Finn, Chelsea},
  date = {2019-12-29},
  eprint = {1912.12773},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.12773},
  urldate = {2020-01-04},
  abstract = {Learning predictive models from interaction with the world allows an agent, such as a robot, to learn about how the world works, and then use this learned model to plan coordinated sequences of actions to bring about desired outcomes. However, learning a model that captures the dynamics of complex skills represents a major challenge: if the agent needs a good model to perform these skills, it might never be able to collect the experience on its own that is required to learn these delicate and complex behaviors. Instead, we can imagine augmenting the training set with observational data of other agents, such as humans. Such data is likely more plentiful, but represents a different embodiment. For example, videos of humans might show a robot how to use a tool, but (i) are not annotated with suitable robot actions, and (ii) contain a systematic distributional shift due to the embodiment differences between humans and robots. We address the first challenge by formulating the corresponding graphical model and treating the action as an observed variable for the interaction data and an unobserved variable for the observation data, and the second challenge by using a domain-dependent prior. In addition to interaction data, our method is able to leverage videos of passive observations in a driving dataset and a dataset of robotic manipulation videos. A robotic planning agent equipped with our method can learn to use tools in a tabletop robotic manipulation setting by observing humans without ever seeing a robotic video of tool use.},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/4RVYBHP8/Schmeckpeper_et_al_2019_Learning_Predictive_Models_From_Observation_and_Interaction.pdf;/Users/vitay/Documents/Zotero/storage/TC5R5VLD/1912.html}
}

@inproceedings{Schmeckpeper2020,
  title = {Learning {{Predictive Models}} from {{Observation}} and {{Interaction}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Schmeckpeper, Karl and Xie, Annie and Rybkin, Oleh and Tian, Stephen and Daniilidis, Kostas and Levine, Sergey and Finn, Chelsea},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {708--725},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-58565-5_42},
  abstract = {Learning predictive models from interaction with the world allows an agent, such as a robot, to learn about how the world works, and then use this learned model to plan coordinated sequences of actions to bring about desired outcomes. However, learning a model that captures the dynamics of complex skills represents a major challenge: if the agent needs a good model to perform these skills, it might never be able to collect the experience on its own that is required to learn these delicate and complex behaviors. Instead, we can imagine augmenting the training set with observational data of other agents, such as humans. Such data is likely more plentiful, but cannot always be combined with data from the original agent. For example, videos of humans might show a robot how to use a tool, but (i) are not annotated with suitable robot actions, and (ii) contain a systematic distributional shift due to the embodiment differences between humans and robots. We address the first challenge by formulating the corresponding graphical model and treating the action as an observed variable for the interaction data and an unobserved variable for the observation data, and the second challenge by using a domain-dependent prior. In addition to interaction data, our method is able to leverage videos of passive observations in a driving dataset and a dataset of robotic manipulation videos to improve video prediction performance. In a real-world tabletop robotic manipulation setting, our method is able to significantly improve control performance by learning a model from both robot data and observations of humans.},
  isbn = {978-3-030-58565-5},
  langid = {english},
  keywords = {Action representations,Robotic manipulation,Video prediction,Visual planning}
}

@unpublished{Schmeckpeper2020a,
  title = {Reinforcement {{Learning}} with {{Videos}}: {{Combining Offline Observations}} with {{Interaction}}},
  shorttitle = {Reinforcement {{Learning}} with {{Videos}}},
  author = {Schmeckpeper, Karl and Rybkin, Oleh and Daniilidis, Kostas and Levine, Sergey and Finn, Chelsea},
  date = {2020-11-12},
  eprint = {2011.06507},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2011.06507},
  urldate = {2020-11-17},
  abstract = {Reinforcement learning is a powerful framework for robots to acquire skills from experience, but often requires a substantial amount of online data collection. As a result, it is difficult to collect sufficiently diverse experiences that are needed for robots to generalize broadly. Videos of humans, on the other hand, are a readily available source of broad and interesting experiences. In this paper, we consider the question: can we perform reinforcement learning directly on experience collected by humans? This problem is particularly difficult, as such videos are not annotated with actions and exhibit substantial visual domain shift relative to the robot's embodiment. To address these challenges, we propose a framework for reinforcement learning with videos (RLV). RLV learns a policy and value function using experience collected by humans in combination with data collected by robots. In our experiments, we find that RLV is able to leverage such videos to learn challenging vision-based skills with less than half as many samples as RL methods that learn from scratch.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/GNYGKANZ/Schmeckpeper_et_al_2020_Reinforcement_Learning_with_Videos.pdf;/Users/vitay/Documents/Zotero/storage/JMDNCMZZ/2011.html}
}

@unpublished{Schoettler2019,
  title = {Deep {{Reinforcement Learning}} for {{Industrial Insertion Tasks}} with {{Visual Inputs}} and {{Natural Rewards}}},
  author = {Schoettler, Gerrit and Nair, Ashvin and Luo, Jianlan and Bahl, Shikhar and Ojea, Juan Aparicio and Solowjow, Eugen and Levine, Sergey},
  date = {2019-06-13},
  eprint = {1906.05841},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1906.05841},
  urldate = {2019-06-18},
  abstract = {Connector insertion and many other tasks commonly found in modern manufacturing settings involve complex contact dynamics and friction. Since it is difficult to capture related physical effects with first-order modeling, traditional control methods often result in brittle and inaccurate controllers, which have to be manually tuned. Reinforcement learning (RL) methods have been demonstrated to be capable of learning controllers in such environments from autonomous interaction with the environment, but running RL algorithms in the real world poses sample efficiency and safety challenges. Moreover, in practical real-world settings we cannot assume access to perfect state information or dense reward signals. In this paper, we consider a variety of difficult industrial insertion tasks with visual inputs and different natural reward specifications, namely sparse rewards and goal images. We show that methods that combine RL with prior information, such as classical controllers or demonstrations, can solve these tasks from a reasonable amount of real-world interaction.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/I3MYZP9Z/Schoettler et al_2019_Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs.pdf;/Users/vitay/Documents/Zotero/storage/UPTIU9EV/1906.html}
}

@unpublished{Schoettler2020,
  title = {Meta-{{Reinforcement Learning}} for {{Robotic Industrial Insertion Tasks}}},
  author = {Schoettler, Gerrit and Nair, Ashvin and Ojea, Juan Aparicio and Levine, Sergey and Solowjow, Eugen},
  date = {2020-04-29},
  eprint = {2004.14404},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2004.14404},
  urldate = {2020-05-05},
  abstract = {Robotic insertion tasks are characterized by contact and friction mechanics, making them challenging for conventional feedback control methods due to unmodeled physical effects. Reinforcement learning (RL) is a promising approach for learning control policies in such settings. However, RL can be unsafe during exploration and might require a large amount of real-world training data, which is expensive to collect. In this paper, we study how to use meta-reinforcement learning to solve the bulk of the problem in simulation by solving a family of simulated industrial insertion tasks and then adapt policies quickly in the real world. We demonstrate our approach by training an agent to successfully perform challenging real-world insertion tasks using less than 20 trials of real-world experience.},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/DBYYHJ4U/Schoettler_et_al_2020_Meta-Reinforcement_Learning_for_Robotic_Industrial_Insertion_Tasks.pdf;/Users/vitay/Documents/Zotero/storage/IEFBEKI6/2004.html}
}

@unpublished{Schrittwieser2019,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  date = {2019-11-19},
  eprint = {1911.08265},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1911.08265},
  urldate = {2019-11-24},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/MHMRTIYK/Schrittwieser et al_2019_Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.pdf;/Users/vitay/Documents/Zotero/storage/IZA7T6NF/1911.html}
}

@unpublished{Schrittwieser2021,
  title = {Online and {{Offline Reinforcement Learning}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Hubert, Thomas and Mandhane, Amol and Barekatain, Mohammadamin and Antonoglou, Ioannis and Silver, David},
  date = {2021-04-13},
  eprint = {2104.06294},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.06294},
  urldate = {2021-04-17},
  abstract = {Learning efficiently from small amounts of data has long been the focus of model-based reinforcement learning, both for the online case when interacting with the environment and the offline case when learning from a fixed dataset. However, to date no single unified algorithm could demonstrate state-of-the-art results in both settings. In this work, we describe the Reanalyse algorithm which uses model-based policy and value improvement operators to compute new improved training targets on existing data points, allowing efficient learning for data budgets varying by several orders of magnitude. We further show that Reanalyse can also be used to learn entirely from demonstrations without any environment interactions, as in the case of offline Reinforcement Learning (offline RL). Combining Reanalyse with the MuZero algorithm, we introduce MuZero Unplugged, a single unified algorithm for any data budget, including offline RL. In contrast to previous work, our algorithm does not require any special adaptations for the off-policy or offline RL settings. MuZero Unplugged sets new state-of-the-art results in the RL Unplugged offline RL benchmark as well as in the online RL benchmark of Atari in the standard 200 million frame setting.},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/8SB29DLL/Schrittwieser_et_al_2021_Online_and_Offline_Reinforcement_Learning_by_Planning_with_a_Learned_Model.pdf;/Users/vitay/Documents/Zotero/storage/ZGLEI5SW/2104.html}
}

@unpublished{Schulman2015,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  date = {2015-06},
  eprint = {1506.02438},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.02438},
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  file = {/Users/vitay/Documents/Zotero/storage/8QLIMRG7/Schulman et al_2015_High-Dimensional Continuous Control Using Generalized Advantage Estimation.pdf}
}

@inproceedings{Schulman2015a,
  title = {Trust {{Region Policy Optimization}}},
  booktitle = {Proceedings of the 31 St {{International Conference}} on {{Machine Learning}}},
  author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  date = {2015-06},
  pages = {1889--1897},
  url = {http://proceedings.mlr.press/v37/schulman15.html},
  file = {/Users/vitay/Documents/Zotero/storage/ESWH62ME/Schulman et al_2015_Trust Region Policy Optimization.pdf}
}

@unpublished{Schulman2017,
  title = {Equivalence {{Between Policy Gradients}} and {{Soft Q-Learning}}},
  author = {Schulman, John and Chen, Xi and Abbeel, Pieter},
  date = {2017-04-21},
  eprint = {1704.06440},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1704.06440},
  urldate = {2019-06-12},
  abstract = {Two of the leading approaches for model-free reinforcement learning are policy gradient methods and \$Q\$-learning methods. \$Q\$-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the \$Q\$-values they estimate are very inaccurate. A partial explanation may be that \$Q\$-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between \$Q\$-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that "soft" (entropy-regularized) \$Q\$-learning is exactly equivalent to a policy gradient method. We also point out a connection between \$Q\$-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of \$Q\$-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a \$Q\$-learning method that closely matches the learning dynamics of A3C without using a target network or \$\textbackslash epsilon\$-greedy exploration schedule.},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/NFH3MA5J/Schulman et al_2017_Equivalence Between Policy Gradients and Soft Q-Learning.pdf;/Users/vitay/Documents/Zotero/storage/GF58N5TU/1704.html}
}

@unpublished{Schulman2017a,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date = {2017-07},
  eprint = {1707.06347},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.06347},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  file = {/Users/vitay/Documents/Zotero/storage/DAHLLUNV/Schulman et al_2017_Proximal Policy Optimization Algorithms.pdf}
}

@online{Sekar2020,
  title = {Planning to {{Explore}} via {{Self-Supervised World Models}}},
  author = {Sekar, Ramanan and Rybkin, Oleh and Daniilidis, Kostas and Abbeel, Pieter and Hafner, Danijar and Pathak, Deepak},
  date = {2020-06-30},
  eprint = {2005.05960},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2005.05960},
  urldate = {2024-01-29},
  abstract = {Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at https://ramanans1.github.io/plan2explore/},
  langid = {english},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/JGH5R43L/Sekar et al. - 2020 - Planning to Explore via Self-Supervised World Mode.pdf}
}

@unpublished{Shah2020,
  title = {{{ViNG}}: {{Learning Open-World Navigation}} with {{Visual Goals}}},
  shorttitle = {{{ViNG}}},
  author = {Shah, Dhruv and Eysenbach, Benjamin and Kahn, Gregory and Rhinehart, Nicholas and Levine, Sergey},
  date = {2020-12-17},
  eprint = {2012.09812},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2012.09812},
  urldate = {2020-12-23},
  abstract = {We propose a learning-based navigation system for reaching visually indicated goals and demonstrate this system on a real mobile robot platform. Learning provides an appealing alternative to conventional methods for robotic navigation: instead of reasoning about environments in terms of geometry and maps, learning can enable a robot to learn about navigational affordances, understand what types of obstacles are traversable (e.g., tall grass) or not (e.g., walls), and generalize over patterns in the environment. However, unlike conventional planning algorithms, it is harder to change the goal for a learned policy during deployment. We propose a method for learning to navigate towards a goal image of the desired destination. By combining a learned policy with a topological graph constructed out of previously observed data, our system can determine how to reach this visually indicated goal even in the presence of variable appearance and lighting. Three key insights, waypoint proposal, graph pruning and negative mining, enable our method to learn to navigate in real-world environments using only offline data, a setting where prior methods struggle. We instantiate our method on a real outdoor ground robot and show that our system, which we call ViNG, outperforms previously-proposed methods for goal-conditioned reinforcement learning, including other methods that incorporate reinforcement learning and search. We also study how ViNG generalizes to unseen environments and evaluate its ability to adapt to such an environment with growing experience. Finally, we demonstrate ViNG on a number of real-world applications, such as last-mile delivery and warehouse inspection. We encourage the reader to check out the videos of our experiments and demonstrations at our project website https://sites.google.com/view/ving-robot},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/MNCXC84J/Shah_et_al_2020_ViNG.pdf;/Users/vitay/Documents/Zotero/storage/DCNNKDDL/2012.html}
}

@inproceedings{Sharma2019,
  title = {Dynamics-Aware Unsupervised Skill Discovery},
  booktitle = {Task-Agnostic {{Reinforcement Learning Workshop}}},
  author = {Sharma, Archit and Gu, Shane and Levine, Sergey and Kumar, Vikash and Hausman, Karol},
  date = {2019},
  pages = {11},
  url = {https://tarl2019.github.io/assets/papers/sharma2019dynamicsaware.pdf},
  abstract = {Model-based Reinforcement Learning (MBRL) shows the potential to facilitate learning of many different tasks by taking advantage of the learned model of the world. Learning a global model that works in every part of state-space, however, can be exceedingly challenging, especially in complex, dynamical environments. To overcome this problem, we present a method that is able to discover skills together with their ‘skill dynamics models’ in an unsupervised fashion. By finding the skills whose dynamics are easier to learn, we are able to effectively partition the space into local models and their corresponding policies. In addition, we show how a simple MBRL algorithm can leverage the learned skills to solve a set of downstream task without any additional learning. Our results indicate that our zero-shot online planning method that uses skill dynamics can significantly outperform strong baselines that were trained specifically on downstream tasks.},
  eventtitle = {{{ICLR}} 2019},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/KKYPCCTH/Sharma_et_al_2019_Dynamics-aware_unsupervised_skill_discovery.pdf}
}

@unpublished{Sharma2020,
  title = {Emergent {{Real-World Robotic Skills}} via {{Unsupervised Off-Policy Reinforcement Learning}}},
  author = {Sharma, Archit and Ahn, Michael and Levine, Sergey and Kumar, Vikash and Hausman, Karol and Gu, Shixiang},
  date = {2020-04-27},
  eprint = {2004.12974},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2004.12974},
  urldate = {2020-05-02},
  abstract = {Reinforcement learning provides a general framework for learning robotic skills while minimizing engineering effort. However, most reinforcement learning algorithms assume that a well-designed reward function is provided, and learn a single behavior for that single reward function. Such reward functions can be difficult to design in practice. Can we instead develop efficient reinforcement learning methods that acquire diverse skills without any reward function, and then repurpose these skills for downstream tasks? In this paper, we demonstrate that a recently proposed unsupervised skill discovery algorithm can be extended into an efficient off-policy method, making it suitable for performing unsupervised reinforcement learning in the real world. Firstly, we show that our proposed algorithm provides substantial improvement in learning efficiency, making reward-free real-world training feasible. Secondly, we move beyond the simulation environments and evaluate the algorithm on real physical hardware. On quadrupeds, we observe that locomotion skills with diverse gaits and different orientations emerge without any rewards or demonstrations. We also demonstrate that the learned skills can be composed using model predictive control for goal-oriented navigation, without any additional training.},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/8U5XL72V/Sharma_et_al_2020_Emergent_Real-World_Robotic_Skills_via_Unsupervised_Off-Policy_Reinforcement.pdf;/Users/vitay/Documents/Zotero/storage/U48YUZRC/2004.html}
}

@inproceedings{Silver2014,
  title = {Deterministic {{Policy Gradient Algorithms}}},
  booktitle = {Proc. {{ICML}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  editor = {Xing, Eric P and Jebara, Tony},
  date = {2014},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {32},
  pages = {387--395},
  publisher = {PMLR},
  url = {http://proceedings.mlr.press/v32/silver14.html},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces.},
  file = {/Users/vitay/Documents/Zotero/storage/2Q74BK77/Silver et al_2014_Deterministic Policy Gradient Algorithms.pdf}
}

@article{Silver2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  date = {2016-01},
  journaltitle = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  url = {https://www.nature.com/articles/nature16961},
  urldate = {2020-01-26},
  abstract = {A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/2JP65C32/Silver_et_al_2016_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search.pdf;/Users/vitay/Documents/Zotero/storage/H6GT26IP/nature16961.html}
}

@article{Silver2016a,
  title = {The {{Predictron}}: {{End-To-End Learning}} and {{Planning}}},
  author = {Silver, David and family=Hasselt, given=Hado, prefix=van, useprefix=true and Hessel, Matteo and Schaul, Tom and Guez, Arthur and Harley, Tim and Dulac-Arnold, Gabriel and Reichert, David and Rabinowitz, Neil and Barreto, Andre and Degris, Thomas},
  date = {2016-12},
  url = {http://arxiv.org/abs/1612.08810},
  abstract = {One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple "imagined" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.},
  file = {/Users/vitay/Documents/Zotero/storage/B3BW6WN3/Silver et al_2016_The Predictron.pdf}
}

@article{Silver2018,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  date = {2018-12-07},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {362},
  number = {6419},
  pages = {1140--1144},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aar6404},
  url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aar6404},
  urldate = {2020-01-26},
  abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from selfplay. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess) as well as Go.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/J3GUF2JA/Silver_et_al_2018_A_general_reinforcement_learning_algorithm_that_masters_chess,_shogi,_and_Go.pdf}
}

@article{Silver2021,
  title = {Reward {{Is Enough}}},
  author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
  date = {2021-05-24},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  pages = {103535},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103535},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370221000862},
  urldate = {2021-05-27},
  abstract = {In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.},
  langid = {english},
  keywords = {Artificial general intelligence,Artificial Intelligence,Reinforcement learning,reward},
  file = {/Users/vitay/Documents/Zotero/storage/629UUCJU/S0004370221000862.html}
}

@unpublished{Simeonov2020,
  title = {A {{Long Horizon Planning Framework}} for {{Manipulating Rigid Pointcloud Objects}}},
  author = {Simeonov, Anthony and Du, Yilun and Kim, Beomjoon and Hogan, Francois R. and Tenenbaum, Joshua and Agrawal, Pulkit and Rodriguez, Alberto},
  date = {2020-11-16},
  eprint = {2011.08177},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2011.08177},
  urldate = {2020-11-22},
  abstract = {We present a framework for solving long-horizon planning problems involving manipulation of rigid objects that operates directly from a point-cloud observation, i.e. without prior object models. Our method plans in the space of object subgoals and frees the planner from reasoning about robot-object interaction dynamics by relying on a set of generalizable manipulation primitives. We show that for rigid bodies, this abstraction can be realized using low-level manipulation skills that maintain sticking contact with the object and represent subgoals as 3D transformations. To enable generalization to unseen objects and improve planning performance, we propose a novel way of representing subgoals for rigid-body manipulation and a graph-attention based neural network architecture for processing point-cloud inputs. We experimentally validate these choices using simulated and real-world experiments on the YuMi robot. Results demonstrate that our method can successfully manipulate new objects into target configurations requiring long-term planning. Overall, our framework realizes the best of the worlds of task-and-motion planning (TAMP) and learning-based approaches. Project website: https://anthonysimeonov.github.io/rpo-planning-framework/.},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/PIA6SESA/Simeonov_et_al_2020_A_Long_Horizon_Planning_Framework_for_Manipulating_Rigid_Pointcloud_Objects.pdf;/Users/vitay/Documents/Zotero/storage/V2SHKIZT/2011.html}
}

@unpublished{Song2019,
  title = {V-{{MPO}}: {{On-Policy Maximum}} a {{Posteriori Policy Optimization}} for {{Discrete}} and {{Continuous Control}}},
  shorttitle = {V-{{MPO}}},
  author = {Song, H. Francis and Abdolmaleki, Abbas and Springenberg, Jost Tobias and Clark, Aidan and Soyer, Hubert and Rae, Jack W. and Noury, Seb and Ahuja, Arun and Liu, Siqi and Tirumala, Dhruva and Heess, Nicolas and Belov, Dan and Riedmiller, Martin and Botvinick, Matthew M.},
  date = {2019-09-26},
  eprint = {1909.12238},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1909.12238},
  urldate = {2019-10-01},
  abstract = {Some of the most successful applications of deep reinforcement learning to challenging domains in discrete and continuous control have used policy gradient methods in the on-policy setting. However, policy gradients can suffer from large variance that may limit performance, and in practice require carefully tuned entropy regularization to prevent policy collapse. As an alternative to policy gradient algorithms, we introduce V-MPO, an on-policy adaptation of Maximum a Posteriori Policy Optimization (MPO) that performs policy iteration based on a learned state-value function. We show that V-MPO surpasses previously reported scores for both the Atari-57 and DMLab-30 benchmark suites in the multi-task setting, and does so reliably without importance weighting, entropy regularization, or population-based tuning of hyperparameters. On individual DMLab and Atari levels, the proposed algorithm can achieve scores that are substantially higher than has previously been reported. V-MPO is also applicable to problems with high-dimensional, continuous action spaces, which we demonstrate in the context of learning to control simulated humanoids with 22 degrees of freedom from full state observations and 56 degrees of freedom from pixel observations, as well as example OpenAI Gym tasks where V-MPO achieves substantially higher asymptotic scores than previously reported.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/ZG3ACFVX/Song et al_2019_V-MPO.pdf;/Users/vitay/Documents/Zotero/storage/LPMUTVNV/1909.html}
}

@inproceedings{Spielberg2017,
  title = {Deep Reinforcement Learning Approaches for Process Control},
  booktitle = {2017 6th {{International Symposium}} on {{Advanced Control}} of {{Industrial Processes}} ({{AdCONIP}})},
  author = {Spielberg, S.P.K. and Gopaluni, R.B. and Loewen, P.D.},
  date = {2017-05},
  pages = {201--206},
  publisher = {IEEE},
  doi = {10.1109/ADCONIP.2017.7983780},
  url = {http://ieeexplore.ieee.org/document/7983780/},
  isbn = {978-1-5090-4397-2},
  file = {/Users/vitay/Documents/Zotero/storage/VQB8QGIU/Spielberg et al_2017_Deep reinforcement learning approaches for process control.pdf}
}

@article{Srinivas2018,
  title = {Universal {{Planning Networks}}},
  author = {Srinivas, Aravind and Jabri, Allan and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
  date = {2018-04},
  url = {http://arxiv.org/abs/1804.00645},
  abstract = {A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.},
  file = {/Users/vitay/Documents/Zotero/storage/UVJST5ZL/Srinivas et al_2018_Universal Planning Networks.pdf}
}

@article{Stachenfeld2017,
  title = {The Hippocampus as a Predictive Map},
  author = {Stachenfeld, Kimberly L and Botvinick, Matthew M and Gershman, Samuel J},
  date = {2017-10},
  journaltitle = {Nature Neuroscience},
  volume = {20},
  number = {11},
  pages = {1643--1653},
  doi = {10.1038/nn.4650},
  url = {http://www.nature.com/doifinder/10.1038/nn.4650},
  abstract = {The authors show how predictive representations are useful for maximizing future reward, particularly in spatial domains. They develop a predictive-map model of hippocampal place cells and entorhinal grid cells that captures a wide variety of effects from human and rodent literature.},
  keywords = {Hippocampus,Learning and memory,Reward},
  file = {/Users/vitay/Documents/Zotero/storage/KMQRJJR6/Stachenfeld et al_2017_The hippocampus as a predictive map.pdf;/Users/vitay/Documents/Zotero/storage/SR9DRT9A/Stachenfeld et al_2017_The hippocampus as a predictive map2.pdf}
}

@article{Stanton2018,
  title = {Deep {{Curiosity Search}}: {{Intra-Life Exploration Improves Performance}} on {{Challenging Deep Reinforcement Learning Problems}}},
  author = {Stanton, Christopher and Clune, Jeff},
  date = {2018-06},
  url = {http://arxiv.org/abs/1806.00553},
  abstract = {Traditional exploration methods in RL require agents to perform random actions to find rewards. But these approaches struggle on sparse-reward domains like Montezuma's Revenge where the probability that any random action sequence leads to reward is extremely low. Recent algorithms have performed well on such tasks by encouraging agents to visit new states or perform new actions in relation to all prior training episodes (which we call across-training novelty). But such algorithms do not consider whether an agent exhibits intra-life novelty: doing something new within the current episode, regardless of whether those behaviors have been performed in previous episodes. We hypothesize that across-training novelty might discourage agents from revisiting initially non-rewarding states that could become important stepping stones later in training. We introduce Deep Curiosity Search (DeepCS), which encourages intra-life exploration by rewarding agents for visiting as many different states as possible within each episode, and show that DeepCS matches the performance of current state-of-the-art methods on Montezuma's Revenge. We further show that DeepCS improves exploration on Gravitar (another difficult, sparse-reward game) and performs well on the dense-reward game Amidar. Surprisingly, DeepCS doubles A2C performance on Seaquest, a game we would not have expected to benefit from intra-life exploration because the arena is small and already easily navigated by naive exploration techniques. In one run, DeepCS achieves a maximum training score of 80,000 points on Seaquest, higher than any methods other than Ape-X. The strong performance of DeepCS on these sparse- and dense-reward tasks suggests that encouraging intra-life novelty is an interesting, new approach for improving performance in Deep RL and motivates further research into hybridizing across-training and intra-life exploration methods.},
  file = {/Users/vitay/Documents/Zotero/storage/EW89XUTT/Stanton_Clune_2018_Deep Curiosity Search.pdf}
}

@article{Stollenga2014,
  title = {Deep {{Networks}} with {{Internal Selective Attention}} through {{Feedback Connections}}},
  author = {Stollenga, Marijn and Masci, Jonathan and Gomez, Faustino and Schmidhuber, Juergen},
  date = {2014-07},
  url = {http://arxiv.org/abs/1407.3068},
  abstract = {Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNets feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.},
  file = {/Users/vitay/Documents/Zotero/storage/A6W46UU8/Stollenga et al_2014_Deep Networks with Internal Selective Attention through Feedback Connections.pdf}
}

@unpublished{Stooke2019,
  title = {Rlpyt: {{A Research Code Base}} for {{Deep Reinforcement Learning}} in {{PyTorch}}},
  shorttitle = {Rlpyt},
  author = {Stooke, Adam and Abbeel, Pieter},
  date = {2019-09-03},
  eprint = {1909.01500},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1909.01500},
  urldate = {2019-09-10},
  abstract = {Since the recent advent of deep reinforcement learning for game play and simulated robotic control, a multitude of new algorithms have flourished. Most are model-free algorithms which can be categorized into three families: deep Q-learning, policy gradients, and Q-value policy gradients. These have developed along separate lines of research, such that few, if any, code bases incorporate all three kinds. Yet these algorithms share a great depth of common deep reinforcement learning machinery. We are pleased to share rlpyt, which implements all three algorithm families on top of a shared, optimized infrastructure, in a single repository. It contains modular implementations of many common deep RL algorithms in Python using PyTorch, a leading deep learning library. rlpyt is designed as a high-throughput code base for small- to medium-scale research in deep RL. This white paper summarizes its features, algorithms implemented, and relation to prior work, and concludes with detailed implementation and usage notes. rlpyt is available at https://github.com/astooke/rlpyt.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/XJMY6B7D/Stooke_Abbeel_2019_rlpyt.pdf;/Users/vitay/Documents/Zotero/storage/MMTPFCYG/1909.html}
}

@unpublished{Stooke2020,
  title = {Decoupling {{Representation Learning}} from {{Reinforcement Learning}}},
  author = {Stooke, Adam and Lee, Kimin and Abbeel, Pieter and Laskin, Michael},
  date = {2020-09-14},
  eprint = {2009.08319},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2009.08319},
  urldate = {2020-09-22},
  abstract = {In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning. To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss. In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments. Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others. We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks. Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation. Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at https://github.com/astooke/rlpyt/rlpyt/ul.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/Q98VQLEW/Stooke_et_al_2020_Decoupling_Representation_Learning_from_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/W4T6AV3T/2009.html}
}

@article{Sutton1981,
  title = {Toward a Modern Theory of Adaptive Networks: Expectation and Prediction.},
  author = {Sutton, R S and Barto, A G},
  date = {1981-03},
  journaltitle = {Psychological review},
  volume = {88},
  number = {2},
  eprint = {7291377},
  eprinttype = {pubmed},
  pages = {135--70},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/7291377},
  keywords = {Animals,Association Learning,Association Learning: physiology,Conditioning (Psychology),Conditioning (Psychology): physiology,Conditioning Classical,Conditioning Classical: physiology,Models Neurological,Nervous System Physiological Phenomena,Neurons,Neurons: physiology,Perception,Perception: physiology,Synapses,Synapses: physiology}
}

@article{Sutton1988,
  title = {Learning to {{Predict}} by the {{Methods}} of {{Temporal Di}} Erences},
  author = {Sutton, Richard S},
  date = {1988},
  journaltitle = {Machine Learning},
  number = {3},
  pages = {9--44},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction|that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the di erence between predicted and actual outcomes, the new methods assign credit by means of the di erence between temporally successive predictions. Although such temporal-di erence methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-di erence methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-di erence methods can be applied to advantage.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/ANXRCJ3A/Sutton_1988_Learning to Predict by the Methods of Temporal Di erences.pdf}
}

@article{Sutton1990,
  title = {Integrated {{Architectures}} for {{Learning}}, {{Planning}}, and {{Reacting Based}} on {{Approximating Dynamic Programming}}},
  author = {Sutton, Richard S.},
  date = {1990-01},
  journaltitle = {Machine Learning Proceedings 1990},
  pages = {216--224},
  issn = {9781558601413},
  doi = {10.1016/B978-1-55860-141-3.50030-4},
  url = {https://www.sciencedirect.com/science/article/pii/B9781558601413500304},
  abstract = {This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world. In this paper, I present and show results for two Dyna architectures. The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas such as evaluation functions and universal plans (reactive systems). Using a navigation task, results are shown for a simple Dyna-PI system that simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. The Dyna-Q architecture is based on Watkins's Q-learning, a new kind of reinforcement learning. Dyna-Q uses a less familiar set of data structures than does Dyna-PI, but is arguably simpler to implement and use. We show that Dyna-Q architectures are easy to adapt for use in changing environments.}
}

@incollection{Sutton1990a,
  title = {Time-Derivative Models of {{Pavlovian}} Reinforcement},
  booktitle = {Learning and {{Computational Neuroscience}}: {{Foundations}} of {{Adaptive Networks}}},
  author = {Sutton, R. S. and Barto, A. G.},
  date = {1990},
  pages = {497--537},
  publisher = {MIT Press},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.81.98},
  file = {/Users/vitay/Documents/Zotero/storage/Y7ZCHJ9P/Sutton_Barto_1990_Time-derivative models of Pavlovian reinforcement.pdf}
}

@book{Sutton1998,
  title = {Reinforcement {{Learning}}: {{An}} Introduction},
  author = {Sutton, R. S. and Barto, A. G.},
  date = {1998},
  publisher = {MIT press},
  location = {Cambridge, MA}
}

@inproceedings{Sutton1999,
  title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
  date = {1999},
  pages = {1057--1063},
  publisher = {MIT Press},
  url = {https://dl.acm.org/citation.cfm?id=3009806},
  file = {/Users/vitay/Documents/Zotero/storage/V3UIMQCS/Sutton_et_al_1999_Policy_gradient_methods_for_reinforcement_learning_with_function_approximation.pdf}
}

@article{Sutton2000,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  author = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  date = {2000},
  pages = {7},
  abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/DCNMF7BU/Sutton et al_2000_Policy Gradient Methods for Reinforcement Learning with Function Approximation.pdf}
}

@book{Sutton2017,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  author = {Sutton, R. S. and Barto, A. G.},
  date = {2017},
  edition = {2},
  publisher = {MIT Press},
  location = {Cambridge, MA},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  file = {/Users/vitay/Documents/Zotero/storage/EBAIFI57/Sutton_Barto_2017_Reinforcement_Learning.pdf}
}

@article{Szita2006,
  title = {Learning {{Tetris Using}} the {{Noisy Cross-Entropy Method}}},
  author = {Szita, István and Lörincz, András},
  date = {2006-12},
  journaltitle = {Neural Computation},
  volume = {18},
  number = {12},
  pages = {2936--2941},
  doi = {10.1162/neco.2006.18.12.2936},
  url = {http://www.mitpressjournals.org/doi/10.1162/neco.2006.18.12.2936},
  abstract = {The cross-entropy method is an efficient and general optimization algorithm. However, its applicability in reinforcement learning (RL) seems to be limited because it often converges to suboptimal policies. We apply noise for preventing early convergence of the cross-entropy method, using Tetris, a computer game, for demonstration. The resulting policy outperforms previous RL algorithms by almost two orders of magnitude.}
}

@article{Tan2019,
  title = {{{DeepMNavigate}}: {{Deep Reinforced Multi-Robot Navigation Unifying Local}} \& {{Global Collision Avoidance}}},
  author = {Tan, Qingyang and Fan, Tingxiang and Pan, Jia and Manocha, Dinesh},
  date = {2019},
  pages = {7},
  abstract = {We present a novel algorithm (DeepMNavigate) for global multi-agent navigation in dense scenarios using deep reinforcement learning. Our approach uses local and global information for each robot based on motion information maps. We use a three-layer CNN that uses these maps as input and generate a suitable action to drive each robot to its goal position. Our approach is general, learns an optimal policy using a multi-scenario, multi-state training algorithm, and can directly handle raw sensor measurements for local observations. We demonstrate the performance on complex, dense benchmarks with narrow passages on environments with tens of agents. We highlight the algorithm’s benefits over prior learning methods and geometric decentralized algorithms in complex scenarios.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/R24ZI7CV/Tan et al_2019_DeepMNavigate2.pdf}
}

@inproceedings{Tang2010,
  title = {On a {{Connection}} between {{Importance Sampling}} and the {{Likelihood Ratio Policy Gradient}}},
  booktitle = {Adv. {{Neural}} Inf. {{Process}}. {{Syst}}.},
  author = {Tang, Jie and Abbeel, Pieter},
  date = {2010},
  url = {http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf},
  abstract = {Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an im-portance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U (θ) at the current policy parameteri-zation θ, rather than to obtain a more complete estimate of U (θ), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which lever-ages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient meth-ods. Our algorithm outperforms standard likelihood ratio policy gradient algo-rithms on several testbeds.}
}

@online{Tang2024,
  title = {Deep {{Reinforcement Learning}} for {{Robotics}}: {{A Survey}} of {{Real-World Successes}}},
  shorttitle = {Deep {{Reinforcement Learning}} for {{Robotics}}},
  author = {Tang, Chen and Abbatematteo, Ben and Hu, Jiaheng and Chandra, Rohan and Martín-Martín, Roberto and Stone, Peter},
  date = {2024-09-16},
  eprint = {2408.03539},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2408.03539},
  url = {http://arxiv.org/abs/2408.03539},
  urldate = {2024-10-29},
  abstract = {Reinforcement learning (RL), particularly its combination with deep neural networks referred to as deep RL (DRL), has shown tremendous promise across a wide range of applications, suggesting its potential for enabling the development of sophisticated robotic behaviors. Robotics problems, however, pose fundamental difficulties for the application of RL, stemming from the complexity and cost of interacting with the physical world. This article provides a modern survey of DRL for robotics, with a particular focus on evaluating the real-world successes achieved with DRL in realizing several key robotic competencies. Our analysis aims to identify the key factors underlying those exciting successes, reveal underexplored areas, and provide an overall characterization of the status of DRL in robotics. We highlight several important avenues for future work, emphasizing the need for stable and sample-efficient real-world RL paradigms, holistic approaches for discovering and integrating various competencies to tackle complex long-horizon, open-world tasks, and principled development and evaluation procedures. This survey is designed to offer insights for both RL practitioners and roboticists toward harnessing RL's power to create generally capable real-world robotic systems.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/VS8V8Y56/Tang et al. - 2024 - Deep Reinforcement Learning for Robotics A Survey of Real-World Successes.pdf}
}

@unpublished{Teh2017,
  title = {Distral: {{Robust Multitask Reinforcement Learning}}},
  shorttitle = {Distral},
  author = {Teh, Yee Whye and Bapst, Victor and Czarnecki, Wojciech Marian and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
  date = {2017-07-13},
  eprint = {1707.04175},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1707.04175},
  urldate = {2020-01-26},
  abstract = {Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill \& transfer learning). Instead of sharing parameters between the different workers, we propose to share a "distilled" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/ZXMUJFDG/Teh_et_al_2017_Distral.pdf;/Users/vitay/Documents/Zotero/storage/RNEGP8LA/1707.html}
}

@incollection{Tesauro1995,
  title = {{{TD-Gammon}}: {{A Self-Teaching Backgammon Program}}},
  shorttitle = {{{TD-Gammon}}},
  booktitle = {Applications of {{Neural Networks}}},
  author = {Tesauro, Gerald},
  editor = {Murray, Alan F.},
  date = {1995},
  pages = {267--285},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-1-4757-2379-3_11},
  url = {https://doi.org/10.1007/978-1-4757-2379-3_11},
  urldate = {2020-10-08},
  abstract = {This chapter describes TD-Gammon, a neural network that is able to teach itself to play backgammon solely by playing against itself and learning from the results. TD-Gammon uses a recently proposed reinforcement learning algorithm called TD(λ) (Sutton, 1988), and is apparently the first application of this algorithm to a complex nontrivial task. Despite starting from random initial weights (and hence random initial strategy), TD-Gammon achieves a surprisingly strong level of play. With zero knowledge built in at the start of learning (i.e. given only a “raw” description of the board state), the network learns to play the entire game at a strong intermediate level that surpasses not only conventional commercial programs, but also comparable networks trained via supervised learning on a large corpus of human expert games. The hidden units in the network have apparently discovered useful features, a longstanding goal of computer games research.Furthermore, when a set of hand-crafted features is added to the network’s input representation, the result is a truly staggering level of performance: TD-Gammon is now estimated to play at a strong master level that is extremely close to the world’s best human players. We discuss possible principles underlying the success of TD-Gammon, and the prospects for successful real-world applications of TD learning in other domains.},
  isbn = {978-1-4757-2379-3},
  langid = {english},
  keywords = {Hide Unit,Random Initial Weight,Reinforcement Learning,Temporal Difference Learning,Training Game}
}

@inproceedings{Todorov2008,
  title = {General Duality between Optimal Control and Estimation},
  booktitle = {2008 47th {{IEEE Conference}} on {{Decision}} and {{Control}}},
  author = {Todorov, E.},
  date = {2008-12},
  pages = {4286--4292},
  doi = {10.1109/CDC.2008.4739438},
  abstract = {Optimal control and estimation are dual in the LQG setting, as Kalman discovered, however this duality has proven difficult to extend beyond LQG. Here we obtain a more natural form of LQG duality by replacing the Kalman-Bucy filter with the information filter. We then generalize this result to non-linear stochastic systems, discrete stochastic systems, and deterministic systems. All forms of duality are established by relating exponentiated costs to probabilities. Unlike the LQG setting where control and estimation are in one-to-one correspondence, in the general case control turns out to be a larger problem class than estimation and only a sub-class of control problems have estimation duals. These are problems where the Bellman equation is intrinsically linear. Apart from their theoretical significance, our results make it possible to apply estimation algorithms to control problems and vice versa.},
  eventtitle = {2008 47th {{IEEE Conference}} on {{Decision}} and {{Control}}},
  keywords = {Control systems,Costs,Density measurement,deterministic systems,discrete stochastic systems,discrete time systems,Equations,Gaussian noise,Information filtering,Information filters,Kalman filters,linear quadratic Gaussian control,LQG duality,nonlinear control systems,nonlinear stochastic systems,optimal control,Optimal control,stochastic systems,Stochastic systems},
  file = {/Users/vitay/Documents/Zotero/storage/BYVB9PKM/Todorov_2008_General duality between optimal control and estimation.pdf;/Users/vitay/Documents/Zotero/storage/B7KJ9LSX/4739438.html}
}

@inproceedings{Toussaint2009,
  title = {Robot {{Trajectory Optimization Using Approximate Inference}}},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Toussaint, Marc},
  date = {2009},
  series = {{{ICML}} '09},
  pages = {1049--1056},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/1553374.1553508},
  url = {http://doi.acm.org/10.1145/1553374.1553508},
  urldate = {2019-02-24},
  abstract = {The general stochastic optimal control (SOC) problem in robotics scenarios is often too complex to be solved exactly and in near real time. A classical approximate solution is to first compute an optimal (deterministic) trajectory and then solve a local linear-quadratic-gaussian (LQG) perturbation model to handle the system stochasticity. We present a new algorithm for this approach which improves upon previous algorithms like iLQG. We consider a probabilistic model for which the maximum likelihood (ML) trajectory coincides with the optimal trajectory and which, in the LQG case, reproduces the classical SOC solution. The algorithm then utilizes approximate inference methods (similar to expectation propagation) that efficiently generalize to non-LQG systems. We demonstrate the algorithm on a simulated 39-DoF humanoid robot.},
  isbn = {978-1-60558-516-1},
  venue = {Montreal, Quebec, Canada},
  file = {/Users/vitay/Documents/Zotero/storage/GFSFX9IP/Toussaint_2009_Robot Trajectory Optimization Using Approximate Inference.pdf}
}

@online{Uehara2024,
  title = {Understanding {{Reinforcement Learning-Based Fine-Tuning}} of {{Diffusion Models}}: {{A Tutorial}} and {{Review}}},
  shorttitle = {Understanding {{Reinforcement Learning-Based Fine-Tuning}} of {{Diffusion Models}}},
  author = {Uehara, Masatoshi and Zhao, Yulai and Biancalani, Tommaso and Levine, Sergey},
  date = {2024-07-18},
  eprint = {2407.13734},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio, stat},
  url = {http://arxiv.org/abs/2407.13734},
  urldate = {2024-09-13},
  abstract = {This tutorial provides a comprehensive survey of methods for fine-tuning diffusion models to optimize downstream reward functions. While diffusion models are widely known to provide excellent generative modeling capability, practical applications in domains such as biology require generating samples that maximize some desired metric (e.g., translation efficiency in RNA, docking score in molecules, stability in protein). In these cases, the diffusion model can be optimized not only to generate realistic samples but also to maximize the measure of interest explicitly. Such methods are based on concepts from reinforcement learning (RL). We explain the application of various RL algorithms, including PPO, differentiable optimization, rewardweighted MLE, value-weighted sampling, and path consistency learning, tailored specifically for fine-tuning diffusion models. We aim to explore fundamental aspects such as the strengths and limitations of different RL-based fine-tuning algorithms across various scenarios, the benefits of RL-based fine-tuning compared to non-RL-based approaches, and the formal objectives of RL-based fine-tuning (target distributions). Additionally, we aim to examine their connections with related topics such as classifier guidance, Gflownets, flow-based diffusion models, path integral control theory, and sampling from unnormalized distributions such as MCMC. The code of this tutorial is available at https://github.com/masa-ue/RLfinetuning Diffusion Bioseq.},
  langid = {english},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/3V84RDL5/Uehara et al. - 2024 - Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models A Tutorial and Review.pdf}
}

@article{Uhlenbeck1930,
  title = {On the {{Theory}} of the {{Brownian Motion}}},
  author = {Uhlenbeck, G. E. and Ornstein, L.S},
  date = {1930},
  journaltitle = {Physical Review},
  volume = {36},
  doi = {10.1103/PhysRev.36.823}
}

@unpublished{Urakami2019,
  title = {{{DoorGym}}: {{A Scalable Door Opening Environment And Baseline Agent}}},
  shorttitle = {{{DoorGym}}},
  author = {Urakami, Yusuke and Hodgkinson, Alec and Carlin, Casey and Leu, Randall and Rigazio, Luca and Abbeel, Pieter},
  date = {2019-08-05},
  eprint = {1908.01887},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1908.01887},
  urldate = {2019-08-10},
  abstract = {Reinforcement Learning (RL) has brought forth ideas of autonomous robots that can navigate real-world environments with ease, aiding humans in a variety of tasks. RL agents have just begun to make their way out of simulation into the real world. Once in the real world, benchmark tasks often fail to transfer into useful skills. We introduce DoorGym, a simulation environment intended to be the first step to move RL from toy environments towards useful atomic skills that can be composed and extended towards a broader goal. DoorGym is an open-source door simulation framework designed to be highly configurable. We also provide a baseline PPO (Proximal Policy Optimization) and SAC (Soft Actor-Critic)implementation, which achieves a success rate of up to 70\% for common tasks in this environment. Environment kit available here:https://github.com/PSVL/DoorGym/},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/2FTAKP6V/Urakami et al_2019_DoorGym.pdf;/Users/vitay/Documents/Zotero/storage/VWKHPSZM/1908.html}
}

@inproceedings{vanHasselt2010,
  title = {Double {{Q-learning}}},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {family=Hasselt, given=Hado, prefix=van, useprefix=true},
  date = {2010},
  pages = {2613--2621},
  publisher = {Curran Associates Inc.},
  url = {https://dl.acm.org/citation.cfm?id=2997187}
}

@unpublished{vanHasselt2015,
  title = {Deep {{Reinforcement Learning}} with {{Double Q-learning}}},
  author = {family=Hasselt, given=Hado, prefix=van, useprefix=true and Guez, Arthur and Silver, David},
  date = {2015-09},
  eprint = {1509.06461},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1509.06461},
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  file = {/Users/vitay/Documents/Zotero/storage/AFJHTT9A/van Hasselt et al_2015_Deep Reinforcement Learning with Double Q-learning.pdf}
}

@unpublished{vanHasselt2016,
  title = {Learning Values across Many Orders of Magnitude},
  author = {family=Hasselt, given=Hado, prefix=van, useprefix=true and Guez, Arthur and Hessel, Matteo and Mnih, Volodymyr and Silver, David},
  date = {2016-08-16},
  eprint = {1602.07714},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1602.07714},
  urldate = {2019-12-04},
  abstract = {Most learning algorithms are not invariant to the scale of the function that is being approximated. We propose to adaptively normalize the targets used in learning. This is useful in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were all clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using the adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/Z6794DEJ/van_Hasselt_et_al_2016_Learning_values_across_many_orders_of_magnitude.pdf;/Users/vitay/Documents/Zotero/storage/5U7SAM8F/1602.html}
}

@inproceedings{vanHasselt2019,
  title = {Learning Values across Many Orders of Magnitude},
  booktitle = {{{NeurIPS}}},
  author = {family=Hasselt, given=Hado, prefix=van, useprefix=true and Guez, Arthur and Hessel, Matteo and Mnih, Volodymyr and Silver, David},
  date = {2019},
  pages = {9},
  abstract = {Most learning algorithms are not invariant to the scale of the signal that is being approximated. We propose to adaptively normalize the targets used in the learning updates. This is important in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/NX5EPZQW/van_Hasselt_et_al_Learning_values_across_many_orders_of_magnitude.pdf}
}

@unpublished{Veerapaneni2019,
  title = {Entity {{Abstraction}} in {{Visual Model-Based Reinforcement Learning}}},
  author = {Veerapaneni, Rishi and Co-Reyes, John D. and Chang, Michael and Janner, Michael and Finn, Chelsea and Wu, Jiajun and Tenenbaum, Joshua B. and Levine, Sergey},
  date = {2019-10-29},
  eprint = {1910.12827},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.12827},
  urldate = {2019-11-03},
  abstract = {This paper tests the hypothesis that modeling a scene in terms of entities and their local interactions, as opposed to modeling the scene globally, provides a significant benefit in generalizing to physical tasks in a combinatorial space the learner has not encountered before. We present object-centric perception, prediction, and planning (OP3), which to the best of our knowledge is the first entity-centric dynamic latent variable framework for model-based reinforcement learning that acquires entity representations from raw visual observations without supervision and uses them to predict and plan. OP3 enforces entity-abstraction -- symmetric processing of each entity representation with the same locally-scoped function -- which enables it to scale to model different numbers and configurations of objects from those in training. Our approach to solving the key technical challenge of grounding these entity representations to actual objects in the environment is to frame this variable binding problem as an inference problem, and we developing an interactive inference algorithm that uses temporal continuity and interactive feedback to bind information about object properties to the entity variables. On block-stacking tasks, OP3 generalizes to novel block configurations and more objects than observed during training, outperforming an oracle model that assumes access to object supervision and achieving two to three times better accuracy than a state-of-the-art video prediction model.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/KLUNHX7N/Veerapaneni et al_2019_Entity Abstraction in Visual Model-Based Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/42Q3TZVR/1910.html}
}

@article{Veerapaneni2019a,
  title = {Object {{Abstraction}} in {{Visual Model-Based Reinforcement Learning}}},
  author = {Veerapaneni, Rishi and Co-Reyes, John D and Chang, Michael and Janner, Michael and Finn, Chelsea and Wu, Jiajun and Tenenbaum, Joshua and Levine, Sergey},
  date = {2019},
  pages = {12},
  url = {https://pgr-workshop.github.io/img/PGR008.pdf},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/Y5Z29LC4/Veerapaneni_et_al_2019_Object_Abstraction_in_Visual_Model-Based_Reinforcement_Learning.pdf}
}

@unpublished{Veeriah2019,
  title = {Discovery of {{Useful Questions}} as {{Auxiliary Tasks}}},
  author = {Veeriah, Vivek and Hessel, Matteo and Xu, Zhongwen and Lewis, Richard and Rajendran, Janarthanan and Oh, Junhyuk and family=Hasselt, given=Hado, prefix=van, useprefix=true and Silver, David and Singh, Satinder},
  date = {2019-09-10},
  eprint = {1909.04607},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1909.04607},
  urldate = {2019-09-14},
  abstract = {Arguably, intelligent agents ought to be able to discover their own questions so that in learning answers for them they learn unanticipated useful knowledge and skills; this departs from the focus in much of machine learning on agents learning answers to externally defined questions. We present a novel method for a reinforcement learning (RL) agent to discover questions formulated as general value functions or GVFs, a fairly rich form of knowledge representation. Specifically, our method uses non-myopic meta-gradients to learn GVF-questions such that learning answers to them, as an auxiliary task, induces useful representations for the main task faced by the RL agent. We demonstrate that auxiliary tasks based on the discovered GVFs are sufficient, on their own, to build representations that support main task learning, and that they do so better than popular hand-designed auxiliary tasks from the literature. Furthermore, we show, in the context of Atari 2600 videogames, how such auxiliary tasks, meta-learned alongside the main task, can improve the data efficiency of an actor-critic agent.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/PFAA4ZIT/Veeriah et al_2019_Discovery of Useful Questions as Auxiliary Tasks.pdf;/Users/vitay/Documents/Zotero/storage/G9ZKVIGY/1909.html}
}

@unpublished{Veeriah2021,
  title = {Discovery of {{Options}} via {{Meta-Learned Subgoals}}},
  author = {Veeriah, Vivek and Zahavy, Tom and Hessel, Matteo and Xu, Zhongwen and Oh, Junhyuk and Kemaev, Iurii and family=Hasselt, given=Hado, prefix=van, useprefix=true and Silver, David and Singh, Satinder},
  date = {2021-02-12},
  eprint = {2102.06741},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.06741},
  urldate = {2021-02-21},
  abstract = {Temporal abstractions in the form of options have been shown to help reinforcement learning (RL) agents learn faster. However, despite prior work on this topic, the problem of discovering options through interaction with an environment remains a challenge. In this paper, we introduce a novel meta-gradient approach for discovering useful options in multi-task RL environments. Our approach is based on a manager-worker decomposition of the RL agent, in which a manager maximises rewards from the environment by learning a task-dependent policy over both a set of task-independent discovered-options and primitive actions. The option-reward and termination functions that define a subgoal for each option are parameterised as neural networks and trained via meta-gradients to maximise their usefulness. Empirical analysis on gridworld and DeepMind Lab tasks show that: (1) our approach can discover meaningful and diverse temporally-extended options in multi-task RL domains, (2) the discovered options are frequently used by the agent while learning to solve the training tasks, and (3) that the discovered options help a randomly initialised manager learn faster in completely new tasks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/MEKC3PFC/Veeriah_et_al_2021_Discovery_of_Options_via_Meta-Learned_Subgoals.pdf;/Users/vitay/Documents/Zotero/storage/ZJXHFL3Q/2102.html}
}

@unpublished{Vezhnevets2017,
  title = {{{FeUdal Networks}} for {{Hierarchical Reinforcement Learning}}},
  author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  date = {2017-03-06},
  eprint = {1703.01161},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.01161},
  urldate = {2020-01-31},
  abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/vitay/Documents/Zotero/storage/FMLIND6X/Vezhnevets_et_al_2017_FeUdal_Networks_for_Hierarchical_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/CR8R9ZRU/1703.html}
}

@unpublished{Vinyals2017,
  title = {{{StarCraft II}}: {{A New Challenge}} for {{Reinforcement Learning}}},
  shorttitle = {{{StarCraft II}}},
  author = {Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and Küttler, Heinrich and Agapiou, John and Schrittwieser, Julian and Quan, John and Gaffney, Stephen and Petersen, Stig and Simonyan, Karen and Schaul, Tom and family=Hasselt, given=Hado, prefix=van, useprefix=true and Silver, David and Lillicrap, Timothy and Calderone, Kevin and Keet, Paul and Brunasso, Anthony and Lawrence, David and Ekermo, Anders and Repp, Jacob and Tsing, Rodney},
  date = {2017-08-16},
  eprint = {1708.04782},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1708.04782},
  urldate = {2019-11-01},
  abstract = {This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/L765DLIE/Vinyals et al_2017_StarCraft II.pdf;/Users/vitay/Documents/Zotero/storage/R4GZI7FT/1708.html}
}

@article{Vinyals2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  date = {2019-10-30},
  journaltitle = {Nature},
  shortjournal = {Nature},
  pages = {1--5},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  url = {https://www.nature.com/articles/s41586-019-1724-z},
  urldate = {2019-11-03},
  abstract = {AlphaStar uses a multi-agent reinforcement learning algorithm and has reached Grandmaster level, ranking among the top 0.2\% of human players for the real-time strategy game StarCraft II.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/F8MDWLW4/s41586-019-1724-z.html}
}

@inproceedings{Vuong2019,
  title = {Sharing {{Experience}} in {{Multitask Reinforcement Learning}}},
  booktitle = {Proceedings of the 28th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Vuong, Tung-Long and Nguyen, Do-Van and Nguyen, Tai-Long and Bui, Cong-Minh and Kieu, Hai-Dang and Ta, Viet-Cuong and Tran, Quoc-Long and Le, Thanh-Ha},
  date = {2019},
  series = {{{IJCAI}}'19},
  pages = {3642--3648},
  publisher = {AAAI Press},
  url = {http://dl.acm.org/citation.cfm?id=3367471.3367547},
  urldate = {2019-10-19},
  abstract = {In multitask reinforcement learning, tasks often have sub-tasks that share the same solution, even though the overall tasks are different. If the shared-portions could be effectively identified, then the learning process could be improved since all the samples between tasks in the shared space could be used. In this paper, we propose a Sharing Experience Framework (SEF) for simultaneously training of multiple tasks. In SEF, a confidence sharing agent uses task-specific rewards from the environment to identify similar parts that should be shared across tasks and defines those parts as shared-regions between tasks. The shared-regions are expected to guide task-policies sharing their experience during the learning process. The experiments highlight that our framework improves the performance and the stability of learning task-policies, and is possible to help task-policies avoid local optimums.},
  isbn = {978-0-9992411-4-1},
  venue = {Macao, China}
}

@unpublished{Wang2016,
  title = {Dueling {{Network Architectures}} for {{Deep Reinforcement Learning}}},
  author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and family=Hasselt, given=Hado, prefix=van, useprefix=true and Lanctot, Marc and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2016-04-05},
  eprint = {1511.06581},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.06581},
  urldate = {2019-11-21},
  abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/LN7D58DH/Wang et al_2016_Dueling Network Architectures for Deep Reinforcement Learning2.pdf;/Users/vitay/Documents/Zotero/storage/554AKXT3/1511.html}
}

@unpublished{Wang2017,
  title = {Learning to Reinforcement Learn},
  author = {Wang, Jane X. and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  date = {2017-01-23},
  eprint = {1611.05763},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.05763},
  urldate = {2021-02-05},
  abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/BLG8F4M4/Wang_et_al_2017_Learning_to_reinforcement_learn.pdf;/Users/vitay/Documents/Zotero/storage/NL4V34SN/1611.html}
}

@unpublished{Wang2017a,
  title = {Sample {{Efficient Actor-Critic}} with {{Experience Replay}}},
  author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2017-11},
  eprint = {1611.01224},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.01224},
  abstract = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.},
  file = {/Users/vitay/Documents/Zotero/storage/3H44URLZ/Wang et al_2017_Sample Efficient Actor-Critic with Experience Replay.pdf}
}

@unpublished{Wang2019,
  title = {Benchmarking {{Model-Based Reinforcement Learning}}},
  author = {Wang, Tingwu and Bao, Xuchan and Clavera, Ignasi and Hoang, Jerrick and Wen, Yeming and Langlois, Eric and Zhang, Shunshi and Zhang, Guodong and Abbeel, Pieter and Ba, Jimmy},
  date = {2019-07-03},
  eprint = {1907.02057},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.02057},
  urldate = {2019-07-23},
  abstract = {Model-based reinforcement learning (MBRL) is widely seen as having the potential to be significantly more sample efficient than model-free RL. However, research in model-based RL has not been very standardized. It is fairly common for authors to experiment with self-designed environments, and there are several separate lines of research, which are sometimes closed-sourced or not reproducible. Accordingly, it is an open question how these various existing MBRL algorithms perform relative to each other. To facilitate research in MBRL, in this paper we gather a wide collection of MBRL algorithms and propose over 18 benchmarking environments specially designed for MBRL. We benchmark these algorithms with unified problem settings, including noisy environments. Beyond cataloguing performance, we explore and unify the underlying algorithmic differences across MBRL algorithms. We characterize three key research challenges for future MBRL research: the dynamics bottleneck, the planning horizon dilemma, and the early-termination dilemma. Finally, to maximally facilitate future research on MBRL, we open-source our benchmark in http://www.cs.toronto.edu/\textasciitilde tingwuwang/mbrl.html.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/ZUQDEIWE/Wang et al_2019_Benchmarking Model-Based Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/P6SKXP9G/1907.html}
}

@unpublished{Wang2019a,
  title = {Paired {{Open-Ended Trailblazer}} ({{POET}}): {{Endlessly Generating Increasingly Complex}} and {{Diverse Learning Environments}} and {{Their Solutions}}},
  shorttitle = {Paired {{Open-Ended Trailblazer}} ({{POET}})},
  author = {Wang, Rui and Lehman, Joel and Clune, Jeff and Stanley, Kenneth O.},
  date = {2019-02-20},
  eprint = {1901.01753},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1901.01753},
  urldate = {2021-02-06},
  abstract = {While the history of machine learning so far largely encompasses a series of problems posed by researchers and algorithms that learn their solutions, an important question is whether the problems themselves can be generated by the algorithm at the same time as they are being solved. Such a process would in effect build its own diverse and expanding curricula, and the solutions to problems at various stages would become stepping stones towards solving even more challenging problems later in the process. The Paired Open-Ended Trailblazer (POET) algorithm introduced in this paper does just that: it pairs the generation of environmental challenges and the optimization of agents to solve those challenges. It simultaneously explores many different paths through the space of possible problems and solutions and, critically, allows these stepping-stone solutions to transfer between problems if better, catalyzing innovation. The term open-ended signifies the intriguing potential for algorithms like POET to continue to create novel and increasingly complex capabilities without bound. Our results show that POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved by direct optimization alone, or even through a direct-path curriculum-building control algorithm introduced to highlight the critical role of open-endedness in solving ambitious challenges. The ability to transfer solutions from one environment to another proves essential to unlocking the full potential of the system as a whole, demonstrating the unpredictable nature of fortuitous stepping stones. We hope that POET will inspire a new push towards open-ended discovery across many domains, where algorithms like POET can blaze a trail through their interesting possible manifestations and solutions.},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/vitay/Documents/Zotero/storage/SSRS8FT5/Wang_et_al_2019_Paired_Open-Ended_Trailblazer_(POET).pdf;/Users/vitay/Documents/Zotero/storage/NEWF49R3/1901.html}
}

@unpublished{Wang2021,
  title = {Alchemy: {{A}} Structured Task Distribution for Meta-Reinforcement Learning},
  shorttitle = {Alchemy},
  author = {Wang, Jane X. and King, Michael and Porcel, Nicolas and Kurth-Nelson, Zeb and Zhu, Tina and Deck, Charlie and Choy, Peter and Cassin, Mary and Reynolds, Malcolm and Song, Francis and Buttimore, Gavin and Reichert, David P. and Rabinowitz, Neil and Matthey, Loic and Hassabis, Demis and Lerchner, Alexander and Botvinick, Matthew},
  date = {2021-02-04},
  eprint = {2102.02926},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.02926},
  urldate = {2021-02-12},
  abstract = {There has been rapidly growing interest in meta-learning as a method for increasing the flexibility and sample efficiency of reinforcement learning. One problem in this area of research, however, has been a scarcity of adequate benchmark tasks. In general, the structure underlying past benchmarks has either been too simple to be inherently interesting, or too ill-defined to support principled analysis. In the present work, we introduce a new benchmark for meta-RL research, which combines structural richness with structural transparency. Alchemy is a 3D video game, implemented in Unity, which involves a latent causal structure that is resampled procedurally from episode to episode, affording structure learning, online inference, hypothesis testing and action sequencing based on abstract domain knowledge. We evaluate a pair of powerful RL agents on Alchemy and present an in-depth analysis of one of these agents. Results clearly indicate a frank and specific failure of meta-learning, providing validation for Alchemy as a challenging benchmark for meta-RL. Concurrent with this report, we are releasing Alchemy as public resource, together with a suite of analysis tools and sample agent trajectories.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/X66AUBI8/Wang_et_al_2021_Alchemy.pdf;/Users/vitay/Documents/Zotero/storage/XDUPKRP3/2102.html}
}

@online{Wang2024,
  title = {One-{{Step Diffusion Policy}}: {{Fast Visuomotor Policies}} via {{Diffusion Distillation}}},
  shorttitle = {One-{{Step Diffusion Policy}}},
  author = {Wang, Zhendong and Li, Zhaoshuo and Mandlekar, Ajay and Xu, Zhenjia and Fan, Jiaojiao and Narang, Yashraj and Fan, Linxi and Zhu, Yuke and Balaji, Yogesh and Zhou, Mingyuan and Liu, Ming-Yu and Zeng, Yu},
  date = {2024-10-28},
  eprint = {2410.21257},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.21257},
  url = {http://arxiv.org/abs/2410.21257},
  urldate = {2024-11-02},
  abstract = {Diffusion models, praised for their success in generative tasks, are increasingly being applied to robotics, demonstrating exceptional performance in behavior cloning. However, their slow generation process stemming from iterative denoising steps poses a challenge for real-time applications in resource-constrained robotics setups and dynamically changing environments. In this paper, we introduce the One-Step Diffusion Policy (OneDP), a novel approach that distills knowledge from pre-trained diffusion policies into a single-step action generator, significantly accelerating response times for robotic control tasks. We ensure the distilled generator closely aligns with the original policy distribution by minimizing the Kullback-Leibler (KL) divergence along the diffusion chain, requiring only \$2\textbackslash\%\$-\$10\textbackslash\%\$ additional pre-training cost for convergence. We evaluated OneDP on 6 challenging simulation tasks as well as 4 self-designed real-world tasks using the Franka robot. The results demonstrate that OneDP not only achieves state-of-the-art success rates but also delivers an order-of-magnitude improvement in inference speed, boosting action prediction frequency from 1.5 Hz to 62 Hz, establishing its potential for dynamic and computationally constrained robotic applications. We share the project page at https://research.nvidia.com/labs/dir/onedp/.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/KUMC4R9F/Wang et al. - 2024 - One-Step Diffusion Policy Fast Visuomotor Policies via Diffusion Distillation.pdf}
}

@thesis{Watkins1989,
  title = {Learning from Delayed Rewards},
  author = {Watkins, Christopher JCH},
  date = {1989}
}

@article{Watter2015,
  title = {Embed to {{Control}}: {{A Locally Linear Latent Dynamics Model}} for {{Control}} from {{Raw Images}}},
  author = {Watter, Manuel and Springenberg, Jost Tobias and Boedecker, Joschka and Riedmiller, Martin},
  date = {2015},
  url = {https://arxiv.org/pdf/1506.07365.pdf},
  abstract = {We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is con-strained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.},
  file = {/Users/vitay/Documents/Zotero/storage/I3YNEAAW/Watter et al_2015_Embed to Control.pdf}
}

@unpublished{Watters2019,
  title = {{{COBRA}}: {{Data-Efficient Model-Based RL}} through {{Unsupervised Object Discovery}} and {{Curiosity-Driven Exploration}}},
  shorttitle = {{{COBRA}}},
  author = {Watters, Nicholas and Matthey, Loic and Bosnjak, Matko and Burgess, Christopher P. and Lerchner, Alexander},
  date = {2019-08-14},
  eprint = {1905.09275},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1905.09275},
  urldate = {2020-02-24},
  abstract = {Data efficiency and robustness to task-irrelevant perturbations are long-standing challenges for deep reinforcement learning algorithms. Here we introduce a modular approach to addressing these challenges in a continuous control environment, without using hand-crafted or supervised information. Our Curious Object-Based seaRch Agent (COBRA) uses task-free intrinsically motivated exploration and unsupervised learning to build object-based models of its environment and action space. Subsequently, it can learn a variety of tasks through model-based search in very few steps and excel on structured hold-out tests of policy robustness.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/CH5ZDM6W/Watters_et_al_2019_COBRA.pdf;/Users/vitay/Documents/Zotero/storage/LCVSPFAH/1905.html}
}

@unpublished{Weber2017,
  title = {Imagination-{{Augmented Agents}} for {{Deep Reinforcement Learning}}},
  author = {Weber, Théophane and Racanière, Sébastien and Reichert, David P. and Buesing, Lars and Guez, Arthur and Rezende, Danilo Jimenez and Badia, Adria Puigdomènech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
  date = {2017-07},
  eprint = {1707.06203},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.06203},
  abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
  file = {/Users/vitay/Documents/Zotero/storage/F3RPSHTV/Weber et al_2017_Imagination-Augmented Agents for Deep Reinforcement Learning.pdf}
}

@incollection{Wierstra2007,
  title = {Solving {{Deep Memory POMDPs}} with {{Recurrent Policy Gradients}}},
  author = {Wierstra, Daan and Foerster, Alexander and Peters, Jan and Schmidhuber, Jürgen},
  date = {2007},
  pages = {697--706},
  publisher = {Springer, Berlin, Heidelberg},
  doi = {10.1007/978-3-540-74690-4_71},
  url = {http://link.springer.com/10.1007/978-3-540-74690-4_71},
  file = {/Users/vitay/Documents/Zotero/storage/YMGLVDPN/Wierstra et al_2007_Solving Deep Memory POMDPs with Recurrent Policy Gradients.pdf}
}

@article{Williams1991,
  title = {Function Optimization Using Connectionist Reinforcement Learning Algorithms},
  author = {Williams, Ronald J and Peng, Jing},
  date = {1991},
  journaltitle = {Connection Science},
  volume = {3},
  number = {3},
  pages = {241--268}
}

@article{Williams1992,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, R. J.},
  date = {1992},
  journaltitle = {Machine Learning},
  volume = {8},
  pages = {229--256},
  file = {/Users/vitay/Documents/Zotero/storage/WQQV6GAD/Williams_1992_Simple_statistical_gradient-following_algorithms_for_connectionist.pdf}
}

@online{Wu2022,
  title = {{{DayDreamer}}: {{World Models}} for {{Physical Robot Learning}}},
  shorttitle = {{{DayDreamer}}},
  author = {Wu, Philipp and Escontrela, Alejandro and Hafner, Danijar and Goldberg, Ken and Abbeel, Pieter},
  date = {2022-06-28},
  eprint = {2206.14176},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.14176},
  url = {http://arxiv.org/abs/2206.14176},
  urldate = {2022-08-05},
  abstract = {To solve tasks in complex environments, robots need to learn from experience. Deep reinforcement learning is a common approach to robot learning but requires a large amount of trial and error to learn, limiting its deployment in the physical world. As a consequence, many advances in robot learning rely on simulators. On the other hand, learning inside of simulators fails to capture the complexity of the real world, is prone to simulator inaccuracies, and the resulting behaviors do not adapt to changes in the world. The Dreamer algorithm has recently shown great promise for learning from small amounts of interaction by planning within a learned world model, outperforming pure reinforcement learning in video games. Learning a world model to predict the outcomes of potential actions enables planning in imagination, reducing the amount of trial and error needed in the real environment. However, it is unknown whether Dreamer can facilitate faster learning on physical robots. In this paper, we apply Dreamer to 4 robots to learn online and directly in the real world, without simulators. Dreamer trains a quadruped robot to roll off its back, stand up, and walk from scratch and without resets in only 1 hour. We then push the robot and find that Dreamer adapts within 10 minutes to withstand perturbations or quickly roll over and stand back up. On two different robotic arms, Dreamer learns to pick and place multiple objects directly from camera images and sparse rewards, approaching human performance. On a wheeled robot, Dreamer learns to navigate to a goal position purely from camera images, automatically resolving ambiguity about the robot orientation. Using the same hyperparameters across all experiments, we find that Dreamer is capable of online learning in the real world, establishing a strong baseline. We release our infrastructure for future applications of world models to robot learning.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/DVK5IH68/Wu_et_al_2022_DayDreamer.pdf;/Users/vitay/Documents/Zotero/storage/T84VN4YG/2206.html}
}

@unpublished{Xiao2020,
  title = {Thinking {{While Moving}}: {{Deep Reinforcement Learning}} with {{Concurrent Control}}},
  shorttitle = {Thinking {{While Moving}}},
  author = {Xiao, Ted and Jang, Eric and Kalashnikov, Dmitry and Levine, Sergey and Ibarz, Julian and Hausman, Karol and Herzog, Alexander},
  date = {2020-04-14},
  eprint = {2004.06089},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2004.06089},
  urldate = {2020-04-17},
  abstract = {We study reinforcement learning in settings where sampling an action from the policy must be done concurrently with the time evolution of the controlled system, such as when a robot must decide on the next action while still performing the previous action. Much like a person or an animal, the robot must think and move at the same time, deciding on its next action before the previous one has completed. In order to develop an algorithmic framework for such concurrent control problems, we start with a continuous-time formulation of the Bellman equations, and then discretize them in a way that is aware of system delays. We instantiate this new class of approximate dynamic programming methods via a simple architectural extension to existing value-based deep reinforcement learning algorithms. We evaluate our methods on simulated benchmark tasks and a large-scale robotic grasping task where the robot must "think while moving".},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,I.2.9,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/9FRC4Z25/Xiao_et_al_2020_Thinking_While_Moving.pdf;/Users/vitay/Documents/Zotero/storage/5UIGSSI4/2004.html}
}

@unpublished{Xiao2022,
  title = {Masked {{Visual Pre-training}} for {{Motor Control}}},
  author = {Xiao, Tete and Radosavovic, Ilija and Darrell, Trevor and Malik, Jitendra},
  date = {2022-03-11},
  eprint = {2203.06173},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.06173},
  urldate = {2022-04-14},
  abstract = {This paper shows that self-supervised visual pre-training from real-world images is effective for learning motor control tasks from pixels. We first train the visual representations by masked modeling of natural images. We then freeze the visual encoder and train neural network controllers on top with reinforcement learning. We do not perform any task-specific fine-tuning of the encoder; the same visual representations are used for all motor control tasks. To the best of our knowledge, this is the first self-supervised model to exploit real-world images at scale for motor control. To accelerate progress in learning from pixels, we contribute a benchmark suite of hand-designed tasks varying in movements, scenes, and robots. Without relying on labels, state-estimation, or expert demonstrations, we consistently outperform supervised encoders by up to 80\% absolute success rate, sometimes even matching the oracle state performance. We also find that in-the-wild images, e.g., from YouTube or Egocentric videos, lead to better visual representations for various manipulation tasks than ImageNet images.},
  file = {/Users/vitay/Documents/Zotero/storage/XD8V3PVQ/Xiao_et_al_2022_Masked_Visual_Pre-training_for_Motor_Control.pdf;/Users/vitay/Documents/Zotero/storage/GE5HVD66/2203.html}
}

@article{Yan2015,
  title = {Model-Based Credit Assignment for Model-Free Deep Reinforcement Learning},
  author = {Yan, Dong and Weng, Jiayi and Huang, Shiyu and Li, Chongxuan and Zhou, Yichi and Su, Hang and Zhu, Jun},
  date = {2015},
  volume = {14},
  number = {8},
  pages = {11},
  abstract = {Credit assignment is one of the most critical problems in reinforcement learning to discover which actions are responsible for rewards. It becomes more serious as reinforcement learning is applied to real-world scenarios where the decision process may involve thousands of actions. In this paper, we propose a novel framework that utilizes a computation process to assign credits for hundreds of thousands of non-terminal stateaction pairs, in order to accelerate the learning speed. Specifically, we first abstract the states and actions of the original problem into a compact representation, which reduces the problem to a tractable size. Then, we solve the abstracted problem to obtain the optimal value function, which is the expected returns of future rewards. Finally, we use the derived value function to assign credits for state-action pairs of the original problem. We conduct extensive experiments on Doom, a complex 3D video game in which the reward signal is sparse. The experiment results demonstrate that our agent outperforms previous state-of-the-art agents in terms of both kill count and death number with a large margin. The effectiveness also manifests in an online competition of Doom, in which we achieved the 2nd place in the final.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/PDS8U4EN/Yan et al. - 2015 - Model-based credit assignment for model-free deep .pdf}
}

@unpublished{Yan2020,
  title = {Learning {{Predictive Representations}} for {{Deformable Objects Using Contrastive Estimation}}},
  author = {Yan, Wilson and Vangipuram, Ashwin and Abbeel, Pieter and Pinto, Lerrel},
  date = {2020-03-11},
  eprint = {2003.05436},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2003.05436},
  urldate = {2020-03-17},
  abstract = {Using visual model-based learning for deformable object manipulation is challenging due to difficulties in learning plannable visual representations along with complex dynamic models. In this work, we propose a new learning framework that jointly optimizes both the visual representation model and the dynamics model using contrastive estimation. Using simulation data collected by randomly perturbing deformable objects on a table, we learn latent dynamics models for these objects in an offline fashion. Then, using the learned models, we use simple model-based planning to solve challenging deformable object manipulation tasks such as spreading ropes and cloths. Experimentally, we show substantial improvements in performance over standard model-based learning techniques across our rope and cloth manipulation suite. Finally, we transfer our visual manipulation policies trained on data purely collected in simulation to a real PR2 robot through domain randomization.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/2NHSI26R/Yan_et_al_2020_Learning_Predictive_Representations_for_Deformable_Objects_Using_Contrastive.pdf;/Users/vitay/Documents/Zotero/storage/MY62X5UR/2003.html}
}

@unpublished{Yang2020a,
  title = {{{Plan2Vec}}: {{Unsupervised Representation Learning}} by {{Latent Plans}}},
  shorttitle = {{{Plan2Vec}}},
  author = {Yang, Ge and Zhang, Amy and Morcos, Ari S. and Pineau, Joelle and Abbeel, Pieter and Calandra, Roberto},
  date = {2020-05-07},
  eprint = {2005.03648},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2005.03648},
  urldate = {2020-05-12},
  abstract = {In this paper we introduce plan2vec, an unsupervised representation learning approach that is inspired by reinforcement learning. Plan2vec constructs a weighted graph on an image dataset using near-neighbor distances, and then extrapolates this local metric to a global embedding by distilling path-integral over planned path. When applied to control, plan2vec offers a way to learn goal-conditioned value estimates that are accurate over long horizons that is both compute and sample efficient. We demonstrate the effectiveness of plan2vec on one simulated and two challenging real-world image datasets. Experimental results show that plan2vec successfully amortizes the planning cost, enabling reactive planning that is linear in memory and computation complexity rather than exhaustive over the entire state space.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/CCV8ID4H/Yang_et_al_2020_Plan2Vec.pdf;/Users/vitay/Documents/Zotero/storage/8HKRKXUL/2005.html}
}

@unpublished{Yang2021,
  title = {Reinforcement {{Learning}} for {{Adaptive Mesh Refinement}}},
  author = {Yang, Jiachen and Dzanic, Tarik and Petersen, Brenden and Kudo, Jun and Mittal, Ketan and Tomov, Vladimir and Camier, Jean-Sylvain and Zhao, Tuo and Zha, Hongyuan and Kolev, Tzanio and Anderson, Robert and Faissol, Daniel},
  date = {2021-03-01},
  eprint = {2103.01342},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/2103.01342},
  urldate = {2021-05-14},
  abstract = {Large-scale finite element simulations of complex physical systems governed by partial differential equations crucially depend on adaptive mesh refinement (AMR) to allocate computational budget to regions where higher resolution is required. Existing scalable AMR methods make heuristic refinement decisions based on instantaneous error estimation and thus do not aim for long-term optimality over an entire simulation. We propose a novel formulation of AMR as a Markov decision process and apply deep reinforcement learning (RL) to train refinement policies directly from simulation. AMR poses a new problem for RL in that both the state dimension and available action set changes at every step, which we solve by proposing new policy architectures with differing generality and inductive bias. The model sizes of these policy architectures are independent of the mesh size and hence scale to arbitrarily large and complex simulations. We demonstrate in comprehensive experiments on static function estimation and the advection of different fields that RL policies can be competitive with a widely-used error estimator and generalize to larger, more complex, and unseen test problems.},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  file = {/Users/vitay/Documents/Zotero/storage/SNCV2UJ4/Yang_et_al_2021_Reinforcement_Learning_for_Adaptive_Mesh_Refinement.pdf;/Users/vitay/Documents/Zotero/storage/89UFTIGY/2103.html}
}

@online{Ye2021,
  title = {Mastering {{Atari Games}} with {{Limited Data}}},
  author = {Ye, Weirui and Liu, Shaohuai and Kurutach, Thanard and Abbeel, Pieter and Gao, Yang},
  date = {2021-12-11},
  eprint = {2111.00210},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.00210},
  url = {http://arxiv.org/abs/2111.00210},
  urldate = {2023-01-30},
  abstract = {Reinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 194.3\% mean human performance and 109.0\% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at https://github.com/YeWR/EfficientZero. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/4GD29V67/Ye_et_al_2021_Mastering_Atari_Games_with_Limited_Data.pdf}
}

@unpublished{Yu2019,
  title = {Meta-{{World}}: {{A Benchmark}} and {{Evaluation}} for {{Multi-Task}} and {{Meta Reinforcement Learning}}},
  shorttitle = {Meta-{{World}}},
  author = {Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  date = {2019-10-23},
  eprint = {1910.10897},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.10897},
  urldate = {2019-10-30},
  abstract = {Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 6 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/MGY5V6DL/Yu et al_2019_Meta-World.pdf;/Users/vitay/Documents/Zotero/storage/NQFMTPFE/1910.html}
}

@article{Yu2019a,
  title = {Multi-{{Task Reinforcement Learning}} without {{Interference}}},
  author = {Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  date = {2019},
  pages = {9},
  abstract = {While deep reinforcement learning systems have demonstrated impressive results in domains ranging from game playing and robotic control, sample efficiency remains a major challenge, particularly as these algorithms learn individual tasks from scratch. Multi-task and goal-conditioned reinforcement learning have emerged as promising approaches for sharing structure across multiple tasks to enable more efficient learning. However, challenges in optimization have hamstrung such methods from realizing efficiency gains compared to learning tasks independently from scratch. Motivated by these challenges, we develop a general approach that can change the multi-task optimization landscape to alleviate conflicting gradients across tasks. In particular, we introduce two instantiations of this approach, one architectural and one algorithmic, that prevent gradients for different tasks from interfering with one another. On two challenging multi-task RL problems, we find that our approaches leads to greater final performance and learning efficiency in comparison to prior approaches.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/EB2TMDMC/Yu_et_al_2019_Multi-Task_Reinforcement_Learning_without_Interference.pdf}
}

@unpublished{Yu2020,
  title = {{{MOPO}}: {{Model-based Offline Policy Optimization}}},
  shorttitle = {{{MOPO}}},
  author = {Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  date = {2020-11-22},
  eprint = {2005.13239},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2005.13239},
  urldate = {2021-10-05},
  abstract = {Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a large batch of previously collected data. This problem setting offers the promise of utilizing such datasets to acquire policies without any costly or dangerous active exploration. However, it is also challenging, due to the distributional shift between the offline training data and those states visited by the learned policy. Despite significant recent progress, the most successful prior methods are model-free and constrain the policy to the support of data, precluding generalization to unseen states. In this paper, we first observe that an existing model-based RL algorithm already produces significant gains in the offline setting compared to model-free approaches. However, standard model-based RL methods, designed for the online setting, do not provide an explicit mechanism to avoid the offline setting's distributional shift issue. Instead, we propose to modify the existing model-based RL methods by applying them with rewards artificially penalized by the uncertainty of the dynamics. We theoretically show that the algorithm maximizes a lower bound of the policy's return under the true MDP. We also characterize the trade-off between the gain and risk of leaving the support of the batch data. Our algorithm, Model-based Offline Policy Optimization (MOPO), outperforms standard model-based RL algorithms and prior state-of-the-art model-free offline RL algorithms on existing offline RL benchmarks and two challenging continuous control tasks that require generalizing from data collected for a different task. The code is available at https://github.com/tianheyu927/mopo.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/XXWUCDJ8/Yu_et_al_2020_MOPO.pdf;/Users/vitay/Documents/Zotero/storage/SPZKV59P/2005.html}
}

@online{Yu2020b,
  title = {Reinforcement {{Learning}} in {{Healthcare}}: {{A Survey}}},
  shorttitle = {Reinforcement {{Learning}} in {{Healthcare}}},
  author = {Yu, Chao and Liu, Jiming and Nemati, Shamim},
  date = {2020-04-24},
  eprint = {1908.08796},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1908.08796},
  url = {http://arxiv.org/abs/1908.08796},
  urldate = {2024-10-14},
  abstract = {As a subfield of machine learning, reinforcement learning (RL) aims at empowering one's capabilities in behavioural decision making by using interaction experience with the world and an evaluative feedback. Unlike traditional supervised learning methods that usually rely on one-shot, exhaustive and supervised reward signals, RL tackles with sequential decision making problems with sampled, evaluative and delayed feedback simultaneously. Such distinctive features make RL technique a suitable candidate for developing powerful solutions in a variety of healthcare domains, where diagnosing decisions or treatment regimes are usually characterized by a prolonged and sequential procedure. This survey discusses the broad applications of RL techniques in healthcare domains, in order to provide the research community with systematic understanding of theoretical foundations, enabling methods and techniques, existing challenges, and new insights of this emerging paradigm. By first briefly examining theoretical foundations and key techniques in RL research from efficient and representational directions, we then provide an overview of RL applications in healthcare domains ranging from dynamic treatment regimes in chronic diseases and critical care, automated medical diagnosis from both unstructured and structured clinical data, as well as many other control or scheduling domains that have infiltrated many aspects of a healthcare system. Finally, we summarize the challenges and open issues in current research, and point out some potential solutions and directions for future research.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/GBE8Y2AK/Yu et al. - 2020 - Reinforcement Learning in Healthcare A Survey.pdf}
}

@unpublished{Zahavy2020,
  title = {Self-{{Tuning Deep Reinforcement Learning}}},
  author = {Zahavy, Tom and Xu, Zhongwen and Veeriah, Vivek and Hessel, Matteo and Oh, Junhyuk and family=Hasselt, given=Hado, prefix=van, useprefix=true and Silver, David and Singh, Satinder},
  date = {2020-03-02},
  eprint = {2002.12928},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.12928},
  urldate = {2020-03-07},
  abstract = {Reinforcement learning (RL) algorithms often require expensive manual or automated hyperparameter searches in order to perform well on a new domain. This need is particularly acute in modern deep RL architectures which often incorporate many modules and multiple loss functions. In this paper, we take a step towards addressing this issue by using metagradients (Xu et al., 2018) to tune these hyperparameters via differentiable cross validation, whilst the agent interacts with and learns from the environment. We present the Self-Tuning Actor Critic (STAC) which uses this process to tune the hyperparameters of the usual loss function of the IMPALA actor critic agent(Espeholt et. al., 2018), to learn the hyperparameters that define auxiliary loss functions, and to balance trade offs in off policy learning by introducing and adapting the hyperparameters of a novel leaky V-trace operator. The method is simple to use, sample efficient and does not require significant increase in compute. Ablative studies show that the overall performance of STAC improves as we adapt more hyperparameters. When applied to 57 games on the Atari 2600 environment over 200 million frames our algorithm improves the median human normalized score of the baseline from 243\% to 364\%.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/R9NHHVXT/Zahavy_et_al_2020_Self-Tuning_Deep_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/KQ5DF8KQ/2002.html}
}

@inproceedings{Zhang2015,
  title = {Towards {{Vision-Based Deep Reinforcement Learning}} for {{Robotic Motion Control}}},
  booktitle = {Proc. {{Acra}}},
  author = {Zhang, Fangyi and Leitner, Juergen and Milford, Michael and Upcroft, Ben and Corke, Peter},
  date = {2015},
  url = {http://arxiv.org/abs/1511.03791},
  abstract = {This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.},
  file = {/Users/vitay/Documents/Zotero/storage/SVZQGJXJ/Zhang et al_2015_Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control.pdf}
}

@unpublished{Zhang2016,
  title = {Deep {{Reinforcement Learning}} with {{Successor Features}} for {{Navigation}} across {{Similar Environments}}},
  author = {Zhang, Jingwei and Springenberg, Jost Tobias and Boedecker, Joschka and Burgard, Wolfram},
  date = {2016-12-16},
  eprint = {1612.05533},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1612.05533},
  urldate = {2019-03-11},
  abstract = {In this paper we consider the problem of robot navigation in simple maze-like environments where the robot has to rely on its onboard sensors to perform the navigation task. In particular, we are interested in solutions to this problem that do not require localization, mapping or planning. Additionally, we require that our solution can quickly adapt to new situations (e.g., changing navigation goals and environments). To meet these criteria we frame this problem as a sequence of related reinforcement learning tasks. We propose a successor feature based deep reinforcement learning algorithm that can learn to transfer knowledge from previously mastered navigation tasks to new problem instances. Our algorithm substantially decreases the required learning time after the first task instance has been solved, which makes it easily adaptable to changing environments. We validate our method in both simulated and real robot experiments with a Robotino and compare it to a set of baseline methods including classical planning-based navigation.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/F4DDVCTK/Zhang et al_2016_Deep Reinforcement Learning with Successor Features for Navigation across.pdf;/Users/vitay/Documents/Zotero/storage/FFFAX6X5/1612.html}
}

@unpublished{Zhang2018a,
  title = {A {{Study}} on {{Overfitting}} in {{Deep Reinforcement Learning}}},
  author = {Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
  date = {2018-04-18},
  eprint = {1804.06893},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.06893},
  urldate = {2019-01-23},
  abstract = {Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen "robustly": commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/W98IBBRM/Zhang et al_2018_A Study on Overfitting in Deep Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/NAKYG45P/1804.html}
}

@unpublished{Zhang2019,
  title = {Asynchronous {{Methods}} for {{Model-Based Reinforcement Learning}}},
  author = {Zhang, Yunzhi and Clavera, Ignasi and Tsai, Boren and Abbeel, Pieter},
  date = {2019-10-28},
  eprint = {1910.12453},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.12453},
  urldate = {2019-11-01},
  abstract = {Significant progress has been made in the area of model-based reinforcement learning. State-of-the-art algorithms are now able to match the asymptotic performance of model-free methods while being significantly more data efficient. However, this success has come at a price: state-of-the-art model-based methods require significant computation interleaved with data collection, resulting in run times that take days, even if the amount of agent interaction might be just hours or even minutes. When considering the goal of learning in real-time on real robots, this means these state-of-the-art model-based algorithms still remain impractical. In this work, we propose an asynchronous framework for model-based reinforcement learning methods that brings down the run time of these algorithms to be just the data collection time. We evaluate our asynchronous framework on a range of standard MuJoCo benchmarks. We also evaluate our asynchronous framework on three real-world robotic manipulation tasks. We show how asynchronous learning not only speeds up learning w.r.t wall-clock time through parallelization, but also further reduces the sample complexity of model-based approaches by means of improving the exploration and by means of effectively avoiding the policy overfitting to the deficiencies of learned dynamics models.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/8U2SQ2WJ/Zhang et al_2019_Asynchronous Methods for Model-Based Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/NE9RUW9C/1910.html}
}

@article{Zhang2020,
  title = {Deep Reinforcement Learning for Power System Applications: {{An}} Overview},
  shorttitle = {Deep Reinforcement Learning for Power System Applications},
  author = {Zhang, Zidong and Zhang, Dongxia and Qiu, Robert C.},
  date = {2020-03},
  journaltitle = {CSEE Journal of Power and Energy Systems},
  volume = {6},
  number = {1},
  pages = {213--225},
  issn = {2096-0042},
  doi = {10.17775/CSEEJPES.2019.00920},
  abstract = {Due to increasing complexity, uncertainty and data dimensions in power systems, conventional methods often meet bottlenecks when attempting to solve decision and control problems. Therefore, data-driven methods toward solving such problems are being extensively studied. Deep reinforcement learning (DRL) is one of these data-driven methods and is regarded as real artificial intelligence (AI). DRL is a combination of deep learning (DL) and reinforcement learning (RL). This field of research has been applied to solve a wide range of complex sequential decision-making problems, including those in power systems. This paper firstly reviews the basic ideas, models, algorithms and techniques of DRL. Applications in power systems such as energy management, demand response, electricity market, operational control, and others are then considered. In addition, recent advances in DRL including the combination of RL with other classical methods, and the prospect and challenges of applications in power systems are also discussed.},
  eventtitle = {{{CSEE Journal}} of {{Power}} and {{Energy Systems}}},
  file = {/Users/vitay/Documents/Zotero/storage/N9CQHPI2/Zhang_et_al_2020_Deep_reinforcement_learning_for_power_system_applications.pdf}
}

@unpublished{Zhao2019,
  title = {Learning {{Efficient Representation}} for {{Intrinsic Motivation}}},
  author = {Zhao, Ruihan and Tiomkin, Stas and Abbeel, Pieter},
  date = {2019-12-08},
  eprint = {1912.02624},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.02624},
  urldate = {2019-12-10},
  abstract = {Mutual Information between agent Actions and environment States (MIAS) quantifies the influence of agent on its environment. Recently, it was found that the maximization of MIAS can be used as an intrinsic motivation for artificial agents. In literature, the term empowerment is used to represent the maximum of MIAS at a certain state. While empowerment has been shown to solve a broad range of reinforcement learning problems, its calculation in arbitrary dynamics is a challenging problem because it relies on the estimation of mutual information. Existing approaches, which rely on sampling, are limited to low dimensional spaces, because high-confidence distribution-free lower bounds for mutual information require exponential number of samples. In this work, we develop a novel approach for the estimation of empowerment in unknown dynamics from visual observation only, without the need to sample for MIAS. The core idea is to represent the relation between action sequences and future states using a stochastic dynamic model in latent space with a specific form. This allows us to efficiently compute empowerment with the "Water-Filling" algorithm from information theory. We construct this embedding with deep neural networks trained on a sophisticated objective function. Our experimental results show that the designed embedding preserves information-theoretic properties of the original dynamics.},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/8NPE9H8S/Zhao_et_al_2019_Learning_Efficient_Representation_for_Intrinsic_Motivation.pdf;/Users/vitay/Documents/Zotero/storage/868T3NE9/1912.html}
}

@unpublished{Zheng2019,
  title = {What {{Can Learned Intrinsic Rewards Capture}}?},
  author = {Zheng, Zeyu and Oh, Junhyuk and Hessel, Matteo and Xu, Zhongwen and Kroiss, Manuel and family=Hasselt, given=Hado, prefix=van, useprefix=true and Silver, David and Singh, Satinder},
  date = {2019-12-11},
  eprint = {1912.05500},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1912.05500},
  urldate = {2019-12-17},
  abstract = {Reinforcement learning agents can include different components, such as policies, value functions, state representations, and environment models. Any or all of these can be the loci of knowledge, i.e., structures where knowledge, whether given or learned, can be deposited and reused. The objective of an agent is to behave so as to maximise the sum of a suitable scalar function of state: the reward. As far as the learning algorithm is concerned, these rewards are typically given and immutable. In this paper we instead consider the proposition that the reward function itself may be a good locus of knowledge. This is consistent with a common use, in the literature, of hand-designed intrinsic rewards to improve the learning dynamics of an agent. We adopt the multi-lifetime setting of the Optimal Rewards Framework, and propose to meta-learn an intrinsic reward function from experience that allows agents to maximise their extrinsic rewards accumulated until the end of their lifetimes. Rewards as a locus of knowledge provide guidance on "what" the agent should strive to do rather than "how" the agent should behave; the latter is more directly captured in policies or value functions for example. Thus, our focus here is on demonstrating the following: (1) that it is feasible to meta-learn good reward functions, (2) that the learned reward functions can capture interesting kinds of "what" knowledge, and (3) that because of the indirectness of this form of knowledge the learned reward functions can generalise to other kinds of agents and to changes in the dynamics of the environment.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/WQEEUSX6/Zheng_et_al_2019_What_Can_Learned_Intrinsic_Rewards_Capture.pdf;/Users/vitay/Documents/Zotero/storage/F9M5UG6N/1912.html}
}

@unpublished{Zhou2019a,
  title = {Watch, {{Try}}, {{Learn}}: {{Meta-Learning}} from {{Demonstrations}} and {{Reward}}},
  shorttitle = {Watch, {{Try}}, {{Learn}}},
  author = {Zhou, Allan and Jang, Eric and Kappler, Daniel and Herzog, Alex and Khansari, Mohi and Wohlhart, Paul and Bai, Yunfei and Kalakrishnan, Mrinal and Levine, Sergey and Finn, Chelsea},
  date = {2019-06-07},
  eprint = {1906.03352},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.03352},
  urldate = {2019-06-16},
  abstract = {Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/SGRMHPM3/Zhou et al_2019_Watch, Try, Learn.pdf;/Users/vitay/Documents/Zotero/storage/GQSGPYXL/1906.html}
}

@unpublished{Zhu2017,
  title = {Visual {{Semantic Planning}} Using {{Deep Successor Representations}}},
  author = {Zhu, Yuke and Gordon, Daniel and Kolve, Eric and Fox, Dieter and Fei-Fei, Li and Gupta, Abhinav and Mottaghi, Roozbeh and Farhadi, Ali},
  date = {2017-05-23},
  eprint = {1705.08080},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1705.08080},
  urldate = {2019-02-23},
  abstract = {A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world. In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state. Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects. We propose learning these through interacting with a visual and dynamic environment. Our proposed solution involves bootstrapping reinforcement learning with imitation learning. To ensure cross task generalization, we develop a deep predictive model based on successor representations. Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/MR75RGEH/Zhu et al_2017_Visual Semantic Planning using Deep Successor Representations.pdf;/Users/vitay/Documents/Zotero/storage/XRUYGQZB/Zhu et al_2017_Visual Semantic Planning using Deep Successor Representations.pdf;/Users/vitay/Documents/Zotero/storage/4I6JYMKM/1705.html;/Users/vitay/Documents/Zotero/storage/ZW43SWYU/1705.html}
}

@unpublished{Zhu2020a,
  title = {The {{Ingredients}} of {{Real-World Robotic Reinforcement Learning}}},
  author = {Zhu, Henry and Yu, Justin and Gupta, Abhishek and Shah, Dhruv and Hartikainen, Kristian and Singh, Avi and Kumar, Vikash and Levine, Sergey},
  date = {2020-04-26},
  eprint = {2004.12570},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2004.12570},
  urldate = {2020-05-02},
  abstract = {The success of reinforcement learning for real world robotics has been, in many cases limited to instrumented laboratory scenarios, often requiring arduous human effort and oversight to enable continuous learning. In this work, we discuss the elements that are needed for a robotic learning system that can continually and autonomously improve with data collected in the real world. We propose a particular instantiation of such a system, using dexterous manipulation as our case study. Subsequently, we investigate a number of challenges that come up when learning without instrumentation. In such settings, learning must be feasible without manually designed resets, using only on-board perception, and without hand-engineered reward functions. We propose simple and scalable solutions to these challenges, and then demonstrate the efficacy of our proposed system on a set of dexterous robotic manipulation tasks, providing an in-depth analysis of the challenges associated with this learning paradigm. We demonstrate that our complete system can learn without any human intervention, acquiring a variety of vision-based skills with a real-world three-fingered hand. Results and videos can be found at https://sites.google.com/view/realworld-rl/},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/9TQHFV8F/Zhu_et_al_2020_The_Ingredients_of_Real-World_Robotic_Reinforcement_Learning.pdf;/Users/vitay/Documents/Zotero/storage/FV7TAR29/2004.html}
}

@inproceedings{Ziebart2008,
  title = {Maximum {{Entropy Inverse Reinforcement Learning}}},
  author = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
  date = {2008},
  pages = {6},
  abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.},
  eventtitle = {{{AAAI Conference}} on {{Artificial Intelligence}}},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/VYBQWRJ6/Ziebart_et_al_2008_Maximum_Entropy_Inverse_Reinforcement_Learning.pdf}
}
