<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="/usr/share/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<header>
  <h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
  <p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>


<article>

<nav id="TOC" class ="toc">
  <h2><strong>Deep Reinforcement Learning</strong></h2>
  <p class="author">Julien Vitay</p>

<ul>
<li><a href="./0-Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./1.1-BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./1.1-BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./1.1-BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./1.1-BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./1.1-BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./1.1-BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./1.1-BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./1.1-BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.6</span> Actor-critic architectures</a></li>
<li><a href="./1.1-BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.7</span> Function approximation</a></li>
</ul></li>
<li><a href="./1.2-DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./1.2-DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./1.2-DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./1.2-DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./2-Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./2-Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./2-Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./2-Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./2-Valuebased.html#sec:prioritised-replay"><span class="toc-section-number">3.4</span> Prioritised replay</a></li>
<li><a href="./2-Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./2-Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./2-Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./2-Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./3.1-PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./3.1-PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./3.1-PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./3.1-PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./3.1-PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./3.2-ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./3.2-ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./3.2-ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./3.2-ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./3.2-ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./3.3-DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.3</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./3.3-DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.3.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./3.3-DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.3.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
</ul></li>
<li><a href="./3.4-NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.4</span> Natural Gradients</a><ul>
<li><a href="./3.4-NaturalGradient.html#sec:natural-actor-critic-nac"><span class="toc-section-number">4.4.1</span> Natural Actor Critic (NAC)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.4.2</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.4.3</span> Proximal Policy Optimization (PPO)</a></li>
</ul></li>
<li><a href="./3.4-NaturalGradient.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.5</span> Off-policy Actor-Critic</a><ul>
<li><a href="./3.4-NaturalGradient.html#sec:importance-sampling"><span class="toc-section-number">4.5.1</span> Importance sampling</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:linear-off-policy-actor-critic-offpac"><span class="toc-section-number">4.5.2</span> Linear Off-Policy Actor-Critic (OffPAC)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.3</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./3.4-NaturalGradient.html#sec:other-policy-gradient-methods"><span class="toc-section-number">4.6</span> Other policy gradient methods</a><ul>
<li><a href="./3.4-NaturalGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.6.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.6.2</span> Fictitious Self-Play (FSP)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:q-prop"><span class="toc-section-number">4.6.3</span> Q-Prop</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.6.4</span> Normalized Advantage Function (NAF)</a></li>
</ul></li>
<li><a href="./3.4-NaturalGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.7</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.8</span> Gradient-free policy search</a><ul>
<li><a href="./3.4-NaturalGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.8.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./3.4-NaturalGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.8.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./7-Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">5</span> Deep RL in practice</a><ul>
<li><a href="./7-Practice.html#sec:limitations"><span class="toc-section-number">5.1</span> Limitations</a></li>
<li><a href="./7-Practice.html#sec:reward-shaping"><span class="toc-section-number">5.2</span> Reward shaping</a></li>
<li><a href="./7-Practice.html#sec:simulation-environments"><span class="toc-section-number">5.3</span> Simulation environments</a></li>
<li><a href="./7-Practice.html#sec:algorithm-implementations"><span class="toc-section-number">5.4</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./8-References.html#sec:references">References</a></li>
</ul>


</nav>



<h2 id="sec:natural-gradients"><span class="header-section-number">4.4</span> Natural Gradients</h2>
<p>Natural policy gradient <span class="citation" data-cites="Kakade2001">Kakade (<a href="8-References.html#ref-Kakade2001">2001</a>)</span></p>
<h3 id="sec:natural-actor-critic-nac"><span class="header-section-number">4.4.1</span> Natural Actor Critic (NAC)</h3>
<p><span class="citation" data-cites="Peters2008">Peters and Schaal (<a href="8-References.html#ref-Peters2008">2008</a>)</span></p>
<h3 id="sec:trust-region-policy-optimization-trpo"><span class="header-section-number">4.4.2</span> Trust Region Policy Optimization (TRPO)</h3>
<p><span class="citation" data-cites="Schulman2015a">Schulman, Levine, et al. (<a href="8-References.html#ref-Schulman2015a">2015</a>)</span></p>
<h3 id="sec:proximal-policy-optimization-ppo"><span class="header-section-number">4.4.3</span> Proximal Policy Optimization (PPO)</h3>
<p><span class="citation" data-cites="Schulman2017">Schulman et al. (<a href="8-References.html#ref-Schulman2017">2017</a>)</span></p>
<p>Explanations from OpenAI: <a href="https://blog.openai.com/openai-baselines-ppo/#content" class="uri">https://blog.openai.com/openai-baselines-ppo/#content</a></p>
<h2 id="sec:off-policy-actor-critic"><span class="header-section-number">4.5</span> Off-policy Actor-Critic</h2>
<h3 id="sec:importance-sampling"><span class="header-section-number">4.5.1</span> Importance sampling</h3>
<p><span class="citation" data-cites="Meuleau200">Meuleau et al. (<a href="8-References.html#ref-Meuleau200">2000</a>)</span> <span class="citation" data-cites="Tang2010">Tang and Abbeel (<a href="8-References.html#ref-Tang2010">2010</a>)</span> <span class="citation" data-cites="Levine2013">Levine and Koltun (<a href="8-References.html#ref-Levine2013">2013</a>)</span></p>
<h3 id="sec:linear-off-policy-actor-critic-offpac"><span class="header-section-number">4.5.2</span> Linear Off-Policy Actor-Critic (OffPAC)</h3>
<p><span class="citation" data-cites="Degris2012">Degris, White, and Sutton (<a href="8-References.html#ref-Degris2012">2012</a>)</span></p>
<h3 id="sec:actor-critic-with-experience-replay-acer"><span class="header-section-number">4.5.3</span> Actor-Critic with Experience Replay (ACER)</h3>
<p><span class="citation" data-cites="Wang2017">Wang et al. (<a href="8-References.html#ref-Wang2017">2017</a>)</span></p>
<h2 id="sec:other-policy-gradient-methods"><span class="header-section-number">4.6</span> Other policy gradient methods</h2>
<h3 id="sec:stochastic-value-gradient-svg"><span class="header-section-number">4.6.1</span> Stochastic Value Gradient (SVG)</h3>
<p><span class="citation" data-cites="Heess2015">Heess et al. (<a href="8-References.html#ref-Heess2015">2015</a>)</span></p>
<h3 id="sec:fictitious-self-play-fsp"><span class="header-section-number">4.6.2</span> Fictitious Self-Play (FSP)</h3>
<p><span class="citation" data-cites="Heinrich2015">Heinrich, Lanctot, and Silver (<a href="8-References.html#ref-Heinrich2015">2015</a>)</span> <span class="citation" data-cites="Heinrich2016">Heinrich and Silver (<a href="8-References.html#ref-Heinrich2016">2016</a>)</span></p>
<h3 id="sec:q-prop"><span class="header-section-number">4.6.3</span> Q-Prop</h3>
<p><span class="citation" data-cites="Gu2016">Gu, Lillicrap, Ghahramani, et al. (<a href="8-References.html#ref-Gu2016">2016</a>)</span></p>
<h3 id="sec:normalized-advantage-function-naf"><span class="header-section-number">4.6.4</span> Normalized Advantage Function (NAF)</h3>
<p><span class="citation" data-cites="Gu2016b">Gu, Lillicrap, Sutskever, et al. (<a href="8-References.html#ref-Gu2016b">2016</a>)</span></p>
<h2 id="sec:comparison-between-value-based-and-policy-gradient-methods"><span class="header-section-number">4.7</span> Comparison between value-based and policy gradient methods</h2>
<p><a href="https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html" class="uri">https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html</a></p>
<p>Advantages of policy gradient:</p>
<ul>
<li>Better convergence properties.</li>
<li>Effective in high-dimensional or continuous action spaces.</li>
<li>Can learn stochastic policies.</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Typically converge to a local rather than global optimum.</li>
<li>Evaluating a policy is typically inefficient and high variance.</li>
</ul>
<p>Policy gradient methods are therefore usually less sample efficient, but can be more stable than value-based methods (Duan et al., 2016).</p>
<h2 id="sec:gradient-free-policy-search"><span class="header-section-number">4.8</span> Gradient-free policy search</h2>
<p>The policy gradient methods presented above rely on backpropagation and gradient descent/ascent to update the parameters of the policy and maximize the objective function. Gradient descent is generally slow, sample inefficient and subject to local minima, but is nevertheless the go-to method in neural networks. However, it is not the only optimization that can be used in deep RL. This section presents some of the alternatives.</p>
<h3 id="sec:cross-entropy-method-cem"><span class="header-section-number">4.8.1</span> Cross-entropy Method (CEM)</h3>
<p><span class="citation" data-cites="Szita2006">Szita and LÃ¶rincz (<a href="8-References.html#ref-Szita2006">2006</a>)</span></p>
<h3 id="sec:evolutionary-search-es"><span class="header-section-number">4.8.2</span> Evolutionary Search (ES)</h3>
<p><span class="citation" data-cites="Salimans2017">Salimans et al. (<a href="8-References.html#ref-Salimans2017">2017</a>)</span></p>
<p>Explanations from OpenAI: <a href="https://blog.openai.com/evolution-strategies/" class="uri">https://blog.openai.com/evolution-strategies/</a></p>
<p>Deep neuroevolution at Uber: <a href="https://eng.uber.com/deep-neuroevolution/" class="uri">https://eng.uber.com/deep-neuroevolution/</a></p>

<br>
<div class="arrows">
<a href="3.3-DPG.html" class="previous">&laquo; Previous</a>
<a href="7-Practice.html" class="next">Next &raquo;</a>
</div>


</article>
</body>
</html>
