<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - 11&nbsp; Miscellaneous model-free algorithm</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./4-RAM.html" rel="next">
<link href="./3.7-DistributionalRL.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Miscellaneous model-free algorithm</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Deep Reinforcement Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1.1-BasicRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Basics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1.2-DeepLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Deep learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-Valuebased.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Value-based methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.1-PolicyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Policy Gradient methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.2-ActorCritic.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advantage Actor-Critic methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.3-ImportanceSampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Off-policy Actor-Critic</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.4-DPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Deterministic Policy Gradient (DPG)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.5-NaturalGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Natural Gradients</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.6-EntropyRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Maximum Entropy RL</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.7-DistributionalRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Distributional learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.8-OtherPolicyGradient.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Miscellaneous model-free algorithm</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-RAM.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Recurrent Attention Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-ModelBased.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-Hierarchical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Hierarchical Reinforcement Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-Inverse.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Inverse Reinforcement Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-Robotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Deep RL for robotics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-Practice.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Deep RL in practice</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#stochastic-value-gradient-svg" id="toc-stochastic-value-gradient-svg" class="nav-link active" data-scroll-target="#stochastic-value-gradient-svg"><span class="toc-section-number">11.0.1</span>  Stochastic Value Gradient (SVG)</a></li>
  <li><a href="#q-prop" id="toc-q-prop" class="nav-link" data-scroll-target="#q-prop"><span class="toc-section-number">11.0.2</span>  Q-Prop</a></li>
  <li><a href="#normalized-advantage-function-naf" id="toc-normalized-advantage-function-naf" class="nav-link" data-scroll-target="#normalized-advantage-function-naf"><span class="toc-section-number">11.0.3</span>  Normalized Advantage Function (NAF)</a></li>
  <li><a href="#fictitious-self-play-fsp" id="toc-fictitious-self-play-fsp" class="nav-link" data-scroll-target="#fictitious-self-play-fsp"><span class="toc-section-number">11.0.4</span>  Fictitious Self-Play (FSP)</a></li>
  <li><a href="#comparison-between-value-based-and-policy-gradient-methods" id="toc-comparison-between-value-based-and-policy-gradient-methods" class="nav-link" data-scroll-target="#comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">11.1</span>  Comparison between value-based and policy gradient methods</a></li>
  <li><a href="#gradient-free-policy-search" id="toc-gradient-free-policy-search" class="nav-link" data-scroll-target="#gradient-free-policy-search"><span class="toc-section-number">11.2</span>  Gradient-free policy search</a>
  <ul class="collapse">
  <li><a href="#cross-entropy-method-cem" id="toc-cross-entropy-method-cem" class="nav-link" data-scroll-target="#cross-entropy-method-cem"><span class="toc-section-number">11.2.1</span>  Cross-entropy Method (CEM)</a></li>
  <li><a href="#evolutionary-search-es" id="toc-evolutionary-search-es" class="nav-link" data-scroll-target="#evolutionary-search-es"><span class="toc-section-number">11.2.2</span>  Evolutionary Search (ES)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Miscellaneous model-free algorithm</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="stochastic-value-gradient-svg" class="level3" data-number="11.0.1">
<h3 data-number="11.0.1" class="anchored" data-anchor-id="stochastic-value-gradient-svg"><span class="header-section-number">11.0.1</span> Stochastic Value Gradient (SVG)</h3>
<p><span class="citation" data-cites="Heess2015">Heess et al. (<a href="references.html#ref-Heess2015" role="doc-biblioref">2015</a>)</span></p>
</section>
<section id="q-prop" class="level3" data-number="11.0.2">
<h3 data-number="11.0.2" class="anchored" data-anchor-id="q-prop"><span class="header-section-number">11.0.2</span> Q-Prop</h3>
<p><span class="citation" data-cites="Gu2016">Gu et al. (<a href="references.html#ref-Gu2016" role="doc-biblioref">2016b</a>)</span></p>
</section>
<section id="normalized-advantage-function-naf" class="level3" data-number="11.0.3">
<h3 data-number="11.0.3" class="anchored" data-anchor-id="normalized-advantage-function-naf"><span class="header-section-number">11.0.3</span> Normalized Advantage Function (NAF)</h3>
<p><span class="citation" data-cites="Gu2016a">Gu et al. (<a href="references.html#ref-Gu2016a" role="doc-biblioref">2016a</a>)</span></p>
</section>
<section id="fictitious-self-play-fsp" class="level3" data-number="11.0.4">
<h3 data-number="11.0.4" class="anchored" data-anchor-id="fictitious-self-play-fsp"><span class="header-section-number">11.0.4</span> Fictitious Self-Play (FSP)</h3>
<p><span class="citation" data-cites="Heinrich2015">Heinrich et al. (<a href="references.html#ref-Heinrich2015" role="doc-biblioref">2015</a>)</span> <span class="citation" data-cites="Heinrich2016">Heinrich and Silver (<a href="references.html#ref-Heinrich2016" role="doc-biblioref">2016</a>)</span></p>
</section>
<section id="comparison-between-value-based-and-policy-gradient-methods" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="comparison-between-value-based-and-policy-gradient-methods"><span class="header-section-number">11.1</span> Comparison between value-based and policy gradient methods</h2>
<p>Having now reviewed both value-based methods (DQN and its variants) and policy gradient methods (A3C, DDPG, PPO), the question is which method to choose? While not much happens right now for value-based methods, policy gradient methods are attracting a lot of attention, as they are able to learn policies in continuous action spaces, what is very important in robotics. <a href="https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html" class="uri">https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html</a> summarizes the advantages and inconvenients of policy gradient methods.</p>
<p>Advantages of PG:</p>
<ul>
<li>Better convergence properties, more stable <span class="citation" data-cites="Duan2016">(<a href="references.html#ref-Duan2016" role="doc-biblioref">Duan et al., 2016</a>)</span>.</li>
<li>Effective in high-dimensional or continuous action spaces.</li>
<li>Can learn stochastic policies.</li>
</ul>
<p>Disadvantages of PG:</p>
<ul>
<li>Typically converge to a local rather than global optimum.</li>
<li>Evaluating a policy is often inefficient and having a high variance.</li>
<li>Worse sample efficiency (but it is getting better).</li>
</ul>
</section>
<section id="gradient-free-policy-search" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="gradient-free-policy-search"><span class="header-section-number">11.2</span> Gradient-free policy search</h2>
<p>The policy gradient methods presented above rely on backpropagation and gradient descent/ascent to update the parameters of the policy and maximize the objective function. Gradient descent is generally slow, sample inefficient and subject to local minima, but is nevertheless the go-to method in neural networks. However, it is not the only optimization that can be used in deep RL. This section presents quickly some of the alternatives.</p>
<section id="cross-entropy-method-cem" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="cross-entropy-method-cem"><span class="header-section-number">11.2.1</span> Cross-entropy Method (CEM)</h3>
<p><span class="citation" data-cites="Szita2006">Szita and Lörincz (<a href="references.html#ref-Szita2006" role="doc-biblioref">2006</a>)</span></p>
</section>
<section id="evolutionary-search-es" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="evolutionary-search-es"><span class="header-section-number">11.2.2</span> Evolutionary Search (ES)</h3>
<p><span class="citation" data-cites="Salimans2017">Salimans et al. (<a href="references.html#ref-Salimans2017" role="doc-biblioref">2017</a>)</span></p>
<p>Explanations from OpenAI: <a href="https://blog.openai.com/evolution-strategies/" class="uri">https://blog.openai.com/evolution-strategies/</a></p>
<p>Deep neuroevolution at Uber: <a href="https://eng.uber.com/deep-neuroevolution/" class="uri">https://eng.uber.com/deep-neuroevolution/</a></p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-Duan2016" class="csl-entry" role="doc-biblioentry">
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016). Benchmarking <span>Deep Reinforcement Learning</span> for <span>Continuous Control</span>. Available at: <a href="http://arxiv.org/abs/1604.06778">http://arxiv.org/abs/1604.06778</a>.
</div>
<div id="ref-Gu2016a" class="csl-entry" role="doc-biblioentry">
Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and Levine, S. (2016a). Q-<span>Prop</span>: <span>Sample-Efficient Policy Gradient</span> with <span>An Off-Policy Critic</span>. Available at: <a href="http://arxiv.org/abs/1611.02247">http://arxiv.org/abs/1611.02247</a>.
</div>
<div id="ref-Gu2016" class="csl-entry" role="doc-biblioentry">
Gu, S., Lillicrap, T., Sutskever, I., and Levine, S. (2016b). Continuous <span>Deep Q-Learning</span> with <span class="nocase">Model-based Acceleration</span>. Available at: <a href="http://arxiv.org/abs/1603.00748">http://arxiv.org/abs/1603.00748</a>.
</div>
<div id="ref-Heess2015" class="csl-entry" role="doc-biblioentry">
Heess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T. (2015). Learning continuous control policies by stochastic value gradients. <em>Proc. International Conference on Neural Information Processing Systems</em>, 2944–2952. Available at: <a href="http://dl.acm.org/citation.cfm?id=2969569">http://dl.acm.org/citation.cfm?id=2969569</a>.
</div>
<div id="ref-Heinrich2015" class="csl-entry" role="doc-biblioentry">
Heinrich, J., Lanctot, M., and Silver, D. (2015). Fictitious <span>Self-Play</span> in <span>Extensive-Form Games</span>. 805–813. Available at: <a href="http://proceedings.mlr.press/v37/heinrich15.html">http://proceedings.mlr.press/v37/heinrich15.html</a>.
</div>
<div id="ref-Heinrich2016" class="csl-entry" role="doc-biblioentry">
Heinrich, J., and Silver, D. (2016). Deep <span>Reinforcement Learning</span> from <span>Self-Play</span> in <span>Imperfect-Information Games</span>. Available at: <a href="http://arxiv.org/abs/1603.01121">http://arxiv.org/abs/1603.01121</a>.
</div>
<div id="ref-Salimans2017" class="csl-entry" role="doc-biblioentry">
Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017). Evolution <span>Strategies</span> as a <span>Scalable Alternative</span> to <span>Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1703.03864">http://arxiv.org/abs/1703.03864</a>.
</div>
<div id="ref-Szita2006" class="csl-entry" role="doc-biblioentry">
Szita, I., and Lörincz, A. (2006). Learning <span>Tetris Using</span> the <span>Noisy Cross-Entropy Method</span>. <em>Neural Computation</em> 18, 2936–2941. doi:<a href="https://doi.org/10.1162/neco.2006.18.12.2936">10.1162/neco.2006.18.12.2936</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./3.7-DistributionalRL.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Distributional learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./4-RAM.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Recurrent Attention Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>