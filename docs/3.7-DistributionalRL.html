<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - 10&nbsp; Distributional learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./3.8-OtherPolicyGradient.html" rel="next">
<link href="./3.6-EntropyRL.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Distributional learning</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Deep Reinforcement Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1.1-BasicRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Basics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1.2-DeepLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Deep learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-Valuebased.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Value-based methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.1-PolicyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Policy Gradient methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.2-ActorCritic.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advantage Actor-Critic methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.3-ImportanceSampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Off-policy Actor-Critic</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.4-DPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Deterministic Policy Gradient (DPG)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.5-NaturalGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Natural Gradients</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.6-EntropyRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Maximum Entropy RL</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.7-DistributionalRL.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Distributional learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.8-OtherPolicyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Miscellaneous model-free algorithm</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-RAM.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Recurrent Attention Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-ModelBased.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-Hierarchical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Hierarchical Reinforcement Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-Inverse.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Inverse Reinforcement Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-Robotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Deep RL for robotics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-Practice.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Deep RL in practice</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#categorical-dqn" id="toc-categorical-dqn" class="nav-link active" data-scroll-target="#categorical-dqn"><span class="toc-section-number">10.0.1</span>  Categorical DQN</a></li>
  <li><a href="#the-reactor" id="toc-the-reactor" class="nav-link" data-scroll-target="#the-reactor"><span class="toc-section-number">10.0.2</span>  The Reactor</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Distributional learning</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>All RL methods based on the Bellman equations use the expectation operator to average returns and compute the values of states and actions:</p>
<p><span class="math display">\[
    Q^\pi(s, a) = \mathbb{E}_{s, a \sim \pi}[R(s, a)]
\]</span></p>
<p>The variance of the returns is not considered in the action selection scheme, and most methods actually try to reduce this variance as it impairs the convergence of neural networks. Decision theory states that only the mean should matter on the long-term, but one can imagine tasks where the variance is an important factor for the decision. Imagine you are in a game where you have two actions available: the first one brings returns of 10 and 20, with a probability of 0.5 each (to simplify), while the second one brings returns of -10 and +40 with probability 0.5 too. Both actions have the same Q-value of 15 (a return which is actually never experienced), so one can theoretically pick whatever action, both are optimal in the Bellman’s sense.</p>
<p>However, this is only true when playing <strong>long enough</strong>. If, after learning, one is only allowed one try on that game, it is obviously safer (but less fun) to choose the first action, as one wins at worse 10, while it is -10 with the second action. Knowing the distribution of the returns can allow to distinguish risky choices from safe ones more easily and adapt the behavior. Another advantage would be that by learning the distribution of the returns instead of just their mean, one actually gathers more information about the environment dynamics: it can only help the convergence of the algorithm towards the optimal policy.</p>
<section id="categorical-dqn" class="level3" data-number="10.0.1">
<h3 data-number="10.0.1" class="anchored" data-anchor-id="categorical-dqn"><span class="header-section-number">10.0.1</span> Categorical DQN</h3>
<p><span class="citation" data-cites="Bellemare2017">Bellemare et al. (<a href="references.html#ref-Bellemare2017" role="doc-biblioref">2017</a>)</span> proposed to learn the <strong>value distribution</strong> (the probability distribution of the returns) through a modification of the Bellman equation. They show that learning the complete distribution of rewards instead of their mean leads to performance improvements on Atari games over modern variants of DQN.</p>
<p>Their proposed <strong>categorical DQN</strong> (also called C51) has an architecture based on DQN, but where the output layer predicts the distribution of the returns for each action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, instead of its mean <span class="math inline">\(Q^\pi(s, a)\)</span>. In practice, each action <span class="math inline">\(a\)</span> is represented by <span class="math inline">\(N\)</span> output neurons, who encode the support of the distribution of returns. If the returns take values between <span class="math inline">\(V_\text{min}\)</span> and <span class="math inline">\(V_\text{max}\)</span>, one can represent their distribution <span class="math inline">\(\mathcal{Z}\)</span> by taking <span class="math inline">\(N\)</span> discrete “bins” (called <em>atoms</em> in the paper) in that range. <a href="#fig-distributionallearning">Figure&nbsp;<span>10.1</span></a> shows how the distribution of returns between -10 and 10 can be represented using 21 atoms.</p>
<div id="fig-distributionallearning" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/distributionallearning.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;10.1: Example of a value distribution using 21 atoms between -10 and 10. The average return is 3, but its variance is explicitly represented.</figcaption><p></p>
</figure>
</div>
<p>Of course, the main problem is to know in advance the range of returns <span class="math inline">\([V_\text{min}, V_\text{max}]\)</span> (it depends largely on the choice of the discount rate <span class="math inline">\(\gamma\)</span>), but you can infer it from training another algorithm such as DQN beforehand. <span class="citation" data-cites="Dabney2017">Dabney et al. (<a href="references.html#ref-Dabney2017" role="doc-biblioref">2017</a>)</span> got rid of this problem with quantile regression. In the paper, the authors found out experimentally that 51 is the most efficient number of atoms (hence the name C51).</p>
<p>Let’s note <span class="math inline">\(z_i\)</span> these atoms with <span class="math inline">\(1 \leq i &lt; N\)</span>. The atom probability that the return associated to a state-action pair <span class="math inline">\((s, a)\)</span> lies within the bin associated to the atom <span class="math inline">\(z_i\)</span> is noted <span class="math inline">\(p_i(s, a)\)</span>. These probabilities can be predicted by a neural network, typically by using a softmax function over outputs <span class="math inline">\(f_i(s, a; \theta)\)</span>:</p>
<p><span class="math display">\[
    p_i(s, a; \theta) = \frac{\exp f_i(s, a; \theta)}{\sum_{j=1}^{N} \exp f_j(s, a; \theta)}
\]</span></p>
<p>The distribution of the returns <span class="math inline">\(\mathcal{Z}\)</span> is simply a sum over the atoms (represented by the Dirac distribution <span class="math inline">\(\delta_{z_i}\)</span>):</p>
<p><span class="math display">\[
    \mathcal{Z}_\theta(s, a) = \sum_{i=1}^{N} p_i(s, a; \theta) \, \delta_{z_i}
\]</span></p>
<p>If these probabilities are correctly estimated, the Q-value is easy to compute as the mean of the distribution:</p>
<p><span class="math display">\[
    Q_\theta(s, a) = \mathbb{E} [\mathcal{Z}_\theta(s, a)] = \sum_{i=1}^{N} p_i(s, a; \theta) \, z_i
\]</span></p>
<p>These Q-values can then be used for action selection as in the regular DQN. The problem is now to learn the value distribution <span class="math inline">\(\mathcal{Z}_\theta\)</span>, i.e.&nbsp;to find a learning rule / loss function for the <span class="math inline">\(p_i(s, a; \theta)\)</span>. Let’s consider a single transition <span class="math inline">\((s, a, r, s')\)</span> and select the greedy action <span class="math inline">\(a'\)</span> in <span class="math inline">\(s'\)</span> using the current policy <span class="math inline">\(\pi_\theta\)</span>. The value distribution <span class="math inline">\(\mathcal{Z}_\theta\)</span> can be evaluated by applying recursively the Bellman operator <span class="math inline">\(\mathcal{T}\)</span>:</p>
<p><span class="math display">\[
    \mathcal{T} \, \mathcal{Z}_\theta(s, a) = \mathcal{R}(s, a) + \gamma \, \mathcal{Z}_\theta(s', a')
\]</span></p>
<p>where <span class="math inline">\(\mathcal{R}(s, a)\)</span> is the distribution of immediate rewards after <span class="math inline">\((s, a)\)</span>. This use of the Bellman operator is the same as in Q-learning:</p>
<p><span class="math display">\[
    \mathcal{T} \, \mathcal{Q}_\theta(s, a) = \mathbb{E}[r(s, a)] + \gamma \, \mathcal{Q}_\theta(s', a')
\]</span></p>
<p>In Q-learning, one minimizes the difference (mse) between <span class="math inline">\(\mathcal{T} \, \mathcal{Q}_\theta(s, a)\)</span> and <span class="math inline">\(\mathcal{Q}_\theta(s, a)\)</span>, which are expectations (so we only manipulate scalars). Here, we will minimize the statistical distance between the distributions <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> and <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span> themselves, using for example the KL divergence, Wasserstein metric, total variation or whatnot.</p>
<p>The problem is mostly that the distributions <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> and <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span> do not have the same support: for a particular atom <span class="math inline">\(z_i\)</span>, <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> can have a non-zero probability <span class="math inline">\(p_i(s, a)\)</span>, while <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span> has a zero probability. Besides, the probabilities must sum to 1, so one cannot update the <span class="math inline">\(z_i\)</span> independently from one another.</p>
<p>The proposed method consists of three steps:</p>
<ol type="1">
<li>Computation of the Bellman update <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span>. They simply compute translated values for each <span class="math inline">\(z_i\)</span> according to:</li>
</ol>
<p><span class="math display">\[
    \mathcal{T} \, z_i = r + \gamma \, z_i
\]</span></p>
<p>and clip the obtained value to <span class="math inline">\([V_\text{min}, V_\text{max}]\)</span>. The reward <span class="math inline">\(r\)</span> translates the distribution of atoms, while the discount rate <span class="math inline">\(\gamma\)</span> scales it. <a href="#fig-distributionallearning2">Figure&nbsp;<span>10.2</span></a> shows the distribution of <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> compared to <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span>. Note that the atoms of the two distributions are not aligned.</p>
<div id="fig-distributionallearning2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/distributionallearning2.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;10.2: Computation of the Bellman update <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span>. The atoms of the two distributions are not aligned.</figcaption><p></p>
</figure>
</div>
<ol start="2" type="1">
<li>Distribution of the probabilities of <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> on the support of <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span>. The projected atom <span class="math inline">\(\mathcal{T} \, z_i\)</span> lie between two “real” atoms <span class="math inline">\(z_l\)</span> and <span class="math inline">\(z_u\)</span>, with a non-integer index <span class="math inline">\(b\)</span> (for example <span class="math inline">\(b = 3.4\)</span>, <span class="math inline">\(l = 3\)</span> and <span class="math inline">\(u=4\)</span>). The corresponding probability <span class="math inline">\(p_{b}(s', a'; \theta)\)</span> of the next greedy action <span class="math inline">\((s', a')\)</span> is “spread” to its neighbors through a local interpolation depending on the distances between <span class="math inline">\(b\)</span>, <span class="math inline">\(l\)</span> and <span class="math inline">\(u\)</span>:</li>
</ol>
<p><span class="math display">\[
    \Delta p_{l}(s', a'; \theta) = p_{b}(s', a'; \theta) \, (b - u)
\]</span> <span class="math display">\[
    \Delta p_{u}(s', a'; \theta) = p_{b}(s', a'; \theta) \, (l - b)
\]</span></p>
<p><a href="#fig-distributionallearning3">Figure&nbsp;<span>10.3</span></a> shows how the projected update distribution <span class="math inline">\(\Phi \, \mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> now matches the support of <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span></p>
<div id="fig-distributionallearning3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/distributionallearning3.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;10.3: Projected update <span class="math inline">\(\Phi \, \mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> on the support of <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span>. The atoms are now aligned, the statistical distance between the two distributions can be minimized.</figcaption><p></p>
</figure>
</div>
<p>The projection of the Bellman update onto an atom <span class="math inline">\(z_i\)</span> can be summarized by the following equation:</p>
<p><span class="math display">\[
    (\Phi \, \mathcal{T} \, \mathcal{Z}_\theta(s, a))_i = \sum_{j=1}^N \big [1 - \frac{| [\mathcal{T}\, z_j]_{V_\text{min}}^{V_\text{max}} - z_i|}{\Delta z} \big ]_0^1 \, p_j (s', a'; \theta)
\]</span></p>
<p>where <span class="math inline">\([\cdot]_a^b\)</span> bounds its argument in <span class="math inline">\([a, b]\)</span> and <span class="math inline">\(\Delta z\)</span> is the step size between two atoms.</p>
<ol start="3" type="1">
<li>Minimizing the statistical distance between <span class="math inline">\(\Phi \, \mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> and <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span>. Now that the Bellman update has the same support as the value distribution, we can minimize the KL divergence between the two for a single transition:</li>
</ol>
<p><span class="math display">\[
    \mathcal{L}(\theta) = D_\text{KL} (\Phi \, \mathcal{T} \, \mathcal{Z}_{\theta'}(s, a) | \mathcal{Z}_\theta(s, a))
\]</span></p>
<p>using a target network <span class="math inline">\(\theta'\)</span> for the target. It is to be noted that minimizing the KL divergence is the same as minimizing the cross-entropy between the two, as in classification tasks:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) =  - \sum_i (\Phi \, \mathcal{T} \, \mathcal{Z}_{\theta'}(s, a))_i \log p_i (s, a; \theta)
\]</span></p>
<p>The projected Bellman update plays the role of the one-hot encoded target vector in classification (except that it is not one-hot encoded). DQN performs a regression on the Q-values (mse loss), while categorical DQN performs a classification (cross-entropy loss). Apart from the way the target is computed, categorical DQN is very similar to DQN: architecture, experience replay memory, target networks, etc.</p>
<p><a href="#fig-categoricaldqn">Figure&nbsp;<span>10.4</span></a> illustrates how the predicted value distribution changes when playing Space invaders (also have a look at the Youtube video at <a href="https://www.youtube.com/watch?v=yFBwyPuO2Vg" class="uri">https://www.youtube.com/watch?v=yFBwyPuO2Vg</a>). C51 outperforms DQN on most Atari games, both in terms of the achieved performance and the sample complexity.</p>
<div id="fig-categoricaldqn" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/categoricaldqn.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;10.4: Evolution of the value distribution for the categorical DQN playing Space Invaders. Animation taken from <a href="https://deepmind.com/blog/going-beyond-average-reinforcement-learning/" class="uri">https://deepmind.com/blog/going-beyond-average-reinforcement-learning/</a></figcaption><p></p>
</figure>
</div>
<iframe width="600" height="300" src="https://www.youtube.com/embed/yFBwyPuO2Vg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
<p><strong>Additional resources:</strong></p>
<ul>
<li><a href="https://deepmind.com/blog/going-beyond-average-reinforcement-learning" class="uri">https://deepmind.com/blog/going-beyond-average-reinforcement-learning</a></li>
<li><a href="https://physai.sciencesconf.org/data/pages/distributional_RL_Remi_Munos.pdf" class="uri">https://physai.sciencesconf.org/data/pages/distributional_RL_Remi_Munos.pdf</a></li>
<li><a href="https://flyyufelix.github.io/2017/10/24/distributional-bellman.html" class="uri">https://flyyufelix.github.io/2017/10/24/distributional-bellman.html</a>, with keras code for C51.</li>
</ul>
</section>
<section id="the-reactor" class="level3" data-number="10.0.2">
<h3 data-number="10.0.2" class="anchored" data-anchor-id="the-reactor"><span class="header-section-number">10.0.2</span> The Reactor</h3>
<p>The <strong>Reactor</strong> (Retrace Actor) of <span class="citation" data-cites="Gruslys2017">Gruslys et al. (<a href="references.html#ref-Gruslys2017" role="doc-biblioref">2017</a>)</span> combines many architectural and algorithmic contributions seen until now in order to provide an algorithm that is both sample efficient and with a good run-time performance. A3C has for example a better run-time performance (smaller wall-clock time for the training) than DQN or categorical DQN thanks to the use of multiple actor-learners in parallel, but its sample complexity is actually higher (as it is on-policy).</p>
<p>The Reactor combines and improves on:</p>
<ul>
<li>An actor-critic architecture using policy gradient with importance sampling,</li>
<li>Off-policy corrected returns computed by the Retrace algorithm,</li>
<li>Distributional learning of the Q-values in the critic,</li>
<li>Prioritized experience replay for sequences.</li>
</ul>
<p>One could consider REACTOR as the distributional version of ACER. We will not go into all the details here, but simply outline the main novelties.</p>
<p>The Reactor is composed of an actor <span class="math inline">\(\pi_\theta(s, a)\)</span> and a critic <span class="math inline">\(Q_\varphi(s, a)\)</span>. The actor is trained using policy gradient with importance sampling, as in Off-PAC. For a single state <span class="math inline">\(s\)</span> and an action <span class="math inline">\(\hat{a}\)</span> sampled by the behavior policy <span class="math inline">\(b\)</span>, the gradient of the objective is defined as:</p>
<p><span class="math display">\[
\begin{aligned}
    \nabla_\theta J(\theta) = \frac{\pi_\theta(s, \hat{a})}{b(s, \hat{a})} &amp; \, (R(s, \hat{a}) - Q_\varphi(s, \hat{a})) \, \nabla_\theta \log \pi_\theta(s, \hat{a}) \\
    &amp; + \sum_a Q_\varphi(s, a) \, \nabla_\theta \pi_\theta(s, a) \\
\end{aligned}
\]</span></p>
<p>The first term comes from Off-PAC and only concerns the chosen action <span class="math inline">\(\hat{a}\)</span> from the behavior policy. The actual return <span class="math inline">\(R(s, a)\)</span> is compared to its estimate <span class="math inline">\(Q_\varphi(s, \hat{a})\)</span> in order to reduce its variance. The second term <span class="math inline">\(\sum_a Q_\varphi(s, a) \, \nabla_\theta \pi_\theta(s, a)\)</span> depends on all available actions in <span class="math inline">\(s\)</span>. Its role is to reduce the bias of the first term, without adding any variance as it is only based on estimates. As the value of the state is defined by <span class="math inline">\(V^\pi(s) = \sum_a \pi(s, a) \, Q^\pi(s, a)\)</span>, maximizing this term maximizes the value of the state, i.e.&nbsp;the associated returns. This rule is called <strong>leave-one-out</strong> (LOO), as one action is left out from the sum and estimated from actual returns instead of other estimates.</p>
<p>For a better control on the variance, the behavior probability <span class="math inline">\(b(s, a)\)</span> is replaced by a parameter <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \beta \, (R(s, \hat{a}) - Q_\varphi(s, \hat{a})) \, \nabla_\theta \pi_\theta(s, \hat{a}) + \sum_a Q_\varphi(s, a) \, \nabla_\theta \pi_\theta(s, a)
\]</span></p>
<p><span class="math inline">\(\beta\)</span> is defined as <span class="math inline">\(\min (c, \frac{1}{b(s, \hat{a})})\)</span>, where <span class="math inline">\(c&gt;1\)</span> is a constant. This truncated term is similar to what was used in ACER. The rule is now called <strong><span class="math inline">\(\beta\)</span>-LOO</strong> and is a novel proposition of the Reactor.</p>
<p>The second importance contribution of the Reactor is how to combine the Retrace algorithm (<span class="citation" data-cites="Munos2016">Munos et al. (<a href="references.html#ref-Munos2016" role="doc-biblioref">2016</a>)</span>) for estimating the return <span class="math inline">\(R(s, \hat{a})\)</span> on multiple steps, with the distributional learning method of Categorical DQN. As Retrace uses n-steps returns iteratively, the n-step distributional Bellman target can updated using the <span class="math inline">\(n\)</span> future rewards:</p>
<p><span class="math display">\[
    z_i^n = \mathcal{T}^n \, z_i = \sum_{k=t}^{t+n} \gamma^{k-t} r_k + \gamma^n \, z_i
\]</span></p>
<p>We leave out the details on how Retrace is combined with these distributional Bellman updates: the notation is complicated but the idea is simple. The last importance contribution of the paper is the use of <strong>prioritized sequence replay</strong>. Prioritized experience replay allows to select in priority transitions from the replay buffer which are the most surprising, i.e.&nbsp;where the TD error is the highest. These transitions are the ones carrying the most information. A similar principle can be applied to sequences of transitions, which are needed by the n-step updates. They devised a specific sampling algorithm in order to achieve this and reduce the variance of the samples.</p>
<p>The last particularities of the Reactor is that it uses a LSTM layer to make the problem Markovian (instead of stacking four frames as in DQN) and train multiple actor-learners as in A3C. The algorithm is trained on CPU, with 10 or 20 actor-learners. The Reactor outperforms DQN and its variants, A3C and ACER on Atari games. Importantly, Reactor only needs one day of training on CPU, compared to the 8 days of GPU training needed by DQN.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-Bellemare2017" class="csl-entry" role="doc-biblioentry">
Bellemare, M. G., Dabney, W., and Munos, R. (2017). A <span>Distributional Perspective</span> on <span>Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1707.06887">http://arxiv.org/abs/1707.06887</a>.
</div>
<div id="ref-Dabney2017" class="csl-entry" role="doc-biblioentry">
Dabney, W., Rowland, M., Bellemare, M. G., and Munos, R. (2017). Distributional <span>Reinforcement Learning</span> with <span>Quantile Regression</span>. Available at: <a href="http://arxiv.org/abs/1710.10044">http://arxiv.org/abs/1710.10044</a> [Accessed June 28, 2019].
</div>
<div id="ref-Gruslys2017" class="csl-entry" role="doc-biblioentry">
Gruslys, A., Dabney, W., Azar, M. G., Piot, B., Bellemare, M., and Munos, R. (2017). The <span>Reactor</span>: <span>A</span> fast and sample-efficient <span>Actor-Critic</span> agent for <span>Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1704.04651">http://arxiv.org/abs/1704.04651</a>.
</div>
<div id="ref-Munos2016" class="csl-entry" role="doc-biblioentry">
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016). Safe and <span>Efficient Off-Policy Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1606.02647">http://arxiv.org/abs/1606.02647</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./3.6-EntropyRL.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Maximum Entropy RL</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./3.8-OtherPolicyGradient.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Miscellaneous model-free algorithm</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>