<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - 3&nbsp; Value-based methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./3.1-PolicyGradient.html" rel="next">
<link href="./1.2-DeepLearning.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Value-based methods</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Deep Reinforcement Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1.1-BasicRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Basics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1.2-DeepLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Deep learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-Valuebased.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Value-based methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.1-PolicyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Policy Gradient methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.2-ActorCritic.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advantage Actor-Critic methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.3-ImportanceSampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Off-policy Actor-Critic</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.4-DPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Deterministic Policy Gradient (DPG)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.5-NaturalGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Natural Gradients</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.6-EntropyRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Maximum Entropy RL</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.7-DistributionalRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Distributional learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.8-OtherPolicyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Miscellaneous model-free algorithm</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-RAM.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Recurrent Attention Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-ModelBased.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-Hierarchical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Hierarchical Reinforcement Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-Inverse.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Inverse Reinforcement Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-Robotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Deep RL for robotics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-Practice.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Deep RL in practice</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#limitations-of-deep-neural-networks-for-function-approximation" id="toc-limitations-of-deep-neural-networks-for-function-approximation" class="nav-link active" data-scroll-target="#limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span>  Limitations of deep neural networks for function approximation</a></li>
  <li><a href="#deep-q-network-dqn" id="toc-deep-q-network-dqn" class="nav-link" data-scroll-target="#deep-q-network-dqn"><span class="toc-section-number">3.2</span>  Deep Q-Network (DQN)</a></li>
  <li><a href="#double-dqn" id="toc-double-dqn" class="nav-link" data-scroll-target="#double-dqn"><span class="toc-section-number">3.3</span>  Double DQN</a></li>
  <li><a href="#prioritized-experience-replay" id="toc-prioritized-experience-replay" class="nav-link" data-scroll-target="#prioritized-experience-replay"><span class="toc-section-number">3.4</span>  Prioritized experience replay</a></li>
  <li><a href="#duelling-network" id="toc-duelling-network" class="nav-link" data-scroll-target="#duelling-network"><span class="toc-section-number">3.5</span>  Duelling network</a></li>
  <li><a href="#rainbow-dqn" id="toc-rainbow-dqn" class="nav-link" data-scroll-target="#rainbow-dqn"><span class="toc-section-number">3.6</span>  Rainbow DQN</a></li>
  <li><a href="#distributed-dqn-gorila" id="toc-distributed-dqn-gorila" class="nav-link" data-scroll-target="#distributed-dqn-gorila"><span class="toc-section-number">3.7</span>  Distributed DQN (GORILA)</a></li>
  <li><a href="#deep-recurrent-q-learning-drqn" id="toc-deep-recurrent-q-learning-drqn" class="nav-link" data-scroll-target="#deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.8</span>  Deep Recurrent Q-learning (DRQN)</a></li>
  <li><a href="#recurrent-replay-distributed-dqn-r2d2" id="toc-recurrent-replay-distributed-dqn-r2d2" class="nav-link" data-scroll-target="#recurrent-replay-distributed-dqn-r2d2"><span class="toc-section-number">3.9</span>  Recurrent Replay Distributed DQN (R2D2)</a></li>
  <li><a href="#other-variants-of-dqn" id="toc-other-variants-of-dqn" class="nav-link" data-scroll-target="#other-variants-of-dqn"><span class="toc-section-number">3.10</span>  Other variants of DQN</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Value-based methods</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="limitations-of-deep-neural-networks-for-function-approximation" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="limitations-of-deep-neural-networks-for-function-approximation"><span class="header-section-number">3.1</span> Limitations of deep neural networks for function approximation</h2>
<p>The goal of value-based deep RL is to approximate the Q-value of each possible state-action pair using a deep (convolutional) neural network. As shown on <a href="#fig-functionapprox2">Figure&nbsp;<span>3.1</span></a>, the network can either take a state-action pair as input and return a single output value, or take only the state as input and return the Q-value of all possible actions (only possible if the action space is discrete), In both cases, the goal is to learn estimates <span class="math inline">\(Q_\theta(s, a)\)</span> with a NN with parameters <span class="math inline">\(\theta\)</span>.</p>
<div id="fig-functionapprox2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/functionapprox.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.1: Function approximators can either associate a state-action pair <span class="math inline">\((s, a)\)</span> to its Q-value (left), or associate a state <span class="math inline">\(s\)</span> to the Q-values of all actions possible in that state (right).</figcaption><p></p>
</figure>
</div>
<p>When using Q-learning, we have already seen that the problem is a regression problem, where the following mse loss function has to be minimized:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \mathbb{E}_\pi[(r_t + \gamma \, \max_{a'} Q_\theta(s', a') - Q_\theta(s, a))^2]
\]</span></p>
<p>In short, we want to reduce the prediction error, i.e.&nbsp;the mismatch between the estimate of the value of an action <span class="math inline">\(Q_\theta(s, a)\)</span> and the real return, here approximated with <span class="math inline">\(r(s, a, s') + \gamma \, \text{max}_{a'} Q_\theta(s', a')\)</span>.</p>
<p>We can compute this loss by gathering enough samples <span class="math inline">\((s, a, r, s')\)</span> (i.e.&nbsp;single transitions), concatenating them randomly in minibatches, and let the DNN learn to minimize the prediction error using backpropagation and SGD, indirectly improving the policy. The following pseudocode would describe the training procedure when gathering transitions <strong>online</strong>, i.e.&nbsp;when directly interacting with the environment:</p>
<hr>
<ul>
<li>Initialize value network <span class="math inline">\(Q_{\theta}\)</span> with random weights.</li>
<li>Initialize empty minibatch <span class="math inline">\(\mathcal{D}\)</span> of maximal size <span class="math inline">\(n\)</span>.</li>
<li>Observe the initial state <span class="math inline">\(s_0\)</span>.</li>
<li>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:
<ul>
<li>Select the action <span class="math inline">\(a_t\)</span> based on the behavior policy derived from <span class="math inline">\(Q_\theta(s_t, a)\)</span> (e.g.&nbsp;softmax).</li>
<li>Perform the action <span class="math inline">\(a_t\)</span> and observe the next state <span class="math inline">\(s_{t+1}\)</span> and the reward <span class="math inline">\(r_{t+1}\)</span>.</li>
<li>Predict the Q-value of the greedy action in the next state <span class="math inline">\(\max_{a'} Q_\theta(s_{t+1}, a')\)</span></li>
<li>Store <span class="math inline">\((s_t, a_t, r_{t+1} + \gamma \, \max_{a'} Q_\theta(s_{t+1}, a'))\)</span> in the minibatch.</li>
<li>If minibatch <span class="math inline">\(\mathcal{D}\)</span> is full:
<ul>
<li>Train the value network <span class="math inline">\(Q_{\theta}\)</span> on <span class="math inline">\(\mathcal{D}\)</span> to minimize <span class="math inline">\(\mathcal{L}(\theta) = \mathbb{E}_\mathcal{D}[(r(s, a, s') + \gamma \, \text{max}_{a'} Q_\theta(s', a') - Q_\theta(s, a))^2]\)</span></li>
<li>Empty the minibatch <span class="math inline">\(\mathcal{D}\)</span>.</li>
</ul></li>
</ul></li>
</ul>
<hr>
<p>However, the definition of the loss function uses the mathematical expectation operator <span class="math inline">\(E\)</span> over all transitions, which can only be approximated by <strong>randomly</strong> sampling the distribution (the MDP). This implies that the samples concatenated in a minibatch should be independent from each other (i.i.d). When gathering transitions online, the samples are correlated: <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> will be followed by <span class="math inline">\((s_{t+1}, a_{t+1}, r_{t+2}, s_{t+2})\)</span>, etc. When playing video games, two successive frames will be very similar (a few pixels will change, or even none if the sampling rate is too high) and the optimal action will likely not change either (to catch the ball in pong, you will need to perform the same action - going left - many times in a row).</p>
<p><strong>Correlated inputs/outputs</strong> are very bad for deep neural networks: the DNN will overfit and fall into a very bad local minimum. That is why stochastic gradient descent works so well: it randomly samples values from the training set to form minibatches and minimize the loss function on these uncorrelated samples (hopefully). If all samples of a minibatch were of the same class (e.g.&nbsp;zeros in MNIST), the network would converge poorly. This is the first problem preventing an easy use of deep neural networks as function approximators in RL.</p>
<p>The second major problem is the <strong>non-stationarity</strong> of the targets in the loss function. In classification or regression, the desired values <span class="math inline">\(\mathbf{t}\)</span> are fixed throughout learning: the class of an object does not change in the middle of the training phase.</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = - \mathbb{E}_{\mathbf{x}, \mathbf{t} \in \mathcal{D}}[ ||\mathbf{t} - \mathbf{y}||^2]
\]</span></p>
<p>In Q-learning, the target <span class="math inline">\(r(s, a, s') + \gamma \, \max_{a'} Q_\theta(s', a')\)</span> will change during learning, as <span class="math inline">\(Q_\theta(s', a')\)</span> depends on the weights <span class="math inline">\(\theta\)</span> and will hopefully increase as the performance improves. This is the second problem of deep RL: deep NN are particularly bad on non-stationary problems, especially feedforward networks. They iteratively converge towards the desired value, but have troubles when the target also moves (like a dog chasing its tail).</p>
</section>
<section id="deep-q-network-dqn" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="deep-q-network-dqn"><span class="header-section-number">3.2</span> Deep Q-Network (DQN)</h2>
<p><span class="citation" data-cites="Mnih2015">Mnih et al. (<a href="references.html#ref-Mnih2015" role="doc-biblioref">2015</a>)</span> (originally arXived in <span class="citation" data-cites="Mnih2013">Mnih et al. (<a href="references.html#ref-Mnih2013" role="doc-biblioref">2013</a>)</span>) proposed an elegant solution to the problems of correlated inputs/outputs and non-stationarity inherent to RL. This article is a milestone of deep RL and it is fair to say that it started or at least strongly renewed the interest for deep RL.</p>
<p>The first idea proposed by <span class="citation" data-cites="Mnih2015">Mnih et al. (<a href="references.html#ref-Mnih2015" role="doc-biblioref">2015</a>)</span> solves the problem of correlated input/outputs and is actually quite simple: instead of feeding successive transitions into a minibatch and immediately training the NN on it, transitions are stored in a huge buffer called <strong>experience replay memory</strong> (ERM) or <strong>replay buffer</strong> able to store 100000 transitions. When the buffer is full, new transitions replace the old ones. SGD can now randomly sample the ERM to form minibatches and train the NN.</p>
<div id="fig-erm" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ERM.png" class="img-fluid figure-img" style="width:40.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.2: Experience replay memory. Interactions with the environment are stored in the ERM. Random minibatches are sampled from it to train the DQN value network.</figcaption><p></p>
</figure>
</div>
<p>The second idea solves the non-stationarity of the targets <span class="math inline">\(r(s, a, s') + \gamma \, \max_{a'} Q_\theta(s', a')\)</span>. Instead of computing it with the current parameters <span class="math inline">\(\theta\)</span> of the NN, they are computed with an old version of the NN called the <strong>target network</strong> with parameters <span class="math inline">\(\theta'\)</span>. The target network is updated only infrequently (every thousands of iterations or so) with the learned weights <span class="math inline">\(\theta\)</span>. As this target network does not change very often, the targets stay constant for a long period of time, and the problem becomes more stationary.</p>
<p>The resulting algorithm is called <strong>Deep Q-Network (DQN)</strong>. It is summarized by the following pseudocode:</p>
<hr>
<ul>
<li>Initialize value network <span class="math inline">\(Q_{\theta}\)</span> with random weights.</li>
<li>Copy <span class="math inline">\(Q_{\theta}\)</span> to create the target network <span class="math inline">\(Q_{\theta'}\)</span>.</li>
<li>Initialize experience replay memory <span class="math inline">\(\mathcal{D}\)</span> of maximal size <span class="math inline">\(N\)</span>.</li>
<li>Observe the initial state <span class="math inline">\(s_0\)</span>.</li>
<li>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:
<ul>
<li>Select the action <span class="math inline">\(a_t\)</span> based on the behavior policy derived from <span class="math inline">\(Q_\theta(s_t, a)\)</span> (e.g.&nbsp;softmax).</li>
<li>Perform the action <span class="math inline">\(a_t\)</span> and observe the next state <span class="math inline">\(s_{t+1}\)</span> and the reward <span class="math inline">\(r_{t+1}\)</span>.</li>
<li>Store <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> in the experience replay memory.</li>
<li>Every <span class="math inline">\(T_\text{train}\)</span> steps:
<ul>
<li>Sample a minibatch <span class="math inline">\(\mathcal{D}_s\)</span> randomly from <span class="math inline">\(\mathcal{D}\)</span>.</li>
<li>For each transition <span class="math inline">\((s, a, r, s')\)</span> in the minibatch:
<ul>
<li>Predict the Q-value of the greedy action in the next state <span class="math inline">\(\max_{a'} Q_{\theta'}(s', a')\)</span> using the target network.</li>
<li>Compute the target value <span class="math inline">\(y = r + \gamma \, \max_{a'} Q_{\theta'}(s', a')\)</span>.</li>
</ul></li>
<li>Train the value network <span class="math inline">\(Q_{\theta}\)</span> on <span class="math inline">\(\mathcal{D}_s\)</span> to minimize <span class="math inline">\(\mathcal{L}(\theta) = \mathbb{E}_{\mathcal{D}_s}[(y - Q_\theta(s, a))^2]\)</span></li>
</ul></li>
<li>Every <span class="math inline">\(T_\text{target}\)</span> steps:
<ul>
<li>Update the target network with the trained value network: <span class="math inline">\(\theta' \leftarrow \theta\)</span></li>
</ul></li>
</ul></li>
</ul>
<hr>
<p>In this document, pseudocode will omit many details to simplify the explanations (for example here, the case where a state is terminal - the game ends - and the next state has to be chosen from the distribution of possible starting states). Refer to the original publication for more exact algorithms.</p>
<p>The first thing to notice is that experienced transitions are not immediately used for learning, but simply stored in the ERM to be sampled later. Due to the huge size of the ERM, it is even likely that the recently experienced transition will only be used for learning hundreds or thousands of steps later. Meanwhile, very old transitions, generated using an initially bad policy, can be used to train the network for a very long time.</p>
<p>The second thing is that the target network is not updated very often (<span class="math inline">\(T_\text{target}=10000\)</span>), so the target values are going to be wrong a long time. More recent algorithms such as DDPG use a smoothed version of the current weights, as proposed in <span class="citation" data-cites="Lillicrap2015">Lillicrap et al. (<a href="references.html#ref-Lillicrap2015" role="doc-biblioref">2015</a>)</span>:</p>
<p><span class="math display">\[
    \theta' = \tau \, \theta + (1-\tau) \, \theta'
\]</span></p>
<p>If this rule is applied after each step with a very small rate <span class="math inline">\(\tau\)</span>, the target network will slowly track the learned network, but never be the same.</p>
<p>These two facts make DQN extremely slow to learn: millions of transitions are needed to obtain a satisfying policy. This is called the <strong>sample complexity</strong>, i.e.&nbsp;the number of transitions needed to obtain a satisfying performance. DQN finds very good policies, but at the cost of a very long training time.</p>
<p>DQN was initially applied to solve various Atari 2600 games. Video frames were used as observations and the set of possible discrete actions was limited (left/right/up/down, shoot, etc). The CNN used is depicted on <a href="#fig-dqn">Figure&nbsp;<span>3.3</span></a>. It has two convolutional layers, no max-pooling, 2 fully-connected layer and one output layer representing the Q-value of all possible actions in the games.</p>
<div id="fig-dqn" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/dqn.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.3: Architecture of the CNN used in the original DQN paper. Taken from <span class="citation" data-cites="Mnih2015">Mnih et al. (<a href="references.html#ref-Mnih2015" role="doc-biblioref">2015</a>)</span>.</figcaption><p></p>
</figure>
</div>
<p>The problem of partial observability is solved by concatenating the four last video frames into a single tensor used as input to the CNN. The convolutional layers become able through learning to extract the speed information from it. Some of the Atari games (Pinball, Breakout) were solved with a performance well above human level, especially when they are mostly reactive. Games necessitating more long-term planning (Montezuma’ Revenge) were still poorly learned, though.</p>
<p>Beside being able to learn using delayed and sparse rewards in highly dimensional input spaces, the true <em>tour de force</em> of DQN is that it was able to learn the 49 Atari games using the same architecture and hyperparameters, showing the generality of the approach.</p>
</section>
<section id="double-dqn" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="double-dqn"><span class="header-section-number">3.3</span> Double DQN</h2>
<p>In DQN, the experience replay memory and the target network were decisive in allowing the CNN to learn the tasks through RL. Their drawback is that they drastically slow down learning and increase the sample complexity. Additionally, DQN has stability issues: the same network may not converge the same way in different runs. One first improvement on DQN was proposed by <span class="citation" data-cites="vanHasselt2015">van Hasselt et al. (<a href="references.html#ref-vanHasselt2015" role="doc-biblioref">2015</a>)</span> and called <strong>double DQN</strong>.</p>
<p>The idea is that the target value <span class="math inline">\(y = r(s, a, s') + \gamma \, \max_{a'} Q_{\theta'}(s', a')\)</span> is frequently over-estimating the true return because of the max operator. Especially at the beginning of learning when Q-values are far from being correct, if an action is over-estimated (<span class="math inline">\(Q_{\theta'}(s', a)\)</span> is higher that its true value) and selected by the target network as the next greedy action, the learned Q-value <span class="math inline">\(Q_{\theta}(s, a)\)</span> will also become over-estimated, what will propagate to all previous actions on the long-term. <span class="citation" data-cites="vanHasselt2010">van Hasselt (<a href="references.html#ref-vanHasselt2010" role="doc-biblioref">2010</a>)</span> showed that this over-estimation is inevitable in regular Q-learning and proposed <strong>double learning</strong>.</p>
<p>The idea is to train independently two value networks: one will be used to find the greedy action (the action with the maximal Q-value), the other to estimate the Q-value itself. Even if the first network choose an over-estimated action as the greedy action, the other might provide a less over-estimated value for it, solving the problem.</p>
<p>Applying double learning to DQN is particularly straightforward: there are already two value networks, the trained network and the target network. Instead of using the target network to both select the greedy action in the next state and estimate its Q-value, here the trained network <span class="math inline">\(\theta\)</span> is used to select the greedy action <span class="math inline">\(a^* = \text{argmax}_{a'} Q_\theta (s', a')\)</span> while the target network only estimates its Q-value. The target value becomes:</p>
<p><span class="math display">\[
    y = r(s, a, s') + \gamma \, Q_{\theta'}(s', \text{argmax}_{a'} Q_\theta (s', a'))
\]</span></p>
<p>This induces only a small modification of the DQN algorithm and significantly improves its performance and stability:</p>
<hr>
<ul>
<li>Every <span class="math inline">\(T_\text{train}\)</span> steps:
<ul>
<li>Sample a minibatch <span class="math inline">\(\mathcal{D}_s\)</span> randomly from <span class="math inline">\(\mathcal{D}\)</span>.</li>
<li>For each transition <span class="math inline">\((s, a, r, s')\)</span> in the minibatch:
<ul>
<li>Select the greedy action in the next state <span class="math inline">\(a^* = \text{argmax}_{a'} Q_\theta (s', a')\)</span> using the trained network.</li>
<li>Predict its Q-value <span class="math inline">\(Q_{\theta'}(s', a^*)\)</span> using the target network.</li>
<li>Compute the target value <span class="math inline">\(y = r + \gamma \, Q_{\theta'}(s', a*)\)</span>.</li>
</ul></li>
</ul></li>
</ul>
<hr>
</section>
<section id="prioritized-experience-replay" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="prioritized-experience-replay"><span class="header-section-number">3.4</span> Prioritized experience replay</h2>
<p>Another drawback of the original DQN is that the experience replay memory is sampled uniformly. Novel and interesting transitions are selected with the same probability as old well-predicted transitions, what slows down learning. The main idea of <strong>prioritized experience replay</strong> <span class="citation" data-cites="Schaul2015">(<a href="references.html#ref-Schaul2015" role="doc-biblioref">Schaul et al., 2015</a>)</span> is to order the transitions in the experience replay memory in decreasing order of their TD error:</p>
<p><span class="math display">\[
    \delta = r(s, a, s') + \gamma \, Q_{\theta'}(s', \text{argmax}_{a'} Q_\theta (s', a')) - Q_\theta(s, a)
\]</span></p>
<p>and sample with a higher probability those surprising transitions to form a minibatch. However, non-surprising transitions might become relevant again after enough training, as the <span class="math inline">\(Q_\theta(s, a)\)</span> change, so prioritized replay has a softmax function over the TD error to ensure “exploration” of memorized transitions. This data structure has of course a non-negligible computational cost, but accelerates learning so much that it is worth it. See <a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/" class="uri">https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/</a> for a presentation of double DQN with prioritized replay.</p>
</section>
<section id="duelling-network" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="duelling-network"><span class="header-section-number">3.5</span> Duelling network</h2>
<p>The classical DQN architecture uses a single NN to predict directly the value of all possible actions <span class="math inline">\(Q_\theta(s, a)\)</span>. The value of an action depends on two factors:</p>
<ul>
<li>the value of the underlying state <span class="math inline">\(s\)</span>: in some states, all actions are bad, you lose whatever you do.</li>
<li>the interest of that action: some actions are better than others for a given state.</li>
</ul>
<p>This leads to the definition of the <strong>advantage</strong> <span class="math inline">\(A^\pi(s,a)\)</span> of an action:</p>
<p><span id="eq-advantagefunction"><span class="math display">\[
    A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)
\tag{3.1}\]</span></span></p>
<p>The advantage of the optimal action in <span class="math inline">\(s\)</span> is equal to zero: the expected return in <span class="math inline">\(s\)</span> is the same as the expected return when being in <span class="math inline">\(s\)</span> and taking <span class="math inline">\(a\)</span>, as the optimal policy will choose <span class="math inline">\(a\)</span> in <span class="math inline">\(s\)</span> anyway. The advantage of all other actions is negative: they bring less reward than the optimal action (by definition), so they are less advantageous. Note that this is only true if your estimate of <span class="math inline">\(V^\pi(s)\)</span> is correct.</p>
<p><span class="citation" data-cites="Baird1993">Baird (<a href="references.html#ref-Baird1993" role="doc-biblioref">1993</a>)</span> has shown that it is advantageous to decompose the Q-value of an action into the value of the state and the advantage of the action (<em>advantage updating</em>):</p>
<p><span class="math display">\[
    Q^\pi(s, a) = V^\pi(s) + A^\pi(s, a)
\]</span></p>
<p>If you already know that the value of a state is very low, you do not need to bother exploring and learning the value of all actions in that state, they will not bring much. Moreover, the advantage function has <strong>less variance</strong> than the Q-values, which is a very good property when using neural networks for function approximation. The variance of the Q-values comes from the fact that they are estimated based on other estimates, which themselves evolve during learning (non-stationarity of the targets) and can drastically change during exploration (stochastic policies). The advantages only track the <em>relative</em> change of the value of an action compared to its state, what is going to be much more stable over time.</p>
<p>The range of values taken by the advantages is also much smaller than the Q-values. Let’s suppose we have two states with values -10 and 10, and two actions with advantages 0 and -1 (it does not matter which one). The Q-values will vary between -11 (the worst action in the worst state) and 10 (the best action in the best state), while the advantage only varies between -1 and 0. It is therefore going to be much easier for a neural network to learn the advantages than the Q-values, as they are theoretically not bounded.</p>
<div id="fig-duelling" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/duelling.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.4: Duelling network architecture. Top: classical feedforward architecture to predict Q-values. Bottom: Duelling networks predicting state values and advantage functions to form the Q-values. Taken from <span class="citation" data-cites="Wang2016">Wang et al. (<a href="references.html#ref-Wang2016" role="doc-biblioref">2016</a>)</span>.</figcaption><p></p>
</figure>
</div>
<p><span class="citation" data-cites="Wang2016">Wang et al. (<a href="references.html#ref-Wang2016" role="doc-biblioref">2016</a>)</span> incorporated the idea of <em>advantage updating</em> in a double DQN architecture with prioritized replay (<a href="#fig-duelling">Figure&nbsp;<span>3.4</span></a>). As in DQN, the last layer represents the Q-values of the possible actions and has to minimize the mse loss:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \mathbb{E}_\pi([r(s, a, s') + \gamma \, Q_{\theta', \alpha', \beta'}(s', \text{argmax}_{a'} Q_{\theta, \alpha, \beta} (s', a')) - Q_{\theta, \alpha, \beta}(s, a)]^2)
\]</span></p>
<p>The difference is that the previous fully-connected layer is forced to represent the value of the input state <span class="math inline">\(V_{\theta, \beta}(s)\)</span> and the advantage of each action <span class="math inline">\(A_{\theta, \alpha}(s, a)\)</span> separately. There are two separate sets of weights in the network, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, to predict these two values, sharing representations from the early convolutional layers through weights <span class="math inline">\(\theta\)</span>. The output layer performs simply a parameter-less summation of both sub-networks:</p>
<p><span class="math display">\[
    Q_{\theta, \alpha, \beta}(s, a) = V_{\theta, \beta}(s) + A_{\theta, \alpha}(s, a)
\]</span></p>
<p>The issue with this formulation is that one could add a constant to <span class="math inline">\(V_{\theta, \beta}(s)\)</span> and substract it from <span class="math inline">\(A_{\theta, \alpha}(s, a)\)</span> while obtaining the same result. An easy way to constrain the summation is to normalize the advantages, so that the greedy action has an advantage of zero as expected:</p>
<p><span class="math display">\[
    Q_{\theta, \alpha, \beta}(s, a) = V_{\theta, \beta}(s) + (A_{\theta, \alpha}(s, a) - \max_a A_{\theta, \alpha}(s, a))
\]</span></p>
<p>By doing this, the advantages are still free, but the state value will have to take the correct value. <span class="citation" data-cites="Wang2016">Wang et al. (<a href="references.html#ref-Wang2016" role="doc-biblioref">2016</a>)</span> found that it is actually better to replace the <span class="math inline">\(\max\)</span> operator by the mean of the advantages. In this case, the advantages only need to change as fast as their mean, instead of having to compensate quickly for any change in the greedy action as the policy improves:</p>
<p><span class="math display">\[
    Q_{\theta, \alpha, \beta}(s, a) = V_{\theta, \beta}(s) + (A_{\theta, \alpha}(s, a) - \frac{1}{|\mathcal{A}|} \sum_a A_{\theta, \alpha}(s, a))
\]</span></p>
<p>Apart from this specific output layer, everything works as usual, especially the gradient of the mse loss function can travel backwards using backpropagation to update the weights <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. The resulting architecture outperforms double DQN with prioritized replay on most Atari games, particularly games with repetitive actions.</p>
</section>
<section id="rainbow-dqn" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="rainbow-dqn"><span class="header-section-number">3.6</span> Rainbow DQN</h2>
<p>As we have seen. the original formulation of DQN <span class="citation" data-cites="Mnih2015">(<a href="references.html#ref-Mnih2015" role="doc-biblioref">Mnih et al., 2015</a>)</span> has seen many improvements over the years.</p>
<ul>
<li><strong>Double DQN</strong> <span class="citation" data-cites="vanHasselt2015">(<a href="references.html#ref-vanHasselt2015" role="doc-biblioref">van Hasselt et al., 2015</a>)</span> separates the selection of the greedy action in the next state from its evaluation in order to prevent over-estimation of Q-values:</li>
</ul>
<p><span class="math display">\[\mathcal{L}(\theta) = \mathbb{E}_\mathcal{D} [(r + \gamma \, Q_{\theta'}(s´, \text{argmax}_{a'} Q_{\theta}(s', a')) - Q_\theta(s, a))^2]\]</span></p>
<ul>
<li><strong>Prioritized Experience Replay</strong> <span class="citation" data-cites="Schaul2015">(<a href="references.html#ref-Schaul2015" role="doc-biblioref">Schaul et al., 2015</a>)</span> selects transitions from the ERM proportionally to their current TD error:</li>
</ul>
<p><span class="math display">\[P(k) = \frac{(|\delta_k| + \epsilon)^\alpha}{\sum_k (|\delta_k| + \epsilon)^\alpha}\]</span></p>
<ul>
<li><strong>Dueling DQN</strong> <span class="citation" data-cites="Wang2016">(<a href="references.html#ref-Wang2016" role="doc-biblioref">Wang et al., 2016</a>)</span> splits learning of Q-values into learning of advantages and state values:</li>
</ul>
<p><span class="math display">\[Q_\theta(s, a) = V_\alpha(s) + A_\beta(s, a)\]</span></p>
<ul>
<li><strong>Categorical DQN</strong> <span class="citation" data-cites="Bellemare2017">(<a href="references.html#ref-Bellemare2017" role="doc-biblioref">Bellemare et al., 2017</a>)</span> learns the distribution of returns instead of their expectation:</li>
</ul>
<p><span class="math display">\[\mathcal{L}(\theta) = \mathbb{E}_{\mathcal{D}_s}[ - \mathbf{t}_k \, \log Z_\theta(s_k, a_k)]\]</span></p>
<ul>
<li><strong>n-step returns</strong> <span class="citation" data-cites="Sutton2017">(<a href="references.html#ref-Sutton2017" role="doc-biblioref">Sutton and Barto, 2017</a>)</span> reduce the bias of the estimation by taking the next <span class="math inline">\(n\)</span> rewards into account, at the cost of a slightly higher variance.</li>
</ul>
<p><span class="math display">\[\mathcal{L}(\theta) = \mathbb{E}_\mathcal{D} [(\sum_{k=1}^n r_{t+k} + \gamma \max_a Q_\theta(s_{t+n+1}, a) - Q_\theta(s_t, a_t))^2\]</span></p>
<ul>
<li><strong>Noisy DQN</strong> <span class="citation" data-cites="Fortunato2017">(<a href="references.html#ref-Fortunato2017" role="doc-biblioref">Fortunato et al., 2017</a>)</span> ensures exploration by adding noise to the parameters of the network instead of a softmax / <span class="math inline">\(\epsilon\)</span>-greedy action selection over the Q-values.</li>
</ul>
<p>All these improvements exceed the performance of vanilla DQN on most if not all Atari game. But which ones are the most important?</p>
<p><span class="citation" data-cites="Hessel2017">Hessel et al. (<a href="references.html#ref-Hessel2017" role="doc-biblioref">2017</a>)</span> designed a <strong>Rainbow DQN</strong> integrating all these improvements. Not only does the combined network outperform all the DQN variants, but each of its components is important for its performance as shown by ablation studies (apart from double learning and duelling networks), see <a href="#fig-rainbow">Figure&nbsp;<span>3.5</span></a>.</p>
<div id="fig-rainbow" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/rainbow.png" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.5: Performance of the Rainbow DQN compared to other DQN variants (left) and ablation studies. Figures taken from <span class="citation" data-cites="Hessel2017">Hessel et al. (<a href="references.html#ref-Hessel2017" role="doc-biblioref">2017</a>)</span>.</figcaption><p></p>
</figure>
</div>
</section>
<section id="distributed-dqn-gorila" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="distributed-dqn-gorila"><span class="header-section-number">3.7</span> Distributed DQN (GORILA)</h2>
<p>The main limitation of deep RL is the slowness of learning, which is mainly influenced by two factors:</p>
<ul>
<li>the <strong>sample complexity</strong>, i.e.&nbsp;the number of transitions needed to learn a satisfying policy.</li>
<li>the <strong>online interaction</strong> with the environment (states are visited one after the other).</li>
</ul>
<p>The second factor is particularly critical in real-world applications like robotics: physical robots evolve in real time, so the acquisition speed of transitions will be limited. Even in simulation (video games, robot emulators), the simulator might turn out to be much slower than training the underlying neural network. Google Deepmind proposed the GORILA (General Reinforcement Learning Architecture) framework to speed up the training of DQN networks using distributed actors and learners <span class="citation" data-cites="Nair2015">(<a href="references.html#ref-Nair2015" role="doc-biblioref">Nair et al., 2015</a>)</span>. The framework is quite general and the distribution granularity can change depending on the task.</p>
<div id="fig-gorila" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/gorila-global.png" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.6: GORILA architecture. Multiple actors interact with multiple copies of the environment and store their experiences in a (distributed) experience replay memory. Multiple DQN learners sample from the ERM and compute the gradient of the loss function w.r.t the parameters <span class="math inline">\(\theta\)</span>. A master network (parameter server, possibly distributed) gathers the gradients, apply weight updates and synchronizes regularly both the actors and the learners with new parameters. Taken from <span class="citation" data-cites="Nair2015">Nair et al. (<a href="references.html#ref-Nair2015" role="doc-biblioref">2015</a>)</span>.</figcaption><p></p>
</figure>
</div>
<p>In GORILA, multiple actors interact with the environment to gather transitions. Each actor has an independent copy of the environment, so they can gather <span class="math inline">\(N\)</span> times more samples per second if there are <span class="math inline">\(N\)</span> actors. This is possible in simulation (starting <span class="math inline">\(N\)</span> instances of the same game in parallel) but much more complicated for real-world systems (but see <span class="citation" data-cites="Gu2017">Gu et al. (<a href="references.html#ref-Gu2017" role="doc-biblioref">2017</a>)</span> for an example where multiple identical robots are used to gather experiences in parallel).</p>
<p>The experienced transitions are sent as in DQN to an experience replay memory, which may be distributed or centralized. Multiple DQN learners will then sample a minibatch from the ERM and compute the DQN loss on this minibatch (also using a target network). All learners start with the same parameters <span class="math inline">\(\theta\)</span> and simply compute the gradient of the loss function <span class="math inline">\(\frac{\partial \mathcal{L}(\theta)}{\partial \theta}\)</span> on the minibatch. The gradients are sent to a parameter server (a master network) which uses the gradients to apply the optimizer (e.g.&nbsp;SGD) and find new values for the parameters <span class="math inline">\(\theta\)</span>. Weight updates can also be applied in a distributed manner. This distributed method to train a network using multiple learners is now quite standard in deep learning: on multiple GPU systems, each GPU has a copy of the network and computes gradients on a different minibatch, while a master network integrates these gradients and updates the slaves.</p>
<p>The parameter server regularly updates the actors (to gather samples with the new policy) and the learners (to compute gradients w.r.t the new parameter values). Such a distributed system can greatly accelerate learning, but it can be quite tricky to find the optimum number of actors and learners (too many learners might degrade the stability) or their update rate (if the learners are not updated frequently enough, the gradients might not be correct).</p>
<p>Further variants of distributed DQN learning include Ape-X <span class="citation" data-cites="Horgan2018">(<a href="references.html#ref-Horgan2018" role="doc-biblioref">Horgan et al., 2018</a>)</span> and IMPALA <span class="citation" data-cites="Espeholt2018">(<a href="references.html#ref-Espeholt2018" role="doc-biblioref">Espeholt et al., 2018</a>)</span>. A similar idea is at the core of the A3C algorithm, which is a policy gradient method.</p>
</section>
<section id="deep-recurrent-q-learning-drqn" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="deep-recurrent-q-learning-drqn"><span class="header-section-number">3.8</span> Deep Recurrent Q-learning (DRQN)</h2>
<p>The Atari games used as a benchmark for value-based methods are <strong>partially observable MDPs</strong> (POMDP), i.e.&nbsp;a single frame does not contain enough information to predict what is going to happen next (e.g.&nbsp;the speed and direction of the ball on the screen is not known). In DQN, partial observability is solved by stacking four consecutive frames and using the resulting tensor as an input to the CNN. if this approach worked well for most Atari games, it has several limitations (as explained in <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc" class="uri">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc</a>):</p>
<ol type="1">
<li>It increases the size of the experience replay memory, as four video frames have to be stored for each transition.</li>
<li>It solves only short-term dependencies (instantaneous speeds). If the partial observability has long-term dependencies (an object has been hidden a long time ago but now becomes useful), the input to the neural network will not have that information. This is the main explanation why the original DQN performed so poorly on games necessitating long-term planning like Montezuma’s revenge.</li>
</ol>
<p>Building on previous ideas from the Schmidhuber’s group <span class="citation" data-cites="Bakker2001 Wierstra2007">(<a href="references.html#ref-Bakker2001" role="doc-biblioref">Bakker, 2001</a>; <a href="references.html#ref-Wierstra2007" role="doc-biblioref">Wierstra et al., 2007</a>)</span>, <span class="citation" data-cites="Hausknecht2015">Hausknecht and Stone (<a href="references.html#ref-Hausknecht2015" role="doc-biblioref">2015</a>)</span> replaced one of the fully-connected layers of the DQN network by a LSTM layer while using single frames as inputs. The resulting <strong>deep recurrent q-learning</strong> (DRQN) network became able to solve POMDPs thanks to the astonishing learning abilities of LSTMs: the LSTM layer learn to remember which part of the sensory information will be useful to take decisions later.</p>
<p>However, LSTMs are not a magical solution either. They are trained using <em>truncated BPTT</em>, i.e.&nbsp;on a limited history of states. Long-term dependencies exceeding the truncation horizon cannot be learned. Additionally, all states in that horizon (i.e.&nbsp;all frames) have to be stored in the ERM to train the network, increasing drastically its size. Despite these limitations, DRQN is a much more elegant solution to the partial observability problem, letting the network decide which horizon it needs to solve long-term dependencies.</p>
</section>
<section id="recurrent-replay-distributed-dqn-r2d2" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="recurrent-replay-distributed-dqn-r2d2"><span class="header-section-number">3.9</span> Recurrent Replay Distributed DQN (R2D2)</h2>
<p><span class="citation" data-cites="Kapturowski2019">Kapturowski et al. (<a href="references.html#ref-Kapturowski2019" role="doc-biblioref">2019</a>)</span></p>
</section>
<section id="other-variants-of-dqn" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="other-variants-of-dqn"><span class="header-section-number">3.10</span> Other variants of DQN</h2>
<p><strong>Average-DQN</strong> <span class="citation" data-cites="Anschel2016">(<a href="references.html#ref-Anschel2016" role="doc-biblioref">Anschel et al., 2016</a>)</span> proposes to increase the stability and performance of DQN by replacing the single target network (a copy of the trained network) by an average of the last parameter values, in other words an average of many past target networks.</p>
<p><span class="citation" data-cites="He2016">He et al. (<a href="references.html#ref-He2016" role="doc-biblioref">2016</a>)</span> proposed <strong>fast reward propagation</strong> through optimality tightening to speedup learning: when rewards are sparse, they require a lot of episodes to propagate these rare rewards to all actions leading to it. Their method combines immediate rewards (single steps) with actual returns (as in Monte-Carlo) via a constrained optimization approach.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-Anschel2016" class="csl-entry" role="doc-biblioentry">
Anschel, O., Baram, N., and Shimkin, N. (2016). Averaged-<span>DQN</span>: <span>Variance Reduction</span> and <span>Stabilization</span> for <span>Deep Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1611.01929">http://arxiv.org/abs/1611.01929</a>.
</div>
<div id="ref-Baird1993" class="csl-entry" role="doc-biblioentry">
Baird, L. C. (1993). Advantage updating. <span>Wright-Patterson Air Force Base</span> Available at: <a href="http://leemon.com/papers/1993b.pdf">http://leemon.com/papers/1993b.pdf</a>.
</div>
<div id="ref-Bakker2001" class="csl-entry" role="doc-biblioentry">
Bakker, B. (2001). Reinforcement <span>Learning</span> with <span>Long Short-Term Memory</span>. in <em>Advances in <span>Neural Information Processing Systems</span> 14 (<span>NIPS</span> 2001)</em>, 1475–1482. Available at: <a href="https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory">https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory</a>.
</div>
<div id="ref-Bellemare2017" class="csl-entry" role="doc-biblioentry">
Bellemare, M. G., Dabney, W., and Munos, R. (2017). A <span>Distributional Perspective</span> on <span>Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1707.06887">http://arxiv.org/abs/1707.06887</a>.
</div>
<div id="ref-Espeholt2018" class="csl-entry" role="doc-biblioentry">
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., et al. (2018). <span>IMPALA</span>: <span>Scalable Distributed Deep-RL</span> with <span>Importance Weighted Actor-Learner Architectures</span>. doi:<a href="https://doi.org/10.48550/arXiv.1802.01561">10.48550/arXiv.1802.01561</a>.
</div>
<div id="ref-Fortunato2017" class="csl-entry" role="doc-biblioentry">
Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., et al. (2017). Noisy <span>Networks</span> for <span>Exploration</span>. Available at: <a href="http://arxiv.org/abs/1706.10295">http://arxiv.org/abs/1706.10295</a> [Accessed March 2, 2020].
</div>
<div id="ref-Gu2017" class="csl-entry" role="doc-biblioentry">
Gu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep <span>Reinforcement Learning</span> for <span>Robotic Manipulation</span> with <span>Asynchronous Off-Policy Updates</span>. in <em>Proc. <span>ICRA</span></em> Available at: <a href="http://arxiv.org/abs/1610.00633">http://arxiv.org/abs/1610.00633</a>.
</div>
<div id="ref-Hausknecht2015" class="csl-entry" role="doc-biblioentry">
Hausknecht, M., and Stone, P. (2015). Deep <span>Recurrent Q-Learning</span> for <span>Partially Observable MDPs</span>. Available at: <a href="http://arxiv.org/abs/1507.06527">http://arxiv.org/abs/1507.06527</a>.
</div>
<div id="ref-He2016" class="csl-entry" role="doc-biblioentry">
He, F. S., Liu, Y., Schwing, A. G., and Peng, J. (2016). Learning to <span>Play</span> in a <span>Day</span>: <span>Faster Deep Reinforcement Learning</span> by <span>Optimality Tightening</span>. Available at: <a href="http://arxiv.org/abs/1611.01606">http://arxiv.org/abs/1611.01606</a>.
</div>
<div id="ref-Hessel2017" class="csl-entry" role="doc-biblioentry">
Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., et al. (2017). Rainbow: <span>Combining Improvements</span> in <span>Deep Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1710.02298">http://arxiv.org/abs/1710.02298</a>.
</div>
<div id="ref-Horgan2018" class="csl-entry" role="doc-biblioentry">
Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van Hasselt, H., et al. (2018). Distributed <span>Prioritized Experience Replay</span>. Available at: <a href="http://arxiv.org/abs/1803.00933">http://arxiv.org/abs/1803.00933</a> [Accessed December 14, 2019].
</div>
<div id="ref-Kapturowski2019" class="csl-entry" role="doc-biblioentry">
Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W. (2019). Recurrent experience replay in distributed reinforcement learning. in, 19. Available at: <a href="https://openreview.net/pdf?id=r1lyTjAqYX">https://openreview.net/pdf?id=r1lyTjAqYX</a>.
</div>
<div id="ref-Lillicrap2015" class="csl-entry" role="doc-biblioentry">
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., et al. (2015). Continuous control with deep reinforcement learning. <em>CoRR</em>. Available at: <a href="http://arxiv.org/abs/1509.02971">http://arxiv.org/abs/1509.02971</a>.
</div>
<div id="ref-Mnih2013" class="csl-entry" role="doc-biblioentry">
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., et al. (2013). Playing <span>Atari</span> with <span>Deep Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1312.5602">http://arxiv.org/abs/1312.5602</a>.
</div>
<div id="ref-Mnih2015" class="csl-entry" role="doc-biblioentry">
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). Human-level control through deep reinforcement learning. <em>Nature</em> 518, 529–533. doi:<a href="https://doi.org/10.1038/nature14236">10.1038/nature14236</a>.
</div>
<div id="ref-Nair2015" class="csl-entry" role="doc-biblioentry">
Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., et al. (2015). Massively <span>Parallel Methods</span> for <span>Deep Reinforcement Learning</span>. Available at: <a href="https://arxiv.org/pdf/1507.04296.pdf">https://arxiv.org/pdf/1507.04296.pdf</a>.
</div>
<div id="ref-Schaul2015" class="csl-entry" role="doc-biblioentry">
Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized <span>Experience Replay</span>. Available at: <a href="http://arxiv.org/abs/1511.05952">http://arxiv.org/abs/1511.05952</a>.
</div>
<div id="ref-Sutton2017" class="csl-entry" role="doc-biblioentry">
Sutton, R. S., and Barto, A. G. (2017). <em>Reinforcement <span>Learning</span>: <span>An Introduction</span></em>. 2nd ed. <span>Cambridge, MA</span>: <span>MIT Press</span> Available at: <a href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a>.
</div>
<div id="ref-vanHasselt2010" class="csl-entry" role="doc-biblioentry">
van Hasselt, H. (2010). Double <span class="nocase">Q-learning</span>. in <em>Proceedings of the 23rd <span>International Conference</span> on <span>Neural Information Processing Systems</span> - <span>Volume</span> 2</em> (<span>Curran Associates Inc.</span>), 2613–2621. Available at: <a href="https://dl.acm.org/citation.cfm?id=2997187">https://dl.acm.org/citation.cfm?id=2997187</a>.
</div>
<div id="ref-vanHasselt2015" class="csl-entry" role="doc-biblioentry">
van Hasselt, H., Guez, A., and Silver, D. (2015). Deep <span>Reinforcement Learning</span> with <span class="nocase">Double Q-learning</span>. Available at: <a href="http://arxiv.org/abs/1509.06461">http://arxiv.org/abs/1509.06461</a>.
</div>
<div id="ref-Wang2016" class="csl-entry" role="doc-biblioentry">
Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de Freitas, N. (2016). Dueling <span>Network Architectures</span> for <span>Deep Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1511.06581">http://arxiv.org/abs/1511.06581</a> [Accessed November 21, 2019].
</div>
<div id="ref-Wierstra2007" class="csl-entry" role="doc-biblioentry">
Wierstra, D., Foerster, A., Peters, J., and Schmidhuber, J. (2007). <span>“Solving <span>Deep Memory POMDPs</span> with <span>Recurrent Policy Gradients</span>,”</span> in (<span>Springer, Berlin, Heidelberg</span>), 697–706. doi:<a href="https://doi.org/10.1007/978-3-540-74690-4_71">10.1007/978-3-540-74690-4_71</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./1.2-DeepLearning.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Deep learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./3.1-PolicyGradient.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Policy Gradient methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>