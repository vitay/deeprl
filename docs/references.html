<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - References</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./9-Practice.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">References</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Deep Reinforcement Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1.1-BasicRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Basics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1.2-DeepLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Deep learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-Valuebased.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Value-based methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.1-PolicyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Policy Gradient methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.2-ActorCritic.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advantage Actor-Critic methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.3-ImportanceSampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Off-policy Actor-Critic</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.4-DPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Deterministic Policy Gradient (DPG)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.5-NaturalGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Natural Gradients</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.6-EntropyRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Maximum Entropy RL</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.7-DistributionalRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Distributional learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.8-OtherPolicyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Miscellaneous model-free algorithm</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-RAM.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Recurrent Attention Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-ModelBased.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-Hierarchical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Hierarchical Reinforcement Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-Inverse.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Inverse Reinforcement Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-Robotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Deep RL for robotics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-Practice.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Deep RL in practice</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link active">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">References</h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Agrawal2016" class="csl-entry" role="doc-biblioentry">
Agrawal, P., Nair, A., Abbeel, P., Malik, J., and Levine, S. (2016).
Learning to <span>Poke</span> by <span>Poking</span>: <span>Experiential
Learning</span> of <span>Intuitive Physics</span>. Available at: <a href="http://arxiv.org/abs/1606.07419">http://arxiv.org/abs/1606.07419</a>.
</div>
<div id="ref-Amari1998" class="csl-entry" role="doc-biblioentry">
Amari, S.-I. (1998). Natural gradient works efficiently in learning.
<em>Neural Computation</em> 10, 251–276.
</div>
<div id="ref-Amarjyoti2017" class="csl-entry" role="doc-biblioentry">
Amarjyoti, S. (2017). Deep <span>Reinforcement Learning</span> for
<span>Robotic Manipulation-The</span> state of the art. Available at: <a href="http://arxiv.org/abs/1701.08878">http://arxiv.org/abs/1701.08878</a>.
</div>
<div id="ref-Andrychowicz2017" class="csl-entry" role="doc-biblioentry">
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R.,
Welinder, P., et al. (2017). Hindsight <span>Experience Replay</span>.
Available at: <a href="http://arxiv.org/abs/1707.01495">http://arxiv.org/abs/1707.01495</a>.
</div>
<div id="ref-Anschel2016" class="csl-entry" role="doc-biblioentry">
Anschel, O., Baram, N., and Shimkin, N. (2016).
Averaged-<span>DQN</span>: <span>Variance Reduction</span> and
<span>Stabilization</span> for <span>Deep Reinforcement Learning</span>.
Available at: <a href="http://arxiv.org/abs/1611.01929">http://arxiv.org/abs/1611.01929</a>.
</div>
<div id="ref-Arjovsky2017" class="csl-entry" role="doc-biblioentry">
Arjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein
<span>GAN</span>. Available at: <a href="http://arxiv.org/abs/1701.07875">http://arxiv.org/abs/1701.07875</a>.
</div>
<div id="ref-Arulkumaran2017" class="csl-entry" role="doc-biblioentry">
Arulkumaran, K., Deisenroth, M. P., Brundage, M., and Bharath, A. A.
(2017). A <span>Brief Survey</span> of <span>Deep Reinforcement
Learning</span>. Available at: <a href="https://arxiv.org/pdf/1708.05866.pdf">https://arxiv.org/pdf/1708.05866.pdf</a>.
</div>
<div id="ref-Ba2014" class="csl-entry" role="doc-biblioentry">
Ba, J., Mnih, V., and Kavukcuoglu, K. (2014). Multiple <span>Object
Recognition</span> with <span>Visual Attention</span>. Available at: <a href="http://arxiv.org/abs/1412.7755">http://arxiv.org/abs/1412.7755</a>.
</div>
<div id="ref-Baird1993" class="csl-entry" role="doc-biblioentry">
Baird, L. C. (1993). Advantage updating. <span>Wright-Patterson Air
Force Base</span> Available at: <a href="http://leemon.com/papers/1993b.pdf">http://leemon.com/papers/1993b.pdf</a>.
</div>
<div id="ref-Bakker2001" class="csl-entry" role="doc-biblioentry">
Bakker, B. (2001). Reinforcement <span>Learning</span> with <span>Long
Short-Term Memory</span>. in <em>Advances in <span>Neural Information
Processing Systems</span> 14 (<span>NIPS</span> 2001)</em>, 1475–1482.
Available at: <a href="https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory">https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory</a>.
</div>
<div id="ref-Barth-Maron2018" class="csl-entry" role="doc-biblioentry">
Barth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., TB,
D., et al. (2018). Distributed <span>Distributional Deterministic Policy
Gradients</span>. Available at: <a href="http://arxiv.org/abs/1804.08617">http://arxiv.org/abs/1804.08617</a>.
</div>
<div id="ref-Bellemare2017" class="csl-entry" role="doc-biblioentry">
Bellemare, M. G., Dabney, W., and Munos, R. (2017). A
<span>Distributional Perspective</span> on <span>Reinforcement
Learning</span>. Available at: <a href="http://arxiv.org/abs/1707.06887">http://arxiv.org/abs/1707.06887</a>.
</div>
<div id="ref-Cho2014" class="csl-entry" role="doc-biblioentry">
Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,
Schwenk, H., et al. (2014). Learning <span>Phrase Representations</span>
using <span>RNN Encoder-Decoder</span> for <span>Statistical Machine
Translation</span>. Available at: <a href="http://arxiv.org/abs/1406.1078">http://arxiv.org/abs/1406.1078</a>.
</div>
<div id="ref-Chou2017" class="csl-entry" role="doc-biblioentry">
Chou, P.-W., Maturana, D., and Scherer, S. (2017). Improving
<span>Stochastic Policy Gradients</span> in <span>Continuous
Control</span> with <span>Deep Reinforcement Learning</span> using the
<span>Beta Distribution</span>. in <em>International
<span>Conference</span> on <span>Machine Learning</span></em> Available
at: <a href="http://proceedings.mlr.press/v70/chou17a/chou17a.pdf">http://proceedings.mlr.press/v70/chou17a/chou17a.pdf</a>.
</div>
<div id="ref-Clavera2018" class="csl-entry" role="doc-biblioentry">
Clavera, I., Nagabandi, A., Fearing, R. S., Abbeel, P., Levine, S., and
Finn, C. (2018). Learning to <span>Adapt</span>:
<span>Meta-Learning</span> for <span>Model-Based Control</span>.
Available at: <a href="http://arxiv.org/abs/1803.11347">http://arxiv.org/abs/1803.11347</a>.
</div>
<div id="ref-Co-Reyes2018" class="csl-entry" role="doc-biblioentry">
Co-Reyes, J. D., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., and
Levine, S. (2018). Self-<span>Consistent Trajectory Autoencoder</span>:
<span>Hierarchical Reinforcement Learning</span> with <span>Trajectory
Embeddings</span>.
</div>
<div id="ref-Corneil2018" class="csl-entry" role="doc-biblioentry">
Corneil, D., Gerstner, W., and Brea, J. (2018). Efficient
<span>Model-Based Deep Reinforcement Learning</span> with
<span>Variational State Tabulation</span>. Available at: <a href="http://arxiv.org/abs/1802.04325">http://arxiv.org/abs/1802.04325</a>.
</div>
<div id="ref-Dabney2017" class="csl-entry" role="doc-biblioentry">
Dabney, W., Rowland, M., Bellemare, M. G., and Munos, R. (2017).
Distributional <span>Reinforcement Learning</span> with <span>Quantile
Regression</span>. Available at: <a href="http://arxiv.org/abs/1710.10044">http://arxiv.org/abs/1710.10044</a>
[Accessed June 28, 2019].
</div>
<div id="ref-Degris2012" class="csl-entry" role="doc-biblioentry">
Degris, T., White, M., and Sutton, R. S. (2012). Linear <span>Off-Policy
Actor-Critic</span>. in <em>Proceedings of the 2012 <span>International
Conference</span> on <span>Machine Learning</span></em> Available at: <a href="http://arxiv.org/abs/1205.4839">http://arxiv.org/abs/1205.4839</a>.
</div>
<div id="ref-Ding2019" class="csl-entry" role="doc-biblioentry">
Ding, Y., Florensa, C., Phielipp, M., and Abbeel, P. (2019).
Goal-conditioned <span>Imitation Learning</span>. in (<span>Long Beach,
California</span>: <span>PMLR</span>), 8. Available at: <a href="https://openreview.net/pdf?id=HkglHcSj2N">https://openreview.net/pdf?id=HkglHcSj2N</a>.
</div>
<div id="ref-Dosovitskiy2016" class="csl-entry" role="doc-biblioentry">
Dosovitskiy, A., and Koltun, V. (2016). Learning to <span>Act</span> by
<span>Predicting</span> the <span>Future</span>. Available at: <a href="http://arxiv.org/abs/1611.01779">http://arxiv.org/abs/1611.01779</a>.
</div>
<div id="ref-Duan2016" class="csl-entry" role="doc-biblioentry">
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016).
Benchmarking <span>Deep Reinforcement Learning</span> for
<span>Continuous Control</span>. Available at: <a href="http://arxiv.org/abs/1604.06778">http://arxiv.org/abs/1604.06778</a>.
</div>
<div id="ref-Espeholt2018" class="csl-entry" role="doc-biblioentry">
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., et
al. (2018). <span>IMPALA</span>: <span>Scalable Distributed
Deep-RL</span> with <span>Importance Weighted Actor-Learner
Architectures</span>. doi:<a href="https://doi.org/10.48550/arXiv.1802.01561">10.48550/arXiv.1802.01561</a>.
</div>
<div id="ref-Feinberg2018" class="csl-entry" role="doc-biblioentry">
Feinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez, J. E., and
Levine, S. (2018). Model-<span>Based Value Estimation</span> for
<span>Efficient Model-Free Reinforcement Learning</span>. Available at:
<a href="http://arxiv.org/abs/1803.00101">http://arxiv.org/abs/1803.00101</a>.
</div>
<div id="ref-Fortunato2017" class="csl-entry" role="doc-biblioentry">
Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves,
A., et al. (2017). Noisy <span>Networks</span> for
<span>Exploration</span>. Available at: <a href="http://arxiv.org/abs/1706.10295">http://arxiv.org/abs/1706.10295</a>
[Accessed March 2, 2020].
</div>
<div id="ref-Gers2001" class="csl-entry" role="doc-biblioentry">
Gers, F. (2001). Long <span>Short-Term Memory</span> in <span>Recurrent
Neural Networks</span>. Available at: <a href="http://www.felixgers.de/papers/phd.pdf">http://www.felixgers.de/papers/phd.pdf</a>.
</div>
<div id="ref-Goodfellow2014" class="csl-entry" role="doc-biblioentry">
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,
D., Ozair, S., et al. (2014). Generative <span>Adversarial
Networks</span>. Available at: <a href="http://arxiv.org/abs/1406.2661">http://arxiv.org/abs/1406.2661</a>.
</div>
<div id="ref-Goodfellow2016" class="csl-entry" role="doc-biblioentry">
Goodfellow, I., Bengio, Y., and Courville, A. (2016). <em>Deep
<span>Learning</span></em>. <span>MIT Press</span> Available at: <a href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a>.
</div>
<div id="ref-Goyal2018" class="csl-entry" role="doc-biblioentry">
Goyal, A., Brakel, P., Fedus, W., Lillicrap, T., Levine, S., Larochelle,
H., et al. (2018). Recall <span>Traces</span>: <span>Backtracking
Models</span> for <span>Efficient Reinforcement Learning</span>.
Available at: <a href="http://arxiv.org/abs/1804.00379">http://arxiv.org/abs/1804.00379</a>.
</div>
<div id="ref-Gruslys2017" class="csl-entry" role="doc-biblioentry">
Gruslys, A., Dabney, W., Azar, M. G., Piot, B., Bellemare, M., and
Munos, R. (2017). The <span>Reactor</span>: <span>A</span> fast and
sample-efficient <span>Actor-Critic</span> agent for <span>Reinforcement
Learning</span>. Available at: <a href="http://arxiv.org/abs/1704.04651">http://arxiv.org/abs/1704.04651</a>.
</div>
<div id="ref-Gu2017" class="csl-entry" role="doc-biblioentry">
Gu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep
<span>Reinforcement Learning</span> for <span>Robotic
Manipulation</span> with <span>Asynchronous Off-Policy Updates</span>.
in <em>Proc. <span>ICRA</span></em> Available at: <a href="http://arxiv.org/abs/1610.00633">http://arxiv.org/abs/1610.00633</a>.
</div>
<div id="ref-Gu2016a" class="csl-entry" role="doc-biblioentry">
Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and Levine, S.
(2016a). Q-<span>Prop</span>: <span>Sample-Efficient Policy
Gradient</span> with <span>An Off-Policy Critic</span>. Available at: <a href="http://arxiv.org/abs/1611.02247">http://arxiv.org/abs/1611.02247</a>.
</div>
<div id="ref-Gu2016" class="csl-entry" role="doc-biblioentry">
Gu, S., Lillicrap, T., Sutskever, I., and Levine, S. (2016b). Continuous
<span>Deep Q-Learning</span> with <span class="nocase">Model-based
Acceleration</span>. Available at: <a href="http://arxiv.org/abs/1603.00748">http://arxiv.org/abs/1603.00748</a>.
</div>
<div id="ref-Ha2018" class="csl-entry" role="doc-biblioentry">
Ha, D., and Schmidhuber, J. (2018). World <span>Models</span>. doi:<a href="https://doi.org/10.5281/zenodo.1207631">10.5281/zenodo.1207631</a>.
</div>
<div id="ref-Haarnoja2018" class="csl-entry" role="doc-biblioentry">
Haarnoja, T., Hartikainen, K., Abbeel, P., and Levine, S. (2018a).
Latent <span>Space Policies</span> for <span>Hierarchical Reinforcement
Learning</span>. Available at: <a href="http://arxiv.org/abs/1804.02808">http://arxiv.org/abs/1804.02808</a>.
</div>
<div id="ref-Haarnoja2017" class="csl-entry" role="doc-biblioentry">
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement
<span>Learning</span> with <span>Deep Energy-Based Policies</span>.
Available at: <a href="http://arxiv.org/abs/1702.08165">http://arxiv.org/abs/1702.08165</a>
[Accessed February 13, 2019].
</div>
<div id="ref-Haarnoja2018a" class="csl-entry" role="doc-biblioentry">
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., et
al. (2018b). Soft <span>Actor-Critic Algorithms</span> and
<span>Applications</span>. Available at: <a href="http://arxiv.org/abs/1812.05905">http://arxiv.org/abs/1812.05905</a>
[Accessed February 5, 2019].
</div>
<div id="ref-Hafner2011" class="csl-entry" role="doc-biblioentry">
Hafner, R., and Riedmiller, M. (2011). Reinforcement learning in
feedback control. <em>Machine Learning</em> 84, 137–169. doi:<a href="https://doi.org/10.1007/s10994-011-5235-x">10.1007/s10994-011-5235-x</a>.
</div>
<div id="ref-Harutyunyan2016" class="csl-entry" role="doc-biblioentry">
Harutyunyan, A., Bellemare, M. G., Stepleton, T., and Munos, R. (2016).
Q(λ) with off-policy corrections. Available at: <a href="http://arxiv.org/abs/1602.04951">http://arxiv.org/abs/1602.04951</a>.
</div>
<div id="ref-Hausknecht2015" class="csl-entry" role="doc-biblioentry">
Hausknecht, M., and Stone, P. (2015). Deep <span>Recurrent
Q-Learning</span> for <span>Partially Observable MDPs</span>. Available
at: <a href="http://arxiv.org/abs/1507.06527">http://arxiv.org/abs/1507.06527</a>.
</div>
<div id="ref-He2016" class="csl-entry" role="doc-biblioentry">
He, F. S., Liu, Y., Schwing, A. G., and Peng, J. (2016). Learning to
<span>Play</span> in a <span>Day</span>: <span>Faster Deep Reinforcement
Learning</span> by <span>Optimality Tightening</span>. Available at: <a href="http://arxiv.org/abs/1611.01606">http://arxiv.org/abs/1611.01606</a>.
</div>
<div id="ref-He2015" class="csl-entry" role="doc-biblioentry">
He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep <span>Residual
Learning</span> for <span>Image Recognition</span>. Available at: <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.
</div>
<div id="ref-Heess2015" class="csl-entry" role="doc-biblioentry">
Heess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T.
(2015). Learning continuous control policies by stochastic value
gradients. <em>Proc. International Conference on Neural Information
Processing Systems</em>, 2944–2952. Available at: <a href="http://dl.acm.org/citation.cfm?id=2969569">http://dl.acm.org/citation.cfm?id=2969569</a>.
</div>
<div id="ref-Heinrich2015" class="csl-entry" role="doc-biblioentry">
Heinrich, J., Lanctot, M., and Silver, D. (2015). Fictitious
<span>Self-Play</span> in <span>Extensive-Form Games</span>. 805–813.
Available at: <a href="http://proceedings.mlr.press/v37/heinrich15.html">http://proceedings.mlr.press/v37/heinrich15.html</a>.
</div>
<div id="ref-Heinrich2016" class="csl-entry" role="doc-biblioentry">
Heinrich, J., and Silver, D. (2016). Deep <span>Reinforcement
Learning</span> from <span>Self-Play</span> in
<span>Imperfect-Information Games</span>. Available at: <a href="http://arxiv.org/abs/1603.01121">http://arxiv.org/abs/1603.01121</a>.
</div>
<div id="ref-Henaff2017" class="csl-entry" role="doc-biblioentry">
Henaff, M., Whitney, W. F., and LeCun, Y. (2017). Model-<span>Based
Planning</span> with <span>Discrete</span> and <span>Continuous
Actions</span>. Available at: <a href="http://arxiv.org/abs/1705.07177">http://arxiv.org/abs/1705.07177</a>.
</div>
<div id="ref-Hessel2017" class="csl-entry" role="doc-biblioentry">
Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G.,
Dabney, W., et al. (2017). Rainbow: <span>Combining Improvements</span>
in <span>Deep Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1710.02298">http://arxiv.org/abs/1710.02298</a>.
</div>
<div id="ref-Hochreiter1991" class="csl-entry" role="doc-biblioentry">
Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen
<span>Netzen</span>. Available at: <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf</a>.
</div>
<div id="ref-Hochreiter1997" class="csl-entry" role="doc-biblioentry">
Hochreiter, S., and Schmidhuber, J. (1997). Long short-term memory.
<em>Neural computation</em> 9, 1735–80. Available at: <a href="https://www.ncbi.nlm.nih.gov/pubmed/9377276">https://www.ncbi.nlm.nih.gov/pubmed/9377276</a>.
</div>
<div id="ref-Horgan2018" class="csl-entry" role="doc-biblioentry">
Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van
Hasselt, H., et al. (2018). Distributed <span>Prioritized Experience
Replay</span>. Available at: <a href="http://arxiv.org/abs/1803.00933">http://arxiv.org/abs/1803.00933</a>
[Accessed December 14, 2019].
</div>
<div id="ref-Ioffe2015" class="csl-entry" role="doc-biblioentry">
Ioffe, S., and Szegedy, C. (2015). Batch <span>Normalization</span>:
<span>Accelerating Deep Network Training</span> by <span>Reducing
Internal Covariate Shift</span>. Available at: <a href="http://arxiv.org/abs/1502.03167">http://arxiv.org/abs/1502.03167</a>.
</div>
<div id="ref-Kakade2001" class="csl-entry" role="doc-biblioentry">
Kakade, S. (2001). A <span>Natural Policy Gradient</span>. in
<em>Advances in <span>Neural Information Processing Systems</span>
14</em> Available at: <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a>.
</div>
<div id="ref-Kakade2002" class="csl-entry" role="doc-biblioentry">
Kakade, S., and Langford, J. (2002). Approximately <span>Optimal
Approximate Reinforcement Learning</span>. <em>Proc. 19th International
Conference on Machine Learning</em>, 267–274. Available at: <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601</a>.
</div>
<div id="ref-Kansky2017" class="csl-entry" role="doc-biblioentry">
Kansky, K., Silver, T., Mély, D. A., Eldawy, M., Lázaro-Gredilla, M.,
Lou, X., et al. (2017). Schema <span>Networks</span>: <span class="nocase">Zero-shot Transfer</span> with a <span>Generative Causal
Model</span> of <span>Intuitive Physics</span>. Available at: <a href="http://arxiv.org/abs/1706.04317">http://arxiv.org/abs/1706.04317</a>
[Accessed January 10, 2019].
</div>
<div id="ref-Kapturowski2019" class="csl-entry" role="doc-biblioentry">
Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W.
(2019). Recurrent experience replay in distributed reinforcement
learning. in, 19. Available at: <a href="https://openreview.net/pdf?id=r1lyTjAqYX">https://openreview.net/pdf?id=r1lyTjAqYX</a>.
</div>
<div id="ref-Kingma2013" class="csl-entry" role="doc-biblioentry">
Kingma, D. P., and Welling, M. (2013). Auto-<span>Encoding Variational
Bayes</span>. Available at: <a href="http://arxiv.org/abs/1312.6114">http://arxiv.org/abs/1312.6114</a>.
</div>
<div id="ref-Knight2018" class="csl-entry" role="doc-biblioentry">
Knight, E., and Lerner, O. (2018). Natural <span class="nocase">Gradient
Deep Q-learning</span>. Available at: <a href="http://arxiv.org/abs/1803.07482">http://arxiv.org/abs/1803.07482</a>.
</div>
<div id="ref-Krizhevsky2012" class="csl-entry" role="doc-biblioentry">
Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). <span>ImageNet
Classification</span> with <span>Deep Convolutional Neural
Networks</span>. in <em>Advances in <span>Neural Information Processing
Systems</span> (<span>NIPS</span>)</em> Available at: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>.
</div>
<div id="ref-Levine2016" class="csl-entry" role="doc-biblioentry">
Levine, S., Finn, C., Darrell, T., and Abbeel, P. (2016a).
End-to-<span>End Training</span> of <span>Deep Visuomotor
Policies</span>. <em>JMLR</em> 17. Available at: <a href="http://arxiv.org/abs/1504.00702">http://arxiv.org/abs/1504.00702</a>.
</div>
<div id="ref-Levine2013" class="csl-entry" role="doc-biblioentry">
Levine, S., and Koltun, V. (2013). Guided <span>Policy Search</span>. in
<em>Proceedings of <span>Machine Learning Research</span></em>, 1–9.
Available at: <a href="http://proceedings.mlr.press/v28/levine13.html">http://proceedings.mlr.press/v28/levine13.html</a>.
</div>
<div id="ref-Levine2016a" class="csl-entry" role="doc-biblioentry">
Levine, S., Pastor, P., Krizhevsky, A., and Quillen, D. (2016b).
Learning <span>Hand-Eye Coordination</span> for <span>Robotic
Grasping</span> with <span>Deep Learning</span> and <span>Large-Scale
Data Collection</span>. in <em>Proc. <span>ISER</span></em> Available
at: <a href="http://arxiv.org/abs/1603.02199">http://arxiv.org/abs/1603.02199</a>.
</div>
<div id="ref-Li2017" class="csl-entry" role="doc-biblioentry">
Li, Y. (2017). Deep <span>Reinforcement Learning</span>: <span>An
Overview</span>. Available at: <a href="http://arxiv.org/abs/1701.07274">http://arxiv.org/abs/1701.07274</a>.
</div>
<div id="ref-Lillicrap2015" class="csl-entry" role="doc-biblioentry">
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa,
Y., et al. (2015). Continuous control with deep reinforcement learning.
<em>CoRR</em>. Available at: <a href="http://arxiv.org/abs/1509.02971">http://arxiv.org/abs/1509.02971</a>.
</div>
<div id="ref-Lotzsch2017" class="csl-entry" role="doc-biblioentry">
Lötzsch, W., Vitay, J., and Hamker, F. H. (2017). Training a deep policy
gradient-based neural network with asynchronous learners on a simulated
robotic problem. in <em><span>INFORMATIK</span> 2017.
<span>Gesellschaft</span> für <span>Informatik</span></em>, eds. M. Eibl
and M. Gaedke (<span>Gesellschaft für Informatik, Bonn</span>),
2143–2154. Available at: <a href="https://dl.gi.de/handle/20.500.12116/3986">https://dl.gi.de/handle/20.500.12116/3986</a>.
</div>
<div id="ref-Machado2018" class="csl-entry" role="doc-biblioentry">
Machado, M. C., Bellemare, M. G., and Bowling, M. (2018).
Count-<span>Based Exploration</span> with the <span>Successor
Representation</span>. Available at: <a href="http://arxiv.org/abs/1807.11622">http://arxiv.org/abs/1807.11622</a>
[Accessed February 23, 2019].
</div>
<div id="ref-Meuleau2000" class="csl-entry" role="doc-biblioentry">
Meuleau, N., Peshkin, L., Kaelbling, L. P., and Kim, K. (2000).
Off-<span>Policy Policy Search</span>. Available at: <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894</a>.
</div>
<div id="ref-Mirowski2016" class="csl-entry" role="doc-biblioentry">
Mirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A. J., Banino,
A., et al. (2016). Learning to <span>Navigate</span> in <span>Complex
Environments</span>. Available at: <a href="http://arxiv.org/abs/1611.03673">http://arxiv.org/abs/1611.03673</a>.
</div>
<div id="ref-Mnih2016" class="csl-entry" role="doc-biblioentry">
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley,
T., et al. (2016). Asynchronous <span>Methods</span> for <span>Deep
Reinforcement Learning</span>. in <em>Proc. <span>ICML</span></em>
Available at: <a href="http://arxiv.org/abs/1602.01783">http://arxiv.org/abs/1602.01783</a>.
</div>
<div id="ref-Mnih2014" class="csl-entry" role="doc-biblioentry">
Mnih, V., Heess, N., Graves, A., and Kavukcuoglu, K. (2014). Recurrent
<span>Models</span> of <span>Visual Attention</span>. Available at: <a href="http://arxiv.org/abs/1406.6247">http://arxiv.org/abs/1406.6247</a>.
</div>
<div id="ref-Mnih2013" class="csl-entry" role="doc-biblioentry">
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I.,
Wierstra, D., et al. (2013). Playing <span>Atari</span> with <span>Deep
Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1312.5602">http://arxiv.org/abs/1312.5602</a>.
</div>
<div id="ref-Mnih2015" class="csl-entry" role="doc-biblioentry">
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J.,
Bellemare, M. G., et al. (2015). Human-level control through deep
reinforcement learning. <em>Nature</em> 518, 529–533. doi:<a href="https://doi.org/10.1038/nature14236">10.1038/nature14236</a>.
</div>
<div id="ref-Mousavi2018" class="csl-entry" role="doc-biblioentry">
Mousavi, S. S., Schukat, M., and Howley, E. (2018). <span>“Deep
<span>Reinforcement Learning</span>: <span>An Overview</span>,”</span>
in (<span>Springer, Cham</span>), 426–440. doi:<a href="https://doi.org/10.1007/978-3-319-56991-8_32">10.1007/978-3-319-56991-8_32</a>.
</div>
<div id="ref-Munos2016" class="csl-entry" role="doc-biblioentry">
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016).
Safe and <span>Efficient Off-Policy Reinforcement Learning</span>.
Available at: <a href="http://arxiv.org/abs/1606.02647">http://arxiv.org/abs/1606.02647</a>.
</div>
<div id="ref-Nachum2018" class="csl-entry" role="doc-biblioentry">
Nachum, O., Gu, S., Lee, H., and Levine, S. (2018). Data-<span>Efficient
Hierarchical Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1805.08296">http://arxiv.org/abs/1805.08296</a>.
</div>
<div id="ref-Nachum2017" class="csl-entry" role="doc-biblioentry">
Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017). Bridging the
<span>Gap Between Value</span> and <span>Policy Based Reinforcement
Learning</span>. Available at: <a href="http://arxiv.org/abs/1702.08892">http://arxiv.org/abs/1702.08892</a>
[Accessed June 12, 2019].
</div>
<div id="ref-Nair2015" class="csl-entry" role="doc-biblioentry">
Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De
Maria, A., et al. (2015). Massively <span>Parallel Methods</span> for
<span>Deep Reinforcement Learning</span>. Available at: <a href="https://arxiv.org/pdf/1507.04296.pdf">https://arxiv.org/pdf/1507.04296.pdf</a>.
</div>
<div id="ref-Nielsen2015" class="csl-entry" role="doc-biblioentry">
Nielsen, M. A. (2015). <em>Neural <span>Networks</span> and <span>Deep
Learning</span></em>. <span>Determination Press</span> Available at: <a href="http://neuralnetworksanddeeplearning.com/">http://neuralnetworksanddeeplearning.com/</a>.
</div>
<div id="ref-Niu2011" class="csl-entry" role="doc-biblioentry">
Niu, F., Recht, B., Re, C., and Wright, S. J. (2011).
<span>HOGWILD</span>!: <span>A Lock-Free Approach</span> to
<span>Parallelizing Stochastic Gradient Descent</span>. in <em>Proc.
<span>Advances</span> in <span>Neural Information Processing
Systems</span></em>, 21–21. Available at: <a href="http://arxiv.org/abs/1106.5730">http://arxiv.org/abs/1106.5730</a>.
</div>
<div id="ref-ODonoghue2016" class="csl-entry" role="doc-biblioentry">
O’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2016).
Combining policy gradient and <span class="nocase">Q-learning</span>.
Available at: <a href="http://arxiv.org/abs/1611.01626">http://arxiv.org/abs/1611.01626</a>
[Accessed February 13, 2019].
</div>
<div id="ref-Oh2018" class="csl-entry" role="doc-biblioentry">
Oh, J., Guo, Y., Singh, S., and Lee, H. (2018). Self-<span>Imitation
Learning</span>. Available at: <a href="http://arxiv.org/abs/1806.05635">http://arxiv.org/abs/1806.05635</a>.
</div>
<div id="ref-Pardo2018" class="csl-entry" role="doc-biblioentry">
Pardo, F., Levdik, V., and Kormushev, P. (2018). Q-map: A
<span>Convolutional Approach</span> for <span>Goal-Oriented
Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1810.02927">http://arxiv.org/abs/1810.02927</a>.
</div>
<div id="ref-Peng2018" class="csl-entry" role="doc-biblioentry">
Peng, B., Li, X., Gao, J., Liu, J., Wong, K.-F., and Su, S.-Y. (2018).
Deep <span>Dyna-Q</span>: <span>Integrating Planning</span> for
<span>Task-Completion Dialogue Policy Learning</span>. Available at: <a href="http://arxiv.org/abs/1801.06176">http://arxiv.org/abs/1801.06176</a>.
</div>
<div id="ref-Peshkin2002" class="csl-entry" role="doc-biblioentry">
Peshkin, L., and Shelton, C. R. (2002). Learning from <span>Scarce
Experience</span>. Available at: <a href="http://arxiv.org/abs/cs/0204043">http://arxiv.org/abs/cs/0204043</a>.
</div>
<div id="ref-Peters2008" class="csl-entry" role="doc-biblioentry">
Peters, J., and Schaal, S. (2008). Reinforcement learning of motor
skills with policy gradients. <em>Neural Networks</em> 21, 682–697.
doi:<a href="https://doi.org/10.1016/j.neunet.2008.02.003">10.1016/j.neunet.2008.02.003</a>.
</div>
<div id="ref-Pong2018" class="csl-entry" role="doc-biblioentry">
Pong, V., Gu, S., Dalal, M., and Levine, S. (2018). Temporal
<span>Difference Models</span>: <span>Model-Free Deep RL</span> for
<span>Model-Based Control</span>. Available at: <a href="http://arxiv.org/abs/1802.09081">http://arxiv.org/abs/1802.09081</a>.
</div>
<div id="ref-Popov2017" class="csl-entry" role="doc-biblioentry">
Popov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G.,
Vecerik, M., et al. (2017). Data-efficient <span>Deep Reinforcement
Learning</span> for <span>Dexterous Manipulation</span>. Available at:
<a href="http://arxiv.org/abs/1704.03073">http://arxiv.org/abs/1704.03073</a>.
</div>
<div id="ref-Precup2000" class="csl-entry" role="doc-biblioentry">
Precup, D., Sutton, R. S., and Singh, S. (2000). Eligibility traces for
off-policy policy evaluation. in <em>Proceedings of the
<span>Seventeenth International Conference</span> on <span>Machine
Learning</span>.</em>
</div>
<div id="ref-Ruder2016" class="csl-entry" role="doc-biblioentry">
Ruder, S. (2016). An overview of gradient descent optimization
algorithms. Available at: <a href="http://arxiv.org/abs/1609.04747">http://arxiv.org/abs/1609.04747</a>.
</div>
<div id="ref-Salimans2017" class="csl-entry" role="doc-biblioentry">
Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017).
Evolution <span>Strategies</span> as a <span>Scalable Alternative</span>
to <span>Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1703.03864">http://arxiv.org/abs/1703.03864</a>.
</div>
<div id="ref-Schaul2015" class="csl-entry" role="doc-biblioentry">
Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized
<span>Experience Replay</span>. Available at: <a href="http://arxiv.org/abs/1511.05952">http://arxiv.org/abs/1511.05952</a>.
</div>
<div id="ref-Schoettler2019" class="csl-entry" role="doc-biblioentry">
Schoettler, G., Nair, A., Luo, J., Bahl, S., Ojea, J. A., Solowjow, E.,
et al. (2019). Deep <span>Reinforcement Learning</span> for
<span>Industrial Insertion Tasks</span> with <span>Visual Inputs</span>
and <span>Natural Rewards</span>. Available at: <a href="http://arxiv.org/abs/1906.05841">http://arxiv.org/abs/1906.05841</a>
[Accessed June 18, 2019].
</div>
<div id="ref-Schulman2017" class="csl-entry" role="doc-biblioentry">
Schulman, J., Chen, X., and Abbeel, P. (2017a). Equivalence
<span>Between Policy Gradients</span> and <span>Soft Q-Learning</span>.
Available at: <a href="http://arxiv.org/abs/1704.06440">http://arxiv.org/abs/1704.06440</a>
[Accessed June 12, 2019].
</div>
<div id="ref-Schulman2015a" class="csl-entry" role="doc-biblioentry">
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
(2015a). Trust <span>Region Policy Optimization</span>. in
<em>Proceedings of the 31 st <span>International Conference</span> on
<span>Machine Learning</span></em>, 1889–1897. Available at: <a href="http://proceedings.mlr.press/v37/schulman15.html">http://proceedings.mlr.press/v37/schulman15.html</a>.
</div>
<div id="ref-Schulman2015" class="csl-entry" role="doc-biblioentry">
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
(2015b). High-<span>Dimensional Continuous Control Using Generalized
Advantage Estimation</span>. Available at: <a href="http://arxiv.org/abs/1506.02438">http://arxiv.org/abs/1506.02438</a>.
</div>
<div id="ref-Schulman2017a" class="csl-entry" role="doc-biblioentry">
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
(2017b). Proximal <span>Policy Optimization Algorithms</span>. Available
at: <a href="http://arxiv.org/abs/1707.06347">http://arxiv.org/abs/1707.06347</a>.
</div>
<div id="ref-Silver2016" class="csl-entry" role="doc-biblioentry">
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den
Driessche, G., et al. (2016a). Mastering the game of <span>Go</span>
with deep neural networks and tree search. <em>Nature</em> 529, 484–489.
doi:<a href="https://doi.org/10.1038/nature16961">10.1038/nature16961</a>.
</div>
<div id="ref-Silver2014" class="csl-entry" role="doc-biblioentry">
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and
Riedmiller, M. (2014). Deterministic <span>Policy Gradient
Algorithms</span>. in <em>Proc. <span>ICML</span></em> Proceedings of
<span>Machine Learning Research</span>., eds. E. P. Xing and T. Jebara
(<span>PMLR</span>), 387–395. Available at: <a href="http://proceedings.mlr.press/v32/silver14.html">http://proceedings.mlr.press/v32/silver14.html</a>.
</div>
<div id="ref-Silver2016a" class="csl-entry" role="doc-biblioentry">
Silver, D., van Hasselt, H., Hessel, M., Schaul, T., Guez, A., Harley,
T., et al. (2016b). The <span>Predictron</span>: <span>End-To-End
Learning</span> and <span>Planning</span>. Available at: <a href="http://arxiv.org/abs/1612.08810">http://arxiv.org/abs/1612.08810</a>.
</div>
<div id="ref-Simonyan2015" class="csl-entry" role="doc-biblioentry">
Simonyan, K., and Zisserman, A. (2015). Very <span>Deep Convolutional
Networks</span> for <span>Large-Scale Image Recognition</span>.
<em>International Conference on Learning Representations (ICRL)</em>,
1–14. doi:<a href="https://doi.org/10.1016/j.infsof.2008.09.005">10.1016/j.infsof.2008.09.005</a>.
</div>
<div id="ref-Srinivas2018" class="csl-entry" role="doc-biblioentry">
Srinivas, A., Jabri, A., Abbeel, P., Levine, S., and Finn, C. (2018).
Universal <span>Planning Networks</span>. Available at: <a href="http://arxiv.org/abs/1804.00645">http://arxiv.org/abs/1804.00645</a>.
</div>
<div id="ref-Stollenga2014" class="csl-entry" role="doc-biblioentry">
Stollenga, M., Masci, J., Gomez, F., and Schmidhuber, J. (2014). Deep
<span>Networks</span> with <span>Internal Selective Attention</span>
through <span>Feedback Connections</span>. Available at: <a href="http://arxiv.org/abs/1407.3068">http://arxiv.org/abs/1407.3068</a>.
</div>
<div id="ref-Sutton1990a" class="csl-entry" role="doc-biblioentry">
Sutton, R. S., and Barto, A. G. (1990). <span>“Time-derivative models of
<span>Pavlovian</span> reinforcement,”</span> in <em>Learning and
<span>Computational Neuroscience</span>: <span>Foundations</span> of
<span>Adaptive Networks</span></em> (<span>MIT Press</span>), 497–537.
Available at: <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.81.98">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.81.98</a>.
</div>
<div id="ref-Sutton1998" class="csl-entry" role="doc-biblioentry">
Sutton, R. S., and Barto, A. G. (1998). <em>Reinforcement
<span>Learning</span>: <span>An</span> introduction</em>.
<span>Cambridge, MA</span>: <span>MIT press</span>.
</div>
<div id="ref-Sutton2017" class="csl-entry" role="doc-biblioentry">
Sutton, R. S., and Barto, A. G. (2017). <em>Reinforcement
<span>Learning</span>: <span>An Introduction</span></em>. 2nd ed.
<span>Cambridge, MA</span>: <span>MIT Press</span> Available at: <a href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a>.
</div>
<div id="ref-Sutton1999" class="csl-entry" role="doc-biblioentry">
Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. (1999). Policy
gradient methods for reinforcement learning with function approximation.
in <em>Proceedings of the 12th <span>International Conference</span> on
<span>Neural Information Processing Systems</span></em> (<span>MIT
Press</span>), 1057–1063. Available at: <a href="https://dl.acm.org/citation.cfm?id=3009806">https://dl.acm.org/citation.cfm?id=3009806</a>.
</div>
<div id="ref-Szita2006" class="csl-entry" role="doc-biblioentry">
Szita, I., and Lörincz, A. (2006). Learning <span>Tetris Using</span>
the <span>Noisy Cross-Entropy Method</span>. <em>Neural Computation</em>
18, 2936–2941. doi:<a href="https://doi.org/10.1162/neco.2006.18.12.2936">10.1162/neco.2006.18.12.2936</a>.
</div>
<div id="ref-Tang2010" class="csl-entry" role="doc-biblioentry">
Tang, J., and Abbeel, P. (2010). On a <span>Connection</span> between
<span>Importance Sampling</span> and the <span>Likelihood Ratio Policy
Gradient</span>. in <em>Adv. <span>Neural</span> inf.
<span>Process</span>. <span>Syst</span>.</em> Available at: <a href="http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf">http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf</a>.
</div>
<div id="ref-Todorov2008" class="csl-entry" role="doc-biblioentry">
Todorov, E. (2008). General duality between optimal control and
estimation. in <em>2008 47th <span>IEEE Conference</span> on
<span>Decision</span> and <span>Control</span></em>, 4286–4292. doi:<a href="https://doi.org/10.1109/CDC.2008.4739438">10.1109/CDC.2008.4739438</a>.
</div>
<div id="ref-Toussaint2009" class="csl-entry" role="doc-biblioentry">
Toussaint, M. (2009). Robot <span>Trajectory Optimization Using
Approximate Inference</span>. in <em>Proceedings of the 26th
<span>Annual International Conference</span> on <span>Machine
Learning</span></em> <span>ICML</span> ’09. (<span>New York, NY,
USA</span>: <span>ACM</span>), 1049–1056. doi:<a href="https://doi.org/10.1145/1553374.1553508">10.1145/1553374.1553508</a>.
</div>
<div id="ref-Uhlenbeck1930" class="csl-entry" role="doc-biblioentry">
Uhlenbeck, G. E., and Ornstein, L. S. (1930). On the <span>Theory</span>
of the <span>Brownian Motion</span>. <em>Physical Review</em> 36. doi:<a href="https://doi.org/10.1103/PhysRev.36.823">10.1103/PhysRev.36.823</a>.
</div>
<div id="ref-vanHasselt2010" class="csl-entry" role="doc-biblioentry">
van Hasselt, H. (2010). Double <span class="nocase">Q-learning</span>.
in <em>Proceedings of the 23rd <span>International Conference</span> on
<span>Neural Information Processing Systems</span> - <span>Volume</span>
2</em> (<span>Curran Associates Inc.</span>), 2613–2621. Available at:
<a href="https://dl.acm.org/citation.cfm?id=2997187">https://dl.acm.org/citation.cfm?id=2997187</a>.
</div>
<div id="ref-vanHasselt2015" class="csl-entry" role="doc-biblioentry">
van Hasselt, H., Guez, A., and Silver, D. (2015). Deep
<span>Reinforcement Learning</span> with <span class="nocase">Double
Q-learning</span>. Available at: <a href="http://arxiv.org/abs/1509.06461">http://arxiv.org/abs/1509.06461</a>.
</div>
<div id="ref-Wang2017" class="csl-entry" role="doc-biblioentry">
Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z.,
Munos, R., et al. (2017). Learning to reinforcement learn. Available at:
<a href="http://arxiv.org/abs/1611.05763">http://arxiv.org/abs/1611.05763</a>
[Accessed February 5, 2021].
</div>
<div id="ref-Wang2016" class="csl-entry" role="doc-biblioentry">
Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de
Freitas, N. (2016). Dueling <span>Network Architectures</span> for
<span>Deep Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1511.06581">http://arxiv.org/abs/1511.06581</a>
[Accessed November 21, 2019].
</div>
<div id="ref-Watkins1989" class="csl-entry" role="doc-biblioentry">
Watkins, C. J. (1989). Learning from delayed rewards.
</div>
<div id="ref-Watter2015" class="csl-entry" role="doc-biblioentry">
Watter, M., Springenberg, J. T., Boedecker, J., and Riedmiller, M.
(2015). Embed to <span>Control</span>: <span>A Locally Linear Latent
Dynamics Model</span> for <span>Control</span> from <span>Raw
Images</span>. Available at: <a href="https://arxiv.org/pdf/1506.07365.pdf">https://arxiv.org/pdf/1506.07365.pdf</a>.
</div>
<div id="ref-Weber2017" class="csl-entry" role="doc-biblioentry">
Weber, T., Racanière, S., Reichert, D. P., Buesing, L., Guez, A.,
Rezende, D. J., et al. (2017). Imagination-<span>Augmented Agents</span>
for <span>Deep Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1707.06203">http://arxiv.org/abs/1707.06203</a>.
</div>
<div id="ref-Wierstra2007" class="csl-entry" role="doc-biblioentry">
Wierstra, D., Foerster, A., Peters, J., and Schmidhuber, J. (2007).
<span>“Solving <span>Deep Memory POMDPs</span> with <span>Recurrent
Policy Gradients</span>,”</span> in (<span>Springer, Berlin,
Heidelberg</span>), 697–706. doi:<a href="https://doi.org/10.1007/978-3-540-74690-4_71">10.1007/978-3-540-74690-4_71</a>.
</div>
<div id="ref-Williams1992" class="csl-entry" role="doc-biblioentry">
Williams, R. J. (1992). Simple statistical gradient-following algorithms
for connectionist reinforcement learning. <em>Machine Learning</em> 8,
229–256.
</div>
<div id="ref-Williams1991" class="csl-entry" role="doc-biblioentry">
Williams, R. J., and Peng, J. (1991). Function optimization using
connectionist reinforcement learning algorithms. <em>Connection
Science</em> 3, 241–268.
</div>
<div id="ref-Zhang2015" class="csl-entry" role="doc-biblioentry">
Zhang, F., Leitner, J., Milford, M., Upcroft, B., and Corke, P. (2015).
Towards <span>Vision-Based Deep Reinforcement Learning</span> for
<span>Robotic Motion Control</span>. in <em>Proc. <span>Acra</span></em>
Available at: <a href="http://arxiv.org/abs/1511.03791">http://arxiv.org/abs/1511.03791</a>.
</div>
<div id="ref-Ziebart2008" class="csl-entry" role="doc-biblioentry">
Ziebart, B. D., Maas, A., Bagnell, J. A., and Dey, A. K. (2008). Maximum
<span>Entropy Inverse Reinforcement Learning</span>. in, 6.
</div>
</div>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./9-Practice.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Deep RL in practice</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>