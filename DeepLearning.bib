@online{Abramson2022,
  title = {Creating {{Multimodal Interactive Agents}} with {{Imitation}} and {{Self-Supervised Learning}}},
  author = {Abramson, Josh and Ahuja, Arun and Brussee, Arthur and Carnevale, Federico and Cassin, Mary and Fischer, Felix and Georgiev, Petko and Goldin, Alex and Gupta, Mansi and Harley, Tim and Hill, Felix and Humphreys, Peter C. and Hung, Alden and Landon, Jessica and Lillicrap, Timothy and Merzic, Hamza and Muldal, Alistair and Santoro, Adam and Scully, Guy and family=Glehn, given=Tamara, prefix=von, useprefix=true and Wayne, Greg and Wong, Nathaniel and Yan, Chen and Zhu, Rui},
  date = {2022-02-02},
  eprint = {2112.03763},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.03763},
  url = {http://arxiv.org/abs/2112.03763},
  urldate = {2022-09-13},
  abstract = {A common vision from science fiction is that robots will one day inhabit our physical spaces, sense the world as we do, assist our physical labours, and communicate with us through natural language. Here we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment. We show that imitation learning of human-human interactions in a simulated world, in conjunction with self-supervised learning, is sufficient to produce a multimodal interactive agent, which we call MIA, that successfully interacts with non-adversarial humans 75\% of the time. We further identify architectural and algorithmic techniques that improve performance, such as hierarchical action selection. Altogether, our results demonstrate that imitation of multi-modal, real-time human behaviour may provide a straightforward and surprisingly effective means of imbuing agents with a rich behavioural prior from which agents might then be fine-tuned for specific purposes, thus laying a foundation for training capable agents for interactive robots or digital assistants. A video of MIA's behaviour may be found at https://youtu.be/ZFgRhviF7mY},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/T9ZK2YM2/Abramson_et_al_2022_Creating_Multimodal_Interactive_Agents_with_Imitation_and_Self-Supervised.pdf;/Users/vitay/Documents/Zotero/storage/87GE8QKN/2112.html}
}

@article{Albelwi2017,
  title = {A {{Framework}} for {{Designing}} the {{Architectures}} of {{Deep Convolutional Neural Networks}}},
  author = {Albelwi, Saleh and Mahmood, Ausif},
  date = {2017-06},
  journaltitle = {Entropy},
  volume = {19},
  number = {6},
  pages = {242},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/e19060242},
  url = {https://www.mdpi.com/1099-4300/19/6/242},
  urldate = {2020-09-17},
  abstract = {Recent advances in Convolutional Neural Networks (CNNs) have obtained promising results in difficult deep learning tasks. However, the success of a CNN depends on finding an architecture to fit a given problem. A hand-crafted architecture is a challenging, time-consuming process that requires expert knowledge and effort, due to a large number of architectural design choices. In this article, we present an efficient framework that automatically designs a high-performing CNN architecture for a given problem. In this framework, we introduce a new optimization objective function that combines the error rate and the information learnt by a set of feature maps using deconvolutional networks (deconvnet). The new objective function allows the hyperparameters of the CNN architecture to be optimized in a way that enhances the performance by guiding the CNN through better visualization of learnt features via deconvnet. The actual optimization of the objective function is carried out via the Nelder-Mead Method (NMM). Further, our new objective function results in much faster convergence towards a better architecture. The proposed framework has the ability to explore a CNN architecture’s numerous design choices in an efficient way and also allows effective, distributed execution and synchronization via web services. Empirically, we demonstrate that the CNN architecture designed with our approach outperforms several existing approaches in terms of its error rate. Our results are also competitive with state-of-the-art results on the MNIST dataset and perform reasonably against the state-of-the-art results on CIFAR-10 and CIFAR-100 datasets. Our approach has a significant role in increasing the depth, reducing the size of strides, and constraining some convolutional layers not followed by pooling layers in order to find a CNN architecture that produces a high recognition performance.},
  issue = {6},
  langid = {english},
  keywords = {CNN architecture design,convolutional neural networks (CNNs),correlation coefficient (Corr),deconvolutional networks (deconvnet),deep learning,Nelder-Mead method (NMM),objective function},
  file = {/Users/vitay/Documents/Zotero/storage/ZVTHR4B8/Albelwi_Mahmood_2017_A_Framework_for_Designing_the_Architectures_of_Deep_Convolutional_Neural.pdf;/Users/vitay/Documents/Zotero/storage/R32YDGU7/242.html}
}

@unpublished{Alcorn2019,
  title = {Strike (with) a {{Pose}}: {{Neural Networks Are Easily Fooled}} by {{Strange Poses}} of {{Familiar Objects}}},
  shorttitle = {Strike (with) a {{Pose}}},
  author = {Alcorn, Michael A. and Li, Qi and Gong, Zhitao and Wang, Chengfei and Mai, Long and Ku, Wei-Shinn and Nguyen, Anh},
  date = {2019-04-18},
  eprint = {1811.11553},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1811.11553},
  urldate = {2021-04-09},
  abstract = {Despite excellent performance on stationary test sets, deep neural networks (DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including natural, non-adversarial ones, which are common in real-world settings. In this paper, we present a framework for discovering DNN failures that harnesses 3D renderers and 3D models. That is, we estimate the parameters of a 3D renderer that cause a target DNN to misbehave in response to the rendered image. Using our framework and a self-assembled dataset of 3D objects, we investigate the vulnerability of DNNs to OoD poses of well-known objects in ImageNet. For objects that are readily recognized by DNNs in their canonical poses, DNNs incorrectly classify 97\% of their pose space. In addition, DNNs are highly sensitive to slight pose perturbations. Importantly, adversarial poses transfer across models and datasets. We find that 99.9\% and 99.4\% of the poses misclassified by Inception-v3 also transfer to the AlexNet and ResNet-50 image classifiers trained on the same ImageNet dataset, respectively, and 75.5\% transfer to the YOLOv3 object detector trained on MS COCO.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/UHS2TB3V/Alcorn_et_al_2019_Strike_(with)_a_Pose.pdf;/Users/vitay/Documents/Zotero/storage/HVTFIA4P/1811.html}
}

@unpublished{Arik2020,
  title = {{{TabNet}}: {{Attentive Interpretable Tabular Learning}}},
  shorttitle = {{{TabNet}}},
  author = {Arik, Sercan O. and Pfister, Tomas},
  date = {2020-12-09},
  eprint = {1908.07442},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1908.07442},
  urldate = {2021-07-10},
  abstract = {We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/R98XE52I/Arik_Pfister_2020_TabNet.pdf;/Users/vitay/Documents/Zotero/storage/T9BFRUX9/1908.html}
}

@unpublished{Arjovsky2017,
  title = {Wasserstein {{GAN}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  date = {2017-01},
  eprint = {1701.07875},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1701.07875},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.}
}

@unpublished{Arras2019,
  title = {Explaining and {{Interpreting LSTMs}}},
  author = {Arras, Leila and Arjona-Medina, Jose A. and Widrich, Michael and Montavon, Grégoire and Gillhofer, Michael and Müller, Klaus-Robert and Hochreiter, Sepp and Samek, Wojciech},
  date = {2019},
  volume = {11700},
  eprint = {1909.12114},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  pages = {211--238},
  doi = {10.1007/978-3-030-28954-6_11},
  url = {http://arxiv.org/abs/1909.12114},
  urldate = {2020-10-05},
  abstract = {While neural networks have acted as a strong unifying force in the design of modern AI systems, the neural network architectures themselves remain highly heterogeneous due to the variety of tasks to be solved. In this chapter, we explore how to adapt the Layer-wise Relevance Propagation (LRP) technique used for explaining the predictions of feed-forward networks to the LSTM architecture used for sequential data modeling and forecasting. The special accumulators and gated interactions present in the LSTM require both a new propagation scheme and an extension of the underlying theoretical framework to deliver faithful explanations.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/39WLCJXR/Arras_et_al_2019_Explaining_and_Interpreting_LSTMs.pdf;/Users/vitay/Documents/Zotero/storage/84HA6BE4/1909.html}
}

@unpublished{Atito2021,
  title = {{{SiT}}: {{Self-supervised vIsion Transformer}}},
  shorttitle = {{{SiT}}},
  author = {Atito, Sara and Awais, Muhammad and Kittler, Josef},
  date = {2021-11-14},
  eprint = {2104.03602},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.03602},
  urldate = {2021-12-26},
  abstract = {Self-supervised learning methods are gaining increasing traction in computer vision due to their recent success in reducing the gap with supervised learning. In natural language processing (NLP) self-supervised learning and transformers are already the methods of choice. The recent literature suggests that the transformers are becoming increasingly popular also in computer vision. So far, the vision transformers have been shown to work well when pretrained either using a large scale supervised data or with some kind of co-supervision, e.g. in terms of teacher network. These supervised pretrained vision transformers achieve very good results in downstream tasks with minimal changes. In this work we investigate the merits of self-supervised learning for pretraining image/vision transformers and then using them for downstream classification tasks. We propose Self-supervised vIsion Transformers (SiT) and discuss several self-supervised training mechanisms to obtain a pretext model. The architectural flexibility of SiT allows us to use it as an autoencoder and work with multiple self-supervised tasks seamlessly. We show that a pretrained SiT can be finetuned for a downstream classification task on small scale datasets, consisting of a few thousand images rather than several millions. The proposed approach is evaluated on standard datasets using common protocols. The results demonstrate the strength of the transformers and their suitability for self-supervised learning. We outperformed existing self-supervised learning methods by large margin. We also observed that SiT is good for few shot learning and also showed that it is learning useful representation by simply training a linear classifier on top of the learned features from SiT. Pretraining, finetuning, and evaluation codes will be available under: https://github.com/Sara-Ahmed/SiT.},
  file = {/Users/vitay/Documents/Zotero/storage/8HBMMBZ3/Atito_et_al_2021_SiT.pdf;/Users/vitay/Documents/Zotero/storage/7NTBWMQQ/2104.html}
}

@inproceedings{Atoofi2021,
  title = {Geometric {{Deep Learning}}: {{Graph Neural Networks}}, {{Challenges}}, and {{Breakthroughs}}},
  booktitle = {Tagungsband 23. {{Anwendungsbezogener Workshop}} Zur {{Erfassung}}, {{Modellierung}}, {{Verarbeitung}} Und {{Auswertung}} von {{3D-Daten}},},
  author = {Atoofi, Payam and Vitay, Julien and Hamker, Fred},
  date = {2021-03-12},
  pages = {115--124},
  location = {Berlin, Germany},
  eventtitle = {Workshop {{3D-NordOst}} 2021},
  isbn = {978-3-942709-27-9}
}

@article{Ba2014,
  title = {Multiple {{Object Recognition}} with {{Visual Attention}}},
  author = {Ba, Jimmy and Mnih, Volodymyr and Kavukcuoglu, Koray},
  date = {2014-12},
  url = {http://arxiv.org/abs/1412.7755},
  abstract = {We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.},
  file = {/Users/vitay/Documents/Zotero/storage/FUYRENWJ/Ba et al_2014_Multiple Object Recognition with Visual Attention.pdf}
}

@unpublished{Ba2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  date = {2016-07-21},
  eprint = {1607.06450},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2020-11-23},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/VWLGZIW4/Ba_et_al_2016_Layer_Normalization.pdf;/Users/vitay/Documents/Zotero/storage/P2A4TAJG/1607.html}
}

@unpublished{Badrinarayanan2016,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder-Decoder Architecture}} for {{Image Segmentation}}},
  shorttitle = {{{SegNet}}},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  date = {2016-10-10},
  eprint = {1511.00561},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.00561},
  urldate = {2020-11-29},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/vitay/Documents/Zotero/storage/62ZAMIAV/Badrinarayanan_et_al_2016_SegNet.pdf;/Users/vitay/Documents/Zotero/storage/FXWG8QBJ/1511.html}
}

@unpublished{Bahdanau2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2016-05-19},
  eprint = {1409.0473},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2020-12-24},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/6ZR6JINI/Bahdanau_et_al_2016_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate.pdf;/Users/vitay/Documents/Zotero/storage/WX8KFATB/1409.html}
}

@article{Baker2018,
  title = {Deep Convolutional Networks Do Not Classify Based on Global Object Shape},
  author = {Baker, Nicholas and Lu, Hongjing and Erlikhman, Gennady and Kellman, Philip J.},
  date = {2018-12-07},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {14},
  number = {12},
  pages = {e1006613},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006613},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613},
  urldate = {2019-01-11},
  abstract = {Deep convolutional networks (DCNNs) are achieving previously unseen performance in object classification, raising questions about whether DCNNs operate similarly to human vision. In biological vision, shape is arguably the most important cue for recognition. We tested the role of shape information in DCNNs trained to recognize objects. In Experiment 1, we presented a trained DCNN with object silhouettes that preserved overall shape but were filled with surface texture taken from other objects. Shape cues appeared to play some role in the classification of artifacts, but little or none for animals. In Experiments 2–4, DCNNs showed no ability to classify glass figurines or outlines but correctly classified some silhouettes. Aspects of these results led us to hypothesize that DCNNs do not distinguish object’s bounding contours from other edges, and that DCNNs access some local shape features, but not global shape. In Experiment 5, we tested this hypothesis with displays that preserved local features but disrupted global shape, and vice versa. With disrupted global shape, which reduced human accuracy to 28\%, DCNNs gave the same classification labels as with ordinary shapes. Conversely, local contour changes eliminated accurate DCNN classification but caused no difficulty for human observers. These results provide evidence that DCNNs have access to some local shape information in the form of local edge relations, but they have no access to global object shapes.},
  langid = {english},
  keywords = {Glass,Human performance,Imaging techniques,Neural networks,Sensory perception,Sharks,Vision,Visual object recognition},
  file = {/Users/vitay/Documents/Zotero/storage/KZRZCKWA/Baker et al_2018_Deep convolutional networks do not classify based on global object shape.pdf}
}

@online{Beitner2020,
  title = {{{PyTorch Forecasting}}},
  author = {Beitner, Jan},
  date = {2020},
  url = {https://pytorch-forecasting.readthedocs.io/en/stable/},
  urldate = {2023-08-04},
  organization = {PyTorch Forecasting}
}

@inproceedings{Bender2021,
  title = {On the {{Dangers}} of {{Stochastic Parrots}}: {{Can Language Models Be Too Big}}?},
  shorttitle = {On the {{Dangers}} of {{Stochastic Parrots}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  date = {2021-03-03},
  series = {{{FAccT}} '21},
  pages = {610--623},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3442188.3445922},
  url = {https://doi.org/10.1145/3442188.3445922},
  urldate = {2022-05-03},
  abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  isbn = {978-1-4503-8309-7},
  file = {/Users/vitay/Documents/Zotero/storage/UW3MFLIA/Bender_et_al_2021_On_the_Dangers_of_Stochastic_Parrots.pdf}
}

@unpublished{Bengio2012,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle = {Representation {{Learning}}},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  date = {2012-06-24},
  eprint = {1206.5538},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1206.5538},
  urldate = {2019-03-02},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/RUFE6XYB/Bengio et al_2012_Representation Learning.pdf}
}

@article{Bi2001,
  title = {Synaptic {{Modification}} by {{Correlated Activity}}: {{Hebb}}'s {{Postulate Revisited}}},
  shorttitle = {Synaptic {{Modification}} by {{Correlated Activity}}},
  author = {Bi, Guo-qiang and Poo, Mu-ming},
  date = {2001},
  journaltitle = {Annual Review of Neuroscience},
  volume = {24},
  number = {1},
  eprint = {11283308},
  eprinttype = {pmid},
  pages = {139--166},
  doi = {10.1146/annurev.neuro.24.1.139},
  url = {https://doi.org/10.1146/annurev.neuro.24.1.139},
  urldate = {2021-01-25},
  abstract = {Correlated spiking of pre- and postsynaptic neurons can result in strengthening or weakening of synapses, depending on the temporal order of spiking. Recent findings indicate that there are narrow and cell type–specific temporal windows for such synaptic modification and that the generally accepted input- (or synapse-) specific rule for modification appears not to be strictly adhered to. Spike timing–dependent modifications, together with selective spread of synaptic changes, provide a set of cellular mechanisms that are likely to be important for the development and functioning of neural networks. When an axon of cell A is near enough to excite cell B or repeatedly or consistently takes part in firing it, some growth or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased. Donald Hebb (1949)},
  file = {/Users/vitay/Documents/Zotero/storage/STUXGM8C/Bi_Poo_2001_Synaptic_Modification_by_Correlated_Activity.pdf}
}

@article{Bienenstock1982,
  title = {Theory for the Development of Neuron Selectivity: Orientation Specificity and Binocular Interaction in Visual Cortex.},
  author = {Bienenstock, E L and Cooper, L N and Munro, P W},
  date = {1982-01},
  journaltitle = {J Neurosci},
  volume = {2},
  number = {1},
  pages = {32--48},
  abstract = {The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework. A synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete. The change in the efficacy of a given synapse depends not only on instantaneous pre- and postsynaptic activities but also on a slowly varying time-averaged value of the postsynaptic activity. Assuming an appropriate nonlinear form for this dependence, development of selectivity is obtained under quite general conditions on the sensory environment. One does not require nonlinearity of the neuron's integrative power nor does one need to assume any particular form for intracortical circuitry. This is first illustrated in simple cases, e.g., when the environment consists of only two different stimuli presented alternately in a random manner. The following formal statement then holds: the state of the system converges with probability 1 to points of maximum selectivity in the state space. We next consider the problem of early development of orientation selectivity and binocular interaction in primary visual cortex. Giving the environment an appropriate form, we obtain orientation tuning curves and ocular dominance comparable to what is observed in normally reared adult cats or monkeys. Simulations with binocular input and various types of normal or altered environments show good agreement with the relevant experimental data. Experiments are suggested that could test our theory further.},
  keywords = {Animals,Cats,Cerebral,Dominance,Mathematics,Models,Neurological,Neurons,Orientation,Retina,Senso}
}

@unpublished{Binder2016,
  title = {Layer-Wise {{Relevance Propagation}} for {{Neural Networks}} with {{Local Renormalization Layers}}},
  author = {Binder, Alexander and Montavon, Grégoire and Bach, Sebastian and Müller, Klaus-Robert and Samek, Wojciech},
  date = {2016-04-04},
  eprint = {1604.00825},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1604.00825},
  urldate = {2019-03-21},
  abstract = {Layer-wise relevance propagation is a framework which allows to decompose the prediction of a deep neural network computed over a sample, e.g. an image, down to relevance scores for the single input dimensions of the sample such as subpixels of an image. While this approach can be applied directly to generalized linear mappings, product type non-linearities are not covered. This paper proposes an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks. We evaluate the proposed method for local renormalization layers on the CIFAR-10, Imagenet and MIT Places datasets.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/VLDB2C6E/Binder et al_2016_Layer-wise Relevance Propagation for Neural Networks with Local Renormalization.pdf;/Users/vitay/Documents/Zotero/storage/MPQ5P6J2/1604.html}
}

@unpublished{Bojarski2016,
  title = {End to {{End Learning}} for {{Self-Driving Cars}}},
  author = {Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
  date = {2016-04-25},
  eprint = {1604.07316},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1604.07316},
  urldate = {2020-09-17},
  abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/vitay/Documents/Zotero/storage/5QD5S83K/Bojarski_et_al_2016_End_to_End_Learning_for_Self-Driving_Cars.pdf;/Users/vitay/Documents/Zotero/storage/4ZJ3X5KK/1604.html}
}

@unpublished{Bond-Taylor2021,
  title = {Deep {{Generative Modelling}}: {{A Comparative Review}} of {{VAEs}}, {{GANs}}, {{Normalizing Flows}}, {{Energy-Based}} and {{Autoregressive Models}}},
  shorttitle = {Deep {{Generative Modelling}}},
  author = {Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
  date = {2021-04-14},
  eprint = {2103.04922},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2103.04922},
  urldate = {2021-06-15},
  abstract = {Deep generative modelling is a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which making trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are drawn under a single cohesive framework, comparing and contrasting to explain the premises behind each, while reviewing current state-of-the-art advances and implementations.},
  keywords = {68T01 (Primary) 68T07 (Secondary),Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,G.3,I.4.0,I.5.0,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/ATZ3CKU5/Bond-Taylor_et_al_2021_Deep_Generative_Modelling.pdf;/Users/vitay/Documents/Zotero/storage/D4W6AZQS/2103.html}
}

@unpublished{Bowman2016,
  title = {Generating {{Sentences}} from a {{Continuous Space}}},
  author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
  date = {2016-05-12},
  eprint = {1511.06349},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.06349},
  urldate = {2020-10-06},
  abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/UKG5C87N/Bowman_et_al_2016_Generating_Sentences_from_a_Continuous_Space.pdf;/Users/vitay/Documents/Zotero/storage/4BKITFUJ/1511.html}
}

@article{Brette2005,
  title = {Adaptive {{Exponential Integrate-and-Fire Model}} as an {{Effective Description}} of {{Neuronal Activity}}},
  author = {Brette, Romain and Gerstner, Wulfram},
  date = {2005-11-01},
  journaltitle = {Journal of Neurophysiology},
  volume = {94},
  number = {5},
  pages = {3637--3642},
  publisher = {American Physiological Society},
  issn = {0022-3077},
  doi = {10.1152/jn.00686.2005},
  url = {https://journals.physiology.org/doi/full/10.1152/jn.00686.2005},
  urldate = {2020-09-23},
  abstract = {We introduce a two-dimensional integrate-and-fire model that combines an exponential spike mechanism with an adaptation equation, based on recent theoretical findings. We describe a systematic method to estimate its parameters with simple electrophysiological protocols (current-clamp injection of pulses and ramps) and apply it to a detailed conductance-based model of a regular spiking neuron. Our simple model predicts correctly the timing of 96\% of the spikes (±2 ms) of the detailed model in response to injection of noisy synaptic conductances. The model is especially reliable in high-conductance states, typical of cortical activity in vivo, in which intrinsic conductances were found to have a reduced role in shaping spike trains. These results are promising because this simple model has enough expressive power to reproduce qualitatively several electrophysiological classes described in vitro.},
  file = {/Users/vitay/Documents/Zotero/storage/PAEUF3DG/Brette_Gerstner_2005_Adaptive_Exponential_Integrate-and-Fire_Model_as_an_Effective_Description_of.pdf;/Users/vitay/Documents/Zotero/storage/DRDTJYAA/jn.00686.html}
}

@article{Bronstein2017,
  title = {Geometric {{Deep Learning}}: {{Going}} beyond {{Euclidean}} Data},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  date = {2017-07},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {4},
  pages = {18--42},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2693418},
  abstract = {Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains, such as graphs and manifolds. The purpose of this article is to overview different examples of geometric deep-learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
  keywords = {computational geometry,Computational modeling,Computer architecture,Convolution,Convolutional codes,deep neural models,Euclidean data,Euclidean distance,geometric deep learning,Machine learning,neural nets,Social network services},
  file = {/Users/vitay/Documents/Zotero/storage/66TCR28W/Bronstein_et_al_2017_Geometric_Deep_Learning.pdf;/Users/vitay/Documents/Zotero/storage/LMD9S6MJ/7974879.html}
}

@unpublished{Bronstein2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
  date = {2021-05-02},
  eprint = {2104.13478},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2104.13478},
  urldate = {2021-06-29},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/5GPZ9QMV/Bronstein_et_al_2021_Geometric_Deep_Learning.pdf;/Users/vitay/Documents/Zotero/storage/7ATGPL8W/2104.html}
}

@unpublished{Brown2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2020-12-23},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/vitay/Documents/Zotero/storage/FDLDXRMC/Brown_et_al_2020_Language_Models_are_Few-Shot_Learners.pdf;/Users/vitay/Documents/Zotero/storage/6T3I9RTA/2005.html}
}

@unpublished{Brundage2018,
  title = {The {{Malicious Use}} of {{Artificial Intelligence}}: {{Forecasting}}, {{Prevention}}, and {{Mitigation}}},
  shorttitle = {The {{Malicious Use}} of {{Artificial Intelligence}}},
  author = {Brundage, Miles and Avin, Shahar and Clark, Jack and Toner, Helen and Eckersley, Peter and Garfinkel, Ben and Dafoe, Allan and Scharre, Paul and Zeitzoff, Thomas and Filar, Bobby and Anderson, Hyrum and Roff, Heather and Allen, Gregory C. and Steinhardt, Jacob and Flynn, Carrick and {hÉigeartaigh}, Seán Ó and Beard, Simon and Belfield, Haydn and Farquhar, Sebastian and Lyle, Clare and Crootof, Rebecca and Evans, Owain and Page, Michael and Bryson, Joanna and Yampolskiy, Roman and Amodei, Dario},
  date = {2018-02-20},
  eprint = {1802.07228},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.07228},
  urldate = {2020-12-07},
  abstract = {This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Cryptography and Security},
  file = {/Users/vitay/Documents/Zotero/storage/Q2HKZQQ3/Brundage_et_al_2018_The_Malicious_Use_of_Artificial_Intelligence.pdf;/Users/vitay/Documents/Zotero/storage/HJS7F65N/1802.html}
}

@unpublished{Carlini2018,
  title = {Audio {{Adversarial Examples}}: {{Targeted Attacks}} on {{Speech-to-Text}}},
  shorttitle = {Audio {{Adversarial Examples}}},
  author = {Carlini, Nicholas and Wagner, David},
  date = {2018-01-05},
  eprint = {1801.01944},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1801.01944},
  urldate = {2019-01-10},
  abstract = {We construct targeted audio adversarial examples on automatic speech recognition. Given any audio waveform, we can produce another that is over 99.9\% similar, but transcribes as any phrase we choose (recognizing up to 50 characters per second of audio). We apply our white-box iterative optimization-based attack to Mozilla's implementation DeepSpeech end-to-end, and show it has a 100\% success rate. The feasibility of this attack introduce a new domain to study adversarial examples.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/CZH5IPWL/Carlini_Wagner_2018_Audio Adversarial Examples.pdf}
}

@unpublished{Caron2021,
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  date = {2021-05-24},
  eprint = {2104.14294},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.14294},
  urldate = {2021-12-29},
  abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  file = {/Users/vitay/Documents/Zotero/storage/7UDASATY/Caron_et_al_2021_Emerging_Properties_in_Self-Supervised_Vision_Transformers.pdf;/Users/vitay/Documents/Zotero/storage/V7WC8NNI/2104.html}
}

@article{Cate2017,
  title = {Machine Learning as a Tool for Geologists},
  author = {Caté, Antoine and Perozzi, Lorenzo and Gloaguen, Erwan and Blouin, Martin},
  date = {2017-03-01},
  journaltitle = {The Leading Edge},
  shortjournal = {The Leading Edge},
  volume = {36},
  pages = {215--219},
  doi = {10.1190/tle36030215.1},
  abstract = {Machine learning is becoming an appealing tool in various fields of earth sciences, especially in resources estimation. Six machine learning algorithms have been used to predict the presence of gold mineralization in drill core from geophysical logs acquired at the Lalor deposit, Manitoba, Canada. Results show that the integration of a set of rock physical properties-measured at closely spaced intervals along the drill core-with ensemble machine learning algorithms allows the detection of gold-bearing intervals with an adequate rate of success. Since the resulting prediction is continuous along the drill core, the use of this type of tool in the future will help geologists in selecting sound intervals for assay sampling and in modeling more continuous ore bodies during the entire life of a mine.},
  file = {/Users/vitay/Documents/Zotero/storage/AYLL2BUM/Caté et al_2017_Machine learning as a tool for geologists.pdf}
}

@article{Chen2016,
  title = {{{InfoGAN}}: {{Interpretable Representation Learning}} by {{Information Maximizing Generative Adversarial Nets}}},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  date = {2016-06},
  url = {http://arxiv.org/abs/1606.03657},
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.}
}

@online{Chen2020,
  title = {Big {{Self-Supervised Models}} Are {{Strong Semi-Supervised Learners}}},
  author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020-10-25},
  eprint = {2006.10029},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2006.10029},
  url = {http://arxiv.org/abs/2006.10029},
  urldate = {2022-11-27},
  abstract = {One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels (\$\textbackslash le\$13 labeled images per class) using ResNet-50, a \$10\textbackslash times\$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/C6CF9GYL/Chen_et_al_2020_Big_Self-Supervised_Models_are_Strong_Semi-Supervised_Learners.pdf;/Users/vitay/Documents/Zotero/storage/HUFLTD3D/2006.html}
}

@online{Chen2020a,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020-06-30},
  eprint = {2002.05709},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2002.05709},
  url = {http://arxiv.org/abs/2002.05709},
  urldate = {2023-01-19},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/6NCIV2LJ/Chen_et_al_2020_A_Simple_Framework_for_Contrastive_Learning_of_Visual_Representations.pdf}
}

@article{Cho2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and family=Merrienboer, given=Bart, prefix=van, useprefix=true and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  date = {2014-06},
  url = {http://arxiv.org/abs/1406.1078},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  file = {/Users/vitay/Documents/Zotero/storage/REI9B6VR/Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf}
}

@book{Chollet2017,
  title = {Deep {{Learning}} with {{Python}}},
  author = {Chollet, François},
  date = {2017},
  publisher = {Manning publications},
  url = {https://www.manning.com/books/deep-learning-with-python},
  urldate = {2020-09-10},
  abstract = {Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Fran\&\#231;ois Chollet, this book builds your understanding through intuitive explanations and practical examples.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/A7YECW8E/deep-learning-with-python.html}
}

@unpublished{Chollet2017b,
  title = {Xception: {{Deep Learning}} with {{Depthwise Separable Convolutions}}},
  shorttitle = {Xception},
  author = {Chollet, François},
  date = {2017-04-04},
  eprint = {1610.02357},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1610.02357},
  urldate = {2020-11-20},
  abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/L85R48YA/Chollet_2017_Xception.pdf;/Users/vitay/Documents/Zotero/storage/WLGUSWD4/1610.html}
}

@online{Chollet2019,
  title = {On the {{Measure}} of {{Intelligence}}},
  author = {Chollet, François},
  date = {2019-11-25},
  eprint = {1911.01547},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1911.01547},
  url = {http://arxiv.org/abs/1911.01547},
  urldate = {2022-09-16},
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/BPBJ5FDE/Chollet_2019_On_the_Measure_of_Intelligence.pdf;/Users/vitay/Documents/Zotero/storage/ADVDMI7K/1911.html}
}

@unpublished{Chung2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  date = {2014-12-11},
  eprint = {1412.3555},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1412.3555},
  urldate = {2020-12-23},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/vitay/Documents/Zotero/storage/8ZI5VTU3/Chung_et_al_2014_Empirical_Evaluation_of_Gated_Recurrent_Neural_Networks_on_Sequence_Modeling.pdf;/Users/vitay/Documents/Zotero/storage/ZBJFMUWM/1412.html}
}

@article{Clopath2010,
  title = {Connectivity Reflects Coding: A Model of Voltage-Based {{STDP}} with Homeostasis},
  shorttitle = {Connectivity Reflects Coding},
  author = {Clopath, Claudia and Büsing, Lars and Vasilaki, Eleni and Gerstner, Wulfram},
  date = {2010-03},
  journaltitle = {Nature Neuroscience},
  volume = {13},
  number = {3},
  pages = {344--352},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn.2479},
  url = {https://www.nature.com/articles/nn.2479},
  urldate = {2021-01-30},
  abstract = {The authors develop a model of synaptic plasticity that can account for a large body of experimental data on connection patterns in the cortex. This model uses multiple parameters, including presynaptic spike interval and postsynaptic membrane potential.},
  issue = {3},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/FB6Q765Z/Clopath_et_al_2010_Connectivity_reflects_coding.pdf}
}

@online{Croitoru2022,
  title = {Diffusion {{Models}} in {{Vision}}: {{A Survey}}},
  shorttitle = {Diffusion {{Models}} in {{Vision}}},
  author = {Croitoru, Florinel-Alin and Hondru, Vlad and Ionescu, Radu Tudor and Shah, Mubarak},
  date = {2022-10-06},
  eprint = {2209.04747},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.04747},
  url = {http://arxiv.org/abs/2209.04747},
  urldate = {2022-10-13},
  abstract = {Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/UGNKPLBY/Croitoru_et_al_2022_Diffusion_Models_in_Vision.pdf;/Users/vitay/Documents/Zotero/storage/B9MLEQNF/2209.html}
}

@online{Dai2021a,
  title = {{{UP-DETR}}: {{Unsupervised Pre-training}} for {{Object Detection}} with {{Transformers}}},
  shorttitle = {{{UP-DETR}}},
  author = {Dai, Zhigang and Cai, Bolun and Lin, Yugeng and Chen, Junying},
  date = {2021-04-07},
  eprint = {2011.09094},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2011.09094},
  url = {http://arxiv.org/abs/2011.09094},
  urldate = {2023-06-30},
  abstract = {Object detection with transformers (DETR) reaches competitive performance with Faster R-CNN via a transformer encoder-decoder architecture. Inspired by the great success of pre-training transformers in natural language processing, we propose a pretext task named random query patch detection to Unsupervisedly Pre-train DETR (UP-DETR) for object detection. Specifically, we randomly crop patches from the given image and then feed them as queries to the decoder. The model is pre-trained to detect these query patches from the original image. During the pre-training, we address two critical issues: multi-task learning and multi-query localization. (1) To trade off classification and localization preferences in the pretext task, we freeze the CNN backbone and propose a patch feature reconstruction branch which is jointly optimized with patch detection. (2) To perform multi-query localization, we introduce UP-DETR from single-query patch and extend it to multi-query patches with object query shuffle and attention mask. In our experiments, UP-DETR significantly boosts the performance of DETR with faster convergence and higher average precision on object detection, one-shot detection and panoptic segmentation. Code and pre-training models: https://github.com/dddzg/up-detr.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/HP94RL4C/Dai_et_al_2021_UP-DETR.pdf}
}

@unpublished{dAscoli2021,
  title = {{{ConViT}}: {{Improving Vision Transformers}} with {{Soft Convolutional Inductive Biases}}},
  shorttitle = {{{ConViT}}},
  author = {family=Ascoli, given=Stéphane, prefix=d', useprefix=true and Touvron, Hugo and Leavitt, Matthew and Morcos, Ari and Biroli, Giulio and Sagun, Levent},
  date = {2021-06-10},
  eprint = {2103.10697},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2103.10697},
  urldate = {2021-11-16},
  abstract = {Convolutional architectures have proven extremely successful for vision tasks. Their hard inductive biases enable sample-efficient learning, but come at the cost of a potentially lower performance ceiling. Vision Transformers (ViTs) rely on more flexible self-attention layers, and have recently outperformed CNNs for image classification. However, they require costly pre-training on large external datasets or distillation from pre-trained convolutional networks. In this paper, we ask the following question: is it possible to combine the strengths of these two architectures while avoiding their respective limitations? To this end, we introduce gated positional self-attention (GPSA), a form of positional self-attention which can be equipped with a ``soft" convolutional inductive bias. We initialise the GPSA layers to mimic the locality of convolutional layers, then give each attention head the freedom to escape locality by adjusting a gating parameter regulating the attention paid to position versus content information. The resulting convolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet, while offering a much improved sample efficiency. We further investigate the role of locality in learning by first quantifying how it is encouraged in vanilla self-attention layers, then analysing how it is escaped in GPSA layers. We conclude by presenting various ablations to better understand the success of the ConViT. Our code and models are released publicly at https://github.com/facebookresearch/convit.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/JKCJUQBE/d'Ascoli_et_al_2021_ConViT.pdf;/Users/vitay/Documents/Zotero/storage/299GDVUG/2103.html}
}

@book{Dayan2001,
  title = {Theoretical {{Neuroscience}}: {{Computational}} and {{Mathematical Modeling}} of {{Neural Systems}}},
  author = {Dayan, Peter and Abbott, L. F.},
  date = {2001-09},
  publisher = {The MIT Press},
  url = {http://dl.acm.org/citation.cfm?id=1205781},
  isbn = {0-262-54185-8},
  file = {/Users/vitay/Documents/Zotero/storage/WBL8XVEC/Dayan_Abbott_2001_Theoretical_Neuroscience.pdf}
}

@article{Demircigil2017,
  title = {On a Model of Associative Memory with Huge Storage Capacity},
  author = {Demircigil, Mete and Heusel, Judith and Löwe, Matthias and Upgang, Sven and Vermet, Franck},
  date = {2017-07},
  journaltitle = {Journal of Statistical Physics},
  shortjournal = {J Stat Phys},
  volume = {168},
  number = {2},
  eprint = {1702.01929},
  eprinttype = {arXiv},
  pages = {288--299},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/s10955-017-1806-y},
  url = {http://arxiv.org/abs/1702.01929},
  urldate = {2021-01-19},
  abstract = {In [7] Krotov and Hopfield suggest a generalized version of the well-known Hopfield model of associative memory. In their version they consider a polynomial interaction function and claim that this increases the storage capacity of the model. We prove this claim and take the "limit" as the degree of the polynomial becomes infinite, i.e. an exponential interaction function. With this interaction we prove that model has an exponential storage capacity in the number of neurons, yet the basins of attraction are almost as large as in the standard Hopfield model.},
  keywords = {82C32 60K35 Secondary: 68T05 92B20,Mathematics - Probability},
  file = {/Users/vitay/Documents/Zotero/storage/KTRSLS3U/Demircigil_et_al_2017_On_a_model_of_associative_memory_with_huge_storage_capacity.pdf;/Users/vitay/Documents/Zotero/storage/IXV37D6L/1702.html}
}

@unpublished{Devlin2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  eprint = {1810.04805},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2019-12-05},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/vitay/Documents/Zotero/storage/H3X4PCYW/Devlin_et_al_2019_BERT.pdf;/Users/vitay/Documents/Zotero/storage/EEHESIXN/1810.html}
}

@unpublished{Didolkar2022,
  title = {Temporal {{Latent Bottleneck}}: {{Synthesis}} of {{Fast}} and {{Slow Processing Mechanisms}} in {{Sequence Learning}}},
  shorttitle = {Temporal {{Latent Bottleneck}}},
  author = {Didolkar, Aniket and Gupta, Kshitij and Goyal, Anirudh and Lamb, Alex and Ke, Nan Rosemary and Bengio, Yoshua},
  date = {2022-05-29},
  eprint = {2205.14794},
  eprinttype = {arXiv},
  eprintclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2205.14794},
  urldate = {2022-06-06},
  abstract = {Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of \$K\$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of \$K\$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.},
  file = {/Users/vitay/Documents/Zotero/storage/9EFW787R/Didolkar_et_al_2022_Temporal_Latent_Bottleneck.pdf;/Users/vitay/Documents/Zotero/storage/PUVJXHZE/2205.html}
}

@online{Doersch2016,
  title = {Unsupervised {{Visual Representation Learning}} by {{Context Prediction}}},
  author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
  date = {2016-01-16},
  eprint = {1505.05192},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1505.05192},
  url = {http://arxiv.org/abs/1505.05192},
  urldate = {2023-01-18},
  abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/K6W6HVIR/Doersch_et_al_2016_Unsupervised_Visual_Representation_Learning_by_Context_Prediction.pdf}
}

@unpublished{Dosovitskiy2021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2021-11-29},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/QLMGDS37/Dosovitskiy_et_al_2021_An_Image_is_Worth_16x16_Words.pdf;/Users/vitay/Documents/Zotero/storage/AXIPN756/2010.html}
}

@unpublished{Ducoffe2018,
  title = {Adversarial {{Active Learning}} for {{Deep Networks}}: A {{Margin Based Approach}}},
  shorttitle = {Adversarial {{Active Learning}} for {{Deep Networks}}},
  author = {Ducoffe, Melanie and Precioso, Frederic},
  date = {2018-02-27},
  eprint = {1802.09841},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.09841},
  urldate = {2019-04-09},
  abstract = {We propose a new active learning strategy designed for deep neural networks. The goal is to minimize the number of data annotation queried from an oracle during training. Previous active learning strategies scalable for deep networks were mostly based on uncertain sample selection. In this work, we focus on examples lying close to the decision boundary. Based on theoretical works on margin theory for active learning, we know that such examples may help to considerably decrease the number of annotations. While measuring the exact distance to the decision boundaries is intractable, we propose to rely on adversarial examples. We do not consider anymore them as a threat instead we exploit the information they provide on the distribution of the input space in order to approximate the distance to decision boundaries. We demonstrate empirically that adversarial active queries yield faster convergence of CNNs trained on MNIST, the Shoe-Bag and the Quick-Draw datasets.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/MJ5DZWLK/Ducoffe_Precioso_2018_Adversarial Active Learning for Deep Networks.pdf;/Users/vitay/Documents/Zotero/storage/SEYEXNUR/1802.html}
}

@incollection{Eleftheriadis2017,
  title = {Variational {{Gaussian Process Auto-Encoder}} for {{Ordinal Prediction}} of {{Facial Action Units}}},
  booktitle = {Computer {{Vision}} – {{ACCV}} 2016},
  author = {Eleftheriadis, Stefanos and Rudovic, Ognjen and Deisenroth, Marc Peter and Pantic, Maja},
  editor = {Lai, Shang-Hong and Lepetit, Vincent and Nishino, Ko and Sato, Yoichi},
  date = {2017},
  volume = {10112},
  pages = {154--170},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-54184-6_10},
  url = {http://link.springer.com/10.1007/978-3-319-54184-6_10},
  urldate = {2019-03-02},
  abstract = {We address the task of simultaneous feature fusion and modeling of discrete ordinal outputs. We propose a novel Gaussian process (GP) auto-encoder modeling approach. In particular, we introduce GP encoders to project multiple observed features onto a latent space, while GP decoders are responsible for reconstructing the original features. Inference is performed in a novel variational framework, where the recovered latent representations are further constrained by the ordinal output labels. In this way, we seamlessly integrate the ordinal structure in the learned manifold, while attaining robust fusion of the input features. We demonstrate the representation abilities of our model on benchmark datasets from machine learning and affect analysis. We further evaluate the model on the tasks of feature fusion and joint ordinal prediction of facial action units. Our experiments demonstrate the benefits of the proposed approach compared to the state of the art.},
  isbn = {978-3-319-54183-9 978-3-319-54184-6},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/G7BB7W6K/Eleftheriadis et al_2017_Variational Gaussian Process Auto-Encoder for Ordinal Prediction of Facial.pdf}
}

@online{Ellenberger2024,
  title = {Backpropagation through Space, Time, and the Brain},
  author = {Ellenberger, Benjamin and Haider, Paul and Jordan, Jakob and Max, Kevin and Jaras, Ismael and Kriener, Laura and Benitez, Federico and Petrovici, Mihai A.},
  date = {2024-03-25},
  eprint = {2403.16933},
  eprinttype = {arXiv},
  eprintclass = {cs, eess, q-bio},
  doi = {10.48550/arXiv.2403.16933},
  url = {http://arxiv.org/abs/2403.16933},
  urldate = {2024-05-15},
  abstract = {Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task. However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality. How such networks can perform efficient credit assignment, remains, to a large extent, an open question. In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by defining an energy based on neuron-local mismatches, from which we derive both neuronal dynamics via stationarity and parameter dynamics via gradient descent. The resulting dynamics can be interpreted as a real-time, biologically plausible approximation of BPTT in deep cortical networks with continuous-time neuronal dynamics and continuously active, local synaptic plasticity. In particular, GLE exploits the ability of biological neurons to phase-shift their output rate with respect to their membrane potential, which is essential in both directions of information propagation. For the forward computation, it enables the mapping of time-continuous inputs to neuronal space, performing an effective spatiotemporal convolution. For the backward computation, it permits the temporal inversion of feedback signals, which consequently approximate the adjoint states necessary for useful parameter updates.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/YBAAK7HZ/Ellenberger et al. - 2024 - Backpropagation through space, time, and the brain.pdf}
}

@inproceedings{Eslami2016,
  title = {Attend, {{Infer}}, {{Repeat}}: {{Fast Scene Understanding}} with {{Generative Models}}},
  author = {Eslami, S M Ali and Heess, Nicolas and Weber, Theophane and Tassa, Yuval and Szepesvari, David},
  date = {2016},
  pages = {9},
  abstract = {We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects – counting, locating and classifying the elements of a scene –without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network at unprecedented speed. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.},
  eventtitle = {30th {{Conference}} on {{Neural Information Processing Systems}} ({{NIPS}} 2016)},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/4FZQH7HD/Eslami et al_2016_Attend, Infer, Repeat.pdf}
}

@unpublished{Eykholt2018,
  title = {Robust {{Physical-World Attacks}} on {{Deep Learning Models}}},
  author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
  date = {2018-04-10},
  eprint = {1707.08945},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.08945},
  urldate = {2021-01-14},
  abstract = {Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations.Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm,Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. Witha perturbation in the form of only black and white stickers,we attack a real stop sign, causing targeted misclassification in 100\% of the images obtained in lab settings, and in 84.8\%of the captured video frames obtained on a moving vehicle(field test) for the target classifier.},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/3HSD59HN/Eykholt_et_al_2018_Robust_Physical-World_Attacks_on_Deep_Learning_Models.pdf;/Users/vitay/Documents/Zotero/storage/9AQBSGW4/1707.html}
}

@inproceedings{Farahani2021,
  title = {Geometric {{Deep Learning}} and Solutions for the Industry},
  booktitle = {Tagungsband 23. {{Anwendungsbezogener Workshop}} Zur {{Erfassung}}, {{Modellierung}}, {{Verarbeitung}} Und {{Auswertung}} von {{3D-Daten}},},
  author = {Farahani, Aida and Vitay, Julien and Hamker, Fred},
  date = {2021-03-12},
  pages = {105--113},
  location = {Berlin, Germany},
  eventtitle = {Workshop {{3D-NordOst}} 2021},
  isbn = {978-3-942709-27-9}
}

@unpublished{Feng2018,
  title = {Hypergraph {{Neural Networks}}},
  author = {Feng, Yifan and You, Haoxuan and Zhang, Zizhao and Ji, Rongrong and Gao, Yue},
  date = {2018-09-25},
  eprint = {1809.09401},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.09401},
  urldate = {2019-04-10},
  abstract = {In this paper, we present a hypergraph neural networks (HGNN) framework for data representation learning, which can encode high-order data correlation in a hypergraph structure. Confronting the challenges of learning representation for complex data in real practice, we propose to incorporate such data structure in a hypergraph, which is more flexible on data modeling, especially when dealing with complex data. In this method, a hyperedge convolution operation is designed to handle the data correlation during representation learning. In this way, traditional hypergraph learning procedure can be conducted using hyperedge convolution operations efficiently. HGNN is able to learn the hidden layer representation considering the high-order data structure, which is a general framework considering the complex data correlations. We have conducted experiments on citation network classification and visual object recognition tasks and compared HGNN with graph convolutional networks and other traditional methods. Experimental results demonstrate that the proposed HGNN method outperforms recent state-of-the-art methods. We can also reveal from the results that the proposed HGNN is superior when dealing with multi-modal data compared with existing methods.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/HXN5XWWR/Feng et al_2018_Hypergraph Neural Networks.pdf;/Users/vitay/Documents/Zotero/storage/8ETUJMMW/1809.html}
}

@article{Feng2019,
  title = {{{MeshNet}}: {{Mesh Neural Network}} for {{3D Shape Representation}}},
  shorttitle = {{{MeshNet}}},
  author = {Feng, Yutong and Feng, Yifan and You, Haoxuan and Zhao, Xibin and Gao, Yue},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {8279--8286},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33018279},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/4840},
  urldate = {2022-05-23},
  abstract = {Mesh is an important and powerful type of data for 3D shapes and widely studied in the field of computer vision and computer graphics. Regarding the task of 3D shape representation, there have been extensive research efforts concentrating on how to represent 3D shapes well using volumetric grid, multi-view and point cloud. However, there is little effort on using mesh data in recent years, due to the complexity and irregularity of mesh data. In this paper, we propose a mesh neural network, named MeshNet, to learn 3D shape representation from mesh data. In this method, face-unit and feature splitting are introduced, and a general architecture with available and effective blocks are proposed. In this way, MeshNet is able to solve the complexity and irregularity problem of mesh and conduct 3D shape representation well. We have applied the proposed MeshNet method in the applications of 3D shape classification and retrieval. Experimental results and comparisons with the state-of-the-art methods demonstrate that the proposed MeshNet can achieve satisfying 3D shape classification and retrieval performance, which indicates the effectiveness of the proposed method on 3D shape representation.},
  issue = {01},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/JA4ZIFUU/Feng_et_al_2019_MeshNet.pdf;/Users/vitay/Documents/Zotero/storage/IEFMR9ZF/1811.html;/Users/vitay/Documents/Zotero/storage/VMR9BL3X/1811.html}
}

@online{Fernando2017,
  title = {Self-{{Supervised Video Representation Learning With Odd-One-Out Networks}}},
  author = {Fernando, Basura and Bilen, Hakan and Gavves, Efstratios and Gould, Stephen},
  date = {2017-04-05},
  eprint = {1611.06646},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1611.06646},
  url = {http://arxiv.org/abs/1611.06646},
  urldate = {2023-01-18},
  abstract = {We propose a new self-supervised CNN pre-training technique based on a novel auxiliary task called "odd-one-out learning". In this task, the machine is asked to identify the unrelated or odd element from a set of otherwise related elements. We apply this technique to self-supervised video representation learning where we sample subsequences from videos and ask the network to learn to predict the odd video subsequence. The odd video subsequence is sampled such that it has wrong temporal order of frames while the even ones have the correct temporal order. Therefore, to generate a odd-one-out question no manual annotation is required. Our learning machine is implemented as multi-stream convolutional neural network, which is learned end-to-end. Using odd-one-out networks, we learn temporal representations for videos that generalizes to other related tasks such as action recognition. On action classification, our method obtains 60.3\textbackslash\% on the UCF101 dataset using only UCF101 data for training which is approximately 10\% better than current state-of-the-art self-supervised learning methods. Similarly, on HMDB51 dataset we outperform self-supervised state-of-the art methods by 12.7\% on action classification task.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/QTAI4CP4/Fernando_et_al_2017_Self-Supervised_Video_Representation_Learning_With_Odd-One-Out_Networks.pdf}
}

@article{Finn2015,
  title = {Deep {{Spatial Autoencoders}} for {{Visuomotor Learning}}},
  author = {Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter},
  date = {2015-09},
  url = {http://arxiv.org/abs/1509.06113},
  abstract = {Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm.}
}

@article{Friston2021,
  title = {World Model Learning and Inference},
  author = {Friston, Karl and Moran, Rosalyn J. and Nagai, Yukie and Taniguchi, Tadahiro and Gomi, Hiroaki and Tenenbaum, Josh},
  date = {2021-12-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {144},
  pages = {573--590},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.09.011},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608021003610},
  urldate = {2022-07-28},
  abstract = {Understanding information processing in the brain—and creating general-purpose artificial intelligence—are long-standing aspirations of scientists and engineers worldwide. The distinctive features of human intelligence are high-level cognition and control in various interactions with the world including the self, which are not defined in advance and are vary over time. The challenge of building human-like intelligent machines, as well as progress in brain science and behavioural analyses, robotics, and their associated theoretical formalisations, speaks to the importance of the world-model learning and inference. In this article, after briefly surveying the history and challenges of internal model learning and probabilistic learning, we introduce the free energy principle, which provides a useful framework within which to consider neuronal computation and probabilistic world models. Next, we showcase examples of human behaviour and cognition explained under that principle. We then describe symbol emergence in the context of probabilistic modelling, as a topic at the frontiers of cognitive robotics. Lastly, we review recent progress in creating human-like intelligence by using novel probabilistic programming languages. The striking consensus that emerges from these studies is that probabilistic descriptions of learning and inference are powerful and effective ways to create human-like artificial intelligent machines and to understand intelligence in the context of how humans interact with their world.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/QI92ZQNT/Friston_et_al_2021_World_model_learning_and_inference.pdf;/Users/vitay/Documents/Zotero/storage/7BTT9BQV/S0893608021003610.html}
}

@article{Fukushima1980,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  shorttitle = {Neocognitron},
  author = {Fukushima, Kunihiko},
  date = {1980-04-01},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybernetics},
  volume = {36},
  number = {4},
  pages = {193--202},
  issn = {1432-0770},
  doi = {10.1007/BF00344251},
  url = {https://doi.org/10.1007/BF00344251},
  urldate = {2020-11-22},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/NPZTYNF2/Fukushima_1980_Neocognitron.pdf}
}

@article{Gao2020,
  title = {A {{Survey}} on {{Deep Learning}} for {{Multimodal Data Fusion}}},
  author = {Gao, Jing and Li, Peng and Chen, Zhikui and Zhang, Jianing},
  date = {2020-05-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {32},
  number = {5},
  pages = {829--864},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01273},
  url = {https://doi.org/10.1162/neco_a_01273},
  urldate = {2022-09-15},
  abstract = {With the wide deployments of heterogeneous networks, huge amounts of data with characteristics of high volume, high variety, high velocity, and high veracity are generated. These data, referred to multimodal big data, contain abundant intermodality and cross-modality information and pose vast challenges on traditional data fusion methods. In this review, we present some pioneering deep learning models to fuse these multimodal big data. With the increasing exploration of the multimodal big data, there are still some challenges to be addressed. Thus, this review presents a survey on deep learning for multimodal data fusion to provide readers, regardless of their original community, with the fundamentals of multimodal deep learning fusion method and to motivate new multimodal data fusion techniques of deep learning. Specifically, representative architectures that are widely used are summarized as fundamental to the understanding of multimodal deep learning. Then the current pioneering multimodal data fusion deep learning models are summarized. Finally, some challenges and future topics of multimodal data fusion deep learning models are described.},
  file = {/Users/vitay/Documents/Zotero/storage/LDMVJY9K/Gao_et_al_2020_A_Survey_on_Deep_Learning_for_Multimodal_Data_Fusion.pdf;/Users/vitay/Documents/Zotero/storage/WLKMNVSE/A-Survey-on-Deep-Learning-for-Multimodal-Data.html}
}

@unpublished{Gatys2015,
  title = {A {{Neural Algorithm}} of {{Artistic Style}}},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  date = {2015-09-02},
  eprint = {1508.06576},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/1508.06576},
  urldate = {2021-04-21},
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/vitay/Documents/Zotero/storage/G9KA89KB/Gatys_et_al_2015_A_Neural_Algorithm_of_Artistic_Style.pdf;/Users/vitay/Documents/Zotero/storage/98B7VLES/1508.html}
}

@unpublished{Geirhos2020,
  title = {Shortcut {{Learning}} in {{Deep Neural Networks}}},
  author = {Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
  date = {2020-04-16},
  eprint = {2004.07780},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2004.07780},
  urldate = {2020-04-17},
  abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning's problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/Users/vitay/Documents/Zotero/storage/LEXN7VML/Geirhos_et_al_2020_Shortcut_Learning_in_Deep_Neural_Networks.pdf;/Users/vitay/Documents/Zotero/storage/L2A95H3Q/2004.html}
}

@inproceedings{Gers2000,
  title = {Recurrent Nets That Time and Count},
  booktitle = {Proceedings of the {{IEEE-INNS-ENNS International Joint Conference}} on {{Neural Networks}}. {{IJCNN}} 2000. {{Neural Computing}}: {{New Challenges}} and {{Perspectives}} for the {{New Millennium}}},
  author = {Gers, F. A. and Schmidhuber, J.},
  date = {2000-07},
  volume = {3},
  pages = {189-194 vol.3},
  issn = {1098-7576},
  doi = {10.1109/IJCNN.2000.861302},
  abstract = {The size of the time intervals between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While hidden Markov models tend to ignore this information, recurrent neural networks (RNN) can in principle learn to make use of it. We focus on long short-term memory (LSTM) because it usually outperforms other RNN. Surprisingly, LSTM augmented by "peephole connections" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps, without the help of any short training exemplars. Without external resets or teacher forcing or loss of performance on tasks reported earlier, our LSTM variant also learns to generate very stable sequences of highly nonlinear, precisely timed spikes. This makes LSTM a promising approach for real-world tasks that require to time and count.},
  eventtitle = {Proceedings of the {{IEEE-INNS-ENNS International Joint Conference}} on {{Neural Networks}}. {{IJCNN}} 2000. {{Neural Computing}}: {{New Challenges}} and {{Perspectives}} for the {{New Millennium}}},
  keywords = {counting,counting circuits,Delay,discrete time steps,Event detection,Hidden Markov models,highly-nonlinear precisely-timed spike sequences,Humans,internal cells,learning (artificial intelligence),long short-term memory,LSTM,motor control,Motor drives,multiplicative gates,Pattern recognition,peephole connections,Performance loss,recurrent neural nets,recurrent neural networks,Recurrent neural networks,Rhythm,rhythm detection,RNN,sequences,sequential tasks,stable sequences,time intervals,timing,World Wide Web},
  file = {/Users/vitay/Documents/Zotero/storage/HEX4WIFM/861302.html}
}

@thesis{Gers2001,
  title = {Long {{Short-Term Memory}} in {{Recurrent Neural Networks}}},
  author = {Gers, Felix},
  date = {2001},
  url = {http://www.felixgers.de/papers/phd.pdf}
}

@article{Gerstner2002,
  title = {Mathematical Formulations of {{Hebbian}} Learning},
  author = {Gerstner, Wulfram and Kistler, Werner M.},
  date = {2002-12-01},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  volume = {87},
  number = {5},
  pages = {404--415},
  issn = {1432-0770},
  doi = {10.1007/s00422-002-0353-y},
  url = {https://doi.org/10.1007/s00422-002-0353-y},
  urldate = {2022-03-08},
  abstract = {Several formulations of correlation-based Hebbian learning are reviewed. On the presynaptic side, activity is described either by a firing rate or by presynaptic spike arrival. The state of the postsynaptic neuron can be described by its membrane potential, its firing rate, or the timing of backpropagating action potentials (BPAPs). It is shown that all of the above formulations can be derived from the point of view of an expansion. In the absence of BPAPs, it is natural to correlate presynaptic spikes with the postsynaptic membrane potential. Time windows of spike-time-dependent plasticity arise naturally if the timing of postsynaptic spikes is available at the site of the synapse, as is the case in the presence of BPAPs. With an appropriate choice of parameters, Hebbian synaptic plasticity has intrinsic normalization properties that stabilizes postsynaptic firing rates and leads to subtractive weight normalization.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/8EEG56BE/Gerstner_Kistler_2002_Mathematical_formulations_of_Hebbian_learning.pdf}
}

@book{Gerstner2014,
  title = {Neuronal {{Dynamics}} - a Neuroscience Textbook},
  author = {Gerstner, Wulfram and Kistler, Werner and Naud, Richard and Paninski, Liam},
  date = {2014},
  publisher = {Cambridge University Press.},
  url = {https://neuronaldynamics.epfl.ch/index.html},
  urldate = {2020-09-10},
  file = {/Users/vitay/Documents/Zotero/storage/X56SFVR9/index.html}
}

@online{Gidaris2018,
  title = {Unsupervised {{Representation Learning}} by {{Predicting Image Rotations}}},
  author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  date = {2018-03-20},
  eprint = {1803.07728},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1803.07728},
  url = {http://arxiv.org/abs/1803.07728},
  urldate = {2023-01-18},
  abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4\% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/MKFMAXTZ/Gidaris_et_al_2018_Unsupervised_Representation_Learning_by_Predicting_Image_Rotations.pdf}
}

@unpublished{Gilpin2018,
  ids = {Gilpin2018a},
  title = {Explaining {{Explanations}}: {{An Overview}} of {{Interpretability}} of {{Machine Learning}}},
  shorttitle = {Explaining {{Explanations}}},
  author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  date = {2018-05-31},
  eprint = {1806.00069},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.00069},
  urldate = {2019-03-21},
  abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/HDAQG7VF/Gilpin et al_2018_Explaining Explanations.pdf;/Users/vitay/Documents/Zotero/storage/M6939SBF/Gilpin et al_2018_Explaining Explanations.pdf;/Users/vitay/Documents/Zotero/storage/8V9ISM8M/1806.html;/Users/vitay/Documents/Zotero/storage/STWCKKHR/1806.html}
}

@unpublished{Girshick2014,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014-10-22},
  eprint = {1311.2524},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1311.2524},
  urldate = {2020-11-29},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/\textasciitilde rbg/rcnn.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/RXXRNLR2/Girshick_et_al_2014_Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation.pdf;/Users/vitay/Documents/Zotero/storage/XYP35KJ4/1311.html}
}

@unpublished{Girshick2015,
  title = {Fast {{R-CNN}}},
  author = {Girshick, Ross},
  date = {2015-09-27},
  eprint = {1504.08083},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1504.08083},
  urldate = {2020-11-29},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/VAZSCTRW/Girshick_2015_Fast_R-CNN.pdf;/Users/vitay/Documents/Zotero/storage/MU5JGWJV/1504.html}
}

@unpublished{Gkioxari2020,
  title = {Mesh {{R-CNN}}},
  author = {Gkioxari, Georgia and Malik, Jitendra and Johnson, Justin},
  date = {2020-01-25},
  eprint = {1906.02739},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1906.02739},
  urldate = {2020-02-19},
  abstract = {Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/DBBIEP6Z/Gkioxari_et_al_2020_Mesh_R-CNN.pdf;/Users/vitay/Documents/Zotero/storage/C6DUH745/1906.html}
}

@inproceedings{Glorot2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {{{AISTATS}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  date = {2010},
  pages = {8},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/RLASQJXL/Glorot and Bengio - Understanding the difﬁculty of training deep feedf.pdf}
}

@book{Goertzel2007,
  title = {Artificial {{General Intelligence}}},
  editor = {Goertzel, Ben and Pennachin, Cassio},
  editora = {Gabbay, Dov M. and Siekmann, Jörg and Bundy, A. and Carbonell, J. G. and Pinkal, M. and Uszkoreit, H. and Veloso, M. and Wahlster, W. and Wooldridge, M. J.},
  editoratype = {redactor},
  date = {2007},
  series = {Cognitive {{Technologies}}},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-68677-4},
  url = {http://link.springer.com/10.1007/978-3-540-68677-4},
  urldate = {2022-09-13},
  isbn = {978-3-540-23733-4 978-3-540-68677-4}
}

@unpublished{Goodfellow2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06},
  eprint = {1406.2661},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1406.2661},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.}
}

@unpublished{Goodfellow2015,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  date = {2015-03-20},
  eprint = {1412.6572},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1412.6572},
  urldate = {2021-01-14},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/VCK4LKIH/Goodfellow_et_al_2015_Explaining_and_Harnessing_Adversarial_Examples.pdf;/Users/vitay/Documents/Zotero/storage/F3P8AFGB/1412.html}
}

@book{Goodfellow2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {MIT Press},
  url = {http://www.deeplearningbook.org}
}

@unpublished{Gou2020,
  title = {Knowledge {{Distillation}}: {{A Survey}}},
  shorttitle = {Knowledge {{Distillation}}},
  author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen John and Tao, Dacheng},
  date = {2020-06-09},
  eprint = {2006.05525},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.05525},
  urldate = {2021-01-13},
  abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
  version = {1},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/WB9JWW4V/Gou_et_al_2020_Knowledge_Distillation.pdf;/Users/vitay/Documents/Zotero/storage/RW4TACQK/2006.html}
}

@unpublished{Graves2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  date = {2014-10-20},
  eprint = {1410.5401},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1410.5401},
  urldate = {2019-03-03},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/vitay/Documents/Zotero/storage/VUUZ7C4E/Graves et al_2014_Neural Turing Machines.pdf}
}

@incollection{Grnarova2019,
  title = {A {{Domain Agnostic Measure}} for {{Monitoring}} and {{Evaluating GANs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Grnarova, Paulina and Levy, Kfir Y. and Lucchi, Aurelien and Perraudin, Nathanael and Goodfellow, Ian and Hofmann, Thomas and Krause, Andreas},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Alché-Buc, given=F., prefix=d\textbackslash textquotesingle, useprefix=false and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {12069--12079},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/9377-a-domain-agnostic-measure-for-monitoring-and-evaluating-gans.pdf},
  urldate = {2019-11-19},
  file = {/Users/vitay/Documents/Zotero/storage/H527EUDZ/Grnarova et al_2019_A Domain Agnostic Measure for Monitoring and Evaluating GANs.pdf;/Users/vitay/Documents/Zotero/storage/TVEYJXWN/9377-a-domain-agnostic-measure-for-monitoring-and-evaluating-gans.html}
}

@inproceedings{Gu2017a,
  title = {Dynamic {{Facial Analysis}}: {{From Bayesian Filtering}} to {{Recurrent Neural Network}}},
  shorttitle = {Dynamic {{Facial Analysis}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Gu, J. and Yang, X. and Mello, S. D. and Kautz, J.},
  date = {2017-07},
  pages = {1531--1540},
  doi = {10.1109/CVPR.2017.167},
  abstract = {Facial analysis in videos, including head pose estimation and facial landmark localization, is key for many applications such as facial animation capture, human activity recognition, and human-computer interaction. In this paper, we propose to use a recurrent neural network (RNN) for joint estimation and tracking of facial features in videos. We are inspired by the fact that the computation performed in an RNN bears resemblance to Bayesian filters, which have been used for tracking in many previous methods for facial analysis from videos. Bayesian filters used in these methods, however, require complicated, problem-specific design and tuning. In contrast, our proposed RNN-based method avoids such tracker-engineering by learning from training data, similar to how a convolutional neural network (CNN) avoids feature-engineering for image classification. As an end-to-end network, the proposed RNN-based method provides a generic and holistic solution for joint estimation and tracking of various types of facial features from consecutive video frames. Extensive experimental results on head pose estimation and facial landmark localization from videos demonstrate that the proposed RNN-based method outperforms frame-wise models and Bayesian filtering. In addition, we create a large-scale synthetic dataset for head pose estimation, with which we achieve state-of-the-art performance on a benchmark dataset.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Bayes methods,Bayesian filtering,CNN,computer animation,consecutive video frames,convolutional neural network,dynamic facial analysis,end-to-end network,face recognition,facial animation capture,facial features,facial landmark localization,feature extraction,Head,head pose estimation,human activity recognition,human-computer interaction,image classification,image filtering,Kalman filters,learning (artificial intelligence),pose estimation,Pose estimation,recurrent neural nets,recurrent neural network,RNN-based method,Videos},
  file = {/Users/vitay/Documents/Zotero/storage/P6KERDDG/Gu et al_2017_Dynamic Facial Analysis.pdf;/Users/vitay/Documents/Zotero/storage/2DWZ4Y9N/8099650.html}
}

@inproceedings{Guo2017,
  title = {Deep {{Clustering}} with {{Convolutional Autoencoders}}},
  booktitle = {Neural {{Information Processing}}},
  author = {Guo, Xifeng and Liu, Xinwang and Zhu, En and Yin, Jianping},
  editor = {Liu, Derong and Xie, Shengli and Li, Yuanqing and Zhao, Dongbin and El-Alfy, El-Sayed M.},
  date = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {373--382},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-70096-0_39},
  abstract = {Deep clustering utilizes deep neural networks to learn feature representation that is suitable for clustering tasks. Though demonstrating promising performance in various applications, we observe that existing deep clustering algorithms either do not well take advantage of convolutional neural networks or do not considerably preserve the local structure of data generating distribution in the learned feature space. To address this issue, we propose a deep convolutional embedded clustering algorithm in this paper. Specifically, we develop a convolutional autoencoders structure to learn embedded features in an end-to-end way. Then, a clustering oriented loss is directly built on embedded features to jointly perform feature refinement and cluster assignment. To avoid feature space being distorted by the clustering loss, we keep the decoder remained which can preserve local structure of data in feature space. In sum, we simultaneously minimize the reconstruction loss of convolutional autoencoders and the clustering loss. The resultant optimization problem can be effectively solved by mini-batch stochastic gradient descent and back-propagation. Experiments on benchmark datasets empirically validate the power of convolutional autoencoders for feature learning and the effectiveness of local structure preservation.},
  isbn = {978-3-319-70096-0},
  langid = {english},
  keywords = {Convolutional autoencoders,Convolutional neural networks,Deep clustering,Unsupervised learning},
  file = {/Users/vitay/Documents/Zotero/storage/RZQ8E3MT/Guo_et_al_2017_Deep_Clustering_with_Convolutional_Autoencoders.pdf}
}

@unpublished{Guo2018,
  title = {An Interpretable {{LSTM}} Neural Network for Autoregressive Exogenous Model},
  author = {Guo, Tian and Lin, Tao and Lu, Yao},
  date = {2018-04-14},
  eprint = {1804.05251},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.05251},
  urldate = {2019-03-21},
  abstract = {In this paper, we propose an interpretable LSTM recurrent neural network, i.e., multi-variable LSTM for time series with exogenous variables. Currently, widely used attention mechanism in recurrent neural networks mostly focuses on the temporal aspect of data and falls short of characterizing variable importance. To this end, our multi-variable LSTM equipped with tensorized hidden states is developed to learn variable specific representations, which give rise to both temporal and variable level attention. Preliminary experiments demonstrate comparable prediction performance of multi-variable LSTM w.r.t. encoder-decoder based baselines. More interestingly, variable importance in real datasets characterized by the variable attention is highly in line with that determined by statistical Granger causality test, which exhibits the prospect of multi-variable LSTM as a simple and uniform end-to-end framework for both forecasting and knowledge discovery.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/LQ6N2A5R/Guo et al_2018_An interpretable LSTM neural network for autoregressive exogenous model.pdf;/Users/vitay/Documents/Zotero/storage/YPLPITH5/1804.html}
}

@article{Guo2019,
  title = {High {{Efficient Deep Feature Extraction}} and {{Classification}} of {{Spectral-Spatial Hyperspectral Image Using Cross Domain Convolutional Neural Networks}}},
  author = {Guo, Y. and Cao, H. and Bai, J. and Bai, Y.},
  date = {2019-01},
  journaltitle = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume = {12},
  number = {1},
  pages = {345--356},
  doi = {10.1109/JSTARS.2018.2888808},
  abstract = {Recently, numerous remote sensing applications highly depend on the hyperspectral image (HSI). HSI classification, as a fundamental issue, has attracted increasing attention and become a hot topic in the remote sensing community. We implemented a regularized convolutional neural network (CNN), which adopted dropout and regularization strategies to address the overfitting problem of limited training samples. Although many kinds of the literature have confirmed that it is an effective way for HSI classification to integrate spectrum with spatial context, the scaling issue is not fully exploited. In this paper, we propose a high efficient deep feature extraction and the classification method for the spectral-spatial HSI, which can make full use of multiscale spatial feature obtained by guided filter. The proposed approach is the first attempt to lean a CNN for spectral and multiscale spatial features. Compared to its counterparts, experimental results show that the proposed method can achieve 3\% improvement in accuracy, according to various datasets such as Indian Pines, Pavia University, and Salinas.},
  keywords = {classification method,CNN,Computer science,convolutional neural nets,Convolutional neural network (CNN),cross domain convolutional neural networks,Deep learning,dropout strategies,feature extraction,Feature extraction,geophysical image processing,geophysical techniques,guided filter,high efficient deep feature extraction,HSI classification,hyperspectral image (HSI) classification,hyperspectral imaging,Hyperspectral imaging,image classification,Indian Pines,multiscale spatial feature,multiscale spatial features,Pavia University,Principal component analysis,regularization strategies,regularized convolutional neural network,remote sensing,remote sensing applications,remote sensing community,Salinas,spectral features,spectral-spatial fusion,spectral-spatial HSI,spectral-spatial hyperspectral image classification},
  file = {/Users/vitay/Documents/Zotero/storage/EULZMJT5/8602463.html}
}

@unpublished{Gupta2014,
  title = {Learning {{Rich Features}} from {{RGB-D Images}} for {{Object Detection}} and {{Segmentation}}},
  author = {Gupta, Saurabh and Girshick, Ross and Arbeláez, Pablo and Malik, Jitendra},
  date = {2014-07-22},
  eprint = {1407.5736},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1407.5736},
  urldate = {2020-11-29},
  abstract = {In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3\%, which is a 56\% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24\% relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/KNWID3SE/Gupta_et_al_2014_Learning_Rich_Features_from_RGB-D_Images_for_Object_Detection_and_Segmentation.pdf;/Users/vitay/Documents/Zotero/storage/YBQ27GEX/1407.html}
}

@unpublished{Gupta2022,
  title = {Semi-{{Supervised Cascaded Clustering}} for {{Classification}} of {{Noisy Label Data}}},
  author = {Gupta, Ashit and Deodhar, Anirudh and Mukherjee, Tathagata and Runkana, Venkataramana},
  date = {2022-05-04},
  eprint = {2205.02209},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.02209},
  urldate = {2022-05-05},
  abstract = {The performance of supervised classification techniques often deteriorates when the data has noisy labels. Even the semi-supervised classification approaches have largely focused only on the problem of handling missing labels. Most of the approaches addressing the noisy label data rely on deep neural networks (DNN) that require huge datasets for classification tasks. This poses a serious challenge especially in process and manufacturing industries, where the data is limited and labels are noisy. We propose a semi-supervised cascaded clustering (SSCC) algorithm to extract patterns and generate a cascaded tree of classes in such datasets. A novel cluster evaluation matrix (CEM) with configurable hyperparameters is introduced to localize and eliminate the noisy labels and invoke a pruning criterion on cascaded clustering. The algorithm reduces the dependency on expensive human expertise for assessing the accuracy of labels. A classifier generated based on SSCC is found to be accurate and consistent even when trained on noisy label datasets. It performed better in comparison with the support vector machines (SVM) when tested on multiple noisy-label datasets, including an industrial dataset. The proposed approach can be effectively used for deriving actionable insights in industrial settings with minimal human expertise.},
  file = {/Users/vitay/Documents/Zotero/storage/97W6GZCU/Gupta_et_al_2022_Semi-Supervised_Cascaded_Clustering_for_Classification_of_Noisy_Label_Data.pdf;/Users/vitay/Documents/Zotero/storage/QGAUPKIF/2205.html}
}

@unpublished{Ha2017,
  title = {A {{Neural Representation}} of {{Sketch Drawings}}},
  author = {Ha, David and Eck, Douglas},
  date = {2017-05-19},
  eprint = {1704.03477},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1704.03477},
  urldate = {2021-01-17},
  abstract = {We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on thousands of crude human-drawn images representing hundreds of classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/TSV3FA5T/Ha_Eck_2017_A_Neural_Representation_of_Sketch_Drawings.pdf;/Users/vitay/Documents/Zotero/storage/YN8FX3C3/Ha_Eck_2017_A_Neural_Representation_of_Sketch_Drawings.pdf;/Users/vitay/Documents/Zotero/storage/4SQZXAR7/1704.html;/Users/vitay/Documents/Zotero/storage/Y6UDXLR8/1704.html}
}

@article{Hadsell2020,
  title = {Embracing {{Change}}: {{Continual Learning}} in {{Deep Neural Networks}}},
  shorttitle = {Embracing {{Change}}},
  author = {Hadsell, Raia and Rao, Dushyant and Rusu, Andrei A. and Pascanu, Razvan},
  date = {2020-12-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {12},
  eprint = {33158755},
  eprinttype = {pmid},
  pages = {1028--1040},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2020.09.004},
  url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(20)30219-9},
  urldate = {2020-11-14},
  langid = {english},
  keywords = {artificial intelligence,lifelong,memory,meta-learning,non-stationary},
  file = {/Users/vitay/Documents/Zotero/storage/XBWISPPP/Hadsell_et_al_2020_Embracing_Change.pdf;/Users/vitay/Documents/Zotero/storage/UGYAHNM2/S1364-6613(20)30219-9.html}
}

@unpublished{Hannun2014,
  title = {Deep {{Speech}}: {{Scaling}} up End-to-End Speech Recognition},
  shorttitle = {Deep {{Speech}}},
  author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
  date = {2014-12-19},
  eprint = {1412.5567},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1412.5567},
  urldate = {2020-11-22},
  abstract = {We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/vitay/Documents/Zotero/storage/A4723API/Hannun_et_al_2014_Deep_Speech.pdf;/Users/vitay/Documents/Zotero/storage/55W6QQIJ/1412.html}
}

@article{Hanocka2019,
  title = {{{MeshCNN}}: A Network with an Edge},
  shorttitle = {{{MeshCNN}}},
  author = {Hanocka, Rana and Hertz, Amir and Fish, Noa and Giryes, Raja and Fleishman, Shachar and Cohen-Or, Daniel},
  date = {2019-07-12},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {38},
  number = {4},
  pages = {90:1--90:12},
  issn = {0730-0301},
  doi = {10.1145/3306346.3322959},
  url = {https://doi.org/10.1145/3306346.3322959},
  urldate = {2022-05-23},
  abstract = {Polygonal meshes provide an efficient representation for 3D shapes. They explicitly captureboth shape surface and topology, and leverage non-uniformity to represent large flat regions as well as sharp, intricate features. This non-uniformity and irregularity, however, inhibits mesh analysis efforts using neural networks that combine convolution and pooling operations. In this paper, we utilize the unique properties of the mesh for a direct analysis of 3D shapes using MeshCNN, a convolutional neural network designed specifically for triangular meshes. Analogous to classic CNNs, MeshCNN combines specialized convolution and pooling layers that operate on the mesh edges, by leveraging their intrinsic geodesic connections. Convolutions are applied on edges and the four edges of their incident triangles, and pooling is applied via an edge collapse operation that retains surface topology, thereby, generating new mesh connectivity for the subsequent convolutions. MeshCNN learns which edges to collapse, thus forming a task-driven process where the network exposes and expands the important features while discarding the redundant ones. We demonstrate the effectiveness of MeshCNN on various learning tasks applied to 3D meshes.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/LMAXZLTY/Hanocka_et_al_2019_MeshCNN.pdf;/Users/vitay/Documents/Zotero/storage/P96LDI48/1809.html}
}

@article{Hanocka2020,
  title = {{{Point2Mesh}}: {{A Self-Prior}} for {{Deformable Meshes}}},
  shorttitle = {{{Point2Mesh}}},
  author = {Hanocka, Rana and Metzer, Gal and Giryes, Raja and Cohen-Or, Daniel},
  date = {2020-07-08},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {39},
  number = {4},
  eprint = {2005.11084},
  eprinttype = {arXiv},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3386569.3392415},
  url = {http://arxiv.org/abs/2005.11084},
  urldate = {2021-05-19},
  abstract = {In this paper, we introduce Point2Mesh, a technique for reconstructing a surface mesh from an input point cloud. Instead of explicitly specifying a prior that encodes the expected shape properties, the prior is defined automatically using the input point cloud, which we refer to as a self-prior. The self-prior encapsulates reoccurring geometric repetitions from a single shape within the weights of a deep neural network. We optimize the network weights to deform an initial mesh to shrink-wrap a single input point cloud. This explicitly considers the entire reconstructed shape, since shared local kernels are calculated to fit the overall object. The convolutional kernels are optimized globally across the entire shape, which inherently encourages local-scale geometric self-similarity across the shape surface. We show that shrink-wrapping a point cloud with a self-prior converges to a desirable solution; compared to a prescribed smoothness prior, which often becomes trapped in undesirable local minima. While the performance of traditional reconstruction approaches degrades in non-ideal conditions that are often present in real world scanning, i.e., unoriented normals, noise and missing (low density) parts, Point2Mesh is robust to non-ideal conditions. We demonstrate the performance of Point2Mesh on a large variety of shapes with varying complexity.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/8ETWNDCN/Hanocka_et_al_2020_Point2Mesh.pdf;/Users/vitay/Documents/Zotero/storage/NH2ENXVJ/2005.html}
}

@book{Haykin2009,
  title = {Neural {{Networks}} and {{Learning Machines}}, 3rd {{Edition}}},
  author = {Haykin, Simon S.},
  date = {2009},
  publisher = {Pearson},
  url = {http://dai.fmph.uniba.sk/courses/NN/haykin.neural-networks.3ed.2009.pdf},
  urldate = {2020-09-10},
  abstract = {Neural Networks and Learning Machines, 3rd Edition},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/9SUPHKDU/PGM320370.html}
}

@unpublished{He2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12},
  eprint = {1512.03385},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1512.03385},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}
}

@unpublished{He2015a,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-02-06},
  eprint = {1502.01852},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1502.01852},
  urldate = {2020-11-17},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/X9FZBPYK/He_et_al_2015_Delving_Deep_into_Rectifiers.pdf;/Users/vitay/Documents/Zotero/storage/EDF4VNIL/1502.html}
}

@unpublished{He2018,
  title = {Mask {{R-CNN}}},
  author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
  date = {2018-01-24},
  eprint = {1703.06870},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.06870},
  urldate = {2020-09-17},
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/F2FNPD29/He_et_al_2018_Mask_R-CNN.pdf;/Users/vitay/Documents/Zotero/storage/2UJSG5K6/1703.html}
}

@unpublished{He2020,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  date = {2020-03-23},
  eprint = {1911.05722},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1911.05722},
  urldate = {2021-01-26},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/MGFTBYNV/He_et_al_2020_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning.pdf;/Users/vitay/Documents/Zotero/storage/QWADDD2T/1911.html}
}

@inproceedings{Higgins2016,
  title = {Beta-{{VAE}}: {{Learning Basic Visual Concepts}} with a {{Constrained Variational Framework}}},
  shorttitle = {Beta-{{VAE}}},
  booktitle = {{{ICLR}} 2017},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  date = {2016-11-04},
  url = {https://openreview.net/forum?id=Sy2fzU9gl},
  urldate = {2020-12-05},
  abstract = {We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/RGVLDPA6/Higgins_et_al_2016_beta-VAE.pdf;/Users/vitay/Documents/Zotero/storage/N3VDNA5G/forum.html;/Users/vitay/Documents/Zotero/storage/TDP4WFGI/forum.html}
}

@article{Hinton2006,
  title = {A Fast Learning Algorithm for Deep Belief Nets},
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  date = {2006-07-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Comput.},
  volume = {18},
  number = {7},
  pages = {1527--1554},
  issn = {0899-7667},
  doi = {10.1162/neco.2006.18.7.1527},
  url = {https://doi.org/10.1162/neco.2006.18.7.1527},
  urldate = {2021-01-06},
  abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.}
}

@article{Hinton2006a,
  title = {Reducing the {{Dimensionality}} of {{Data}} with {{Neural Networks}}},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  date = {2006-07-28},
  journaltitle = {Science},
  volume = {313},
  number = {5786},
  eprint = {16873662},
  eprinttype = {pmid},
  pages = {504--507},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1127647},
  url = {https://science.sciencemag.org/content/313/5786/504},
  urldate = {2021-01-06},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data. Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks. Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/ZITMPFQE/Hinton_Salakhutdinov_2006_Reducing_the_Dimensionality_of_Data_with_Neural_Networks.pdf;/Users/vitay/Documents/Zotero/storage/84XBQ5BR/504.html;/Users/vitay/Documents/Zotero/storage/VPEXNHT2/504.html}
}

@unpublished{Hinton2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  date = {2015-03-09},
  eprint = {1503.02531},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1503.02531},
  urldate = {2021-01-13},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/9XCTMFZG/Hinton_et_al_2015_Distilling_the_Knowledge_in_a_Neural_Network.pdf;/Users/vitay/Documents/Zotero/storage/FDVYKD9Y/1503.html}
}

@online{Ho2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-12-16},
  eprint = {2006.11239},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2006.11239},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2024-09-30},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/G9E6K6AD/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@thesis{Hochreiter1991,
  title = {Untersuchungen Zu Dynamischen Neuronalen {{Netzen}}},
  author = {Hochreiter, Sepp},
  date = {1991},
  institution = {TU München},
  url = {http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf}
}

@article{Hochreiter1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-11-15},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://doi.org/10.1162/neco.1997.9.8.1735},
  urldate = {2023-08-04},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@article{Hopfield1982,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities},
  author = {Hopfield, J. J.},
  date = {1982-04-01},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {79},
  number = {8},
  eprint = {6953413},
  eprinttype = {pmid},
  pages = {2554--2558},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.79.8.2554},
  url = {https://www.pnas.org/content/79/8/2554},
  urldate = {2021-01-20},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/UQJRLRCQ/Hopfield_1982_Neural_networks_and_physical_systems_with_emergent_collective_computational.pdf;/Users/vitay/Documents/Zotero/storage/L962K4AT/2554.html}
}

@article{Hopfield1983,
  title = {‘{{Unlearning}}’ Has a Stabilizing Effect in Collective Memories},
  author = {Hopfield, J. J. and Feinstein, D. I. and Palmer, R. G.},
  date = {1983-07},
  journaltitle = {Nature},
  volume = {304},
  number = {5922},
  pages = {158--159},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/304158a0},
  url = {https://www.nature.com/articles/304158a0},
  urldate = {2021-01-19},
  abstract = {Crick and Mitchison1 have presented a hypothesis for the functional role of dream sleep involving an ‘unlearning’ process. We have independently carried out mathematical and computer modelling of learning and ‘unlearning’ in a collective neural network of 30–1,000 neurones. The model network has a content-addressable memory or ‘associative memory’ which allows it to learn and store many memories. A particular memory can be evoked in its entirety when the network is stimulated by any adequate-sized subpart of the information of that memory2. But different memories of the same size are not equally easy to recall. Also, when memories are learned, spurious memories are also created and can also be evoked. Applying an ‘unlearning’ process, similar to the learning processes but with a reversed sign and starting from a noise input, enhances the performance of the network in accessing real memories and in minimizing spurious ones. Although our model was not motivated by higher nervous function, our system displays behaviours which are strikingly parallel to those needed for the hypothesized role of ‘unlearning’ in rapid eye movement (REM) sleep.},
  issue = {5922},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/V76GLL35/Hopfield_et_al_1983_‘Unlearning’_has_a_stabilizing_effect_in_collective_memories.pdf;/Users/vitay/Documents/Zotero/storage/SR2RHHYK/304158a0.html}
}

@online{Hu2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-10-16},
  eprint = {2106.09685},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.09685},
  url = {http://arxiv.org/abs/2106.09685},
  urldate = {2023-05-26},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/5DH7H278/Hu_et_al_2021_LoRA.pdf}
}

@online{Hu2021a,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-10-16},
  eprint = {2106.09685},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.09685},
  url = {http://arxiv.org/abs/2106.09685},
  urldate = {2023-06-06},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/97GAHJXQ/Hu_et_al_2021_LoRA.pdf}
}

@unpublished{Hua2017,
  title = {Pointwise {{Convolutional Neural Networks}}},
  author = {Hua, Binh-Son and Tran, Minh-Khoi and Yeung, Sai-Kit},
  date = {2017-12-14},
  eprint = {1712.05245},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1712.05245},
  urldate = {2019-01-24},
  abstract = {Deep learning with 3D data such as reconstructed point clouds and CAD models has received great research interests recently. However, the capability of using point clouds with convolutional neural network has been so far not fully explored. In this paper, we present a convolutional neural network for semantic segmentation and object recognition with 3D point clouds. At the core of our network is pointwise convolution, a new convolution operator that can be applied at each point of a point cloud. Our fully convolutional network design, while being surprisingly simple to implement, can yield competitive accuracy in both semantic segmentation and object recognition task.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/3APVWYA6/Hua et al_2017_Pointwise Convolutional Neural Networks.pdf;/Users/vitay/Documents/Zotero/storage/DESTWJKQ/1712.html}
}

@unpublished{Huang2018,
  title = {Densely {{Connected Convolutional Networks}}},
  author = {Huang, Gao and Liu, Zhuang and family=Maaten, given=Laurens, prefix=van der, useprefix=true and Weinberger, Kilian Q.},
  date = {2018-01-28},
  eprint = {1608.06993},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1608.06993},
  urldate = {2020-11-22},
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/V9HIY9DK/Huang_et_al_2018_Densely_Connected_Convolutional_Networks.pdf;/Users/vitay/Documents/Zotero/storage/F8DER3MG/1608.html}
}

@unpublished{Huang2020a,
  title = {{{MeshODE}}: {{A Robust}} and {{Scalable Framework}} for {{Mesh Deformation}}},
  shorttitle = {{{MeshODE}}},
  author = {Huang, Jingwei and Jiang, Chiyu Max and Leng, Baiqiang and Wang, Bin and Guibas, Leonidas},
  date = {2020-05-23},
  eprint = {2005.11617},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2005.11617},
  urldate = {2021-05-12},
  abstract = {We present MeshODE, a scalable and robust framework for pairwise CAD model deformation without prespecified correspondences. Given a pair of shapes, our framework provides a novel shape feature-preserving mapping function that continuously deforms one model to the other by minimizing fitting and rigidity losses based on the non-rigid iterative-closest-point (ICP) algorithm. We address two challenges in this problem, namely the design of a powerful deformation function and obtaining a feature-preserving CAD deformation. While traditional deformation directly optimizes for the coordinates of the mesh vertices or the vertices of a control cage, we introduce a deep bijective mapping that utilizes a flow model parameterized as a neural network. Our function has the capacity to handle complex deformations, produces deformations that are guaranteed free of self-intersections, and requires low rigidity constraining for geometry preservation, which leads to a better fitting quality compared with existing methods. It additionally enables continuous deformation between two arbitrary shapes without supervision for intermediate shapes. Furthermore, we propose a robust preprocessing pipeline for raw CAD meshes using feature-aware subdivision and a uniform graph template representation to address artifacts in raw CAD models including self-intersections, irregular triangles, topologically disconnected components, non-manifold edges, and nonuniformly distributed vertices. This facilitates a fast deformation optimization process that preserves global and local details. Our code is publicly available.},
  keywords = {Computer Science - Computational Geometry,Computer Science - Graphics},
  file = {/Users/vitay/Documents/Zotero/storage/JKJWU5ZF/Huang_et_al_2020_MeshODE.pdf;/Users/vitay/Documents/Zotero/storage/KRV8RQA3/2005.html}
}

@unpublished{Iandola2016,
  title = {{{SqueezeNet}}: {{AlexNet-level}} Accuracy with 50x Fewer Parameters and {$<$}0.{{5MB}} Model Size},
  shorttitle = {{{SqueezeNet}}},
  author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
  date = {2016-11-04},
  eprint = {1602.07360},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1602.07360},
  urldate = {2021-01-13},
  abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/HAI85UGL/Iandola_et_al_2016_SqueezeNet.pdf;/Users/vitay/Documents/Zotero/storage/793T6TWE/1602.html}
}

@article{Intrator1992,
  title = {Objective Function Formulation of the {{BCM}} Theory of Visual Cortical Plasticity: {{Statistical}} Connections, Stability Conditions},
  author = {Intrator, Nathan and Cooper, Leon N},
  date = {1992-01},
  journaltitle = {Neural Networks},
  volume = {5},
  number = {1},
  pages = {3--17},
  doi = {10.1016/S0893-6080(05)80003-6},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608005800036},
  abstract = {In this paper, we present an objective function formulation of the Bienenstock, Cooper, and Munro (BCM) theory of visual cortical plasticity that permits us to demonstrate the connection between the unsupervised BCM learning procedure and various statistical methods, in particular, that of Projection Pursuit. This formulation provides a general method for stability analysis of the fixed points of the theory and enables us to analyze the behavior and the evolution of the network under various visual rearing conditions. It also allows comparison with many existing unsupervised methods. This model has been shown successful in various applications such as phoneme and 3D object recognition. We thus have the striking and possibly highly significant result that a biological neuron is performing a sophisticated statistical procedure.},
  keywords = {Dimensionality reduction,Feature extraction,Unsupervised learning}
}

@unpublished{Ioffe2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  date = {2015-02},
  eprint = {1502.03167},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {http://arxiv.org/abs/1502.03167},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.}
}

@article{IsmailFawaz2020,
  title = {{{InceptionTime}}: {{Finding AlexNet}} for Time Series Classification},
  shorttitle = {{{InceptionTime}}},
  author = {Ismail Fawaz, Hassan and Lucas, Benjamin and Forestier, Germain and Pelletier, Charlotte and Schmidt, Daniel F. and Weber, Jonathan and Webb, Geoffrey I. and Idoumghar, Lhassane and Muller, Pierre-Alain and Petitjean, François},
  date = {2020-11-01},
  journaltitle = {Data Mining and Knowledge Discovery},
  shortjournal = {Data Min Knowl Disc},
  volume = {34},
  number = {6},
  pages = {1936--1962},
  issn = {1573-756X},
  doi = {10.1007/s10618-020-00710-y},
  url = {https://doi.org/10.1007/s10618-020-00710-y},
  urldate = {2020-12-01},
  abstract = {This paper brings deep learning at the forefront of research into time series classification (TSC). TSC is the area of machine learning tasked with the categorization (or labelling) of time series. The last few decades of work in this area have led to significant progress in the accuracy of classifiers, with the state of the art now represented by the HIVE-COTE algorithm. While extremely accurate, HIVE-COTE cannot be applied to many real-world datasets because of its high training time complexity in \$\$O(N\textasciicircum 2\textbackslash cdot T\textasciicircum 4)\$\$O(N2·T4)for a dataset with N time series of length T. For example, it takes HIVE-COTE more than 8 days to learn from a small dataset with \$\$N=1500\$\$N=1500time series of short length \$\$T=46\$\$T=46. Meanwhile deep learning has received enormous attention because of its high accuracy and scalability. Recent approaches to deep learning for TSC have been scalable, but less accurate than HIVE-COTE. We introduce InceptionTime—an ensemble of deep Convolutional Neural Network models, inspired by the Inception-v4 architecture. Our experiments show that InceptionTime is on par with HIVE-COTE in terms of accuracy while being much more scalable: not only can it learn from 1500 time series in one hour but it can also learn from 8M time series in 13 h, a quantity of data that is fully out of reach of HIVE-COTE.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/FIZL8EBA/Ismail_Fawaz_et_al_2020_InceptionTime.pdf}
}

@article{Isola2016,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  date = {2016-11},
  url = {http://arxiv.org/abs/1611.07004},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.}
}

@unpublished{Isola2018,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  date = {2018-11-26},
  eprint = {1611.07004},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.07004},
  urldate = {2020-12-07},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/EAV5AMUN/Isola_et_al_2018_Image-to-Image_Translation_with_Conditional_Adversarial_Networks.pdf;/Users/vitay/Documents/Zotero/storage/27QJF8QD/1611.html}
}

@article{Izhikevich2003,
  title = {Simple Model of Spiking Neurons.},
  author = {Izhikevich, E M},
  date = {2003-01},
  journaltitle = {IEEE transactions on neural networks},
  volume = {14},
  number = {6},
  eprint = {18244602},
  eprinttype = {pubmed},
  pages = {1569--72},
  doi = {10.1109/TNN.2003.820440},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/18244602},
  abstract = {A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin-Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.}
}

@report{Jaeger2001,
  title = {The "Echo State" Approach to Analysing and Training Recurrent Neural Networks},
  author = {Jaeger, Herbert},
  date = {2001},
  pages = {GMD Report 148-GMD Report 148},
  institution = {Jacobs Universität Bremen},
  url = {http://www.faculty.jacobs-university.de/hjaeger/pubs/EchoStatesTechRep.pdf}
}

@inproceedings{Jin2019,
  title = {Auto-Keras: {{An}} Efficient Neural Architecture Search System},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD}} International Conference on Knowledge Discovery \& Data Mining},
  author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
  date = {2019},
  pages = {1946--1956},
  organization = {ACM},
  file = {/Users/vitay/Documents/Zotero/storage/U3XNWB67/Jin_et_al_2019_Auto-keras.pdf}
}

@article{Jobe2018,
  title = {Geological {{Feature Prediction Using Image-Based Machine Learning}}},
  author = {Jobe, T. D. and Vital-Brazil, E. and Khait, M.},
  date = {2018-12-01},
  journaltitle = {Petrophysics},
  volume = {59},
  number = {06},
  pages = {750--760},
  issn = {1529-9074},
  url = {https://www.onepetro.org/journal-paper/SPWLA-2018-v59n6a1},
  urldate = {2019-09-27},
  abstract = {Abstract Petrographic description is one of the primary methods by which geoscientists develop an understanding of geologic systems and constrain source, reservoir, and seal characteristics. Geologic facies or rock types are routinely interpreted fr},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/UHWNY9VB/SPWLA-2018-v59n6a1.html}
}

@inproceedings{Joshi2009,
  title = {Rules for Information Maximization in Spiking Neurons Using Intrinsic Plasticity},
  booktitle = {2009 {{International Joint Conference}} on {{Neural Networks}}},
  author = {Joshi, P. and Triesch, J.},
  date = {2009-06},
  pages = {1456--1461},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2009.5178625},
  abstract = {Information theory predicts the need for information maximization as sensory information must be compressed into a limited range of responses that spiking neurons can generate. We propose computational theory and learning rules based on information theory that lead to information maximization using intrinsic plasticity in a stochastically spiking neuron model. Computer simulations are used to verify the theoretical results. Further experiments show that the intrinsic plasticity rules described in this article lead to a desired exponential output distribution, firing-rate homeostasis, and adaptation to sensory deprivation in our model as observed in cortical neurons.},
  eventtitle = {2009 {{International Joint Conference}} on {{Neural Networks}}},
  keywords = {Biomembranes,Cats,computational theory,Computer simulation,cortical neurons,Discrete transforms,exponential distribution,Exponential distribution,exponential output distribution,firing-rate homeostasis,information maximization,information theory,Information theory,intrinsic plasticity,learning rules,Mutual information,neural nets,Neural networks,Neurons,neurophysiology,optimisation,plasticity,sensory deprivation,sensory information,spiking neurons,stochastic processes,stochastically spiking neuron model,Videos},
  file = {/Users/vitay/Documents/Zotero/storage/JFV6ZPQ2/5178625.html}
}

@article{Jumper2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  date = {2021-07-15},
  journaltitle = {Nature},
  pages = {1--11},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  url = {https://www.nature.com/articles/s41586-021-03819-2},
  urldate = {2021-07-20},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the 3-D structure that a protein will adopt based solely on its amino acid sequence, the structure prediction component of the ‘protein folding problem’8, has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even where no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experiment in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  langid = {english},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Research\\
Subject\_term: Computational biophysics;Machine learning;Protein structure predictions;Structural biology\\
Subject\_term\_id: computational-biophysics;machine-learning;protein-structure-predictions;structural-biology},
  file = {/Users/vitay/Documents/Zotero/storage/DNXNEQB5/s41586-021-03819-2.html}
}

@unpublished{Karpatne2017,
  title = {Machine {{Learning}} for the {{Geosciences}}: {{Challenges}} and {{Opportunities}}},
  shorttitle = {Machine {{Learning}} for the {{Geosciences}}},
  author = {Karpatne, Anuj and Ebert-Uphoff, Imme and Ravela, Sai and Babaie, Hassan Ali and Kumar, Vipin},
  date = {2017-11-13},
  eprint = {1711.04708},
  eprinttype = {arXiv},
  eprintclass = {physics},
  url = {http://arxiv.org/abs/1711.04708},
  urldate = {2019-09-27},
  abstract = {Geosciences is a field of great societal relevance that requires solutions to several urgent problems facing our humanity and the planet. As geosciences enters the era of big data, machine learning (ML) -- that has been widely successful in commercial domains -- offers immense potential to contribute to problems in geosciences. However, problems in geosciences have several unique challenges that are seldom found in traditional applications, requiring novel problem formulations and methodologies in machine learning. This article introduces researchers in the machine learning (ML) community to these challenges offered by geoscience problems and the opportunities that exist for advancing both machine learning and geosciences. We first highlight typical sources of geoscience data and describe their properties that make it challenging to use traditional machine learning techniques. We then describe some of the common categories of geoscience problems where machine learning can play a role, and discuss some of the existing efforts and promising directions for methodological development in machine learning. We conclude by discussing some of the emerging research themes in machine learning that are applicable across all problems in the geosciences, and the importance of a deep collaboration between machine learning and geosciences for synergistic advancements in both disciplines.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Physics - Geophysics},
  file = {/Users/vitay/Documents/Zotero/storage/W9RJZ5SQ/Karpatne et al_2017_Machine Learning for the Geosciences.pdf;/Users/vitay/Documents/Zotero/storage/9SWXZPLT/1711.html}
}

@unpublished{Karras2020,
  title = {Analyzing and {{Improving}} the {{Image Quality}} of {{StyleGAN}}},
  author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  date = {2020-03-23},
  eprint = {1912.04958},
  eprinttype = {arXiv},
  eprintclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1912.04958},
  urldate = {2020-12-08},
  abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/WJJGLD7R/Karras_et_al_2020_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN.pdf;/Users/vitay/Documents/Zotero/storage/MD7C8FHG/1912.html}
}

@unpublished{Kendall2016,
  title = {{{PoseNet}}: {{A Convolutional Network}} for {{Real-Time}} 6-{{DOF Camera Relocalization}}},
  shorttitle = {{{PoseNet}}},
  author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto},
  date = {2016-02-18},
  eprint = {1505.07427},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1505.07427},
  urldate = {2020-11-27},
  abstract = {We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. PoseNet code, dataset and an online demonstration is available on our project webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/7MPX3T5W/Kendall_et_al_2016_PoseNet.pdf;/Users/vitay/Documents/Zotero/storage/BEBS6PC9/1505.html}
}

@article{Kheradpisheh2018,
  title = {{{STDP-based}} Spiking Deep Convolutional Neural Networks for Object Recognition},
  author = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J. and Masquelier, Timothée},
  date = {2018-03-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {99},
  pages = {56--67},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2017.12.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608017302903},
  urldate = {2019-04-20},
  abstract = {Previous studies have shown that spike-timing-dependent plasticity (STDP) can be used in spiking neural networks (SNN) to extract visual features of low or intermediate complexity in an unsupervised manner. These studies, however, used relatively shallow architectures, and only one layer was trainable. Another line of research has demonstrated – using rate-based neural networks trained with back-propagation – that having many layers increases the recognition robustness, an approach known as deep learning. We thus designed a deep SNN, comprising several convolutional (trainable with STDP) and pooling layers. We used a temporal coding scheme where the most strongly activated neurons fire first, and less activated neurons fire later or not at all. The network was exposed to natural images. Thanks to STDP, neurons progressively learned features corresponding to prototypical patterns that were both salient and frequent. Only a few tens of examples per category were required and no label was needed. After learning, the complexity of the extracted features increased along the hierarchy, from edge detectors in the first layer to object prototypes in the last layer. Coding was very sparse, with only a few thousands spikes per image, and in some cases the object category could be reasonably well inferred from the activity of a single higher-order neuron. More generally, the activity of a few hundreds of such neurons contained robust category information, as demonstrated using a classifier on Caltech 101, ETH-80, and MNIST databases. We also demonstrate the superiority of STDP over other unsupervised techniques such as random crops (HMAX) or auto-encoders. Taken together, our results suggest that the combination of STDP with latency coding may be a key to understanding the way that the primate visual system learns, its remarkable processing speed and its low energy consumption. These mechanisms are also interesting for artificial vision systems, particularly for hardware solutions.},
  keywords = {Deep learning,Object recognition,Spiking neural network,STDP,Temporal coding},
  file = {/Users/vitay/Documents/Zotero/storage/4YUFQI9R/Kheradpisheh et al_2018_STDP-based spiking deep convolutional neural networks for object recognition.pdf;/Users/vitay/Documents/Zotero/storage/8WASSJYA/S0893608017302903.html}
}

@unpublished{Kim2014,
  title = {Convolutional {{Neural Networks}} for {{Sentence Classification}}},
  author = {Kim, Yoon},
  date = {2014-09-02},
  eprint = {1408.5882},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1408.5882},
  urldate = {2020-11-22},
  abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/vitay/Documents/Zotero/storage/HXZX9EEK/Kim_2014_Convolutional_Neural_Networks_for_Sentence_Classification.pdf;/Users/vitay/Documents/Zotero/storage/53JNKBK9/1408.html}
}

@article{Kim2019a,
  title = {Response Prediction of Nonlinear Hysteretic Systems by Deep Neural Networks},
  author = {Kim, Taeyong and Kwon, Oh-Sung and Song, Junho},
  date = {2019-03-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {111},
  pages = {1--10},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2018.12.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608018303368},
  urldate = {2019-02-10},
  abstract = {Nonlinear hysteretic systems are common in many engineering problems. The maximum response estimation of a nonlinear hysteretic system under stochastic excitations is an important task for designing and maintaining such systems. Although a nonlinear time history analysis is the most rigorous method to accurately estimate the responses in many situations, high computational costs and modelingtime hamper adoption of the approach in a routine engineering practice. Thus, various simplified regression equations are often introduced to replace a nonlinear time history analysis in engineering practices, but the accuracy of the estimated responses is limited. This paper proposes a deep neural network trained by the results of the nonlinear time history analyses as an alternative of such simplified regression equations. To this end, a convolutional neural network (CNN) which is usually applied to abstract features from visual imagery is introduced to analyze the information of the hysteretic behavior of the system, then, merged with neural networks representing the stochastic random excitation to predict the responses of a nonlinear hysteretic system. For verification, the proposed deep neural network is applied to the earthquake engineering area to predict the structural responses under earthquake excitations. The results confirm that the proposed deep neural network provides a superior performance compared to the simplified regression equations which are developed based on a limited dataset. Moreover, to give an insight of the proposed deep neural network, the extracted features from the deep neural network are investigated with various numerical examples. The method is expected to enable engineers to effectively predict the response of the hysteretic system without performing nonlinear time history analyses, and provide a new prospect in the relevant engineering fields. The supporting source code and data are available for download at https://github.com/TyongKim/ERD2.},
  keywords = {Convolutional neural network,Deep learning,Hysteretic behavior,Nonlinear time history analysis,Single degree of freedom system (SDOF),Stochastic excitation},
  file = {/Users/vitay/Documents/Zotero/storage/XH6RGX6N/S0893608018303368.html}
}

@article{King2009,
  title = {Dlib-Ml: {{A Machine Learning Toolkit}}},
  author = {King, Davis E.},
  date = {2009},
  journaltitle = {Journal of Machine Learning Research},
  volume = {10},
  pages = {1755--1758}
}

@unpublished{Kingma2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P and Welling, Max},
  date = {2013-12},
  eprint = {1312.6114},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1312.6114},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  file = {/Users/vitay/Documents/Zotero/storage/WHSNWB6L/Kingma_Welling_2013_Auto-Encoding Variational Bayes.pdf}
}

@inproceedings{Kingma2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  booktitle = {Proc. {{ICLR}}},
  author = {Kingma, Diederik and Ba, Jimmy},
  date = {2014},
  pages = {1--13},
  doi = {10.1145/1830483.1830503},
  url = {http://arxiv.org/abs/1412.6980},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  isbn = {978-1-4503-0072-8}
}

@article{Kirkpatrick2017,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  date = {2017-03-28},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {13},
  pages = {3521--3526},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1611835114},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1611835114},
  urldate = {2019-04-10},
  abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/B8VVY5FJ/Kirkpatrick et al_2017_Overcoming catastrophic forgetting in neural networks.pdf}
}

@unpublished{Ko2020,
  title = {Key {{Points Estimation}} and {{Point Instance Segmentation Approach}} for {{Lane Detection}}},
  author = {Ko, Yeongmin and Lee, Younkwan and Azam, Shoaib and Munir, Farzeen and Jeon, Moongu and Pedrycz, Witold},
  date = {2020-09-13},
  eprint = {2002.06604},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2002.06604},
  urldate = {2021-01-22},
  abstract = {Perception techniques for autonomous driving should be adaptive to various environments. In the case of traffic line detection, an essential perception module, many condition should be considered, such as number of traffic lines and computing power of the target system. To address these problems, in this paper, we propose a traffic line detection method called Point Instance Network (PINet); the method is based on the key points estimation and instance segmentation approach. The PINet includes several stacked hourglass networks that are trained simultaneously. Therefore the size of the trained models can be chosen according to the computing power of the target environment. We cast a clustering problem of the predicted key points as an instance segmentation problem; the PINet can be trained regardless of the number of the traffic lines. The PINet achieves competitive accuracy and false positive on the TuSimple and Culane datasets, popular public datasets for lane detection. Our code is available at https://github.com/koyeongmin/PINet\_new},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/vitay/Documents/Zotero/storage/Q35QHHNV/Ko_et_al_2020_Key_Points_Estimation_and_Point_Instance_Segmentation_Approach_for_Lane.pdf;/Users/vitay/Documents/Zotero/storage/ITEVAFGC/2002.html}
}

@unpublished{Kosiorek2019,
  title = {Stacked {{Capsule Autoencoders}}},
  author = {Kosiorek, Adam R. and Sabour, Sara and Teh, Yee Whye and Hinton, Geoffrey E.},
  date = {2019-12-02},
  eprint = {1906.06818},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.06818},
  urldate = {2021-01-19},
  abstract = {Objects are composed of a set of geometrically organized parts. We introduce an unsupervised capsule autoencoder (SCAE), which explicitly uses geometric relationships between parts to reason about objects. Since these relationships do not depend on the viewpoint, our model is robust to viewpoint changes. SCAE consists of two stages. In the first stage, the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates. In the second stage, SCAE predicts parameters of a few object capsules, which are then used to reconstruct part poses. Inference in this model is amortized and performed by off-the-shelf neural encoders, unlike in previous capsule networks. We find that object capsule presences are highly informative of the object class, which leads to state-of-the-art results for unsupervised classification on SVHN (55\%) and MNIST (98.7\%). The code is available at https://github.com/google-research/google-research/tree/master/stacked\_capsule\_autoencoders},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/FBCGTF3Z/Kosiorek_et_al_2019_Stacked_Capsule_Autoencoders.pdf;/Users/vitay/Documents/Zotero/storage/7ZHCAXRR/1906.html}
}

@unpublished{Krakovna2016,
  title = {Increasing the {{Interpretability}} of {{Recurrent Neural Networks Using Hidden Markov Models}}},
  author = {Krakovna, Viktoriya and Doshi-Velez, Finale},
  date = {2016-06-16},
  eprint = {1606.05320},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1606.05320},
  urldate = {2019-03-22},
  abstract = {As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks (RNNs), state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining an RNN with a hidden Markov model (HMM), a simpler and more transparent model. We explore various combinations of RNNs and HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained first, then a small LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/G84KMQTC/Krakovna_Doshi-Velez_2016_Increasing the Interpretability of Recurrent Neural Networks Using Hidden.pdf;/Users/vitay/Documents/Zotero/storage/9LPGDYQY/1606.html}
}

@inproceedings{Krizhevsky2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called " dropout " that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.}
}

@unpublished{Krotov2016,
  title = {Dense {{Associative Memory}} for {{Pattern Recognition}}},
  author = {Krotov, Dmitry and Hopfield, John J.},
  date = {2016-09-27},
  eprint = {1606.01164},
  eprinttype = {arXiv},
  eprintclass = {cond-mat, q-bio, stat},
  url = {http://arxiv.org/abs/1606.01164},
  urldate = {2021-01-19},
  abstract = {A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/US3YHKAM/Krotov_Hopfield_2016_Dense_Associative_Memory_for_Pattern_Recognition.pdf;/Users/vitay/Documents/Zotero/storage/FGMSHZIZ/1606.html}
}

@unpublished{Krotov2020,
  title = {Large {{Associative Memory Problem}} in {{Neurobiology}} and {{Machine Learning}}},
  author = {Krotov, Dmitry and Hopfield, John},
  date = {2020-08-16},
  eprint = {2008.06996},
  eprinttype = {arXiv},
  eprintclass = {cond-mat, q-bio, stat},
  url = {http://arxiv.org/abs/2008.06996},
  urldate = {2021-01-19},
  abstract = {Dense Associative Memories or modern Hopfield networks permit storage and reliable retrieval of an exponentially large (in the dimension of feature space) number of memories. At the same time, their naive implementation is non-biological, since it seemingly requires the existence of many-body synaptic junctions between the neurons. We show that these models are effective descriptions of a more microscopic (written in terms of biological degrees of freedom) theory that has additional (hidden) neurons and only requires two-body interactions between them. For this reason our proposed microscopic theory is a valid model of large associative memory with a degree of biological plausibility. The dynamics of our network and its reduced dimensional equivalent both minimize energy (Lyapunov) functions. When certain dynamical variables (hidden neurons) are integrated out from our microscopic theory, one can recover many of the models that were previously discussed in the literature, e.g. the model presented in ''Hopfield Networks is All You Need'' paper. We also provide an alternative derivation of the energy function and the update rule proposed in the aforementioned paper and clarify the relationships between various models of this class.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/ZH69HYX4/Krotov_Hopfield_2020_Large_Associative_Memory_Problem_in_Neurobiology_and_Machine_Learning.pdf;/Users/vitay/Documents/Zotero/storage/VEJ8CEGN/2008.html}
}

@article{Laje2013,
  title = {Robust Timing and Motor Patterns by Taming Chaos in Recurrent Neural Networks.},
  author = {Laje, Rodrigo and Buonomano, Dean V},
  date = {2013-07},
  journaltitle = {Nature neuroscience},
  volume = {16},
  number = {7},
  eprint = {23708144},
  eprinttype = {pubmed},
  pages = {925--33},
  doi = {10.1038/nn.3405},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/23708144},
  abstract = {The brain's ability to tell time and produce complex spatiotemporal motor patterns is critical for anticipating the next ring of a telephone or playing a musical instrument. One class of models proposes that these abilities emerge from dynamically changing patterns of neural activity generated in recurrent neural networks. However, the relevant dynamic regimes of recurrent networks are highly sensitive to noise; that is, chaotic. We developed a firing rate model that tells time on the order of seconds and generates complex spatiotemporal patterns in the presence of high levels of noise. This is achieved through the tuning of the recurrent connections. The network operates in a dynamic regime that exhibits coexisting chaotic and locally stable trajectories. These stable patterns function as 'dynamic attractors' and provide a feature that is characteristic of biological systems: the ability to 'return' to the pattern being generated in the face of perturbations.},
  file = {/Users/vitay/Documents/Zotero/storage/JRPQVJGJ/Laje_Buonomano_2013_Robust_timing_and_motor_patterns_by_taming_chaos_in_recurrent_neural_networks.pdf}
}

@unpublished{Lansdell2019,
  title = {Learning to Solve the Credit Assignment Problem},
  author = {Lansdell, Benjamin James and Prakash, Prashanth Ravi and Kording, Konrad Paul},
  date = {2019-10-15},
  eprint = {1906.00889},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/1906.00889},
  urldate = {2019-11-12},
  abstract = {Backpropagation is driving today's artificial neural networks (ANNs). However, despite extensive research, it remains unclear if the brain implements this algorithm. Among neuroscientists, reinforcement learning (RL) algorithms are often seen as a realistic alternative: neurons can randomly introduce change, and use unspecific feedback signals to observe their effect on the cost and thus approximate their gradient. However, the convergence rate of such learning scales poorly with the number of involved neurons. Here we propose a hybrid learning approach. Each neuron uses an RL-type strategy to learn how to approximate the gradients that backpropagation would provide. We provide proof that our approach converges to the true gradient for certain classes of networks. In both feedforward and convolutional networks, we empirically show that our approach learns to approximate the gradient, and can match the performance of gradient-based learning. Learning feedback weights provides a biologically plausible mechanism of achieving good performance, without the need for precise, pre-specified learning rules.},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/vitay/Documents/Zotero/storage/WLFYW5U7/Lansdell et al_2019_Learning to solve the credit assignment problem.pdf;/Users/vitay/Documents/Zotero/storage/VKCSFJ2H/1906.html}
}

@article{Lapuschkin2019,
  title = {Unmasking {{Clever Hans}} Predictors and Assessing What Machines Really Learn},
  author = {Lapuschkin, Sebastian and Wäldchen, Stephan and Binder, Alexander and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
  date = {2019-03-11},
  journaltitle = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {1096},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-08987-4},
  url = {https://www.nature.com/articles/s41467-019-08987-4},
  urldate = {2019-03-12},
  abstract = {Nonlinear machine learning methods have good predictive ability but the lack of transparency of the algorithms can limit their use. Here the authors investigate how these methods approach learning in order to assess the dependability of their decision making.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/ABAREVWX/Lapuschkin_et_al_2019_Unmasking_Clever_Hans_predictors_and_assessing_what_machines_really_learn.pdf;/Users/vitay/Documents/Zotero/storage/F7J7GHVA/s41467-019-08987-4.html}
}

@article{Larisch2023,
  title = {Detecting {{Anomalies}} in {{System Logs With}} a {{Compact Convolutional Transformer}}},
  author = {Larisch, René and Vitay, Julien and Hamker, Fred H.},
  date = {2023},
  journaltitle = {IEEE Access},
  volume = {11},
  pages = {113464--113479},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3323252},
  url = {https://ieeexplore.ieee.org/document/10285328?source=authoralert},
  urldate = {2023-10-24},
  abstract = {Computer systems play an important role to ensure the correct functioning of critical systems such as train stations, power stations, emergency systems, and server infrastructures. To ensure the correct functioning and safety of these computer systems, the detection of abnormal system behavior is crucial. For that purpose, monitoring log data (mirroring the recent and current system status) is very commonly used. Because log data consists mainly of words and numbers, recent work used Transformer-based networks to analyze the log data and predict anomalies. Despite their success in fields such as natural language processing and computer vision, the main disadvantage of Transformers is the huge amount of trainable parameters, leading to long training times. In this work, we use a Compact Convolutional Transformer to detect anomalies in log data. Using convolutional layers leads to a much smaller number of trainable parameters and enable the processing of many consecutive log lines. We evaluate the proposed network on two standard datasets for log data anomaly detection, Blue Gene/L (BGL) and Spirit. Our results demonstrate that the combination of convolutional processing and self-attention improves the performance for anomaly detection in comparison to other self-supervised Transformer-based approaches, and is even on par with supervised approaches.},
  eventtitle = {{{IEEE Access}}},
  file = {/Users/vitay/Documents/Zotero/storage/BV5YTI2Z/Larisch_et_al_2023_Detecting_Anomalies_in_System_Logs_With_a_Compact_Convolutional_Transformer.pdf}
}

@inproceedings{Le2013,
  title = {Building High-Level Features Using Large Scale Unsupervised Learning},
  booktitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Le, Quoc V.},
  date = {2013-05},
  pages = {8595--8598},
  publisher = {IEEE},
  location = {Vancouver, BC, Canada},
  doi = {10.1109/ICASSP.2013.6639343},
  url = {http://ieeexplore.ieee.org/document/6639343/},
  urldate = {2020-11-03},
  abstract = {We consider the problem of building highlevel, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8\% accuracy in recognizing 22,000 object categories from ImageNet, a leap of 70\% relative improvement over the previous state-of-the-art.},
  eventtitle = {{{ICASSP}} 2013 - 2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-4799-0356-6},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/NYEEXGEZ/Le_2013_Building_high-level_features_using_large_scale_unsupervised_learning2.pdf}
}

@inproceedings{Le2018,
  title = {{{PointGrid}}: {{A Deep Network}} for {{3D Shape Understanding}}},
  shorttitle = {{{PointGrid}}},
  author = {Le, Truc and Duan, Ye},
  date = {2018},
  pages = {9204--9214},
  url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Le_PointGrid_A_Deep_CVPR_2018_paper.html},
  urldate = {2019-01-24},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/vitay/Documents/Zotero/storage/GNAP9DIY/Le_Duan_2018_PointGrid.pdf;/Users/vitay/Documents/Zotero/storage/9QXZUHDS/Le_PointGrid_A_Deep_CVPR_2018_paper.html}
}

@article{LeCun1998,
  title = {Gradient {{Based Learning Applied}} to {{Document Recognition}}},
  author = {LeCun, Y and Bottou, L and Bengio, Y and Haffner, P},
  date = {1998},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {0018-9219},
  doi = {10.1109/5.726791},
  abstract = {A long and detailed paper on convolutional nets, graph transformer\textbackslash nnetworks, and discriminative training methods for sequence labeling.\textbackslash nWe show how to build systems that integrate segmentation, feature\textbackslash nextraction, classification, contextual post-processing, and language\textbackslash nmodeling into one single learning machine trained end-to-end. Applications\textbackslash nto handwriting recognition and face detection are described.},
  keywords = {2D shape variability,back-propagation,backpropagation,Character recognition,cheque reading,complex decision surface synthesis,convo-,convolution,convolutional neural network character recognizers,document recogni-,document recognition,document recognition systems,Feature extraction,fi-,field extraction,gradient based learning technique,gradient-based learning,graph transformer networks,GTN,handwritten character recognition,handwritten digit recognition task,Hidden Markov models,high-dimensional patterns,language modeling,lutional neural networks,machine learning,Machine learning,Multi-layer neural network,multilayer neural networks,multilayer perceptrons,multimodule systems,neural networks,Neural networks,nite state transducers,ocr,optical character recognition,Optical character recognition software,Optical computing,Pattern recognition,performance measure minimization,Principal component analysis,segmentation recognition,tion}
}

@online{Lehman2022,
  title = {Evolution through {{Large Models}}},
  author = {Lehman, Joel and Gordon, Jonathan and Jain, Shawn and Ndousse, Kamal and Yeh, Cathy and Stanley, Kenneth O.},
  date = {2022-06-17},
  eprint = {2206.08896},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.08896},
  url = {http://arxiv.org/abs/2206.08896},
  urldate = {2022-07-11},
  abstract = {This paper pursues the insight that large language models (LLMs) trained to generate code can vastly improve the effectiveness of mutation operators applied to programs in genetic programming (GP). Because such LLMs benefit from training data that includes sequential changes and modifications, they can approximate likely changes that humans would make. To highlight the breadth of implications of such evolution through large models (ELM), in the main experiment ELM combined with MAP-Elites generates hundreds of thousands of functional examples of Python programs that output working ambulating robots in the Sodarace domain, which the original LLM had never seen in pre-training. These examples then help to bootstrap training a new conditional language model that can output the right walker for a particular terrain. The ability to bootstrap new models that can output appropriate artifacts for a given context in a domain where zero training data was previously available carries implications for open-endedness, deep learning, and reinforcement learning. These implications are explored here in depth in the hope of inspiring new directions of research now opened up by ELM.},
  pubstate = {prepublished},
  version = {1},
  file = {/Users/vitay/Documents/Zotero/storage/TPEGZDQH/Lehman_et_al_2022_Evolution_through_Large_Models.pdf;/Users/vitay/Documents/Zotero/storage/CWBMHTJ5/2206.html}
}

@unpublished{Li2018,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  date = {2018-11-07},
  eprint = {1712.09913},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1712.09913},
  urldate = {2020-10-02},
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/H2FB6TN6/Li_et_al_2018_Visualizing_the_Loss_Landscape_of_Neural_Nets.pdf;/Users/vitay/Documents/Zotero/storage/I3DSJ3DR/1712.html}
}

@article{Li2019,
  title = {Deep {{Learning}} for {{Hyperspectral Image Classification}}: {{An Overview}}},
  shorttitle = {Deep {{Learning}} for {{Hyperspectral Image Classification}}},
  author = {Li, S. and Song, W. and Fang, L. and Chen, Y. and Ghamisi, P. and Benediktsson, J. A.},
  date = {2019-09},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {57},
  number = {9},
  pages = {6690--6709},
  doi = {10.1109/TGRS.2019.2907932},
  abstract = {Hyperspectral image (HSI) classification has become a hot topic in the field of remote sensing. In general, the complex characteristics of hyperspectral data make the accurate classification of such data challenging for traditional machine learning methods. In addition, hyperspectral imaging often deals with an inherently nonlinear relation between the captured spectral information and the corresponding materials. In recent years, deep learning has been recognized as a powerful feature-extraction tool to effectively address nonlinear problems and widely used in a number of image processing tasks. Motivated by those successful applications, deep learning has also been introduced to classify HSIs and demonstrated good performance. This survey paper presents a systematic review of deep learning-based HSI classification literatures and compares several strategies for this topic. Specifically, we first summarize the main challenges of HSI classification which cannot be effectively overcome by traditional machine learning methods, and also introduce the advantages of deep learning to handle these problems. Then, we build a framework that divides the corresponding works into spectral-feature networks, spatial-feature networks, and spectral-spatial-feature networks to systematically review the recent achievements in deep learning-based HSI classification. In addition, considering the fact that available training samples in the remote sensing field are usually very limited and training deep networks require a large number of samples, we include some strategies to improve classification performance, which can provide some guidelines for future studies on this topic. Finally, several representative deep learning-based classification methods are conducted on real HSIs in our experiments.},
  keywords = {Classification,classification performance,deep learning,Deep learning,deep learning-based HSI classification literatures,deep networks,feature extraction,Feature extraction,feature-extraction tool,geophysical image processing,hyperspectral data,hyperspectral image (HSI),hyperspectral image classification,hyperspectral imaging,Hyperspectral imaging,image classification,image processing tasks,learning (artificial intelligence),Logistics,remote sensing,representative deep learning-based classification methods,spectral information,spectral-spatial-feature networks,traditional machine learning methods,Training},
  file = {/Users/vitay/Documents/Zotero/storage/PV5L3WB7/8697135.html}
}

@article{Lillicrap2016,
  title = {Random Synaptic Feedback Weights Support Error Backpropagation for Deep Learning},
  author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
  date = {2016-11-08},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {7},
  number = {1},
  pages = {1--10},
  issn = {2041-1723},
  doi = {10.1038/ncomms13276},
  url = {https://www.nature.com/articles/ncomms13276},
  urldate = {2020-01-08},
  abstract = {Multi-layered neural architectures that implement learning require elaborate mechanisms for symmetric backpropagation of errors that are biologically implausible. Here the authors propose a simple resolution to this problem of blame assignment that works even with feedback using random synaptic weights.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/HIEUML58/Lillicrap_et_al_2016_Random_synaptic_feedback_weights_support_error_backpropagation_for_deep_learning.pdf;/Users/vitay/Documents/Zotero/storage/BAB8D8GY/ncomms13276.html}
}

@thesis{Linnainmaa1970,
  type = {mathesis},
  title = {The Representation of the Cumulative Rounding Error of an Algorithm as a {{Taylor}} Expansion of the Local Rounding Errors},
  author = {Linnainmaa, Seppo},
  date = {1970},
  institution = {Univ. Helsinki}
}

@unpublished{Liu2016,
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  shorttitle = {{{SSD}}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  date = {2016},
  volume = {9905},
  eprint = {1512.02325},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {21--37},
  doi = {10.1007/978-3-319-46448-0_2},
  url = {http://arxiv.org/abs/1512.02325},
  urldate = {2020-11-29},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300\textbackslash times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500\textbackslash times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/JHH9293C/Liu_et_al_2016_SSD.pdf;/Users/vitay/Documents/Zotero/storage/5PJC2YFK/1512.html}
}

@unpublished{Liu2022,
  title = {A {{ConvNet}} for the 2020s},
  author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  date = {2022-03-02},
  eprint = {2201.03545},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.03545},
  urldate = {2022-05-02},
  abstract = {The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.},
  file = {/Users/vitay/Documents/Zotero/storage/ILIQXFLQ/Liu_et_al_2022_A_ConvNet_for_the_2020s.pdf;/Users/vitay/Documents/Zotero/storage/2VDXEX4P/2201.html}
}

@online{Liu2024a,
  title = {{{KAN}}: {{Kolmogorov-Arnold Networks}}},
  shorttitle = {{{KAN}}},
  author = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Soljačić, Marin and Hou, Thomas Y. and Tegmark, Max},
  date = {2024-05-02},
  eprint = {2404.19756},
  eprinttype = {arXiv},
  eprintclass = {cond-mat, stat},
  url = {http://arxiv.org/abs/2404.19756},
  urldate = {2024-05-21},
  abstract = {Inspired by the Kolmogorov-Arnold representation theorem, we propose KolmogorovArnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (“neurons”), KANs have learnable activation functions on edges (“weights”). KANs have no linear weights at all – every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful “collaborators” helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today’s deep learning models which rely heavily on MLPs.},
  langid = {english},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/FSKJNMNV/Liu et al. - 2024 - KAN Kolmogorov-Arnold Networks.pdf}
}

@inproceedings{Maas2013,
  title = {Rectifier {{Nonlinearities Improve Neural Network Acoustic Models}}},
  booktitle = {{{ICML}}},
  author = {Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y},
  date = {2013},
  pages = {6},
  abstract = {Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectifier nonlinearities produce 2\% absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectifier networks.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/XYS9J8FL/Maas et al. - Rectiﬁer Nonlinearities Improve Neural Network Aco.pdf}
}

@article{Maass2002,
  title = {Real-Time Computing without Stable States: A New Framework for Neural Computation Based on Perturbations.},
  author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
  date = {2002-11},
  journaltitle = {Neural computation},
  volume = {14},
  number = {11},
  eprint = {12433288},
  eprinttype = {pubmed},
  pages = {2531--60},
  doi = {10.1162/089976602760407955},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/12433288},
  abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
  file = {/Users/vitay/Documents/Zotero/storage/2VLNI3Z7/Maass_et_al_2002_Real-time_computing_without_stable_states.pdf}
}

@unpublished{Madiraju2018,
  title = {Deep {{Temporal Clustering}} : {{Fully Unsupervised Learning}} of {{Time-Domain Features}}},
  shorttitle = {Deep {{Temporal Clustering}}},
  author = {Madiraju, Naveen Sai and Sadat, Seid M. and Fisher, Dimitry and Karimabadi, Homa},
  date = {2018-02-03},
  eprint = {1802.01059},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.01059},
  urldate = {2020-12-08},
  abstract = {Unsupervised learning of time series data, also known as temporal clustering, is a challenging problem in machine learning. Here we propose a novel algorithm, Deep Temporal Clustering (DTC), to naturally integrate dimensionality reduction and temporal clustering into a single end-to-end learning framework, fully unsupervised. The algorithm utilizes an autoencoder for temporal dimensionality reduction and a novel temporal clustering layer for cluster assignment. Then it jointly optimizes the clustering objective and the dimensionality reduction objec tive. Based on requirement and application, the temporal clustering layer can be customized with any temporal similarity metric. Several similarity metrics and state-of-the-art algorithms are considered and compared. To gain insight into temporal features that the network has learned for its clustering, we apply a visualization method that generates a region of interest heatmap for the time series. The viability of the algorithm is demonstrated using time series data from diverse domains, ranging from earthquakes to spacecraft sensor data. In each case, we show that the proposed algorithm outperforms traditional methods. The superior performance is attributed to the fully integrated temporal dimensionality reduction and clustering criterion.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/R93J686N/Madiraju_et_al_2018_Deep_Temporal_Clustering.pdf;/Users/vitay/Documents/Zotero/storage/JY6T2QQM/1802.html}
}

@unpublished{Makhzani2016,
  title = {Adversarial {{Autoencoders}}},
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  date = {2016-05-24},
  eprint = {1511.05644},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.05644},
  urldate = {2020-12-08},
  abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/QQFI7XAT/Makhzani_et_al_2016_Adversarial_Autoencoders.pdf;/Users/vitay/Documents/Zotero/storage/J8LVSEN5/1511.html}
}

@unpublished{Malinowski2015,
  title = {Ask {{Your Neurons}}: {{A Neural-based Approach}} to {{Answering Questions}} about {{Images}}},
  shorttitle = {Ask {{Your Neurons}}},
  author = {Malinowski, Mateusz and Rohrbach, Marcus and Fritz, Mario},
  date = {2015-10-01},
  eprint = {1505.01121},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1505.01121},
  urldate = {2020-12-24},
  abstract = {We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/QHI3Z9GK/Malinowski_et_al_2015_Ask_Your_Neurons.pdf;/Users/vitay/Documents/Zotero/storage/NADMJTNS/1505.html}
}

@unpublished{Malkiel2020,
  title = {{{MTAdam}}: {{Automatic Balancing}} of {{Multiple Training Loss Terms}}},
  shorttitle = {{{MTAdam}}},
  author = {Malkiel, Itzik and Wolf, Lior},
  date = {2020-06-25},
  eprint = {2006.14683},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.14683},
  urldate = {2020-10-06},
  abstract = {When training neural models, it is common to combine multiple loss terms. The balancing of these terms requires considerable human effort and is computationally demanding. Moreover, the optimal trade-off between the loss term can change as training progresses, especially for adversarial terms. In this work, we generalize the Adam optimization algorithm to handle multiple loss terms. The guiding principle is that for every layer, the gradient magnitude of the terms should be balanced. To this end, the Multi-Term Adam (MTAdam) computes the derivative of each loss term separately, infers the first and second moments per parameter and loss term, and calculates a first moment for the magnitude per layer of the gradients arising from each loss. This magnitude is used to continuously balance the gradients across all layers, in a manner that both varies from one layer to the next and dynamically changes over time. Our results show that training with the new method leads to fast recovery from suboptimal initial loss weighting and to training outcomes that match conventional training with the prescribed hyperparameters of each method.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/7LJ3ZP4I/Malkiel_Wolf_2020_MTAdam.pdf;/Users/vitay/Documents/Zotero/storage/ZHT3V9Q3/2006.html}
}

@article{Matsuo2022,
  title = {Deep Learning, Reinforcement Learning, and World Models},
  author = {Matsuo, Yutaka and LeCun, Yann and Sahani, Maneesh and Precup, Doina and Silver, David and Sugiyama, Masashi and Uchibe, Eiji and Morimoto, Jun},
  date = {2022-08-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {152},
  pages = {267--275},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2022.03.037},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608022001150},
  urldate = {2022-07-28},
  abstract = {Deep learning (DL) and reinforcement learning (RL) methods seem to be a part of indispensable factors to achieve human-level or super-human AI systems. On the other hand, both DL and RL have strong connections with our brain functions and with neuroscientific findings. In this review, we summarize talks and discussions in the “Deep Learning and Reinforcement Learning” session of the symposium, International Symposium on Artificial Intelligence and Brain Science. In this session, we discussed whether we can achieve comprehensive understanding of human intelligence based on the recent advances of deep learning and reinforcement learning algorithms. Speakers contributed to provide talks about their recent studies that can be key technologies to achieve human-level intelligence.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/2MNA8SCL/Matsuo_et_al_2022_Deep_learning,_reinforcement_learning,_and_world_models.pdf;/Users/vitay/Documents/Zotero/storage/AETJ42WB/S0893608022001150.html}
}

@article{McEliece1987,
  title = {The Capacity of the {{Hopfield}} Associative Memory},
  author = {McEliece, R. and Posner, E. and Rodemich, E. and Venkatesh, S.},
  date = {1987-07},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {33},
  number = {4},
  pages = {461--482},
  issn = {1557-9654},
  doi = {10.1109/TIT.1987.1057328},
  abstract = {Techniques from coding theory are applied to study rigorously the capacity of the Hopfield associative memory. Such a memory storesn-tuple of\textbackslash pm 1's. The components change depending on a hard-limited version of linear functions of all other components. With symmetric connections between components, a stable state is ultimately reached. By building up the connection matrix as a sum-of-outer products ofmfundamental memories, one hopes to be able to recover a certain one of themmemories by using an initialn-tuple probe vector less than a Hamming distancen/2away from the fundamental memory. Ifmfundamental memories are chosen at random, the maximum asympotic value ofmin order that most of themoriginal memories are exactly recoverable isn/(2 \textbackslash log n). With the added restriction that every one of themfundamental memories be recoverable exactly,mcan be no more thann/(4 \textbackslash log n)asymptotically asnapproaches infinity. Extensions are also considered, in particular to capacity under quantization of the outer-product connection matrix. This quantized memory capacity problem is closely related to the capacity of the quantized Gaussian channel.},
  eventtitle = {{{IEEE Transactions}} on {{Information Theory}}},
  keywords = {Associative memories,Coding/decoding,Neural networks},
  file = {/Users/vitay/Documents/Zotero/storage/PEHKUZ8I/McEliece_et_al_1987_The_capacity_of_the_Hopfield_associative_memory.pdf;/Users/vitay/Documents/Zotero/storage/54RPYUK9/1057328.html}
}

@unpublished{McInnes2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  date = {2020-09-17},
  eprint = {1802.03426},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.03426},
  urldate = {2020-09-23},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/SJ3TV2ZN/McInnes_et_al_2020_UMAP.pdf;/Users/vitay/Documents/Zotero/storage/8JEHWB3R/1802.html}
}

@unpublished{Merity2019,
  title = {Single {{Headed Attention RNN}}: {{Stop Thinking With Your Head}}},
  shorttitle = {Single {{Headed Attention RNN}}},
  author = {Merity, Stephen},
  date = {2019-11-27},
  eprint = {1911.11423},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1911.11423},
  urldate = {2020-01-14},
  abstract = {The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/vitay/Documents/Zotero/storage/L5NWJ8QZ/Merity_2019_Single_Headed_Attention_RNN.pdf;/Users/vitay/Documents/Zotero/storage/R9EHLM6D/1911.html}
}

@unpublished{Mikolov2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-09-06},
  eprint = {1301.3781},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1301.3781},
  urldate = {2020-12-24},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/vitay/Documents/Zotero/storage/4KSSQEMI/Mikolov_et_al_2013_Efficient_Estimation_of_Word_Representations_in_Vector_Space.pdf;/Users/vitay/Documents/Zotero/storage/Q2IUJLTM/1301.html}
}

@unpublished{Mirza2014,
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  date = {2014-11},
  eprint = {1411.1784},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1411.1784},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.}
}

@online{Misra2016,
  title = {Shuffle and {{Learn}}: {{Unsupervised Learning}} Using {{Temporal Order Verification}}},
  shorttitle = {Shuffle and {{Learn}}},
  author = {Misra, Ishan and Zitnick, C. Lawrence and Hebert, Martial},
  date = {2016-07-26},
  eprint = {1603.08561},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1603.08561},
  url = {http://arxiv.org/abs/1603.08561},
  urldate = {2023-01-18},
  abstract = {In this paper, we present an approach for learning a visual representation from the raw spatiotemporal signals in videos. Our representation is learned without supervision from semantic labels. We formulate our method as an unsupervised sequential verification task, i.e., we determine whether a sequence of frames from a video is in the correct temporal order. With this simple task and no semantic labels, we learn a powerful visual representation using a Convolutional Neural Network (CNN). The representation contains complementary information to that learned from supervised image datasets like ImageNet. Qualitative results show that our method captures information that is temporally varying, such as human pose. When used as pre-training for action recognition, our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51. To demonstrate its sensitivity to human pose, we show results for pose estimation on the FLIC and MPII datasets that are competitive, or better than approaches using significantly more supervision. Our method can be combined with supervised representations to provide an additional boost in accuracy.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/Q8K7GXID/Misra_et_al_2016_Shuffle_and_Learn.pdf}
}

@unpublished{Mummadi2021,
  title = {Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions?},
  author = {Mummadi, Chaithanya Kumar and Subramaniam, Ranjitha and Hutmacher, Robin and Vitay, Julien and Fischer, Volker and Metzen, Jan Hendrik},
  date = {2021-04-20},
  eprint = {2104.09789},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.09789},
  urldate = {2021-04-21},
  abstract = {Convolutional neural networks (CNNs) learn to extract representations of complex features, such as object shapes and textures to solve image recognition tasks. Recent work indicates that CNNs trained on ImageNet are biased towards features that encode textures and that these alone are sufficient to generalize to unseen test data from the same distribution as the training data but often fail to generalize to out-of-distribution data. It has been shown that augmenting the training data with different image styles decreases this texture bias in favor of increased shape bias while at the same time improving robustness to common corruptions, such as noise and blur. Commonly, this is interpreted as shape bias increasing corruption robustness. However, this relationship is only hypothesized. We perform a systematic study of different ways of composing inputs based on natural images, explicit edge information, and stylization. While stylization is essential for achieving high corruption robustness, we do not find a clear correlation between shape bias and robustness. We conclude that the data augmentation caused by style-variation accounts for the improved corruption robustness and increased shape bias is only a byproduct.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/BYU9GBQB/Mummadi_et_al_2021_Does_enhanced_shape_bias_improve_neural_network_robustness_to_common_corruptions.pdf;/Users/vitay/Documents/Zotero/storage/4R9N6VIF/2104.html}
}

@book{Murphy2022,
  title = {Probabilistic {{Machine Learning}}: {{An}} Introduction},
  author = {Murphy, Kevin Patrick},
  date = {2022},
  publisher = {MIT Press},
  url = {https://probml.github.io/pml-book/book1.html},
  urldate = {2023-01-08},
  file = {/Users/vitay/Documents/Zotero/storage/MNXBRKLT/Murphy_2022_Probabilistic_Machine_Learning.pdf}
}

@unpublished{Nakkiran2019,
  title = {Deep {{Double Descent}}: {{Where Bigger Models}} and {{More Data Hurt}}},
  shorttitle = {Deep {{Double Descent}}},
  author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  date = {2019-12-04},
  eprint = {1912.02292},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.02292},
  urldate = {2020-05-14},
  abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/2AFUDIVB/Nakkiran_et_al_2019_Deep_Double_Descent.pdf;/Users/vitay/Documents/Zotero/storage/98MA3U4G/1912.html}
}

@article{Nascimento2020,
  title = {A Tutorial on Solving Ordinary Differential Equations Using {{Python}} and Hybrid Physics-Informed Neural Network},
  author = {Nascimento, Renato G. and Fricke, Kajetan and Viana, Felipe A. C.},
  date = {2020-11-01},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  shortjournal = {Engineering Applications of Artificial Intelligence},
  volume = {96},
  pages = {103996},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2020.103996},
  url = {https://www.sciencedirect.com/science/article/pii/S095219762030292X},
  urldate = {2021-02-11},
  abstract = {We present a tutorial on how to directly implement integration of ordinary differential equations through recurrent neural networks using Python. In order to simplify the implementation, we leveraged modern machine learning frameworks such as TensorFlow and Keras. Besides, offering implementation of basic models (such as multilayer perceptrons and recurrent neural networks) and optimization methods, these frameworks offer powerful automatic differentiation. With all that, the main advantage of our approach is that one can implement hybrid models combining physics-informed and data-driven kernels, where data-driven kernels are used to reduce the gap between predictions and observations. Alternatively, we can also perform model parameter identification. In order to illustrate our approach, we used two case studies. The first one consisted of performing fatigue crack growth integration through Euler’s forward method using a hybrid model combining a data-driven stress intensity range model with a physics-based crack length increment model. The second case study consisted of performing model parameter identification of a dynamic two-degree-of-freedom system through Runge–Kutta integration. The examples presented here as well as source codes are all open-source under the GitHub repository https://github.com/PML-UCF/pinn\_code\_tutorial.},
  langid = {english},
  keywords = {Hybrid model python implementation,Physics-informed neural network,Scientific machine learning,Uncertainty quantification},
  file = {/Users/vitay/Documents/Zotero/storage/6G3D5KSE/Nascimento_et_al_2020_A_tutorial_on_solving_ordinary_differential_equations_using_Python_and_hybrid.pdf;/Users/vitay/Documents/Zotero/storage/LNCEE9A2/S095219762030292X.html}
}

@article{Nasr2019,
  title = {Number Detectors Spontaneously Emerge in a Deep Neural Network Designed for Visual Object Recognition},
  author = {Nasr, Khaled and Viswanathan, Pooja and Nieder, Andreas},
  date = {2019-05-01},
  journaltitle = {Science Advances},
  volume = {5},
  number = {5},
  pages = {eaav7903},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aav7903},
  url = {https://advances.sciencemag.org/content/5/5/eaav7903},
  urldate = {2019-05-17},
  abstract = {Humans and animals have a “number sense,” an innate capability to intuitively assess the number of visual items in a set, its numerosity. This capability implies that mechanisms to extract numerosity indwell the brain’s visual system, which is primarily concerned with visual object recognition. Here, we show that network units tuned to abstract numerosity, and therefore reminiscent of real number neurons, spontaneously emerge in a biologically inspired deep neural network that was merely trained on visual object recognition. These numerosity-tuned units underlay the network’s number discrimination performance that showed all the characteristics of human and animal number discriminations as predicted by the Weber-Fechner law. These findings explain the spontaneous emergence of the number sense based on mechanisms inherent to the visual system. A deep neural network trained only on visual object recognition develops tuned number detectors reminiscent of real neurons. A deep neural network trained only on visual object recognition develops tuned number detectors reminiscent of real neurons.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/FAQTVVL2/Nasr et al_2019_Number detectors spontaneously emerge in a deep neural network designed for.pdf;/Users/vitay/Documents/Zotero/storage/GRBK2CTY/eaav7903.html}
}

@online{Nichol2021,
  title = {Improved {{Denoising Diffusion Probabilistic Models}}},
  author = {Nichol, Alex and Dhariwal, Prafulla},
  date = {2021-02-18},
  eprint = {2102.09672},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2102.09672},
  url = {http://arxiv.org/abs/2102.09672},
  urldate = {2024-09-30},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/ESNMHUZG/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf}
}

@online{Nichol2021a,
  title = {{{GLIDE}}: {{Towards Photorealistic Image Generation}} and {{Editing}} with {{Text-Guided Diffusion Models}}},
  shorttitle = {{{GLIDE}}},
  author = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  date = {2021-12-20},
  url = {https://arxiv.org/abs/2112.10741v3},
  urldate = {2024-10-01},
  abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
  langid = {english},
  organization = {arXiv.org},
  file = {/Users/vitay/Documents/Zotero/storage/M9XKW42J/Nichol et al. - 2021 - GLIDE Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.pdf}
}

@online{Nichol2022,
  title = {{{GLIDE}}: {{Towards Photorealistic Image Generation}} and {{Editing}} with {{Text-Guided Diffusion Models}}},
  shorttitle = {{{GLIDE}}},
  author = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  date = {2022-03-08},
  eprint = {2112.10741},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.10741},
  url = {http://arxiv.org/abs/2112.10741},
  urldate = {2024-09-30},
  abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/ACEBBDX2/Nichol et al. - 2022 - GLIDE Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.pdf}
}

@book{Nielsen2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  date = {2015},
  publisher = {Determination Press},
  url = {http://neuralnetworksanddeeplearning.com/}
}

@online{Noroozi2017,
  title = {Unsupervised {{Learning}} of {{Visual Representations}} by {{Solving Jigsaw Puzzles}}},
  author = {Noroozi, Mehdi and Favaro, Paolo},
  date = {2017-08-22},
  eprint = {1603.09246},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1603.09246},
  url = {http://arxiv.org/abs/1603.09246},
  urldate = {2023-01-18},
  abstract = {In this paper we study the problem of image representation learning without human annotation. By following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their correct spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. Our proposed method for learning visual representations outperforms state of the art methods in several transfer learning benchmarks.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/2MZJE4VQ/Noroozi_Favaro_2017_Unsupervised_Learning_of_Visual_Representations_by_Solving_Jigsaw_Puzzles.pdf}
}

@unpublished{Nowozin2016,
  title = {F-{{GAN}}: {{Training Generative Neural Samplers}} Using {{Variational Divergence Minimization}}},
  shorttitle = {F-{{GAN}}},
  author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
  date = {2016-06-02},
  eprint = {1606.00709},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1606.00709},
  urldate = {2020-12-09},
  abstract = {Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/vitay/Documents/Zotero/storage/YFSHDYAN/Nowozin_et_al_2016_f-GAN.pdf;/Users/vitay/Documents/Zotero/storage/UMRXS3JH/1606.html}
}

@article{Oja1982a,
  title = {A Simplified Neuron Model as a Principal Component Analyzer.},
  author = {Oja, E},
  date = {1982-01},
  journaltitle = {Journal of Mathematical Biology},
  volume = {15},
  number = {3},
  eprint = {7153672},
  eprinttype = {pubmed},
  pages = {267--73},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/7153672},
  keywords = {Animals,Mathematics,Models Neurological,Neurons,Neurons: physiology,Synapses,Synapses: physiology}
}

@article{Olshausen1997,
  title = {Sparse Coding with an Overcomplete Basis Set: {{A}} Strategy Employed by {{V1}}?},
  shorttitle = {Sparse Coding with an Overcomplete Basis Set},
  author = {Olshausen, Bruno A. and Field, David J.},
  date = {1997-12-01},
  journaltitle = {Vision Research},
  shortjournal = {Vision Research},
  volume = {37},
  number = {23},
  pages = {3311--3325},
  issn = {0042-6989},
  doi = {10.1016/S0042-6989(97)00169-7},
  url = {http://www.sciencedirect.com/science/article/pii/S0042698997001697},
  urldate = {2020-12-08},
  abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
  langid = {english},
  keywords = {Coding,Gabor-wavelet,Natural images,V1},
  file = {/Users/vitay/Documents/Zotero/storage/MFIMCGSY/S0042698997001697.html}
}

@unpublished{Oord2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {family=Oord, given=Aaron, prefix=van den, useprefix=false and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  date = {2016-09-19},
  eprint = {1609.03499},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1609.03499},
  urldate = {2020-11-22},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound},
  file = {/Users/vitay/Documents/Zotero/storage/5D42ZS7V/Oord_et_al_2016_WaveNet.pdf;/Users/vitay/Documents/Zotero/storage/46HMD3JN/1609.html}
}

@article{Park2016,
  title = {Tracking {{Human-like Natural Motion Using Deep Recurrent Neural Networks}}},
  author = {Park, Youngbin and Moon, Sungphill and Suh, Il Hong},
  date = {2016-04},
  url = {http://arxiv.org/abs/1604.04528},
  abstract = {Kinect skeleton tracker is able to achieve considerable human body tracking performance in convenient and a low-cost manner. However, The tracker often captures unnatural human poses such as discontinuous and vibrated motions when self-occlusions occur. A majority of approaches tackle this problem by using multiple Kinect sensors in a workspace. Combination of the measurements from different sensors is then conducted in Kalman filter framework or optimization problem is formulated for sensor fusion. However, these methods usually require heuristics to measure reliability of measurements observed from each Kinect sensor. In this paper, we developed a method to improve Kinect skeleton using single Kinect sensor, in which supervised learning technique was employed to correct unnatural tracking motions. Specifically, deep recurrent neural networks were used for improving joint positions and velocities of Kinect skeleton, and three methods were proposed to integrate the refined positions and velocities for further enhancement. Moreover, we suggested a novel measure to evaluate naturalness of captured motions. We evaluated the proposed approach by comparison with the ground truth obtained using a commercial optical maker-based motion capture system.}
}

@online{Parnami2022,
  title = {Learning from {{Few Examples}}: {{A Summary}} of {{Approaches}} to {{Few-Shot Learning}}},
  shorttitle = {Learning from {{Few Examples}}},
  author = {Parnami, Archit and Lee, Minwoo},
  date = {2022-03-07},
  eprint = {2203.04291},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.04291},
  url = {http://arxiv.org/abs/2203.04291},
  urldate = {2022-10-05},
  abstract = {Few-Shot Learning refers to the problem of learning the underlying pattern in the data just from a few training samples. Requiring a large number of data samples, many deep learning solutions suffer from data hunger and extensively high computation time and resources. Furthermore, data is often not available due to not only the nature of the problem or privacy concerns but also the cost of data preparation. Data collection, preprocessing, and labeling are strenuous human tasks. Therefore, few-shot learning that could drastically reduce the turnaround time of building machine learning applications emerges as a low-cost solution. This survey paper comprises a representative list of recently proposed few-shot learning algorithms. Given the learning dynamics and characteristics, the approaches to few-shot learning problems are discussed in the perspectives of meta-learning, transfer learning, and hybrid approaches (i.e., different variations of the few-shot learning problem).},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/7RBTCJRQ/Parnami_Lee_2022_Learning_from_Few_Examples.pdf;/Users/vitay/Documents/Zotero/storage/RAWDQN65/2203.html}
}

@article{Pascanu2013a,
  title = {Revisiting {{Natural Gradient}} for {{Deep Networks}}},
  author = {Pascanu, Razvan and Bengio, Yoshua},
  date = {2013-01},
  url = {https://arxiv.org/abs/1301.3584}
}

@online{Pathak2016,
  title = {Context {{Encoders}}: {{Feature Learning}} by {{Inpainting}}},
  shorttitle = {Context {{Encoders}}},
  author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
  date = {2016-11-21},
  eprint = {1604.07379},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1604.07379},
  url = {http://arxiv.org/abs/1604.07379},
  urldate = {2023-01-18},
  abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/Y39N9YY6/Pathak_et_al_2016_Context_Encoders.pdf}
}

@article{Pfrommer2018,
  title = {Optimisation of Manufacturing Process Parameters Using Deep Neural Networks as Surrogate Models},
  author = {Pfrommer, Julius and Zimmerling, Clemens and Liu, Jinzhao and Kärger, Luise and Henning, Frank and Beyerer, Jürgen},
  date = {2018-01-01},
  journaltitle = {Procedia CIRP},
  shortjournal = {Procedia CIRP},
  series = {51st {{CIRP Conference}} on {{Manufacturing Systems}}},
  volume = {72},
  pages = {426--431},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2018.03.046},
  url = {https://www.sciencedirect.com/science/article/pii/S221282711830146X},
  urldate = {2021-07-12},
  abstract = {Optimisation of manufacturing process parameters requires resource-intensive search in a high-dimensional parameter space. In some cases, physics-based simulations can replace actual experiments. But they are computationally expensive to evaluate. Surrogate-based optimisation uses a simplified model to guide the search for optimised parameter combinations, where the surrogate model is iteratively improved with new observations. This work applies surrogate-based optimisation to a composite textile draping process. Numerical experiments are conducted with a Finite Element (FE) simulation model. The surrogate model, a deep artificial neural network, is trained to predict the shear angle of more than 24,000 textile elements. Predicting detailed process results instead of a single performance scalar improves the model quality, as more relevant data from every experiment can be used for training. For the textile draping case, the approach is shown to reduce the number of resource-intensive FE simulations required to find optimised parameter configurations. It also improves on the best-known overall solution.},
  langid = {english},
  keywords = {Artificial intelligence in manufacturing,Composite Textile Draping,Deep Learning,Machine Learning,Modelling,Optimisation,Process Parameter Optimisation,Simulation},
  file = {/Users/vitay/Documents/Zotero/storage/S6PUW4BY/Pfrommer_et_al_2018_Optimisation_of_manufacturing_process_parameters_using_deep_neural_networks_as.pdf}
}

@online{Phuong2022,
  title = {Formal {{Algorithms}} for {{Transformers}}},
  author = {Phuong, Mary and Hutter, Marcus},
  date = {2022-07-19},
  eprint = {2207.09238},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.09238},
  url = {http://arxiv.org/abs/2207.09238},
  urldate = {2022-07-27},
  abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/EVR6WEPN/Phuong_Hutter_2022_Formal_Algorithms_for_Transformers.pdf;/Users/vitay/Documents/Zotero/storage/B23MHSCG/2207.html}
}

@unpublished{Qi2016,
  title = {{{PointNet}}: {{Deep Learning}} on {{Point Sets}} for {{3D Classification}} and {{Segmentation}}},
  shorttitle = {{{PointNet}}},
  author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
  date = {2016-12-02},
  eprint = {1612.00593},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1612.00593},
  urldate = {2019-01-24},
  abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/3AJYKPNK/Qi et al_2016_PointNet.pdf;/Users/vitay/Documents/Zotero/storage/F2FQ76GR/1612.html}
}

@unpublished{Radford2015,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  date = {2015-11},
  eprint = {1511.06434},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.06434},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.}
}

@article{Radford2018,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  date = {2018},
  pages = {12},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/H7IQUFUJ/Radford_et_al_2018_Improving_Language_Understanding_by_Generative_Pre-Training.pdf}
}

@unpublished{Rae2021,
  title = {Scaling {{Language Models}}: {{Methods}}, {{Analysis}} \& {{Insights}} from {{Training Gopher}}},
  shorttitle = {Scaling {{Language Models}}},
  author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and family=Driessche, given=George, prefix=van den, useprefix=false and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and family=Autume, given=Cyprien de Masson, prefix=d', useprefix=true and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  date = {2021-12-08},
  eprint = {2112.11446},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.11446},
  urldate = {2021-12-26},
  abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/vitay/Documents/Zotero/storage/CG4WSL6F/Rae_et_al_2021_Scaling_Language_Models.pdf;/Users/vitay/Documents/Zotero/storage/TA4KPM6H/2112.html}
}

@unpublished{Raghavan2019,
  title = {Generative {{Memory}} for {{Lifelong Reinforcement Learning}}},
  author = {Raghavan, Aswin and Hostetler, Jesse and Chai, Sek},
  date = {2019-02-21},
  eprint = {1902.08349},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.08349},
  urldate = {2019-04-10},
  abstract = {Our research is focused on understanding and applying biological memory transfers to new AI systems that can fundamentally improve their performance, throughout their fielded lifetime experience. We leverage current understanding of biological memory transfer to arrive at AI algorithms for memory consolidation and replay. In this paper, we propose the use of generative memory that can be recalled in batch samples to train a multi-task agent in a pseudo-rehearsal manner. We show results motivating the need for task-agnostic separation of latent space for the generative memory to address issues of catastrophic forgetting in lifelong learning.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/BBQS5PX5/Raghavan et al_2019_Generative Memory for Lifelong Reinforcement Learning.pdf;/Users/vitay/Documents/Zotero/storage/TAXX9MST/1902.html}
}

@online{Rahaman2022,
  title = {Neural {{Attentive Circuits}}},
  author = {Rahaman, Nasim and Weiss, Martin and Locatello, Francesco and Pal, Chris and Bengio, Yoshua and Schölkopf, Bernhard and Li, Li Erran and Ballas, Nicolas},
  date = {2022-10-19},
  eprint = {2210.08031},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2210.08031},
  urldate = {2022-10-20},
  abstract = {Recent work has seen the development of general purpose neural architectures that can be trained to perform tasks across diverse data modalities. General purpose models typically make few assumptions about the underlying data-structure and are known to perform well in the large-data regime. At the same time, there has been growing interest in modular neural architectures that represent the data using sparsely interacting modules. These models can be more robust out-of-distribution, computationally efficient, and capable of sample-efficient adaptation to new data. However, they tend to make domain-specific assumptions about the data, and present challenges in how module behavior (i.e., parameterization) and connectivity (i.e., their layout) can be jointly learned. In this work, we introduce a general purpose, yet modular neural architecture called Neural Attentive Circuits (NACs) that jointly learns the parameterization and a sparse connectivity of neural modules without using domain knowledge. NACs are best understood as the combination of two systems that are jointly trained end-to-end: one that determines the module configuration and the other that executes it on an input. We demonstrate qualitatively that NACs learn diverse and meaningful module configurations on the NLVR2 dataset without additional supervision. Quantitatively, we show that by incorporating modularity in this way, NACs improve upon a strong non-modular baseline in terms of low-shot adaptation on CIFAR and CUBs dataset by about 10\%, and OOD robustness on Tiny ImageNet-R by about 2.5\%. Further, we find that NACs can achieve an 8x speedup at inference time while losing less than 3\% performance. Finally, we find NACs to yield competitive results on diverse data modalities spanning point-cloud classification, symbolic processing and text-classification from ASCII bytes, thereby confirming its general purpose nature.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/62CVDUIJ/Rahaman_et_al_2022_Neural_Attentive_Circuits.pdf;/Users/vitay/Documents/Zotero/storage/U27TS5Z5/2210.html}
}

@online{Ramesh2022,
  title = {Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  date = {2022-04-12},
  eprint = {2204.06125},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.06125},
  url = {http://arxiv.org/abs/2204.06125},
  urldate = {2024-10-01},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/26WIAI8W/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf}
}

@unpublished{Ramsauer2020,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  date = {2020-12-22},
  eprint = {2008.02217},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2008.02217},
  urldate = {2021-01-11},
  abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/WS6GUH6C/Ramsauer_et_al_2020_Hopfield_Networks_is_All_You_Need.pdf;/Users/vitay/Documents/Zotero/storage/VCRQXM3V/2008.html}
}

@unpublished{Ranjan2016,
  title = {{{HyperFace}}: {{A Deep Multi-task Learning Framework}} for {{Face Detection}}, {{Landmark Localization}}, {{Pose Estimation}}, and {{Gender Recognition}}},
  shorttitle = {{{HyperFace}}},
  author = {Ranjan, Rajeev and Patel, Vishal M. and Chellappa, Rama},
  date = {2016-03-03},
  eprint = {1603.01249},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1603.01249},
  urldate = {2019-04-25},
  abstract = {We present an algorithm for simultaneous face detection, landmarks localization, pose estimation and gender recognition using deep convolutional neural networks (CNN). The proposed method called, HyperFace, fuses the intermediate layers of a deep CNN using a separate CNN followed by a multi-task learning algorithm that operates on the fused features. It exploits the synergy among the tasks which boosts up their individual performances. Additionally, we propose two variants of HyperFace: (1) HyperFace-ResNet that builds on the ResNet-101 model and achieves significant improvement in performance, and (2) Fast-HyperFace that uses a high recall fast face detector for generating region proposals to improve the speed of the algorithm. Extensive experiments show that the proposed models are able to capture both global and local information in faces and performs significantly better than many competitive algorithms for each of these four tasks.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/QEIYHSZ2/Ranjan et al_2016_HyperFace.pdf;/Users/vitay/Documents/Zotero/storage/F5KEYALQ/1603.html}
}

@unpublished{Ranjan2018,
  title = {Generating {{3D}} Faces Using {{Convolutional Mesh Autoencoders}}},
  author = {Ranjan, Anurag and Bolkart, Timo and Sanyal, Soubhik and Black, Michael J.},
  date = {2018-07-31},
  eprint = {1807.10267},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1807.10267},
  urldate = {2021-05-17},
  abstract = {Learned 3D representations of human faces are useful for computer vision problems such as 3D face tracking and reconstruction from images, as well as graphics applications such as character generation and animation. Traditional models learn a latent representation of a face using linear subspaces or higher-order tensor generalizations. Due to this linearity, they can not capture extreme deformations and non-linear expressions. To address this, we introduce a versatile model that learns a non-linear representation of a face using spectral convolutions on a mesh surface. We introduce mesh sampling operations that enable a hierarchical mesh representation that captures non-linear variations in shape and expression at multiple scales within the model. In a variational setting, our model samples diverse realistic 3D faces from a multivariate Gaussian distribution. Our training data consists of 20,466 meshes of extreme expressions captured over 12 different subjects. Despite limited training data, our trained model outperforms state-of-the-art face models with 50\% lower reconstruction error, while using 75\% fewer parameters. We also show that, replacing the expression space of an existing state-of-the-art face model with our autoencoder, achieves a lower reconstruction error. Our data, model and code are available at http://github.com/anuragranj/coma},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/4GTSNYUX/Ranjan_et_al_2018_Generating_3D_faces_using_Convolutional_Mesh_Autoencoders.pdf;/Users/vitay/Documents/Zotero/storage/5SQVB6B3/1807.html}
}

@unpublished{Razavi2019,
  title = {Generating {{Diverse High-Fidelity Images}} with {{VQ-VAE-2}}},
  author = {Razavi, Ali and family=Oord, given=Aaron, prefix=van den, useprefix=false and Vinyals, Oriol},
  date = {2019-06-02},
  eprint = {1906.00446},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.00446},
  urldate = {2020-12-06},
  abstract = {We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.},
  version = {1},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/QXALXM42/Razavi_et_al_2019_Generating_Diverse_High-Fidelity_Images_with_VQ-VAE-2.pdf;/Users/vitay/Documents/Zotero/storage/NRY56QQQ/1906.html}
}

@unpublished{Redmon2016,
  title = {{{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  shorttitle = {{{YOLO9000}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  date = {2016-12-25},
  eprint = {1612.08242},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1612.08242},
  urldate = {2020-11-29},
  abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/E8XBCI77/Redmon_Farhadi_2016_YOLO9000.pdf;/Users/vitay/Documents/Zotero/storage/S2HLBKQX/1612.html}
}

@unpublished{Redmon2016a,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016-05-09},
  eprint = {1506.02640},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.02640},
  urldate = {2020-09-17},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/7SQ2CXIK/Redmon_et_al_2016_You_Only_Look_Once.pdf;/Users/vitay/Documents/Zotero/storage/BWSGCSA8/1506.html}
}

@unpublished{Redmon2018,
  title = {{{YOLOv3}}: {{An Incremental Improvement}}},
  shorttitle = {{{YOLOv3}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  date = {2018-04-08},
  eprint = {1804.02767},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1804.02767},
  urldate = {2020-11-29},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/4WEANAXC/Redmon_Farhadi_2018_YOLOv3.pdf;/Users/vitay/Documents/Zotero/storage/P6LMJX59/1804.html}
}

@unpublished{Reed2016,
  title = {Generative {{Adversarial Text}} to {{Image Synthesis}}},
  author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
  date = {2016-06-05},
  eprint = {1605.05396},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1605.05396},
  urldate = {2020-09-23},
  abstract = {Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image model- ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/vitay/Documents/Zotero/storage/XLEVHECI/Reed_et_al_2016_Generative_Adversarial_Text_to_Image_Synthesis.pdf;/Users/vitay/Documents/Zotero/storage/7XPG3JKI/1605.html}
}

@unpublished{Reed2022,
  title = {A {{Generalist Agent}}},
  author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2022-05-12},
  eprint = {2205.06175},
  eprinttype = {arXiv},
  eprintclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2205.06175},
  urldate = {2022-05-17},
  abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  file = {/Users/vitay/Documents/Zotero/storage/JD5N36RT/Reed_et_al_2022_A_Generalist_Agent.pdf;/Users/vitay/Documents/Zotero/storage/Z6TD8MM2/2205.html}
}

@unpublished{Ren2016,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date = {2016-01-06},
  eprint = {1506.01497},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.01497},
  urldate = {2020-11-29},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/2KPVHN3D/Ren_et_al_2016_Faster_R-CNN.pdf;/Users/vitay/Documents/Zotero/storage/7PUYXDTZ/1506.html}
}

@unpublished{Ronneberger2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date = {2015-05-18},
  eprint = {1505.04597},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1505.04597},
  urldate = {2020-11-29},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/SKQ6WUGQ/Ronneberger_et_al_2015_U-Net.pdf;/Users/vitay/Documents/Zotero/storage/7NEKS7ZB/1505.html}
}

@unpublished{Rosenfeld2018,
  title = {The {{Elephant}} in the {{Room}}},
  author = {Rosenfeld, Amir and Zemel, Richard and Tsotsos, John K.},
  date = {2018-08-09},
  eprint = {1808.03305},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1808.03305},
  urldate = {2019-01-10},
  abstract = {We showcase a family of common failures of state-of-the art object detectors. These are obtained by replacing image sub-regions by another sub-image that contains a trained object. We call this "object transplanting". Modifying an image in this manner is shown to have a non-local impact on object detection. Slight changes in object position can affect its identity according to an object detector as well as that of other objects in the image. We provide some analysis and suggest possible reasons for the reported phenomena.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/WUZAK9UT/Rosenfeld et al_2018_The Elephant in the Room.pdf}
}

@article{Rossant2011,
  title = {Fitting {{Neuron Models}} to {{Spike Trains}}},
  author = {Rossant, Cyrille and Goodman, Dan F. M. and Fontaine, Bertrand and Platkiewicz, Jonathan and Magnusson, Anna K. and Brette, Romain},
  date = {2011},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {5},
  issn = {1662-453X},
  doi = {10.3389/fnins.2011.00009},
  url = {https://www.frontiersin.org/articles/10.3389/fnins.2011.00009/full},
  urldate = {2019-07-14},
  abstract = {Computational modeling is increasingly used to understand the function of neural circuits in systems neuroscience. These studies require models of individual neurons with realistic input-output properties. Recently, it was found that spiking models can accurately predict the precisely timed spike trains produced by cortical neurons in response to somatically injected currents, if properly fitted. This requires fitting techniques that are efficient and flexible enough to easily test different candidate models. We present a generic solution, based on the Brian simulator (a neural network simulator in Python), which allows the user to define and fit arbitrary neuron models to electrophysiological recordings. It relies on vectorization and parallel computing techniques to achieve efficiency. We demonstrate its use on neural recordings in the barrel cortex and in the auditory brainstem, and confirm that simple adaptive spiking models can accurately predict the response of cortical neurons. Finally, we show how a complex multicompartmental model can be reduced to a simple effective spiking model.},
  langid = {english},
  keywords = {brian,gpu,model fitting,optimization,Parallel Computing,python,simulation,spiking models},
  file = {/Users/vitay/Documents/Zotero/storage/92RCWIUU/Rossant et al_2011_Fitting Neuron Models to Spike Trains.pdf}
}

@article{Ruder2016,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  date = {2016-09},
  url = {http://arxiv.org/abs/1609.04747},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.}
}

@article{Rumelhart1986a,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date = {1986-10},
  journaltitle = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  url = {https://www.nature.com/articles/323533a0},
  urldate = {2020-09-16},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  issue = {6088},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/GW72C9DM/Rumelhart_et_al_1986_Learning_representations_by_back-propagating_errors.pdf;/Users/vitay/Documents/Zotero/storage/V5RQPRD7/323533a0.html}
}

@online{Russell2021,
  title = {Artificial {{Intelligence}}, {{Global Edition}}},
  author = {Russell, Stuart and Norvig, Peter},
  date = {2021-05-13},
  publisher = {Pearson Deutschland},
  url = {https://elibrary.pearson.de/book/99.150005/9781292401171},
  urldate = {2022-09-13},
  abstract = {{$<$}br{$><$}h4{$>$}The most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence{$<$}/h4{$>$} {$<$}br{$><$}p{$>$}The long-anticipated revision of {$<$}strong{$>$}Artificial Intelligence: A Modern Approach{$<$}/strong{$>$} explores the full breadth and depth of the field of artificial intelligence (AI). {$<$}br{$>$}The {$<$}strong{$>$}4th Edition{$<$}/strong{$>$} brings readers up to date on the latest technologies, present concepts in a more unified manner, and offers new or expanded coverage of {$<$}br{$>$}machine learning, deep learning, transfer learning, multi agent systems, robotics, natural language processing, causality, probabilistic programming, {$<$}br{$>$}privacy, fairness, and safe AI.{$<$}/p{$>$} {$<$}br{$><$}br{$>$}},
  isbn = {9781292401171},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/YP8TTSE3/9781292401171.html}
}

@article{Russin2020,
  title = {{{DEEP LEARNING NEEDS A PREFRONTAL CORTEX}}},
  author = {Russin, Jacob and O’Reilly, Randall C and Bengio, Yoshua},
  date = {2020},
  pages = {11},
  abstract = {Research seeking to build artificial systems capable of reproducing elements of human intelligence may benefit from a deeper consideration of the architecture and learning mechanisms of the human brain. In this brief review, we note a connection between many current challenges facing artificial intelligence and the functions of a particular brain area —the prefrontal cortex (PFC). This brain area is known to be involved in executive functions such as reasoning, rule-learning, deliberate or controlled processing, and abstract planning. Motivated by the hypothesis that these functions provide a form of out-of-distribution robustness currently not available in state-of-the-art AI systems, we elaborate on this connection and highlight some computational principles thought to be at work in PFC, with the goal of enhancing the synergy between neuroscience and machine learning.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/BLZ34NUP/Russin et al. - 2020 - DEEP LEARNING NEEDS A PREFRONTAL CORTEX.pdf}
}

@online{Sabour2017,
  title = {Dynamic {{Routing Between Capsules}}},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  date = {2017-11-07},
  eprint = {1710.09829},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1710.09829},
  url = {http://arxiv.org/abs/1710.09829},
  urldate = {2022-09-13},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/IH8IHUEY/Sabour_et_al_2017_Dynamic_Routing_Between_Capsules.pdf;/Users/vitay/Documents/Zotero/storage/7AHE7G38/1710.html}
}

@unpublished{Salimans2016,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  date = {2016-06-10},
  eprint = {1606.03498},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1606.03498},
  urldate = {2020-12-09},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/vitay/Documents/Zotero/storage/YUSXZQVK/Salimans_et_al_2016_Improved_Techniques_for_Training_GANs.pdf;/Users/vitay/Documents/Zotero/storage/AXILU3MN/1606.html}
}

@article{Salinas2020,
  title = {{{DeepAR}}: {{Probabilistic}} Forecasting with Autoregressive Recurrent Networks},
  shorttitle = {{{DeepAR}}},
  author = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
  date = {2020-07-01},
  journaltitle = {International Journal of Forecasting},
  shortjournal = {International Journal of Forecasting},
  volume = {36},
  number = {3},
  pages = {1181--1191},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.07.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0169207019301888},
  urldate = {2023-08-04},
  abstract = {Probabilistic forecasting, i.e., estimating a time series’ future probability distribution given its past, is a key enabler for optimizing business processes. In retail businesses, for example, probabilistic demand forecasts are crucial for having the right inventory available at the right time and in the right place. This paper proposes DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an autoregressive recurrent neural network model on a large number of related time series. We demonstrate how the application of deep learning techniques to forecasting can overcome many of the challenges that are faced by widely-used classical approaches to the problem. By means of extensive empirical evaluations on several real-world forecasting datasets, we show that our methodology produces more accurate forecasts than other state-of-the-art methods, while requiring minimal manual work.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/7423R98X/Salinas_et_al_2020_DeepAR.pdf}
}

@inproceedings{Sathe2022,
  title = {Overview of {{Image Caption Generators}} and {{Its Applications}}},
  booktitle = {Proceeding of {{International Conference}} on {{Computational Science}} and {{Applications}}},
  author = {Sathe, Shreeya and Shinde, Shivani and Chorge, Shriya and Thakare, Shalaka and Kulkarni, Lalit},
  editor = {Bhalla, Subhash and Bedekar, Mangesh and Phalnikar, Rashmi and Sirsikar, Sumedha},
  date = {2022},
  series = {Algorithms for {{Intelligent Systems}}},
  pages = {105--110},
  publisher = {Springer Nature},
  location = {Singapore},
  doi = {10.1007/978-981-19-0863-7_8},
  abstract = {The emergence of deep learning has transformed mundane tasks and reduced human errors. Image Captioning is an application of deep learning which takes an image as input and gives a well-formed description for the required image based on features and objects present in it. Image Caption Generation has branched out into several applications ranging from assistive technology to agriculture and manufacturing sectors. The existing solutions use this deep learning application to aid visually impaired people by helping them understand their environment, in robotic and industrial applications by automating processes and limiting human intervention. In this paper, we present a detailed analysis of different uses of image captioning, technologies implemented, their limitations and the scope to extend utilization in other domains.},
  isbn = {978-981-19086-3-7},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/CRXBP4SA/Sathe_et_al_2022_Overview_of_Image_Caption_Generators_and_Its_Applications.pdf}
}

@unpublished{Scellier2022,
  title = {Agnostic {{Physics-Driven Deep Learning}}},
  author = {Scellier, Benjamin and Mishra, Siddhartha and Bengio, Yoshua and Ollivier, Yann},
  date = {2022-05-30},
  eprint = {2205.15021},
  eprinttype = {arXiv},
  eprintclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2205.15021},
  urldate = {2022-06-06},
  abstract = {This work establishes that a physical system can perform statistical learning without gradient computations, via an Agnostic Equilibrium Propagation (Aeqprop) procedure that combines energy minimization, homeostatic control, and nudging towards the correct response. In Aeqprop, the specifics of the system do not have to be known: the procedure is based only on external manipulations, and produces a stochastic gradient descent without explicit gradient computations. Thanks to nudging, the system performs a true, order-one gradient step for each training sample, in contrast with order-zero methods like reinforcement or evolutionary strategies, which rely on trial and error. This procedure considerably widens the range of potential hardware for statistical learning to any system with enough controllable parameters, even if the details of the system are poorly known. Aeqprop also establishes that in natural (bio)physical systems, genuine gradient-based statistical learning may result from generic, relatively simple mechanisms, without backpropagation and its requirement for analytic knowledge of partial derivatives.},
  file = {/Users/vitay/Documents/Zotero/storage/966H5XZB/Scellier_et_al_2022_Agnostic_Physics-Driven_Deep_Learning.pdf;/Users/vitay/Documents/Zotero/storage/MAPR4REJ/2205.html}
}

@thesis{Schluter2017,
  type = {phdthesis},
  title = {Deep {{Learning}} for {{Event Detection}}, {{Sequence Labelling}} and {{Similarity Estimation}} in {{Music Signals}}},
  author = {Schlüter, Jan},
  date = {2017},
  institution = {Johannes Kepler University Linz},
  url = {http://www.ofai.at/~jan.schlueter/pubs/phd/phd.pdf},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/ZHHUWB7S/Schlüter_Deep_Learning_for_Event_Detection,_Sequence_Labelling_and_Similarity_Estimation.pdf}
}

@inproceedings{Schroder2019,
  title = {Feature {{Map Transformation}} for {{Multi-sensor Fusion}} in {{Object Detection Networks}} for {{Autonomous Driving}}},
  booktitle = {Advances in {{Computer Vision}}},
  author = {Schröder, Enrico and Braun, Sascha and Mählisch, Mirko and Vitay, Julien and Hamker, Fred},
  editor = {Arai, Kohei and Kapoor, Supriya},
  date = {2019},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {118--131},
  publisher = {Springer International Publishing},
  abstract = {We present a general framework for fusing pre-trained object detection networks for multiple sensor modalities in autonomous cars at an intermediate stage. The key innovation is an autoencoder-inspired Transformer module which transforms perspective as well as feature activation characteristics from one sensor modality to another. Transformed feature maps can be combined with those of a modality-native feature extractor to enhance performance and reliability through a simple fusion scheme. Our approach is not limited to specific object detection network types. Compared to other methods, our framework allows fusion of pre-trained object detection networks and fuses sensor modalities at a single stage, resulting in a modular and traceable architecture. We show effectiveness of the proposed scheme by fusing camera and Lidar information to detect objects using our own as well as the KITTI dataset.},
  isbn = {978-3-030-17798-0},
  langid = {english},
  keywords = {Autonomous driving,Lidar,Object detection,Perception,Sensor fusion},
  file = {/Users/vitay/Documents/Zotero/storage/4LAR4RFJ/Schröder et al_2020_Feature Map Transformation for Multi-sensor Fusion in Object Detection Networks.pdf}
}

@inproceedings{Schroder2020,
  title = {Monocular {{3D Object Detection Using Feature Map Transformation}}: {{Towards Learning Perspective-Invariant Scene Representations}}},
  shorttitle = {Monocular {{3D Object Detection Using Feature Map Transformation}}},
  booktitle = {2020 {{Fourth IEEE International Conference}} on {{Robotic Computing}} ({{IRC}})},
  author = {Schröder, E. and Mählisch, M. and Vitay, J. and Hamker, F.},
  date = {2020-11},
  pages = {383--390},
  publisher = {IEEE},
  location = {Taichung, Taiwan},
  doi = {10.1109/IRC.2020.00066},
  abstract = {In this paper we propose to use a feature map transformation network for the task of monocular 3D object detection. Given a monocular camera image, the transformation network encodes features of the scene in an abstract, perspective-invariant latent representation. This latent representation can then be decoded into a bird's-eye view representation to estimate objects' position and rotation in 3D space. In our experiments on the Kitti object detection dataset we show that our model is able to learn to estimate objects' 3D position from a monocular camera image alone without having any explicit geometric model or other prior information on how to perform the transformation. While performing slightly worse than networks which are purpose-built for this task, our approach allows feeding the same bird's-eye view object detection network with input data from different sensor modalities. This can increase redundancy in a safety-critical environment. We present additional experiments to gain insight into the properties of the learned perspective-invariant abstract scene representation.},
  eventtitle = {2020 {{Fourth IEEE International Conference}} on {{Robotic Computing}} ({{IRC}})},
  keywords = {3D Object Detection,Autonomous Driving,Cameras,Feature extraction,Feature Map Transformation,Monocular Object Detection,Object detection,Robot sensing systems,Solid modeling,Task analysis,Three-dimensional displays},
  file = {/Users/vitay/Documents/Zotero/storage/NN6SXM56/9287883.html}
}

@article{Sejnowski2020,
  title = {The Unreasonable Effectiveness of Deep Learning in Artificial Intelligence},
  author = {Sejnowski, Terrence J.},
  date = {2020-01-28},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  eprint = {31992643},
  eprinttype = {pmid},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1907373117},
  url = {https://www.pnas.org/content/early/2020/01/23/1907373117},
  urldate = {2020-03-04},
  abstract = {Deep learning networks have been trained to recognize speech, caption photographs, and translate text between languages at high levels of performance. Although applications of deep learning networks to real-world problems have become ubiquitous, our understanding of why they are so effective is lacking. These empirical results should not be possible according to sample complexity in statistics and nonconvex optimization theory. However, paradoxes in the training and effectiveness of deep learning networks are being investigated and insights are being found in the geometry of high-dimensional spaces. A mathematical theory of deep learning would illuminate how they function, allow us to assess the strengths and weaknesses of different network architectures, and lead to major improvements. Deep learning has provided natural ways for humans to communicate with digital devices and is foundational for building artificial general intelligence. Deep learning was inspired by the architecture of the cerebral cortex and insights into autonomy and general intelligence may be found in other brain regions that are essential for planning and survival, but major breakthroughs will be needed to achieve these goals.},
  langid = {english},
  keywords = {artificial intelligence,deep learning,neural networks},
  file = {/Users/vitay/Documents/Zotero/storage/JVJRHCS3/Sejnowski_2020_The_unreasonable_effectiveness_of_deep_learning_in_artificial_intelligence.pdf;/Users/vitay/Documents/Zotero/storage/8YWCQ6IM/1907373117.html}
}

@unpublished{Sengupta2019,
  title = {A {{Review}} of {{Deep Learning}} with {{Special Emphasis}} on {{Architectures}}, {{Applications}} and {{Recent Trends}}},
  author = {Sengupta, Saptarshi and Basak, Sanchita and Saikia, Pallabi and Paul, Sayak and Tsalavoutis, Vasilios and Atiah, Frederick and Ravi, Vadlamani and Peters, Alan},
  date = {2019-05-30},
  eprint = {1905.13294},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.13294},
  urldate = {2019-09-28},
  abstract = {Deep learning has solved a problem that as little as five years ago was thought by many to be intractable - the automatic recognition of patterns in data; and it can do so with accuracy that often surpasses human beings. It has solved problems beyond the realm of traditional, hand-crafted machine learning algorithms and captured the imagination of practitioners trying to make sense out of the flood of data that now inundates our society. As public awareness of the efficacy of DL increases so does the desire to make use of it. But even for highly trained professionals it can be daunting to approach the rapidly increasing body of knowledge produced by experts in the field. Where does one start? How does one determine if a particular model is applicable to their problem? How does one train and deploy such a network? A primer on the subject can be a good place to start. With that in mind, we present an overview of some of the key multilayer ANNs that comprise DL. We also discuss some new automatic architecture optimization protocols that use multi-agent approaches. Further, since guaranteeing system uptime is becoming critical to many computer applications, we include a section on using neural networks for fault detection and subsequent mitigation. This is followed by an exploratory survey of several application areas where DL has emerged as a game-changing technology: anomalous behavior detection in financial applications or in financial time-series forecasting, predictive and prescriptive analytics, medical image processing and analysis and power systems research. The thrust of this review is to outline emerging areas of application-oriented research within the DL community as well as to provide a reference to researchers seeking to use it in their work for what it does best: statistical pattern recognition with unparalleled learning capacity with the ability to scale with information.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/3IY44EX5/Sengupta et al_2019_A Review of Deep Learning with Special Emphasis on Architectures, Applications.pdf;/Users/vitay/Documents/Zotero/storage/GJMH4GEX/1905.html}
}

@article{Signoroni2019,
  title = {Deep {{Learning Meets Hyperspectral Image Analysis}}: {{A Multidisciplinary Review}}},
  shorttitle = {Deep {{Learning Meets Hyperspectral Image Analysis}}},
  author = {Signoroni, Alberto and Savardi, Mattia and Baronio, Annalisa and Benini, Sergio},
  date = {2019-05},
  journaltitle = {Journal of Imaging},
  volume = {5},
  number = {5},
  pages = {52},
  doi = {10.3390/jimaging5050052},
  url = {https://www.mdpi.com/2313-433X/5/5/52},
  urldate = {2019-09-27},
  abstract = {Modern hyperspectral imaging systems produce huge datasets potentially conveying a great abundance of information; such a resource, however, poses many challenges in the analysis and interpretation of these data. Deep learning approaches certainly offer a great variety of opportunities for solving classical imaging tasks and also for approaching new stimulating problems in the spatial\&ndash;spectral domain. This is fundamental in the driving sector of Remote Sensing where hyperspectral technology was born and has mostly developed, but it is perhaps even more true in the multitude of current and evolving application sectors that involve these imaging technologies. The present review develops on two fronts: on the one hand, it is aimed at domain professionals who want to have an updated overview on how hyperspectral acquisition techniques can combine with deep learning architectures to solve specific tasks in different application fields. On the other hand, we want to target the machine learning and computer vision experts by giving them a picture of how deep learning technologies are applied to hyperspectral data from a multidisciplinary perspective. The presence of these two viewpoints and the inclusion of application fields other than Remote Sensing are the original contributions of this review, which also highlights some potentialities and critical issues related to the observed development trends.},
  langid = {english},
  keywords = {deep learning,hyperspectral imaging,image processing,machine learning,neural networks},
  file = {/Users/vitay/Documents/Zotero/storage/39NIWBC3/Signoroni et al_2019_Deep Learning Meets Hyperspectral Image Analysis.pdf;/Users/vitay/Documents/Zotero/storage/WKWDILEU/52.html}
}

@unpublished{Simon2019,
  title = {Complexer-{{YOLO}}: {{Real-Time 3D Object Detection}} and {{Tracking}} on {{Semantic Point Clouds}}},
  shorttitle = {Complexer-{{YOLO}}},
  author = {Simon, Martin and Amende, Karl and Kraus, Andrea and Honer, Jens and Sämann, Timo and Kaulbersch, Hauke and Milz, Stefan and Gross, Horst Michael},
  date = {2019-04-16},
  eprint = {1904.07537},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.07537},
  urldate = {2019-04-20},
  abstract = {Accurate detection of 3D objects is a fundamental problem in computer vision and has an enormous impact on autonomous cars, augmented/virtual reality and many applications in robotics. In this work we present a novel fusion of neural network based state-of-the-art 3D detector and visual semantic segmentation in the context of autonomous driving. Additionally, we introduce Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable evaluation metric for comparison of object detections, which speeds up our inference time up to 20\textbackslash\% and halves training time. On top, we apply state-of-the-art online multi target feature tracking on the object measurements to further increase accuracy and robustness utilizing temporal information. Our experiments on KITTI show that we achieve same results as state-of-the-art in all related categories, while maintaining the performance and accuracy trade-off and still run in real-time. Furthermore, our model is the first one that fuses visual semantic with 3D object detection.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/EMHMBTGX/Simon et al_2019_Complexer-YOLO.pdf;/Users/vitay/Documents/Zotero/storage/52VIDQ94/1904.html}
}

@article{Simoncelli2001,
  title = {Natural {{Image Statistics}} and {{Neural Representation}}},
  author = {Simoncelli, Eero P and Olshausen, Bruno A},
  date = {2001-03-01},
  journaltitle = {Annual Review of Neuroscience},
  shortjournal = {Annu. Rev. Neurosci.},
  volume = {24},
  number = {1},
  pages = {1193--1216},
  publisher = {Annual Reviews},
  issn = {0147-006X},
  doi = {10.1146/annurev.neuro.24.1.1193},
  url = {https://www.annualreviews.org/doi/10.1146/annurev.neuro.24.1.1193},
  urldate = {2021-01-30},
  abstract = {It has long been assumed that sensory neurons are adapted, through both evolutionary and developmental processes, to the statistical properties of the signals to which they are exposed. Attneave (1954), Barlow (1961) proposed that information theory could provide a link between environmental statistics and neural responses through the concept of coding efficiency. Recent developments in statistical modeling, along with powerful computational tools, have enabled researchers to study more sophisticated statistical models for visual images, to validate these models empirically against large sets of data, and to begin experimentally testing the efficient coding hypothesis for both individual neurons and populations of neurons.},
  file = {/Users/vitay/Documents/Zotero/storage/PCV4W9J8/Simoncelli_Olshausen_2001_Natural_Image_Statistics_and_Neural_Representation.pdf;/Users/vitay/Documents/Zotero/storage/9F5RNK75/annurev.neuro.24.1.html}
}

@article{Simonyan2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2015},
  journaltitle = {International Conference on Learning Representations (ICRL)},
  pages = {1--14},
  issn = {9781450341448},
  doi = {10.1016/j.infsof.2008.09.005},
  url = {http://arxiv.org/abs/1409.1556},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.}
}

@article{Sinz2019,
  title = {Engineering a {{Less Artificial Intelligence}}},
  author = {Sinz, Fabian H. and Pitkow, Xaq and Reimer, Jacob and Bethge, Matthias and Tolias, Andreas S.},
  date = {2019-09-25},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {103},
  number = {6},
  eprint = {31557461},
  eprinttype = {pmid},
  pages = {967--979},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.08.034},
  url = {https://www.cell.com/neuron/abstract/S0896-6273(19)30740-8},
  urldate = {2019-11-02},
  langid = {english},
  keywords = {artificial intelligence,generalization,inductive bias,machine learning,neuroscience,robustness,sensory systems},
  file = {/Users/vitay/Documents/Zotero/storage/XMWGL96C/Sinz et al_2019_Engineering a Less Artificial Intelligence.pdf;/Users/vitay/Documents/Zotero/storage/4A85LP6L/S0896-6273(19)30740-8.html}
}

@incollection{Sohn2015,
  title = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  date = {2015},
  pages = {3483--3491},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf},
  urldate = {2019-10-04},
  file = {/Users/vitay/Documents/Zotero/storage/E8NHNXV5/Sohn et al_2015_Learning Structured Output Representation using Deep Conditional Generative.pdf;/Users/vitay/Documents/Zotero/storage/XQUDKBD7/5775-learning-structured-output-representation-using-deep-conditional-generative-models.html}
}

@unpublished{Springenberg2015,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  shorttitle = {Striving for {{Simplicity}}},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  date = {2015-04-13},
  eprint = {1412.6806},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1412.6806},
  urldate = {2020-11-22},
  abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/vitay/Documents/Zotero/storage/WTBJDX8X/Springenberg_et_al_2015_Striving_for_Simplicity.pdf;/Users/vitay/Documents/Zotero/storage/DI2CZY2I/1412.html}
}

@article{Srivastava2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  date = {2014},
  journaltitle = {Journal of Machine Learning Research},
  volume = {15},
  number = {56},
  pages = {1929--1958},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v15/srivastava14a.html},
  urldate = {2020-11-17},
  file = {/Users/vitay/Documents/Zotero/storage/3VQZDNVC/Srivastava_et_al_2014_Dropout.pdf;/Users/vitay/Documents/Zotero/storage/NQ8RB523/srivastava14a.html}
}

@unpublished{Srivastava2015,
  title = {Highway {{Networks}}},
  author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
  date = {2015-11-03},
  eprint = {1505.00387},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1505.00387},
  urldate = {2020-11-22},
  abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
  keywords = {68T01,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,G.1.6,I.2.6},
  file = {/Users/vitay/Documents/Zotero/storage/8EREW6CV/Srivastava_et_al_2015_Highway_Networks.pdf;/Users/vitay/Documents/Zotero/storage/AH5ZLTMX/1505.html}
}

@article{Sternberg1981,
  title = {People's Conceptions of Intelligence},
  author = {Sternberg, Robert J. and Conway, Barbara E. and Ketron, Jerry L. and Bernstein, Morty},
  date = {1981},
  journaltitle = {Journal of Personality and Social Psychology},
  volume = {41},
  pages = {37--55},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1315},
  doi = {10.1037/0022-3514.41.1.37},
  abstract = {Three experiments investigated experts' and laypersons' conceptions of intelligence. In Exp I, 61 persons studying in a college library, 63 entering a supermarket, and 62 waiting for trains in a railroad station were asked to list behaviors characteristic of either "intelligence," "academic intelligence," "everyday intelligence," or "unintelligence," and to rate themselves on each. In Exp II, 140 experts and 122 laypersons (excluding students) were asked to rate various properties of the behaviors listed in Exp I; the laypersons also rated themselves on the 3 kinds of intelligence and took the Henmon-Nelson Tests of Mental Abilities. In Exp III, 65 laypersons received written descriptions of behaviors characterizing fictitious people and were asked to rate these people's intelligence. Results show that well-formed prototypes corresponding to the various kinds of intelligence, that these prototypes were quite similar for experts and laypersons, were closely related to certain psychological theories of intelligence, and were used in the evaluation of one's own and other's intelligence. Moreover, proximity of one's behavioral self-characterizations to an ideal prototype was strongly related to intelligence. (34 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/vitay/Documents/Zotero/storage/WPINPRNV/1982-05773-001.html}
}

@article{Stollenga2014,
  title = {Deep {{Networks}} with {{Internal Selective Attention}} through {{Feedback Connections}}},
  author = {Stollenga, Marijn and Masci, Jonathan and Gomez, Faustino and Schmidhuber, Juergen},
  date = {2014-07},
  url = {http://arxiv.org/abs/1407.3068},
  abstract = {Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNets feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.},
  file = {/Users/vitay/Documents/Zotero/storage/A6W46UU8/Stollenga et al_2014_Deep Networks with Internal Selective Attention through Feedback Connections.pdf}
}

@article{Sussillo2009,
  title = {Generating Coherent Patterns of Activity from Chaotic Neural Networks.},
  author = {Sussillo, David and Abbott, L F},
  date = {2009-08},
  journaltitle = {Neuron},
  volume = {63},
  number = {4},
  eprint = {19709635},
  eprinttype = {pubmed},
  pages = {544--57},
  doi = {10.1016/j.neuron.2009.07.018},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/19709635},
  abstract = {Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
  file = {/Users/vitay/Documents/Zotero/storage/3RQV8GZY/Sussillo_Abbott_2009_Generating coherent patterns of activity from chaotic neural networks.pdf}
}

@unpublished{Sutskever2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  date = {2014-12-14},
  eprint = {1409.3215},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1409.3215},
  urldate = {2019-11-07},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/ND42WV8C/Sutskever et al_2014_Sequence to Sequence Learning with Neural Networks.pdf;/Users/vitay/Documents/Zotero/storage/NI95LB2V/1409.html}
}

@online{Svyatkovskiy2020,
  title = {{{IntelliCode Compose}}: {{Code Generation Using Transformer}}},
  shorttitle = {{{IntelliCode Compose}}},
  author = {Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel},
  date = {2020-10-29},
  eprint = {2005.08025},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2005.08025},
  urldate = {2022-09-13},
  abstract = {In software development through integrated development environments (IDEs), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and APIs, or arguments. In this paper, we introduce IntelliCode Compose \$-\$ a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages state-of-the-art generative transformer model trained on 1.2 billion lines of source code in Python, \$C\textbackslash\#\$, JavaScript and TypeScript programming languages. IntelliCode Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code IDE and Azure Notebook. Our best model yields an average edit similarity of \$86.7\textbackslash\%\$ and a perplexity of 1.82 for Python programming language.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/EDBKTNII/Svyatkovskiy_et_al_2020_IntelliCode_Compose.pdf;/Users/vitay/Documents/Zotero/storage/HHCF7HW9/2005.html}
}

@unpublished{Szegedy2014,
  title = {Going {{Deeper}} with {{Convolutions}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  date = {2014-09-16},
  eprint = {1409.4842},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1409.4842},
  urldate = {2019-03-02},
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/VH8KV36L/Szegedy et al_2014_Going Deeper with Convolutions.pdf}
}

@unpublished{Szegedy2015,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  date = {2015-12-11},
  eprint = {1512.00567},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1512.00567},
  urldate = {2020-11-20},
  abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/4NHFDXZU/Szegedy_et_al_2015_Rethinking_the_Inception_Architecture_for_Computer_Vision.pdf;/Users/vitay/Documents/Zotero/storage/QRTB4XWX/1512.html}
}

@inproceedings{Taigman2014,
  title = {{{DeepFace}}: {{Closing}} the {{Gap}} to {{Human-Level Performance}} in {{Face Verification}}},
  shorttitle = {{{DeepFace}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
  date = {2014-06},
  pages = {1701--1708},
  publisher = {IEEE},
  location = {Columbus, OH, USA},
  doi = {10.1109/CVPR.2014.220},
  url = {https://ieeexplore.ieee.org/document/6909616},
  urldate = {2020-11-22},
  abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect ⇒ align ⇒ represent ⇒ classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4,000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35\% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27\%, closely approaching human-level performance.},
  eventtitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4799-5118-5},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/FPTINPMJ/Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performan.pdf}
}

@article{Tanaka2019a,
  ids = {Tanaka2018},
  title = {Recent Advances in Physical Reservoir Computing: {{A}} Review},
  shorttitle = {Recent Advances in Physical Reservoir Computing},
  author = {Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  date = {2019-07-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {115},
  pages = {100--123},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.03.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608019300784},
  urldate = {2019-05-24},
  abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
  keywords = {Machine learning,Neural networks,Neuromorphic device,Nonlinear dynamical systems,Reservoir computing},
  file = {/Users/vitay/Documents/Zotero/storage/NYDCDB62/Tanaka et al_2019_Recent advances in physical reservoir computing.pdf;/Users/vitay/Documents/Zotero/storage/WNDC8LK9/Tanaka et al_2018_Recent Advances in Physical Reservoir Computing.pdf;/Users/vitay/Documents/Zotero/storage/ZY7NYR7P/S0893608019300784.html}
}

@unpublished{Tolstikhin2021,
  title = {{{MLP-Mixer}}: {{An}} All-{{MLP Architecture}} for {{Vision}}},
  shorttitle = {{{MLP-Mixer}}},
  author = {Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
  date = {2021-06-11},
  eprint = {2105.01601},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2105.01601},
  urldate = {2022-02-18},
  abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
  file = {/Users/vitay/Documents/Zotero/storage/HMNZRLIJ/Tolstikhin_et_al_2021_MLP-Mixer.pdf;/Users/vitay/Documents/Zotero/storage/4UMZA62D/2105.html}
}

@unpublished{Tzaban2022,
  title = {Stitch It in {{Time}}: {{GAN-Based Facial Editing}} of {{Real Videos}}},
  shorttitle = {Stitch It in {{Time}}},
  author = {Tzaban, Rotem and Mokady, Ron and Gal, Rinon and Bermano, Amit H. and Cohen-Or, Daniel},
  date = {2022-01-21},
  eprint = {2201.08361},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.08361},
  urldate = {2022-02-04},
  abstract = {The ability of Generative Adversarial Networks to encode rich semantics within their latent space has been widely adopted for facial image editing. However, replicating their success with videos has proven challenging. Sets of high-quality facial videos are lacking, and working with videos introduces a fundamental barrier to overcome - temporal coherency. We propose that this barrier is largely artificial. The source video is already temporally coherent, and deviations from this state arise in part due to careless treatment of individual components in the editing pipeline. We leverage the natural alignment of StyleGAN and the tendency of neural networks to learn low frequency functions, and demonstrate that they provide a strongly consistent prior. We draw on these insights and propose a framework for semantic editing of faces in videos, demonstrating significant improvements over the current state-of-the-art. Our method produces meaningful face manipulations, maintains a higher degree of temporal consistency, and can be applied to challenging, high quality, talking head videos which current methods struggle with.},
  file = {/Users/vitay/Documents/Zotero/storage/5FN7PSMB/Tzaban_et_al_2022_Stitch_it_in_Time.pdf;/Users/vitay/Documents/Zotero/storage/JDUJDBQ9/2201.html}
}

@article{vanEngelen2020,
  title = {A Survey on Semi-Supervised Learning},
  author = {family=Engelen, given=Jesper E., prefix=van, useprefix=true and Hoos, Holger H.},
  date = {2020-02-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {109},
  number = {2},
  pages = {373--440},
  issn = {1573-0565},
  doi = {10.1007/s10994-019-05855-6},
  url = {https://doi.org/10.1007/s10994-019-05855-6},
  urldate = {2021-04-13},
  abstract = {Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/JDYYBTYY/van_Engelen_Hoos_2020_A_survey_on_semi-supervised_learning.pdf}
}

@inproceedings{VanEtten2019,
  title = {Satellite {{Imagery Multiscale Rapid Detection}} with {{Windowed Networks}}},
  booktitle = {2019 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Van Etten, Adam},
  date = {2019-01},
  eprint = {1809.09978},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {735--743},
  doi = {10.1109/WACV.2019.00083},
  url = {http://arxiv.org/abs/1809.09978},
  urldate = {2023-03-03},
  abstract = {Detecting small objects over large areas remains a significant challenge in satellite imagery analytics. Among the challenges is the sheer number of pixels and geographical extent per image: a single DigitalGlobe satellite image encompasses over 64 km2 and over 250 million pixels. Another challenge is that objects of interest are often minuscule (\textasciitilde pixels in extent even for the highest resolution imagery), which complicates traditional computer vision techniques. To address these issues, we propose a pipeline (SIMRDWN) that evaluates satellite images of arbitrarily large size at native resolution at a rate of {$>$} 0.2 km2/s. Building upon the tensorflow object detection API paper, this pipeline offers a unified approach to multiple object detection frameworks that can run inference on images of arbitrary size. The SIMRDWN pipeline includes a modified version of YOLO (known as YOLT), along with the models of the tensorflow object detection API: SSD, Faster R-CNN, and R-FCN. The proposed approach allows comparison of the performance of these four frameworks, and can rapidly detect objects of vastly different scales with relatively little training data over multiple sensors. For objects of very different scales (e.g. airplanes versus airports) we find that using two different detectors at different scales is very effective with negligible runtime cost.We evaluate large test images at native resolution and find mAP scores of 0.2 to 0.8 for vehicle localization, with the YOLT architecture achieving both the highest mAP and fastest inference speed.},
  file = {/Users/vitay/Documents/Zotero/storage/Q4UNN9A8/Van_Etten_2019_Satellite_Imagery_Multiscale_Rapid_Detection_with_Windowed_Networks.pdf}
}

@unpublished{Vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-06-12},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2019-08-25},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/UMXVELF5/Vaswani et al_2017_Attention Is All You Need.pdf;/Users/vitay/Documents/Zotero/storage/UHB7LGPB/1706.html}
}

@inproceedings{Vaswani2017a,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}
}

@online{Vaze2022,
  title = {Generalized {{Category Discovery}}},
  author = {Vaze, Sagar and Han, Kai and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2022-06-18},
  eprint = {2201.02609},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.02609},
  url = {http://arxiv.org/abs/2201.02609},
  urldate = {2022-10-14},
  abstract = {In this paper, we consider a highly general image recognition setting wherein, given a labelled and unlabelled set of images, the task is to categorize all images in the unlabelled set. Here, the unlabelled images may come from labelled classes or from novel ones. Existing recognition methods are not able to deal with this setting, because they make several restrictive assumptions, such as the unlabelled instances only coming from known - or unknown - classes, and the number of unknown classes being known a-priori. We address the more unconstrained setting, naming it 'Generalized Category Discovery', and challenge all these assumptions. We first establish strong baselines by taking state-of-the-art algorithms from novel category discovery and adapting them for this task. Next, we propose the use of vision transformers with contrastive representation learning for this open-world setting. We then introduce a simple yet effective semi-supervised \$k\$-means method to cluster the unlabelled data into seen and unseen classes automatically, substantially outperforming the baselines. Finally, we also propose a new approach to estimate the number of classes in the unlabelled data. We thoroughly evaluate our approach on public datasets for generic object classification and on fine-grained datasets, leveraging the recent Semantic Shift Benchmark suite. Project page at https://www.robots.ox.ac.uk/\textasciitilde vgg/research/gcd},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/92LE53J8/Vaze_et_al_2022_Generalized_Category_Discovery.pdf;/Users/vitay/Documents/Zotero/storage/CETSBNEJ/2201.html}
}

@article{Vincent2010,
  title = {Stacked {{Denoising Autoencoders}}: {{Learning Useful Representations}} in a {{Deep Network}} with a {{Local Denoising Criterion}}},
  shorttitle = {Stacked {{Denoising Autoencoders}}},
  author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  date = {2010-12-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {11},
  pages = {3371--3408},
  issn = {1532-4435},
  abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
  file = {/Users/vitay/Documents/Zotero/storage/LTZI5Z5Q/Vincent et al. - Stacked Denoising Autoencoders Learning Useful Re.pdf;/Users/vitay/Documents/Zotero/storage/Q27GNGE3/Vincent_et_al_2010_Stacked_Denoising_Autoencoders.pdf}
}

@unpublished{Vinyals2015,
  title = {Show and {{Tell}}: {{A Neural Image Caption Generator}}},
  shorttitle = {Show and {{Tell}}},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  date = {2015-04-20},
  eprint = {1411.4555},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1411.4555},
  urldate = {2020-12-24},
  abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/67V8FNQD/Vinyals_et_al_2015_Show_and_Tell.pdf;/Users/vitay/Documents/Zotero/storage/6SM52VGA/1411.html}
}

@unpublished{Vinyals2016,
  title = {Matching {{Networks}} for {{One Shot Learning}}},
  author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
  date = {2016-06-13},
  eprint = {1606.04080},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1606.04080},
  urldate = {2019-04-10},
  abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6\% to 93.2\% and from 88.0\% to 93.8\% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/SSRWFKP3/Vinyals et al_2016_Matching Networks for One Shot Learning.pdf;/Users/vitay/Documents/Zotero/storage/9NQ2Y8CM/1606.html}
}

@article{Vogels2011,
  title = {Inhibitory {{Plasticity Balances Excitation}} and {{Inhibition}} in {{Sensory Pathways}} and {{Memory Networks}}},
  author = {Vogels, T. P. and Sprekeler, H. and Zenke, F. and Clopath, C. and Gerstner, W.},
  date = {2011-12-16},
  journaltitle = {Science},
  volume = {334},
  number = {6062},
  eprint = {22075724},
  eprinttype = {pmid},
  pages = {1569--1573},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1211095},
  url = {https://science.sciencemag.org/content/334/6062/1569},
  urldate = {2021-01-30},
  abstract = {Cortical neurons receive balanced excitatory and inhibitory synaptic currents. Such a balance could be established and maintained in an experience-dependent manner by synaptic plasticity at inhibitory synapses. We show that this mechanism provides an explanation for the sparse firing patterns observed in response to natural stimuli and fits well with a recently observed interaction of excitatory and inhibitory receptive field plasticity. The introduction of inhibitory plasticity in suitable recurrent networks provides a homeostatic mechanism that leads to asynchronous irregular network states. Further, it can accommodate synaptic memories with activity patterns that become indiscernible from the background state but can be reactivated by external stimuli. Our results suggest an essential role of inhibitory plasticity in the formation and maintenance of functional cortical circuitry. Plasticity at inhibitory synapses maintains balanced excitatory and inhibitory synaptic inputs at cortical neurons. Plasticity at inhibitory synapses maintains balanced excitatory and inhibitory synaptic inputs at cortical neurons.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/XTZEPAFQ/1569.html}
}

@unpublished{Wang2018a,
  title = {Toward {{Characteristic-Preserving Image-based Virtual Try-On Network}}},
  author = {Wang, Bochao and Zheng, Huabin and Liang, Xiaodan and Chen, Yimin and Lin, Liang and Yang, Meng},
  date = {2018-09-12},
  eprint = {1807.07688},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1807.07688},
  urldate = {2020-11-27},
  abstract = {Image-based virtual try-on systems for fitting new in-shop clothes into a person image have attracted increasing research attention, yet is still challenging. A desirable pipeline should not only transform the target clothes into the most fitting shape seamlessly but also preserve well the clothes identity in the generated image, that is, the key characteristics (e.g. texture, logo, embroidery) that depict the original clothes. However, previous image-conditioned generation works fail to meet these critical requirements towards the plausible virtual try-on performance since they fail to handle large spatial misalignment between the input image and target clothes. Prior work explicitly tackled spatial deformation using shape context matching, but failed to preserve clothing details due to its coarse-to-fine strategy. In this work, we propose a new fully-learnable Characteristic-Preserving Virtual Try-On Network(CP-VTON) for addressing all real-world challenges in this task. First, CP-VTON learns a thin-plate spline transformation for transforming the in-shop clothes into fitting the body shape of the target person via a new Geometric Matching Module (GMM) rather than computing correspondences of interest points as prior works did. Second, to alleviate boundary artifacts of warped clothes and make the results more realistic, we employ a Try-On Module that learns a composition mask to integrate the warped clothes and the rendered image to ensure smoothness. Extensive experiments on a fashion dataset demonstrate our CP-VTON achieves the state-of-the-art virtual try-on performance both qualitatively and quantitatively.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/AJMR9ELC/Wang_et_al_2018_Toward_Characteristic-Preserving_Image-based_Virtual_Try-On_Network.pdf;/Users/vitay/Documents/Zotero/storage/NF8HHEBH/1807.html}
}

@unpublished{Weng2020,
  title = {Sequential {{Forecasting}} of 100,000 {{Points}}},
  author = {Weng, Xinshuo and Wang, Jianren and Levine, Sergey and Kitani, Kris and Rhinehart, Nicholas},
  date = {2020-03-18},
  eprint = {2003.08376},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2003.08376},
  urldate = {2020-03-24},
  abstract = {Predicting the future is a crucial first step to effective control, since systems that can predict the future can select plans that lead to desired outcomes. In this work, we study the problem of future prediction at the level of 3D scenes, represented by point clouds captured by a LiDAR sensor, i.e., directly learning to forecast the evolution of {$>$}100,000 points that comprise a complete scene. We term this Scene Point Cloud Sequence Forecasting (SPCSF). By directly predicting the densest-possible 3D representation of the future, the output contains richer information than other representations such as future object trajectories. We design a method, SPCSFNet, evaluate it on the KITTI and nuScenes datasets, and find that it demonstrates excellent performance on the SPCSF task. To show that SPCSF can benefit downstream tasks such as object trajectory forecasting, we present a new object trajectory forecasting pipeline leveraging SPCSFNet. Specifically, instead of forecasting at the object level as in conventional trajectory forecasting, we propose to forecast at the sensor level and then apply detection and tracking on the predicted sensor data. As a result, our new pipeline can remove the need of object trajectory labels and enable large-scale training with unlabeled sensor data. Surprisingly, we found our new pipeline based on SPCSFNet was able to outperform the conventional pipeline using state-of-the-art trajectory forecasting methods, all of which require future object trajectory labels. Finally, we propose a new evaluation procedure and two new metrics to measure the end-to-end performance of the trajectory forecasting pipeline. Our code will be made publicly available at https://github.com/xinshuoweng/SPCSF.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Robotics},
  file = {/Users/vitay/Documents/Zotero/storage/H2MV78VP/Weng_et_al_2020_Sequential_Forecasting_of_100,000_Points.pdf;/Users/vitay/Documents/Zotero/storage/A63VLRYQ/2003.html}
}

@inproceedings{Werbos1982,
  title = {Applications of Advances in Nonlinear Sensitivity Analysis},
  booktitle = {System {{Modeling}} and {{Optimization}}: {{Proc}}. {{IFIP}}},
  author = {Werbos, P.J.},
  date = {1982},
  publisher = {Springer}
}

@unpublished{Wu2016,
  title = {Google's {{Neural Machine Translation System}}: {{Bridging}} the {{Gap}} between {{Human}} and {{Machine Translation}}},
  shorttitle = {Google's {{Neural Machine Translation System}}},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Łukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  date = {2016-09-26},
  eprint = {1609.08144},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {https://arxiv.org/abs/1609.08144v2},
  urldate = {2019-11-07},
  abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60\% compared to Google's phrase-based production system.},
  langid = {english},
  file = {/Users/vitay/Documents/Zotero/storage/IJGD899I/Wu et al_2016_Google's Neural Machine Translation System.pdf;/Users/vitay/Documents/Zotero/storage/H8LNWI2B/1609.html}
}

@unpublished{Wu2019,
  title = {A {{Comprehensive Survey}} on {{Graph Neural Networks}}},
  author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  date = {2019-01-02},
  eprint = {1901.00596},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1901.00596},
  urldate = {2019-06-17},
  abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into different categories. With a focus on graph convolutional networks, we review alternative architectures that have recently been developed; these learning paradigms include graph attention networks, graph autoencoders, graph generative networks, and graph spatial-temporal networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes and benchmarks of the existing algorithms on different learning tasks. Finally, we propose potential research directions in this fast-growing field.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/ZFD7SLBJ/Wu et al_2019_A Comprehensive Survey on Graph Neural Networks.pdf;/Users/vitay/Documents/Zotero/storage/AY6VTNFN/1901.html}
}

@unpublished{Wu2020,
  title = {Deep {{Transformer Models}} for {{Time Series Forecasting}}: {{The Influenza Prevalence Case}}},
  shorttitle = {Deep {{Transformer Models}} for {{Time Series Forecasting}}},
  author = {Wu, Neo and Green, Bradley and Ben, Xue and O'Banion, Shawn},
  date = {2020-01-22},
  eprint = {2001.08317},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2001.08317},
  urldate = {2021-02-16},
  abstract = {In this paper, we present a new approach to time series forecasting. Time series data are prevalent in many scientific and engineering disciplines. Time series forecasting is a crucial task in modeling time series data, and is an important area of machine learning. In this work we developed a novel method that employs Transformer-based machine learning models to forecast time series data. This approach works by leveraging self-attention mechanisms to learn complex patterns and dynamics from time series data. Moreover, it is a generic framework and can be applied to univariate and multivariate time series data, as well as time series embeddings. Using influenza-like illness (ILI) forecasting as a case study, we show that the forecasting results produced by our approach are favorably comparable to the state-of-the-art.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/DMCE2UMJ/Wu_et_al_2020_Deep_Transformer_Models_for_Time_Series_Forecasting.pdf;/Users/vitay/Documents/Zotero/storage/V96H4HZA/2001.html}
}

@inproceedings{Xu2015,
  title = {Show, {{Attend}} and {{Tell}}: {{Neural Image Caption Generation}} with {{Visual Attention}}},
  shorttitle = {Show, {{Attend}} and {{Tell}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  author = {Xu, Kelvin and Ba, Jimmy Lei and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard S. and Bengio, Yoshua},
  date = {2015},
  series = {{{ICML}}'15},
  pages = {2048--2057},
  publisher = {JMLR.org},
  url = {http://dl.acm.org/citation.cfm?id=3045118.3045336},
  urldate = {2019-04-07},
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.},
  venue = {Lille, France},
  file = {/Users/vitay/Documents/Zotero/storage/DCSIMPIG/Xu et al_2015_Show, Attend and Tell.pdf}
}

@article{Yang2018,
  title = {Hyperspectral {{Image Classification With Deep Learning Models}}},
  author = {Yang, X. and Ye, Y. and Li, X. and Lau, R. Y. K. and Zhang, X. and Huang, X.},
  date = {2018-09},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {56},
  number = {9},
  pages = {5408--5423},
  doi = {10.1109/TGRS.2018.2815613},
  abstract = {Deep learning has achieved great successes in conventional computer vision tasks. In this paper, we exploit deep learning techniques to address the hyperspectral image classification problem. In contrast to conventional computer vision tasks that only examine the spatial context, our proposed method can exploit both spatial context and spectral correlation to enhance hyperspectral image classification. In particular, we advocate four new deep learning models, namely, 2-D convolutional neural network (2-D-CNN), 3-D-CNN, recurrent 2-D CNN (R-2-D-CNN), and recurrent 3-D-CNN (R-3-D-CNN) for hyperspectral image classification. We conducted rigorous experiments based on six publicly available data sets. Through a comparative evaluation with other state-of-the-art methods, our experimental results confirm the superiority of the proposed deep learning models, especially the R-3-D-CNN and the R-2-D-CNN deep learning models.},
  keywords = {2-D convolutional neural network,computer vision,Context modeling,conventional computer vision tasks,convolution,Convolution,Convolutional neural network (CNN),deep learning,deep learning techniques,feedforward neural nets,hyperspectral image,hyperspectral image classification problem,Hyperspectral imaging,image classification,Kernel,learning (artificial intelligence),Machine learning,R-2-D-CNN,R-3-D-CNN,recurrent 2-D CNN,recurrent 3-D-CNN,spatial context,Task analysis},
  file = {/Users/vitay/Documents/Zotero/storage/BMSA93MY/Yang et al_2018_Hyperspectral Image Classification With Deep Learning Models.pdf;/Users/vitay/Documents/Zotero/storage/TGH8LSBL/8340197.html}
}

@article{Young2018,
  title = {Recent {{Trends}} in {{Deep Learning Based Natural Language Processing}} [{{Review Article}}]},
  author = {Young, T. and Hazarika, D. and Poria, S. and Cambria, E.},
  date = {2018-08},
  journaltitle = {IEEE Computational Intelligence Magazine},
  volume = {13},
  number = {3},
  pages = {55--75},
  issn = {1556-603X},
  doi = {10.1109/MCI.2018.2840738},
  abstract = {Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.},
  keywords = {Computational modeling,Context modeling,data structures,deep learning methods,deep learning related models,hierarchical data representations,learning (artificial intelligence),Learning systems,Machine learning,model designs,multiple processing layers,natural language processing,Natural language processing,NLP tasks,Semantics},
  file = {/Users/vitay/Documents/Zotero/storage/CC8VLLUI/Young et al_2018_Recent Trends in Deep Learning Based Natural Language Processing [Review.pdf;/Users/vitay/Documents/Zotero/storage/N67RJ8PU/8416973.html}
}

@article{Yu2011,
  title = {Weighted Bagging: A Modification of {{AdaBoost}} from the Perspective of Importance Sampling},
  author = {Yu, Qingzhao},
  date = {2011-03},
  journaltitle = {Journal of Applied Statistics},
  volume = {38},
  number = {3},
  pages = {451--463},
  doi = {10.1080/02664760903456418},
  url = {http://www.tandfonline.com/doi/abs/10.1080/02664760903456418},
  abstract = {We motivate the success of AdaBoost (ADA) in classification problems by appealing to an importance sampling perspective. Based on this insight, we propose the Weighted Bagging (WB) algorithm, a regularization method that naturally extends ADA to solve both classification and regression problems. WB uses a part of the available data to build models, and a separate part to modify the weights of observations. The method is used with categorical and regression tress and is compared with ADA, Boosting, Bagging, Random Forest and Support Vector Machine. We apply these methods to some real data sets and report some results of simulations. These applications and simulations show the effectiveness of WB.},
  keywords = {AdaBoost,bagging,categorical and regression trees,ensemble learning,gradient-descent boosting}
}

@article{Yuan2018,
  title = {Adversarial {{Examples}}: {{Attacks}} and {{Defenses}} for {{Deep Learning}}},
  author = {Yuan, Xiaoyong and He, Pan and Zhu, Qile and Bhat, Rajendra Rana and Li, Xiaolin},
  date = {2018},
  url = {https://arxiv.org/pdf/1712.07107.pdf},
  abstract = {—With rapid progress and great successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical scenarios. Therefore, the attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples against deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for ad-versarial examples and explore the challenges and the potential solutions.},
  keywords = {adversarial examples,deep learning,Index Terms—deep neural network,security},
  file = {/Users/vitay/Documents/Zotero/storage/FJAH3WD7/Yuan et al_2018_Adversarial Examples.pdf}
}

@unpublished{Yuille2018,
  title = {Deep {{Nets}}: {{What}} Have They Ever Done for {{Vision}}?},
  shorttitle = {Deep {{Nets}}},
  author = {Yuille, Alan L. and Liu, Chenxi},
  date = {2018-05-10},
  eprint = {1805.04025},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1805.04025},
  urldate = {2019-04-04},
  abstract = {This is an opinion paper about the strengths and weaknesses of Deep Nets for vision. They are at the center of recent progress on artificial intelligence and are of growing importance in cognitive science and neuroscience. They have enormous successes but also clear limitations. There is also only partial understanding of their inner workings. It seems unlikely that Deep Nets in their current form will be the best long-term solution either for building general purpose intelligent machines or for understanding the mind/brain, but it is likely that many aspects of them will remain. At present Deep Nets do very well on specific types of visual tasks and on specific benchmarked datasets. But Deep Nets are much less general purpose, flexible, and adaptive than the human visual system. Moreover, methods like Deep Nets may run into fundamental difficulties when faced with the enormous complexity of natural images which can lead to a combinatorial explosion. To illustrate our main points, while keeping the references small, this paper is slightly biased towards work from our group.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/A5WC3QD5/Yuille_Liu_2018_Deep Nets.pdf;/Users/vitay/Documents/Zotero/storage/CEFKVCKV/1805.html}
}

@online{Zeng2022,
  title = {Are {{Transformers Effective}} for {{Time Series Forecasting}}?},
  author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  date = {2022-08-17},
  eprint = {2205.13504},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.13504},
  url = {http://arxiv.org/abs/2205.13504},
  urldate = {2023-03-21},
  abstract = {Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the \textbackslash emph\{permutation-invariant\} self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future. Code is available at: \textbackslash url\{https://github.com/cure-lab/LTSF-Linear\}.},
  pubstate = {prepublished},
  file = {/Users/vitay/Documents/Zotero/storage/JI9MGPLP/Zeng_et_al_2022_Are_Transformers_Effective_for_Time_Series_Forecasting.pdf}
}

@inproceedings{Zhan2018,
  title = {Semi-{{Supervised Classification}} of {{Hyperspectral Data Based}} on {{Generative Adversarial Networks}} and {{Neighborhood Majority Voting}}},
  booktitle = {{{IGARSS}} 2018 - 2018 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Zhan, Y. and Wu, K. and Liu, W. and Qin, J. and Yang, Z. and Medjadba, Y. and Wang, G. and Yu, X.},
  date = {2018-07},
  pages = {5756--5759},
  doi = {10.1109/IGARSS.2018.8518846},
  abstract = {How to classify hyperspectral images using few training samples is an important and challenging problem because the collection of the samples is difficult and expensive. Because semi-supervised approaches can utilize information contained in the unlabeled samples and labeled samples, it is a suitable choice. A novel semi-supervised spectral-spatial classification method for hyperspectral data based on generative adversarial network (GAN) is proposed in this paper. First, we use a custom one-dimensional GAN to train the hyperspectral data to obtain spectral features. After using a new small convolutional neural network (CNN) to classify the spectral features, we use a new classification method based on a majority voting strategy further to improve the classification result. The performance of our method is evaluated on ROSIS image data, and the results show that the proposed method can acquire satisfactory results when compared with traditional methods using a few of labeled samples.},
  eventtitle = {{{IGARSS}} 2018 - 2018 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  keywords = {CNN,convolution,convolutional neural network,custom one-dimensional GAN,Data models,deep learning,Feature extraction,feedforward neural nets,Gallium nitride,generative adversarial network,Generative adversarial networks,generative adversarial networks (GAN),geophysical image processing,hyperspectral data,hyperspectral images,Hyperspectral images classification,hyperspectral imaging,Hyperspectral imaging,image classification,labeled samples,learning (artificial intelligence),neighborhood majority voting,ROSIS image data,semi-supervised learning (SSL),semisupervised spectral-spatial classification method,spectral features,spectral-spatial classification,Training,unlabeled samples},
  file = {/Users/vitay/Documents/Zotero/storage/ABEVTUM4/8518846.html}
}

@unpublished{Zhang2018b,
  title = {Image {{Super-Resolution Using Very Deep Residual Channel Attention Networks}}},
  author = {Zhang, Yulun and Li, Kunpeng and Li, Kai and Wang, Lichen and Zhong, Bineng and Fu, Yun},
  date = {2018-07-12},
  eprint = {1807.02758},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1807.02758},
  urldate = {2021-09-04},
  abstract = {Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low-resolution inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/Z5XVWKVQ/Zhang_et_al_2018_Image_Super-Resolution_Using_Very_Deep_Residual_Channel_Attention_Networks.pdf;/Users/vitay/Documents/Zotero/storage/B6RMUCMN/1807.html}
}

@unpublished{Zhang2018c,
  title = {Interpretable {{Deep Learning}} under {{Fire}}},
  author = {Zhang, Xinyang and Wang, Ningfei and Ji, Shouling and Shen, Hua and Wang, Ting},
  date = {2018-12-03},
  eprint = {1812.00891},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1812.00891},
  urldate = {2019-03-21},
  abstract = {Providing explanations for complicated deep neural network (DNN) models is critical for their usability in security-sensitive domains. A proliferation of interpretation methods have been proposed to help end users understand the inner workings of DNNs, that is, how a DNN arrives at a particular decision for a specific input. This improved interpretability is believed to offer a sense of security by involving human in the decision-making process. However, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulation, about which little is known thus far. In this paper, we conduct the first systematic study on the security of interpretable deep learning systems (IDLSes). We first demonstrate that existing IDLSes are highly vulnerable to adversarial manipulation. We present ACID attacks, a broad class of attacks that generate adversarial inputs which not only mislead target DNNs but also deceive their coupled interpretation models. By empirically investigating three representative types of interpretation models, we show that ACID attacks are effective against all of them. This vulnerability thus seems pervasive in many IDLSes. Further, using both analytical and empirical evidence, we identify the prediction-interpretation "independency" as one possible root cause of this vulnerability: a DNN and its interpretation model are often not fully aligned, resulting in the possibility for the adversary to exploit both models simultaneously. Moreover, by examining the transferability of adversarial inputs across different interpretation models, we expose the fundamental tradeoff among the attack evasiveness with respect to different interpretation methods. These findings shed light on developing potential countermeasures and designing more robust interpretation methods, leading to several promising research directions.},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/vitay/Documents/Zotero/storage/UVEZQ798/Zhang et al_2018_Interpretable Deep Learning under Fire.pdf;/Users/vitay/Documents/Zotero/storage/HCWXRNWV/1812.html}
}

@unpublished{Zhang2018d,
  title = {Visual {{Interpretability}} for {{Deep Learning}}: A {{Survey}}},
  shorttitle = {Visual {{Interpretability}} for {{Deep Learning}}},
  author = {Zhang, Quanshi and Zhu, Song-Chun},
  date = {2018-02-02},
  eprint = {1802.00614},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.00614},
  urldate = {2019-01-26},
  abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always the Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of low interpretability of their black-box representations. We believe that high model interpretability may help people to break several bottlenecks of deep learning, e.g., learning from very few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/4N2825V4/Zhang_Zhu_2018_Visual Interpretability for Deep Learning.pdf;/Users/vitay/Documents/Zotero/storage/DK8W7YB9/1802.html}
}

@unpublished{Zhou2017,
  title = {{{VoxelNet}}: {{End-to-End Learning}} for {{Point Cloud Based 3D Object Detection}}},
  shorttitle = {{{VoxelNet}}},
  author = {Zhou, Yin and Tuzel, Oncel},
  date = {2017-11-16},
  eprint = {1711.06396},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1711.06396},
  urldate = {2020-11-29},
  abstract = {Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/H8M4UE3Z/Zhou_Tuzel_2017_VoxelNet.pdf;/Users/vitay/Documents/Zotero/storage/5WJZP4NE/1711.html}
}

@article{Zhu2020,
  title = {Dark, {{Beyond Deep}}: {{A Paradigm Shift}} to {{Cognitive AI}} with {{Humanlike Common Sense}}},
  shorttitle = {Dark, {{Beyond Deep}}},
  author = {Zhu, Yixin and Gao, Tao and Fan, Lifeng and Huang, Siyuan and Edmonds, Mark and Liu, Hangxin and Gao, Feng and Zhang, Chi and Qi, Siyuan and Nian Wu, Ying and Tenenbaum, Joshua B. and Zhu, Song-Chun},
  date = {2020-02-22},
  journaltitle = {Engineering},
  shortjournal = {Engineering},
  issn = {2095-8099},
  doi = {10.1016/j.eng.2020.01.011},
  url = {http://www.sciencedirect.com/science/article/pii/S2095809920300345},
  urldate = {2020-02-25},
  abstract = {Recent progress in deep learning is essentially based on a “big data for small tasks” paradigm, under which massive amounts of data are used to train a classifier for a single narrow task. In this paper, we call for a shift that flips this paradigm upside down. Specifically, we propose a “small data for big tasks” paradigm, wherein a single artificial intelligence (AI) system is challenged to develop “common sense,” enabling it to solve a wide range of tasks with little training data. We illustrate the potential power of this new paradigm by reviewing models of common sense that synthesize recent breakthroughs in both machine and human vision. We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense. When taken as a unified concept, FPICU is concerned with the questions of “why” and “how,” beyond the dominant “what” and “where” framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes. We therefore coin them the “dark matter” of vision. Just as our universe cannot be understood by merely studying observable matter, we argue that vision cannot be understood without studying FPICU. We demonstrate the power of this perspective to develop cognitive AI systems with humanlike common sense by showing how to observe and apply FPICU with little training data to solve a wide range of challenging tasks, including tool use, planning, utility inference, and social learning. In summary, we argue that the next generation of AI must embrace “dark” humanlike common sense for solving novel tasks.},
  langid = {english},
  keywords = {Artificial intelligence,Causality,Computer vision,Functionality,Intuitive physics,Perceived intent,Utility},
  file = {/Users/vitay/Documents/Zotero/storage/9YHRANYP/Zhu_et_al_2020_Dark,_Beyond_Deep.pdf;/Users/vitay/Documents/Zotero/storage/K38N2GIX/S2095809920300345.html}
}

@unpublished{Zhu2020b,
  title = {Unpaired {{Image-to-Image Translation}} Using {{Cycle-Consistent Adversarial Networks}}},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  date = {2020-08-24},
  eprint = {1703.10593},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.10593},
  urldate = {2020-12-07},
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X \textbackslash rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y \textbackslash rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) \textbackslash approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/vitay/Documents/Zotero/storage/X4RJ7ZS7/Zhu_et_al_2020_Unpaired_Image-to-Image_Translation_using_Cycle-Consistent_Adversarial_Networks.pdf;/Users/vitay/Documents/Zotero/storage/MSZZ4TJB/1703.html}
}
