[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "The goal of this webbook is to keep track of the state-of-the-art in deep reinforcement learning. It starts with basics in reinforcement learning and deep learning to introduce the notations. It then covers different classes of deep RL methods, value-based or policy-based, model-free or model-based, etc. Later sections focus on more advanced topics.\nThis document is meant to stay work in progress forever, as new algorithms will be added as they are published. Feel free to comment, correct, suggest, pull request by writing to julien.vitay@gmail.com.\nSome figures are taken from the original publication (“Source:” in the caption). Their copyright stays to the respective authors, naturally. The rest is my own work and can be distributed, reproduced and modified under CC-BY-SA-NC 4.0.\n\n\n\n\n\n\n\nLicense\n\n\n\n\nExcept where otherwise noted, this work is licensed under a Creative Commons Attribution-Non Commercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "src/0-Introduction.html",
    "href": "src/0-Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "What is Reinforcement Learning?\nDeep reinforcement learning (Deep RL) is the integration of deep learning methods, classically used in supervised or unsupervised learning contexts, to reinforcement learning (RL), a well-studied adaptive control framework used in problems with delayed and partial feedback.\nSupervised learning (SL) trains a discriminative model (classification or regression) by comparing the correct answer (ground truth) available in a training set to compute a prediction error. For neural networks, the prediction error (typically the difference between the ground truth and the predicted output) is used by the backpropagation algorithm to adapt the parameters of the model so that the prediction error is iteratively reduced. Typical examples are convolutional neural networks (CNN) predicting the label associated to an image, a recurrent neural network (RNN) predicting autoregressively the future of a time series, or a fully-connected network performing credit scoring. The major drawback of SL methods is that they typically require a lot of annotated data, which are very expensive to produce.\nUnsupervised learning (UL) only deals with raw data, trying to extract statistical properties without any additional information. One application of UL is dimensionality reduction, which searches how to project highly dimensional data (e.g. images) onto smaller spaces without losing too much information. Algorithms like PCA (Principal Components Analysis) or neural architectures like autoencoders are typically used. Another approach to UL is generative modelling, i.e. learning a model of the distribution of the data allowing to generate new samples. Generative AI (ChatGPT, Midjourney, etc) relies on learning the distribution of vast amounts of text or images in order to generate novel high-quality samples. Self-supervised learning relies on using supervised learning algorithms on raw data by using self-generated pretext tasks, such as masking part of the data in the input and learning to predict it, or guessing the next item in a sequence.\nReinforcement learning (RL) lies somehow in between: the model makes a prediction (an action), but there is no ground truth to compare with. The only feedback it gets from the environment is a unidimensional reward signal that informs how good (or bad) the action was. In the extreme case, this partial feedback can be binary, like winning or losing a game after a sequence of dozens of actions.\nRL setups follow the agent-environment interface (Sutton and Barto, 1998). The agent (for example a robot) is in a given state s_t at time t. This state represents the perception of the robot (camera, internal sensors) but also its position inside the environment, and generally anything relevant information for the task. The agent selects an action a_t according to its policy (or strategy). This action modifies the environment (or world), what brings the agent in a new state s_{t+1}. Furthermore, a reward r_{t+1} is delivered to the agent to valuate the executed action. This interaction loop continues over time, leading to episodes or trajectories of various lengths until a terminal state is reached. The goal of the agent is to find a policy that maximizes the sum of the rewards received over time (more on that later).\nThe key concept in RL is trial and error learning: trying out actions until their outcome is good. The agent selects an action from its repertoire and observes the outcome. If the outcome is positive (reward), the action is reinforced (it becomes more likely to occur again). If the outcome is negative (punishment), the action will be avoided in the future. After enough interactions, the agent has learned which action to perform in a given situation.\nThe agent has to explore its environment via trial-and-error in order to gain knowledge. The agent’s behavior then is roughly divided into two phases:\nThe biggest issue with this approach is that exploring large action spaces might necessitate a lot of trials (a problem referred to as sample complexity). The modern techniques we will see in this book try to reduce the sample complexity.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "src/0-Introduction.html#what-is-reinforcement-learning",
    "href": "src/0-Introduction.html#what-is-reinforcement-learning",
    "title": "Introduction",
    "section": "",
    "text": "Figure 1: Three types of machine learning: Supervised learning uses a ground truth to compute a prediction error that drives learning. Unsupervised learning extracts statistical properties from raw data. Reinforcement learning uses a reward signal from the environment to assess the correctness of an action.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Agent-environment interface. Images generated by ChatGPT.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Trial and error learning. Top: classical trial and error learning, where the agent tries different actions until the outcome is satisfying. Bottom: the outcome of the trial influences via learning whether the corresponding action will be reinforced or avoided in the future.\n\n\n\n\n\nThe exploration phase, where it gathers knowledge about its environment.\nThe exploitation phase, where this knowledge is used to collect as many rewards as possible.\n\n\n\n\n\n\n\n\nFigure 4: Sample complexity: learning to control a modern plane through trial and error might be too long and dangerous. Image generated by ChatGPT.\n\n\n\n\n\n\n\n\n\nSutton and Barto. Reinforcement learning: An Introduction\n\n\n\nThe book “Reinforcement learning: An introduction” (1st and 2nd editions) (Sutton and Barto, 1998, 2017) contains everything you need to know about the basics of RL. The second edition can be found here:\nhttp://incompleteideas.net/book/the-book-2nd.html",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "src/0-Introduction.html#applications-of-reinforcement-learning",
    "href": "src/0-Introduction.html#applications-of-reinforcement-learning",
    "title": "Introduction",
    "section": "Applications of Reinforcement Learning",
    "text": "Applications of Reinforcement Learning\nRL can be used in many control problems, ranging from simple problems to complex robotics, video games or even plasma control.\n\nOptimal control\nThe most basic problems on which RL can be applied are simple control environments with a few degrees of freedom. The gymnasium library (formerly gym) maintained by the Farama foundation provides an API for RL environments as well as the reference implementation of the most popular ones, including control problems, Mujoco robotic simulations and Atari games:\nhttps://gymnasium.farama.org\nSome examples:\n\n\n\n\n\n\nPendulum\n\n\n\nGoal: maintaining the pendulum vertical by changing the applied torque.\n\n\nBefore\n\n\n\nSource: https://keras.io/examples/rl/ddpg_pendulum/\n\n\n\nAfter\n\n\n\n\n\n\n\n\n\n\n\nCartpole\n\n\n\nGoal: maintaining the pole vertical by moving the cart.\n\n\nBefore\n\n\nAfter\n\n\n\nSource: https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288\nSee the Cartpole learning in real life (Deisenroth and Rasmussen, 2011):\n\n\n\n\n\nAtari games\nThese toy control problems were used for decades to test various RL algorithms, although serious applications existed. The big breakthrough in (deep) reinforcement learning occurred in 2013, as a small startup in London, DeepMind Technologies Limited (now Google Deepmind), led by Demis Hassabis, successfully coupled reinforcement learning with deep neural networks for the first time. Their proposed algorithm, the deep Q-network (DQN, Mnih et al., 2013, 2015), used a convolutional neural network (CNN) to learn to play many Atari games by trial and error, without any prior information about the game and only using raw images as inputs.\n\nSimilar pixel-based games were quickly mastered, such as the TORCS simulator with the A3C algporithm (Mnih et al., 2016):\n\nSimulated robotics were addressed by similar techniques:\n\n\n\nAlphaGo\nThe next major breakthrough for Deep RL happened in 2016, as AlphaGo, also created by Deepmind, was able to beat Lee Sedol, world champion of the game of Go. AlphaGo coupled the power of deep neural networks and RL with Monte Carlo Tree Search (MCTS), a popular tree-based search algorithm, to learn through self-play the optimal strategy for the game of Go. This performance was extremely commented, as such a dominance of an AI system was not expected before decades.\n\n\n\nOpenAI Five and AlphaStar\nDespite its complexity, the game of Go is actually quite simple for a computer: there is full observability (the entire state of the game is known to the players), the number of possible actions is quite limited, the consequence of an action is fully predictable, and planning is only necessary over a few dozens of plays. On the contrary, modern video games like DotA 2 or Starcraft have partial observability (the player only sees its immediate surroundings), hundreds of hierarchical actions can be made at each time step, the dynamics of the game are largely unknown, and a game can last hours. In some sense, it is much more difficult to become good at DotA than it is at chess or Go.\nDeepmind and OpenAI recognized this difference and started applying deep RL methods to Starcraft II and DotA 2, respectively, between 2017 and 2020. They managed to beat teams of professional players under limited conditions, but eventually stopped because of the huge training costs (and they had to work on LLMs…).\n\n\n\n\nProcess control\nIn 2016, Deepmind applied its RL algorithms to the control of the cooling systems in Google’s datacenters. The system learned passively from observations of the current cooling sytem what the optimal policy was. When they gave control to the RL agent, it instantly led to a 40% reduction of energy consumption, which, for Google’s data centers, represents a huge amount of money. See Luo et al. (2022) for a recent review of RL controlling cooling systems.\n\n\n\n\n\n\nFigure 5: Source: https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/\n\n\n\nThis showed that the progress in deep RL was not limited to toy problems or video games, but could be relevant for complex process control problems. An amazing illustration of that idea is the magnetic control of tokamak plasmas by Degrave et al. (2022).\n\n\n\n\n\n\nFigure 6: Magnetic control of tokamak plasmas through deep reinforcement learning. Source: Degrave et al. (2022)\n\n\n\nAnother recent illustration is how RL can be used in the design of silicon chips by Nvidia (Roy et al., 2022).\n\n\n\n\n\n\nFigure 7: Optimization of Parallel Prefix Circuits using Deep Reinforcement Learning. Roy et al. (2022)\n\n\n\n\n\nRobotics\nA natural application of RL is in the domain of (autonomous) robotics, where agents / robots can autonomously solve tasks by themselves. A very influential lab working on robotics and Deep RL is RAIL lab of Sergey Levine at Berkeley.\n\nOpenAI worked on dexterity tasks.\n\nAutonomous driving is another promising field of application for deep RL. The english startup Wayve made for example a very exciting demo of their system (see Kendall et al. (2018) and https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning).\n\nA recent contribution by the UZH Robotics and Perception Group leveraged deep RL to control drones flying at high speeds (Kaufmann et al., 2023).\n\n\n\nChatGPT\nRL can also be used to fine-tune generative AI models, such as diffusion models or large language models such as ChatGPT. Generative models typically learn from raw data using pretext tasks: predicting missing parts of the data, or predicting the next item in a sequence. While this allows to learn a world model, it does not lead to meaningful behaviors for a task at hand. Using RL allows to fine-tune the model so that its outputs align with the task. Such a model alignment is central to the acceptance of LLMs.\n\n\n\n\n\n\nFigure 8: Source: https://openai.com/blog/chatgpt/\n\n\n\n\n\nand many more…\nRL finds applications in virtually any field involving sequential decision making, including finance technology (Malibari et al., 2023), inventory management (Madeka et al., 2022), missile guidance (Li et al., 2022) or healthcare (Yu et al., 2020).\n\n\n\n\nDegrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese, F., et al. (2022). Magnetic control of tokamak plasmas through deep reinforcement learning. Nature 602, 414–419. doi:10.1038/s41586-021-04301-9.\n\n\nKaufmann, E., Bauersfeld, L., Loquercio, A., Müller, M., Koltun, V., and Scaramuzza, D. (2023). Champion-level drone racing using deep reinforcement learning. Nature 620, 982–987. doi:10.1038/s41586-023-06419-4.\n\n\nKendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen, J.-M., et al. (2018). Learning to Drive in a Day. Available at: http://arxiv.org/abs/1807.00412 [Accessed December 19, 2018].\n\n\nLi, W., Zhu, Y., and Zhao, D. (2022). Missile guidance with assisted deep reinforcement learning for head-on interception of maneuvering target. Complex Intell. Syst. 8, 1205–1216. doi:10.1007/s40747-021-00577-6.\n\n\nLuo, J., Paduraru, C., Voicu, O., Chervonyi, Y., Munns, S., Li, J., et al. (2022). Controlling Commercial Cooling Systems Using Reinforcement Learning. doi:10.48550/arXiv.2211.07357.\n\n\nMadeka, D., Torkkola, K., Eisenach, C., Luo, A., Foster, D. P., and Kakade, S. M. (2022). Deep Inventory Management. doi:10.48550/arXiv.2210.03137.\n\n\nMalibari, N., Katib, I., and Mehmood, R. (2023). Systematic Review on Reinforcement Learning in the Field of Fintech. doi:10.48550/arXiv.2305.07466.\n\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., et al. (2016). Asynchronous Methods for Deep Reinforcement Learning. in Proc. ICML Available at: http://arxiv.org/abs/1602.01783.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., et al. (2013). Playing Atari with Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1312.5602.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). Human-level control through deep reinforcement learning. Nature 518, 529–533. doi:10.1038/nature14236.\n\n\nRoy, R., Raiman, J., Kant, N., Elkin, I., Kirby, R., Siu, M., et al. (2022). PrefixRL: Optimization of Parallel Prefix Circuits using Deep Reinforcement Learning. doi:10.1109/DAC18074.2021.9586094.\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press.\n\n\nSutton, R. S., and Barto, A. G. (2017). Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press Available at: http://incompleteideas.net/book/the-book-2nd.html.\n\n\nYu, C., Liu, J., and Nemati, S. (2020). Reinforcement Learning in Healthcare: A Survey. doi:10.48550/arXiv.1908.08796.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "src/1.1-Bandits.html",
    "href": "src/1.1-Bandits.html",
    "title": "Sampling and Bandits",
    "section": "",
    "text": "n-armed bandits\nThe n-armed bandit (or multi-armed bandit) is the simplest form of learning by trial and error. Learning and action selection take place in the same single state, with n available actions having different reward distributions. The goal is to find out through trial and error which action provides the most reward on average.\nWe have the choice between N different actions (a_1, ..., a_N). Each action a taken at time t provides a reward r_t drawn from the action-specific probability distribution r(a).\nThe mathematical expectation of that distribution is the expected reward, called the true value of the action Q^*(a).\nQ^*(a) = \\mathbb{E} [r(a)]\nThe reward distribution also has a variance: we usually ignore it in RL, as all we care about is the optimal action a^* (but see distributional RL later).\na^* = \\text{argmax}_a \\, Q^*(a)\nIf we take the optimal action an infinity of times, we maximize the reward intake on average. The question is how to find out the optimal action through trial and error, i.e. without knowing the exact reward distribution r(a). We only have access to samples of r(a) by taking the action a at time t (a trial, play or step).\nr_t \\sim r(a)\nThe received rewards r_t vary around the true value over time. We need to build estimates Q_t(a) of the value of each action based on the samples. These estimates will be very wrong at the beginning, but should get better over time.",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling and Bandits</span>"
    ]
  },
  {
    "objectID": "src/1.1-Bandits.html#n-armed-bandits",
    "href": "src/1.1-Bandits.html#n-armed-bandits",
    "title": "Sampling and Bandits",
    "section": "",
    "text": "Figure 2.1: Example of a bandit with 10 actions. The mean and the variance of each reward distribution are depicted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Sampled reward over time for the same action.",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling and Bandits</span>"
    ]
  },
  {
    "objectID": "src/1.1-Bandits.html#random-sampling",
    "href": "src/1.1-Bandits.html#random-sampling",
    "title": "Sampling and Bandits",
    "section": "Random sampling",
    "text": "Random sampling\n\nExpectation\nAn important metric for a random variable is its mathematical expectation or expected value. For discrete distributions, it is the “mean” realization / outcome weighted by the corresponding probabilities:\n\n    \\mathbb{E}[X] = \\sum_{i=1}^n P(X = x_i) \\, x_i\n\nFor continuous distributions, one needs to integrate the probability density function (pdf) instead of the probabilities:\n\n    \\mathbb{E}[X] = \\int_{x \\in \\mathcal{D}_X} f(x) \\, x \\, dx\n\nOne can also compute the expectation of a function of a random variable:\n\n    \\mathbb{E}[g(X)] = \\int_{x \\in \\mathcal{D}_X} f(x) \\, g(x) \\, dx\n\n\n\nRandom sampling\nIn ML and RL, we deal with random variables whose exact probability distribution is unknown, but we are interested in their expectation or variance anyway.\n\n\n\n\n\n\nFigure 2.3: Samples from the normal distribution are centered around its expected value.\n\n\n\nRandom sampling or Monte Carlo sampling (MC) consists of taking N samples x_i out of the distribution X (discrete or continuous) and computing the sample average:\n\n    \\mathbb{E}[X] = \\mathbb{E}_{x \\sim X} [x] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N x_i\n\nMore samples will be obtained where f(x) is high (x is probable), so the average of the sampled data will be close to the expected value of the distribution.\n\n\n\n\n\n\nLaw of big numbers\n\n\n\nAs the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean.\n\n\nMC estimates are only correct when:\n\nthe samples are i.i.d (independent and identically distributed):\n\nindependent: the samples must be unrelated with each other.\nidentically distributed: the samples must come from the same distribution X.\n\nthe number of samples is large enough.\n\nOne can estimate any function of the random variable with random sampling:\n\n    \\mathbb{E}[f(X)] = \\mathbb{E}_{x \\sim X} [f(x)] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N f(x_i)\n\n\n\nCentral limit theorem\nSuppose we have an unknown distribution X with expected value \\mu = \\mathbb{E}[X] and variance \\sigma^2. We can take randomly N samples from X to compute the sample average:\n\n    S_N = \\frac{1}{N} \\, \\sum_{i=1}^N x_i\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nThe distribution of sample averages is normally distributed with mean \\mu and variance \\frac{\\sigma^2}{N}.\nS_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{N}})\n\n\nIf we perform the sampling multiple times, even with few samples, the average of the sampling averages will be very close to the expected value. The more samples we get, the smaller the variance of the estimates. Although the distribution X can be anything, the sampling averages are normally distributed.\n\n\n\n\n\n\nFigure 2.4: Illustration of the central limit theorem. Source:: https://en.wikipedia.org/wiki/Central_limit_theorem\n\n\n\nCLT shows that the sampling average is an unbiased estimator of the expected value of a distribution:\n\\mathbb{E}(S_N) = \\mathbb{E}(X)\nAn estimator is a random variable used to measure parameters of a distribution (e.g. its expectation). The problem is that estimators can generally be biased.\nTake the example of a thermometer M measuring the temperature T. T is a random variable (normally distributed with \\mu=20 and \\sigma=10) and the measurements M relate to the temperature with the relation:\n\n    M = 0.95 \\, T + 0.65\n\n\n\n\n\n\n\nFigure 2.5: Thermometer and temperature.\n\n\n\nThe thermometer is not perfect, but do random measurements allow us to estimate the expected value of the temperature? We could repeatedly take 100 random samples of the thermometer and see how the distribution of sample averages look like:\n\n\n\n\n\n\nFigure 2.6: Distribution of the sampling averages.\n\n\n\nBut, as the expectation is linear, we actually have:\n\n    \\mathbb{E}[M] = \\mathbb{E}[0.95 \\, T + 0.65] = 0.95 \\, \\mathbb{E}[T] + 0.65 = 19.65 \\neq \\mathbb{E}[T]\n\nThe thermometer is a biased estimator of the temperature.\nLet’s note \\theta a parameter of a probability distribution X that we want to estimate (it does not have to be its mean). An estimator \\hat{\\theta} is a random variable mapping the sample space of X to a set of sample estimates. The bias of an estimator is the mean error made by the estimator:\n\n    \\mathcal{B}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta} - \\theta] = \\mathbb{E}[\\hat{\\theta}] - \\theta\n\nThe variance of an estimator is the deviation of the samples around the expected value:\n\n    \\text{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}] )^2]\n\nIdeally, we would like estimators with a low bias, as the estimations would be correct on average (= equal to the true parameter) and a low variance, as we would not need many estimates to get a correct estimate (CLT: \\frac{\\sigma}{\\sqrt{N}})\n\n\n\n\n\n\nFigure 2.7: Bias-variance trade-off. Source: https://www.machinelearningplus.com/machine-learning/bias-variance-tradeoff/\n\n\n\nUnfortunately, the perfect estimator does not exist. Estimators will have a bias and a variance. For estimators with a high bias, the estimated values will be wrong, and the policy not optimal. For estimators with a high variance, we will need a lot of samples (trial and error) to have correct estimates. One usually talks of a bias/variance trade-off: if you have a small bias, you will have a high variance, or vice versa. There is a sweet spot balancing the two. In machine learning, bias corresponds to underfitting, variance to overfitting.",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling and Bandits</span>"
    ]
  },
  {
    "objectID": "src/1.1-Bandits.html#sampling-based-evaluation",
    "href": "src/1.1-Bandits.html#sampling-based-evaluation",
    "title": "Sampling and Bandits",
    "section": "Sampling-based evaluation",
    "text": "Sampling-based evaluation\n\n\n\n\n\n\nFigure 2.8: Samples and expected reward over time.\n\n\n\nThe expectation of the reward distribution can be approximated by the mean of its samples:\n\n    \\mathbb{E} [r(a)] \\approx  \\frac{1}{N} \\sum_{t=1}^N r_t |_{a_t = a}\n\nSuppose that the action a had been selected t times, producing rewards\n\n    (r_1, r_2, ..., r_t)\n\nThe estimated value of action a at play t is then:\n\n    Q_t (a) = \\frac{r_1 + r_2 + ... + r_t }{t}\n\nOver time, the estimated action-value converges to the true action-value:\n\n   \\lim_{t \\to \\infty} Q_t (a) = Q^* (a)\n\nThe drawback of maintaining the mean of the received rewards is that it consumes a lot of memory:\n\n    Q_t (a) = \\frac{r_1 + r_2 + ... + r_t }{t} = \\frac{1}{t} \\, \\sum_{i=1}^{t} r_i\n\nIt is possible to update an estimate of the mean in an online or incremental manner:\n\n\\begin{aligned}\n    Q_{t+1}(a) &= \\frac{1}{t+1} \\, \\sum_{i=1}^{t+1} r_i = \\frac{1}{t+1} \\, (r_{t+1} + \\sum_{i=1}^{t} r_i )\\\\\n            &= \\frac{1}{t+1} \\, (r_{t+1} + t \\,  Q_{t}(a) ) \\\\\n            &= \\frac{1}{t+1} \\, (r_{t+1} + (t + 1) \\,  Q_{t}(a) - Q_t(a))\n\\end{aligned}\n\nThe estimate at time t+1 depends on the previous estimate at time t and the last reward r_{t+1}:\n\n    Q_{t+1}(a) = Q_t(a) + \\frac{1}{t+1} \\, (r_{t+1} - Q_t(a))\n\nThe problem with the exact mean is that it is only exact when the reward distribution is stationary, i.e. when the probability distribution does not change over time. If the reward distribution is non-stationary, the \\frac{1}{t+1} term will become very small and prevent rapid updates of the mean.\n\n\n\n\n\n\nFigure 2.9\n\n\n\nThe solution is to replace \\frac{1}{t+1} with a fixed parameter called the learning rate (or step size) \\alpha:\n\n\\begin{aligned}\n    Q_{t+1}(a) & = Q_t(a) + \\alpha \\, (r_{t+1} - Q_t(a)) \\\\\n                & \\\\\n                & = (1 - \\alpha) \\, Q_t(a) + \\alpha \\, r_{t+1}\n\\end{aligned}\n\n\n\n\n\n\n\nFigure 2.10\n\n\n\nThe computed value is called an exponentially moving average (or sliding average), as if one used only a small window of the past history.\n\n    Q_{t+1}(a) = Q_t(a) + \\alpha \\, (r_{t+1} - Q_t(a))\n\nor:\n\n    \\Delta Q(a) = \\alpha \\, (r_{t+1} - Q_t(a))\n\nThe moving average adapts very fast to changes in the reward distribution and should be used in non-stationary problems. It is however not exact and sensible to noise. Choosing the right value for \\alpha can be difficult.\nThe form of this update rule is very important to remember:\n\n    \\text{new estimate} = \\text{current estimate} + \\alpha \\, (\\text{target} - \\text{current estimate})\n\nEstimates following this update rule track the mean of their sampled target values. \\text{target} - \\text{current estimate} is the prediction error between the target and the estimate.",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling and Bandits</span>"
    ]
  },
  {
    "objectID": "src/1.1-Bandits.html#action-selection",
    "href": "src/1.1-Bandits.html#action-selection",
    "title": "Sampling and Bandits",
    "section": "Action selection",
    "text": "Action selection\nLet’s suppose we have formed reasonable estimates of the Q-values Q_t(a) at time t. Which action should we do next? If we select the next action a_{t+1} randomly (random agent), we do not maximize the rewards we receive, but we can continue learning the Q-values. Choosing the action to perform next is called action selection and several schemes are possible.\n\nGreedy action selection\nThe greedy action is the action whose expected value is maximal at time t based on our current estimates:\n\n    a^*_t = \\text{argmax}_{a} Q_t(a)\n\nIf our estimates Q_t are correct (i.e. close from Q^*), the greedy action is the optimal action and we maximize the rewards on average. If our estimates are wrong, the agent will perform sub-optimally.\n\n\n\n\n\n\nFigure 2.11: Greedy action selection. The action with the highest expected value is selected all the time.\n\n\n\nThis defines the greedy policy, where the probability of taking the greedy action is 1 and the probability of selecting another action is 0:\n\n    \\pi(a) = \\begin{cases}\n                    1 \\; \\text{if} \\; a = a_t^* \\\\\n                    0 \\; \\text{otherwise.} \\\\\n            \\end{cases}\n\nThe greedy policy is deterministic: the action taken is always the same for a fixed Q_t.\nHowever, the greedy action selection scheme only works when the estimates are good enough. Imagine that estimates are initially bad (e.g. 0), and an action is sampled randomly. If the received reward is positive, the new Q-value of that action becomes positive, so it becomes the greedy action. At the next step, greedy action selection will always select that action, although the second one could have been better but it was never explored.\n\nThis exploration-exploitation dilemma is the hardest problem in RL:\n\nExploitation is using the current estimates to select an action: they might be wrong!\nExploration is selecting non-greedy actions in order to improve their estimates: they might not be optimal!\n\nOne has to balance exploration and exploitation over the course of learning:\n\nMore exploration at the beginning of learning, as the estimates are initially wrong.\nMore exploitation at the end of learning, as the estimates get better.\n\n\n\n\n\n\n\nFigure 2.12: Source: UC Berkeley AI course slides, lecture 11\n\n\n\n\n\n\\epsilon-greedy action selection\n\\epsilon-greedy action selection ensures a trade-off between exploitation and exploration. The greedy action is selected with probability 1 - \\epsilon (with 0 &lt; \\epsilon &lt;1), the others with probability \\epsilon:\n\n    \\pi(a) = \\begin{cases} 1 - \\epsilon \\; \\text{if} \\; a = a_t^* \\\\ \\frac{\\epsilon}{|\\mathcal{A}| - 1} \\; \\text{otherwise.} \\end{cases}\n\n\n\n\n\n\n\nFigure 2.13: \\epsilon-greedy action selection. The greedy action is selected most of the time, but the other actions might be selected from time to time.\n\n\n\nThe parameter \\epsilon controls the level of exploration: the higher \\epsilon, the more exploration. One can set \\epsilon high at the beginning of learning and progressively reduce it to exploit more. However, it chooses equally among all actions: the worst action is as likely to be selected as the next-to-best action.\n\n\n\nSoftmax action selection\nSoftmax action selection defines the probability of choosing an action using all estimated value. It represents the policy using a Gibbs (or Boltzmann) distribution:\n\n    \\pi(a) = \\dfrac{\\exp \\dfrac{Q_t(a)}{\\tau}}{ \\displaystyle\\sum_{a'} \\exp \\dfrac{Q_t(a')}{\\tau}}\n\nwhere \\tau is a positive parameter called the temperature.\n\n\n\n\n\n\nFigure 2.14: Softmax action selection.\n\n\n\nJust as \\epsilon, the temperature \\tau controls the level of exploration:\n\nHigh temperature causes the actions to be nearly equiprobable (random agent).\nLow temperature causes the greediest actions only to be selected (greedy agent).\n\n\n\n\n\n\n\nFigure 2.15: Influence of the temperature parameter. With low temperatures (left), only the greedy action gets selected, while high temperatures (right) make action selection random.\n\n\n\n\n\n\n\n\n\n\nExploration schedule\n\n\n\nA useful technique to cope with the exploration-exploitation dilemma is to slowly decrease the value of \\epsilon or \\tau with the number of plays. This allows for more exploration at the beginning of learning and more exploitation towards the end. It is however hard to find the right decay rate for the exploration parameters.\n\n\n\n\n\nOptimistic initial values\nThe problem with online evaluation is that it depends a lot on the initial estimates Q_0. If the initial estimates are already quite good (e.g. using expert knowledge), the Q-values will converge very fast. If the initial estimates are very wrong, we will need a lot of updates to correctly estimate the true values. This problem is called bootstrapping: the better your initial estimates, the better (and faster) the results.\n\n\\begin{aligned}\n    &Q_{t+1}(a) = (1 - \\alpha) \\, Q_t(a) + \\alpha \\, r_{t+1}  \\\\\n    &\\\\\n    & \\rightarrow Q_1(a) = (1 - \\alpha) \\, Q_0(a) + \\alpha \\, r_1 \\\\\n    & \\rightarrow Q_2(a) = (1 - \\alpha) \\, Q_1(a) + \\alpha \\, r_2 = (1- \\alpha)^2 \\, Q_0(a) + (1-\\alpha)\\alpha \\, r_1 + \\alpha r_2 \\\\\n\\end{aligned}\n\nThe influence of Q_0 on Q_t fades quickly with (1-\\alpha)^t, but that can be lost time or lead to a suboptimal policy. However, we can use this at our advantage with optimistic initialization. By choosing very high initial values for the estimates (they can only decrease), one can ensure that all possible actions will be selected during learning by the greedy method, solving the exploration problem. This leads however to an overestimation of the value of other actions.\n\n\n\nReinforcement comparison\nActions followed by large rewards should be made more likely to reoccur, whereas actions followed by small rewards should be made less likely to reoccur. But what is a large/small reward? Is a reward of 5 large or small? Reinforcement comparison methods only maintain a preference p_t(a) for each action, which is not exactly its Q-value. The preference for an action is updated after each play, according to the update rule:\n\n    p_{t+1}(a_t) =    p_{t}(a_t) + \\beta \\, (r_t - \\tilde{r}_t)\n\nwhere \\tilde{r}_t is the moving average of the recently received rewards (regardless the action):\n\n    \\tilde{r}_{t+1} =  \\tilde{r}_t + \\alpha \\, (r_t - \\tilde{r}_t)\n\nIf an action brings more reward than usual (good surprise), we increase the preference for that action. If an action brings less reward than usual (bad surprise), we decrease the preference for that action. \\beta &gt; 0 and 0 &lt; \\alpha &lt; 1 are two constant parameters.\nPreferences are updated by replacing the action-dependent Q-values by a baseline \\tilde{r}_t:\n\n    p_{t+1}(a_t) =    p_{t}(a_t) + \\beta \\, (r_t - \\tilde{r}_t)\n\nThe preferences can be used to select the action using the softmax method just as the Q-values (without temperature):\n\n    \\pi_t (a) = \\dfrac{\\exp p_t(a)}{ \\displaystyle\\sum_{a'} \\exp p_t(a')}\n\n\nReinforcement comparison can be very effective, as it does not rely only on the rewards received, but also on their comparison with a baseline, the average reward. This idea is at the core of actor-critic architectures which we will see later. The initial average reward \\tilde{r}_{0} can be set optimistically to encourage exploration.\n\n\nGradient bandit algorithm\nInstead of only increasing the preference for the executed action if it brings more reward than usual, we could also decrease the preference for the other actions. The preferences are used to select an action a_t via softmax:\n\n    \\pi_t (a) = \\dfrac{\\exp p_t(a)}{ \\displaystyle\\sum_{a'} \\exp p_t(a')}\n\nUpdate rule for the action taken a_t:\n\n    p_{t+1}(a_t) =    p_{t}(a_t) + \\beta \\, (r_t - \\tilde{r}_t) \\, (1 - \\pi_t(a_t))\n\nUpdate rule for the other actions a \\neq a_t:\n\n    p_{t+1}(a) =    p_{t}(a) - \\beta \\, (r_t - \\tilde{r}_t) \\, \\pi_t(a)\n\nUpdate of the reward baseline:\n\n    \\tilde{r}_{t+1} =  \\tilde{r}_t + \\alpha \\, (r_t - \\tilde{r}_t)\n\nThe preference can increase become quite high, making the policy greedy towards the end. No need for a temperature parameter!\n\n\n\nUpper-Confidence-Bound action selection\nIn the previous methods, exploration is controlled by an external parameter (\\epsilon or \\tau) which is global to each action an must be scheduled. A much better approach would be to decide whether to explore an action based on the uncertainty about its Q-value: If we are certain about the value of an action, there is no need to explore it further, we only have to exploit it if it is good.\nThe central limit theorem tells us that the variance of a sampling estimator decreases with the number of samples:\n\nThe distribution of sample averages is normally distributed with mean \\mu and variance \\frac{\\sigma^2}{N}.\n\nS_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{N}})\nThe more you explore an action a, the smaller the variance of Q_t(a), the more certain you are about the estimation, the less you need to explore it.\nUpper-Confidence-Bound (UCB) action selection is a greedy action selection method that uses an exploration bonus:\n\n    a^*_t = \\text{argmax}_{a} \\left[ Q_t(a) + c \\, \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]\n\nQ_t(a) is the current estimated value of a and N_t(a) is the number of times the action a has already been selected.\nIt realizes a balance between trusting the estimates Q_t(a) and exploring uncertain actions which have not been explored much yet. The term \\sqrt{\\frac{\\ln t}{N_t(a)}} is an estimate of the variance of Q_t(a). The sum of both terms is an upper-bound of the true value \\mu + \\sigma. When an action has not been explored much yet, the uncertainty term will dominate and the action be explored, although its estimated value might be low. When an action has been sufficiently explored, the uncertainty term goes to 0 and we greedily follow Q_t(a).\nThe exploration-exploitation trade-off is automatically adjusted by counting visits to an action.\n\n    a^*_t = \\text{argmax}_{a} \\left[ Q_t(a) + c \\, \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling and Bandits</span>"
    ]
  },
  {
    "objectID": "src/1.2-MDP.html",
    "href": "src/1.2-MDP.html",
    "title": "Markov Decision Process",
    "section": "",
    "text": "Markov Decision Process",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Markov Decision Process</span>"
    ]
  },
  {
    "objectID": "src/1.2-MDP.html#markov-decision-process",
    "href": "src/1.2-MDP.html#markov-decision-process",
    "title": "Markov Decision Process",
    "section": "",
    "text": "Definition\nReinforcement Learning methods apply to problems where an agent interacts with an environment in discrete time steps (Figure 3.1). At time t, the agent is in state s_t and decides to perform an action a_t. At the next time step, it arrives in the state s_{t+1} and obtains the reward r_{t+1}. In the genral case, transitions can be stochastic (there is a probability of arriving in a given state after an action), as well as the rewards (as in the bandits previously seen). The goal of the agent is to maximize the reward obtained on the long term.\n\n\n\n\n\n\nFigure 3.1: Interaction between an agent and its environment. Source: Sutton and Barto (1998).\n\n\n\nThese problems are formalized as Markov Decision Processes (MDP) and defined by six quantities &lt;\\mathcal{S}, \\mathcal{A}, p_0, \\mathcal{P}, \\mathcal{R}, \\gamma&gt;. For a finite MDP, we have:\n\nThe state space \\mathcal{S} = \\{ s_i\\}_{i=1}^N, where each state respects the Markov property.\nThe action space \\mathcal{A} = \\{ a_i\\}_{i=1}^M.\nAn initial state distribution p_0(s_0) (from which states the agent is most likely to start).\nThe state transition probability function, defining the probability of arriving in the state s' at time t+1 after being in the state s and performing the action a at time t:\n\n\n\\begin{aligned}\n    \\mathcal{P}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow & P(\\mathcal{S}) \\\\\n    p(s' | s, a) & =  P (s_{t+1} = s' | s_t = s, a_t = a) \\\\\n\\end{aligned}\n\n\nThe expected reward function defining the (stochastic) reward obtained after performing a in state s and arriving in s':\n\n\n\\begin{aligned}\n    \\mathcal{R}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow & \\Re \\\\\n    r(s, a, s') &=  \\mathbb{E} (r_{t+1} | s_t = s, a_t = a, s_{t+1} = s') \\\\\n\\end{aligned}\n\n\nThe discount factor \\gamma \\in [0, 1].\n\nIn deep RL, the state and action spaces can be infinite, but let’s focus on finite MDPs for now.\nThe behavior of the agent over time is a trajectory (also called episode, history or roll-out) \\tau = (s_0, a_0, s_1, a_, \\ldots, s_T, a_T) defined by the dynamics of the MDP. Each transition occurs with a probability p(s'|s, a) and provides a certain amount of reward defined by r(s, a, s'). In episodic tasks, the horizon T is finite, while in continuing tasks T is infinite.\n\n\nMarkov property\nThe state of the agent represents all the information needed to take decisions and solve the task. For a robot navigating in an environment, this may include all its sensors, its positions as tracked by a GPS, but also the relative position of all objects / persons it may interact with. For a board game, the description of the board is usually enough.\nImportantly, the Markov property states that:\n\nThe future is independent of the past given the present.\n\nIn mathematical terms for a transition (s_t, a_t, s_{t+1}):\n\n    p(s_{t+1}|s_t, a_t) = p(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \\dots s_0, a_0)\n\ni.e. you do not need the full history of the agent to predict where it will arrive after an action. In simple problems, this is just a question of providing enough information to the description of a state: if a transition depends on what happened in the past, just put that information in the state description.\nA state representation with the Markov property should therefore not only contain all the important information available at time t, but also information from the past that is necessary to take a decision.\nIf the Markov property is not met, RL methods may not converge (or poorly). In many problems, one does not have access to the true states of the agent, but one can only indirectly observe them. For example, in a video game, the true state is defined by a couple of variables: coordinates (x, y) of the two players, position of the ball, speed, etc. However, in Atari games all you have access to are the raw pixels: sometimes the ball may be hidden behind a wall or a tree, but it still exists in the state space. Speed information is also not observable in a single frame.\nIn a Partially Observable Markov Decision Process (POMDP), observations o_t come from a space \\mathcal{O} and are linked to underlying states using the density function p(o_t| s_t). Observations are usually not Markov, so the full history of observations h_t = (o_0, a_0, \\dots o_t, a_t) is needed to solve the problem. We will see later how recurrent neural networks can help with POMDPs.\n\n\nRewards and returns\nAs with n-armed bandits, we only care about the expected reward received during a transition s \\rightarrow s' (on average), but the actual reward received r_{t+1} may vary around the expected value with some unknown variance. In hard RL, we only care about the expected reward and ignore its variance, as we suppose that we can take actions an infinity of times. However, distributional RL investigates the role of this variance (see Section Categorical DQN).\nr(s, a, s') =  \\mathbb{E} (r_{t+1} | s_t = s, a_t = a, s_{t+1} = s')\n\n\n\n\n\n\nFigure 3.2: Reward distributions for several actions in a single state.\n\n\n\nAn important distinction in practice is between sparse vs. dense rewards. Sparse rewards take non-zero values only during certain transitions: game won/lost, goal achieved, timeout, etc. Dense rewards provide non-zero values during each transition: distance to goal, energy consumption, speed of the robot, etc. As we will see later, MDPs with sparse rewards are much harder to learn.\n\n\n\n\n\n\nFigure 3.3: Dense vs. sparse rewards. Source: https://forns.lmu.build/classes/spring-2020/cmsi-432/lecture-13-2.html\n\n\n\nOver time, the MDP will be in a sequence of states (possibly infinite):\ns_0 \\rightarrow s_1 \\rightarrow s_2  \\rightarrow \\ldots \\rightarrow s_T\nand collect a sequence of rewards:\nr_1 \\rightarrow r_2 \\rightarrow r_3  \\rightarrow \\ldots \\rightarrow r_{T}\n\n\n\n\n\n\nFigure 3.4: Sequence of transitions over time in a MDP.\n\n\n\nIn a MDP, we are interested in maximizing the return R_t, i.e. the discounted sum of future rewards after the step t:\n\n    R_t = r_{t+1} + \\gamma \\, r_{t+2} + \\gamma^2 \\, r_{t+3} + \\ldots = \\sum_{k=0}^\\infty \\gamma^k \\, r_{t+k+1}\n\nThe return is sometimes called the reward-to-go: how much reward will I collect from now on? Of course, you can never know the return at time t: transitions and rewards are probabilistic, so the received rewards in the future are not exactly predictable at t. R_t is therefore purely theoretical: RL is all about estimating the return.\nMore generally, for a trajectory (episode) \\tau = (s_0, a_0, r_1, s_1, a_1, \\ldots, s_T), one can define its return as:\n R(\\tau) = \\sum_{t=0}^{T} \\gamma^t \\, r_{t+1} \nThe discount factor (or discount rate, or discount) \\gamma \\in [0, 1] is a very important parameter in RL: It defines the present value of future rewards. Receiving 10 euros now has a higher value than receiving 10 euros in ten years, although the reward is the same: you do not have to wait. The value of receiving a reward r after k+1 time steps is \\gamma^k \\, r, meaning that immediate rewards are better than delayed rewards.\n\\gamma determines the relative importance of future rewards for the behavior:\n\nif \\gamma is close to 0, only the immediately available rewards will count: the agent is greedy or myopic.\nif \\gamma is close to 1, even far-distance rewards will be taken into account: the agent is farsighted.\n\nAnother important property is that, when \\gamma &lt; 1, \\gamma^k tends to 0 when k goes to infinity: this makes sure that the return is always finite. We can therefore try to maximize it.\n\n    R_t = r_{t+1} + \\gamma \\, r_{t+2} + \\gamma^2 \\, r_{t+3} + \\ldots = \\sum_{k=0}^\\infty \\gamma^k \\, r_{t+k+1}\n\n\n\n\n\n\n\nFigure 3.5: The value of \\gamma^k decays over time. The closer \\gamma is to 1, the slower the decay.\n\n\n\nFor episodic tasks (which break naturally into finite episodes of length T, e.g. plays of a game, trips through a maze), the return is always finite and easy to compute at the end of the episode. The discount factor can be set to 1.\n\n    R_t = \\sum_{k=0}^{T} r_{t+k+1}\n\nFor continuing tasks (which can not be split into episodes), the return could become infinite if \\gamma = 1. The discount factor has to be smaller than 1.\n\n    R_t = \\sum_{k=0}^{\\infty} \\gamma^k \\, r_{t+k+1}\n\n\n\n\n\n\n\nWhy the reward on the long term?\n\n\n\n\n\n\n\n\n\nFigure 3.6: Example of a MDP with two actions in state s_1, lading to two different returns. The states s_5 and s_6 are terminal states, where no reward is received anymore.\n\n\n\nIn the MDP above, selecting the action a_1 in s_1 does not bring reward immediately (r_1 = 0) but allows to reach s_5 in the future and get a reward of 10. Selecting a_2 in s_1 brings immediately a reward of 1, but that will be all.\nDepending on the value of \\gamma, the optimal action might be a_1 or a_2, depending on which one brings more reward on the long term.\nWhen selecting a_1 in s_1, the discounted return is:\n\n    R = 0 + \\gamma \\, 0 + \\gamma^2 \\, 0 + \\gamma^3 \\, 10 + \\ldots = 10 \\, \\gamma^3\n\nwhile it is R= 1 for the action a_2.\nFor high values of \\gamma, 10\\, \\gamma^3 is higher than one, so the action a_1 is the optimal action. For small values of \\gamma, 10\\, \\gamma^3 becomes smaller than one, and the action a_2 becomes the optimal action. The discount rate \\gamma can totally change the optimal behavior of the agent, that is why it is part of the MDP definition and not just a hyperparameter.\n\n\n\n\nPolicy\nThe probability that an agent selects a particular action a in a given state s is called the policy \\pi.\n\n\\begin{align}\n    \\pi &: \\mathcal{S} \\times \\mathcal{A} \\rightarrow P(\\mathcal{S})\\\\\n    (s, a) &\\rightarrow \\pi(s, a)  = P(a_t = a | s_t = s) \\\\\n\\end{align}\n\nThe policy can be deterministic (one action has a probability of 1, the others 0) or stochastic. In all cases, the sum of the probabilities in a given state must be one:\n\n    \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) = 1\n\nThe goal of an agent is to find a policy that maximizes the sum of received rewards on the long term, i.e. the return R_t at each each time step. This policy is called the optimal policy \\pi^*. It maximizes the following objective function:\n\n    \\pi^* = \\text{argmax} \\, \\mathcal{J}(\\pi) = \\text{argmax} \\,  \\mathbb{E}_{\\tau \\sim \\rho_\\pi} [R(\\tau)]\n\nwhere \\rho_\\pi is the density distribution of the trajectories generated by the policy \\pi.\nIn summary, RL is an adaptive optimal control method for Markov Decision Processes using (sparse) rewards as a partial feedback. At each time step t, the agent observes its Markov state s_t \\in \\mathcal{S}, produces an action a_t \\in \\mathcal{A}(s_t), receives a reward according to this action r_{t+1} \\in \\Re and updates its state: s_{t+1} \\in \\mathcal{S}. The agent generates trajectories \\tau = (s_0, a_0, r_1, s_1, a_1, \\ldots, s_T) depending on its policy \\pi(s ,a). The goal is to find the optimal policy \\pi^* (s, a) that maximizes in expectation the return of each possible trajectory under that policy.",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Markov Decision Process</span>"
    ]
  },
  {
    "objectID": "src/1.2-MDP.html#value-functions",
    "href": "src/1.2-MDP.html#value-functions",
    "title": "Markov Decision Process",
    "section": "Value functions",
    "text": "Value functions\nA central notion in RL is to estimate the value (or utility) of every state and action of the MDP. The state-value V^{\\pi} (s) of a state s is defined as the mathematical expectation of the return when starting from that state and thereafter following the agent’s current policy \\pi:\n  V^{\\pi} (s) = \\mathbb{E}_{\\rho_\\pi} ( R_t | s_t = s) = \\mathbb{E}_{\\rho_\\pi} ( \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} |s_t=s ) \nThe mathematical expectation operator \\mathbb{E}(\\cdot) is indexed by \\rho_\\pi, the probability distribution of states achievable with \\pi. Indeed, several trajectories are possible after the state s:\n\nThe state transition probability function p(s' | s, a) leads to different states s', even if the same actions are taken.\nThe expected reward function r(s, a, s') provides stochastic rewards, even if the transition (s, a, s') is the same.\nThe policy \\pi itself is stochastic.\n\nOnly rewards that are obtained using the policy \\pi should be taken into account, not the complete distribution of states and rewards.\nThe value of a state is not intrinsic to the state itself, it depends on the policy: One could be in a state which is very close to the goal (only one action left to win the game), but if the policy is very bad, the “good” action will not be chosen and the state will have a small value.\nThe value of taking an action a in a state s under policy \\pi is the expected return starting\nSimilarly, the action-value (or Q-value) for a state-action pair (s, a) under the policy \\pi is defined as:\n\n\\begin{align}\n    Q^{\\pi} (s, a)  & = \\mathbb{E}_{\\rho_\\pi} ( R_t | s_t = s, a_t =a) \\\\\n                    & = \\mathbb{E}_{\\rho_\\pi} ( \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} |s_t=s, a_t=a) \\\\\n\\end{align}\n\nThe Q-value of an action is sometimes called its utility: is it worth taking this action?",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Markov Decision Process</span>"
    ]
  },
  {
    "objectID": "src/1.2-MDP.html#bellman-equations",
    "href": "src/1.2-MDP.html#bellman-equations",
    "title": "Markov Decision Process",
    "section": "Bellman equations",
    "text": "Bellman equations\n\nRelationship between V and Q\nThe value of a state V^{\\pi}(s) depends on the value Q^{\\pi} (s, a) of the action that will be chosen by the policy \\pi in s:\n\n        V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi(s,a)} [Q^{\\pi} (s, a)] = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, Q^{\\pi} (s, a)\n\nIf the policy \\pi is deterministic (the same action is chosen every time), the value of the state is the same as the value of that action (same expected return). If the policy \\pi is stochastic (actions are chosen with different probabilities), the value of the state is the weighted average (i.e. expectation) of the value of the actions.\n➡️ If the Q-values are known, the V-values can be found easily.\nWe can note that the return at time t depends on the immediate reward r_{t+1} and the return at the next time step t+1:\n\n\\begin{aligned}\n    R_t &= r_{t+1} + \\gamma \\, r_{t+2} +  \\gamma^2  \\, r_{t+3} + \\dots + \\gamma^k \\, r_{t+k+1} + \\dots \\\\\n        &= r_{t+1} + \\gamma \\, ( r_{t+2} +  \\gamma \\, r_{t+3} + \\dots + \\gamma^{k-1} \\, r_{t+k+1} + \\dots) \\\\\n        &= r_{t+1} + \\gamma \\,  R_{t+1} \\\\\n\\end{aligned}\n\nWhen taking the mathematical expectation of that identity, we obtain:\n\n    \\mathbb{E}_{\\rho_\\pi}[R_t] = r(s_t, a_t, s_{t+1}) + \\gamma \\, \\mathbb{E}_{\\rho_\\pi}[R_{t+1}]\n\nIt becomes clear that the value of an action depends on the immediate reward received just after the action, as well as the value of the next state:\n\n        Q^{\\pi}(s_t, a_t) = r(s_t, a_t, s_{t+1}) + \\gamma \\,  V^{\\pi} (s_{t+1})\n\nHowever, this is only for a fixed (s_t, a_t, s_{t+1}) transition. Taking transition probabilities into account, one can obtain the Q-values through the equation:\n\n    Q^{\\pi}(s, a) = \\mathbb{E}_{s' \\sim p(s'|s, a)} [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ] = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ]\n\nThe value of an action depends on:\n\nthe states s' one can arrive after the action (with a probability p(s' | s, a)).\nthe value of that state V^{\\pi} (s'), weighted by \\gamma as it is one step in the future.\nthe reward received immediately after taking that action r(s, a, s') (as it is not included in the value of s').\n\n➡️ If the V-values are known, the Q-values can be found easily by a 1-step look-ahead, i.e. looking at the achievable states.\n\n\nBellman equations\nPutting together those two equations, a fundamental property of value functions used throughout reinforcement learning is that they satisfy a particular recursive relationship:\n\n\\begin{aligned}\n        V^{\\pi}(s)  &= \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, Q^{\\pi} (s, a)\\\\\n                    &= \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ]\n\\end{aligned}\n\nThis equation is called the Bellman equation for V^{\\pi}. It expresses the relationship between the value of a state V^\\pi(s) and the value of its successors V^\\pi(s'), depending on the dynamics of the MDP (p(s' | s, a) and r(s, a, s')) and the current policy \\pi. The interesting property of the Bellman equation for RL is that it admits one and only one solution V^{\\pi}(s).\nThe same recursive relationship stands for Q^{\\pi}(s, a):\n\n\\begin{aligned}\n        Q^{\\pi}(s, a)  &= \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ] \\\\\n                    &=  \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, \\sum_{a' \\in \\mathcal{A}(s')} \\pi(s', a') \\, Q^{\\pi} (s', a')]\n\\end{aligned}\n\nwhich is called the Bellman equation for Q^{\\pi}.\n\n\nOptimal Bellman equations\nThe optimal policy is the policy that gathers the maximum of reward on the long term. Value functions define a partial ordering over policies:\n\n\n\n\n\n\nPartial ordering\n\n\n\nA policy \\pi is better than another policy \\pi' if its expected return is greater or equal than that of \\pi' for all states s.\n\n        \\pi \\geq \\pi' \\Leftrightarrow V^{\\pi}(s) \\geq V^{\\pi'}(s) \\quad \\forall s \\in \\mathcal{S}\n\n\n\nFor a MDP, there exists at least one policy that is better than all the others: this is the optimal policy \\pi^*. We note V^*(s) and Q^*(s, a) the optimal value of the different states and actions under \\pi^*.\n\n   V^* (s) = \\max_{\\pi} V^{\\pi}(s) \\quad \\forall s \\in \\mathcal{S}\n\n\n    Q^* (s, a) = \\max_{\\pi} Q^{\\pi}(s, a) \\quad \\forall s \\in \\mathcal{S}, \\quad \\forall a \\in \\mathcal{A}\n\nWhen the policy is optimal \\pi^*, the link between the V and Q values is even easier. The V and Q values are maximal for the optimal policy: there is no better alternative.\nThe optimal action a^* to perform in the state s is the one with the highest optimal Q-value Q^*(s, a).\n\n    a^* = \\text{argmax}_a \\, Q^*(s, a)\n\nBy definition, this action will bring the maximal return when starting in s. The optimal policy is therefore greedy with respect to Q^*(s, a), i.e. deterministic.\n\n    \\pi^*(s, a) = \\begin{cases}\n                1 \\; \\text{if} \\; a = a^* \\\\\n                0 \\; \\text{otherwise.}\n                \\end{cases}\n\nAs the optimal policy is deterministic, the optimal value of a state is equal to the value of the optimal action:\n\n    V^* (s)  = \\max_{a \\in \\mathcal{A}(s)} Q^{\\pi^*} (s, a)\n\nThe expected return after being in s is the same as the expected return after being in s and choosing the optimal action a^*, as this is the only action that can be taken. This allows to define the Bellman optimality equation for V^*:\n\n    V^* (s)  = \\max_{a \\in \\mathcal{A}(s)} \\sum_{s' \\in \\mathcal{S}}  p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{*} (s') ]\n\nThe same Bellman optimality equation stands for Q^*:\n\n    Q^* (s, a)  = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s')  + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q^* (s', a') ]\n\nThe optimal value of (s, a) depends on the optimal action in the next state s'.",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Markov Decision Process</span>"
    ]
  },
  {
    "objectID": "src/1.2-MDP.html#sec-dp",
    "href": "src/1.2-MDP.html#sec-dp",
    "title": "Markov Decision Process",
    "section": "Dynamic programming",
    "text": "Dynamic programming\n\n\n\n\n\n\nFigure 3.7: Generalized Policy Iteration. Source: Sutton and Barto (1998).\n\n\n\nIn general, RL algorithms iterate over two steps:\n\nPolicy evaluation\n\nFor a given policy \\pi, the value of all states V^\\pi(s) or all state-action pairs Q^\\pi(s, a) is calculated or estimated.\n\nPolicy improvement\n\nFrom the current estimated values V^\\pi(s) or Q^\\pi(s, a), a new better policy \\pi is derived.\n\n\nAfter enough iterations, the policy converges to the optimal policy (if the states are Markov).\nThis alternation between policy evaluation and policy improvement is called generalized policy iteration (GPI). One particular form of GPI is dynamic programming, where the Bellman equations are used to evaluate a policy.\n\nExact solution\nLet’s note \\mathcal{P}_{ss'}^\\pi the transition probability between s and s' (dependent on the policy \\pi) and \\mathcal{R}_{s}^\\pi the expected reward in s (also dependent):\n\n  \\mathcal{P}_{ss'}^\\pi = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, p(s' | s, a)\n\n\n  \\mathcal{R}_{s}^\\pi = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} \\, p(s' | s, a) \\ r(s, a, s')\n\nThe Bellman equation becomes V^{\\pi} (s)  = \\mathcal{R}_{s}^\\pi + \\gamma \\, \\displaystyle\\sum_{s' \\in \\mathcal{S}} \\, \\mathcal{P}_{ss'}^\\pi \\, V^{\\pi} (s'). As we have a fixed policy during the evaluation, the Bellman equation is simplified.\nLet’s now put the Bellman equations in a matrix-vector form.\n\n      V^{\\pi} (s)  = \\mathcal{R}_{s}^\\pi + \\gamma \\, \\sum_{s' \\in \\mathcal{S}} \\, \\mathcal{P}_{ss'}^\\pi \\, V^{\\pi} (s')\n\nWe first define the vector of state values \\mathbf{V}^\\pi:\n\n  \\mathbf{V}^\\pi = \\begin{bmatrix}\n      V^\\pi(s_1) \\\\ V^\\pi(s_2) \\\\ \\vdots \\\\ V^\\pi(s_n) \\\\\n  \\end{bmatrix}\n\nand the vector of expected reward \\mathbf{R}^\\pi:\n\n  \\mathbf{R}^\\pi = \\begin{bmatrix}\n      \\mathcal{R}^\\pi(s_1) \\\\ \\mathcal{R}^\\pi(s_2) \\\\ \\vdots \\\\ \\mathcal{R}^\\pi(s_n) \\\\\n  \\end{bmatrix}\n\nThe state transition matrix \\mathcal{P}^\\pi is defined as:\n\n  \\mathcal{P}^\\pi = \\begin{bmatrix}\n      \\mathcal{P}_{s_1 s_1}^\\pi & \\mathcal{P}_{s_1 s_2}^\\pi & \\ldots & \\mathcal{P}_{s_1 s_n}^\\pi \\\\\n      \\mathcal{P}_{s_2 s_1}^\\pi & \\mathcal{P}_{s_2 s_2}^\\pi & \\ldots & \\mathcal{P}_{s_2 s_n}^\\pi \\\\\n      \\vdots & \\vdots & \\vdots & \\vdots \\\\\n      \\mathcal{P}_{s_n s_1}^\\pi & \\mathcal{P}_{s_n s_2}^\\pi & \\ldots & \\mathcal{P}_{s_n s_n}^\\pi \\\\\n  \\end{bmatrix}\n\nYou can simply check that:\n\n  \\begin{bmatrix}\n      V^\\pi(s_1) \\\\ V^\\pi(s_2) \\\\ \\vdots \\\\ V^\\pi(s_n) \\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n      \\mathcal{R}^\\pi(s_1) \\\\ \\mathcal{R}^\\pi(s_2) \\\\ \\vdots \\\\ \\mathcal{R}^\\pi(s_n) \\\\\n  \\end{bmatrix}\n  + \\gamma \\, \\begin{bmatrix}\n      \\mathcal{P}_{s_1 s_1}^\\pi & \\mathcal{P}_{s_1 s_2}^\\pi & \\ldots & \\mathcal{P}_{s_1 s_n}^\\pi \\\\\n      \\mathcal{P}_{s_2 s_1}^\\pi & \\mathcal{P}_{s_2 s_2}^\\pi & \\ldots & \\mathcal{P}_{s_2 s_n}^\\pi \\\\\n      \\vdots & \\vdots & \\vdots & \\vdots \\\\\n      \\mathcal{P}_{s_n s_1}^\\pi & \\mathcal{P}_{s_n s_2}^\\pi & \\ldots & \\mathcal{P}_{s_n s_n}^\\pi \\\\\n  \\end{bmatrix} \\times \\begin{bmatrix}\n      V^\\pi(s_1) \\\\ V^\\pi(s_2) \\\\ \\vdots \\\\ V^\\pi(s_n) \\\\\n  \\end{bmatrix}\n\nleads to the same equations as:\n\n      V^{\\pi} (s)  = \\mathcal{R}_{s}^\\pi + \\gamma \\, \\sum_{s' \\in \\mathcal{S}} \\, \\mathcal{P}_{ss'}^\\pi \\, V^{\\pi} (s')\n\nfor all states s. The Bellman equations for all states s can therefore be written with a matrix-vector notation as:\n\n  \\mathbf{V}^\\pi = \\mathbf{R}^\\pi + \\gamma \\, \\mathcal{P}^\\pi \\, \\mathbf{V}^\\pi\n\nIf we know \\mathcal{P}^\\pi and \\mathbf{R}^\\pi (dynamics of the MDP for the policy \\pi), we can simply obtain the state values:\n\n  (\\mathbb{I} - \\gamma \\, \\mathcal{P}^\\pi ) \\times \\mathbf{V}^\\pi = \\mathbf{R}^\\pi\n\nwhere \\mathbb{I} is the identity matrix, what gives:\n\n  \\mathbf{V}^\\pi = (\\mathbb{I} - \\gamma \\, \\mathcal{P}^\\pi )^{-1} \\times \\mathbf{R}^\\pi\n\nIf we have n states, the matrix \\mathcal{P}^\\pi has n^2 elements. Inverting \\mathbb{I} - \\gamma \\, \\mathcal{P}^\\pi requires at least \\mathcal{O}(n^{2.37}) operations. Forget it if you have more than a thousand states (1000^{2.37} \\approx 13 million operations). In dynamic programming, we will use iterative methods to estimate \\mathbf{V}^\\pi.\n\n\nPolicy iteration\nThe idea of iterative policy evaluation (IPE) is to consider a sequence of consecutive state-value functions which should converge from initially wrong estimates V_0(s) towards the real state-value function V^{\\pi}(s).\n\n      V_0 \\rightarrow V_1 \\rightarrow V_2 \\rightarrow \\ldots \\rightarrow V_k \\rightarrow V_{k+1} \\rightarrow \\ldots \\rightarrow V^\\pi\n\n\n\n\nIterative policy estimaiton. Source: David Silver. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n\n\nThe value function at step k+1 V_{k+1}(s) is computed using the previous estimates V_{k}(s) and the Bellman equation transformed into an update rule.\n\n  \\mathbf{V}_{k+1} = \\mathbf{R}^\\pi + \\gamma \\, \\mathcal{P}^\\pi \\, \\mathbf{V}_k\n\nV_\\infty = V^{\\pi} is a fixed point of this update rule because of the uniqueness of the solution to the Bellman equation.\n\n\n\n\n\n\nIterative policy evaluation\n\n\n\n\nFor a fixed policy \\pi, initialize V(s)=0 \\; \\forall s \\in \\mathcal{S}.\nwhile not converged:\n\nfor all states s:\n\nV_\\text{target}(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n\n\\delta =0\nfor all states s:\n\n\\delta = \\max(\\delta, |V(s) - V_\\text{target}(s)|)\nV(s) = V_\\text{target}(s)\n\nif \\delta &lt; \\delta_\\text{threshold}:\n\nconverged = True\n\n\n\n\n\nFor each state s, we would like to know if we should deterministically choose an action a \\neq \\pi(s) or not in order to improve the policy. The value of an action a in the state s for the policy \\pi is given by:\n\n     Q^{\\pi} (s, a) = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ]\n\nIf the Q-value of an action a is higher than the one currently selected by the deterministic policy:\nQ^{\\pi} (s, a) &gt; Q^{\\pi} (s, \\pi(s)) = V^{\\pi}(s)\nthen it is better to select a once in s and thereafter follow \\pi. If there is no better action, we keep the previous policy for this state. This corresponds to a greedy action selection over the Q-values, defining a deterministic policy \\pi(s):\n\\pi(s) \\leftarrow \\text{argmax}_a \\, Q^{\\pi} (s, a) = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ]\nAfter the policy improvement, the Q-value of each deterministic action \\pi(s) has increased or stayed the same.\n\\text{argmax}_a \\; Q^{\\pi} (s, a) = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ] \\geq Q^\\pi(s, \\pi(s))\nThis defines an improved policy \\pi', where all states and actions have a higher value than previously. Greedy action selection over the state value function implements policy improvement:\n\\pi' \\leftarrow \\text{Greedy}(V^\\pi)\n\n\n\n\n\n\nGreedy policy improvement:\n\n\n\n\nfor each state s \\in \\mathcal{S}:\n\n\\pi(s) \\leftarrow \\text{argmax}_a \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ]\n\n\n\n\nOnce a policy \\pi has been improved using V^{\\pi} to yield a better policy \\pi', we can then compute V^{\\pi'} and improve it again to yield an even better policy \\pi''. The algorithm policy iteration successively uses policy evaluation and policy improvement to find the optimal policy.\n\n  \\pi_0 \\xrightarrow[]{E} V^{\\pi_0} \\xrightarrow[]{I} \\pi_1 \\xrightarrow[]{E} V^{\\pi^1} \\xrightarrow[]{I}  ... \\xrightarrow[]{I} \\pi^* \\xrightarrow[]{E} V^{*}\n\nThe optimal policy being deterministic, policy improvement can be greedy over the state-action values. If the policy does not change after policy improvement, the optimal policy has been found.\n\n\n\n\n\n\nPolicy iteration\n\n\n\n\nInitialize a deterministic policy \\pi(s) and set V(s)=0 \\; \\forall s \\in \\mathcal{S}.\nwhile \\pi is not optimal:\n\nwhile not converged: # Policy evaluation\n\nfor all states s:\n\nV_\\text{target}(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n\nfor all states s:\n\nV(s) = V_\\text{target}(s)\n\n\nfor each state s \\in \\mathcal{S}: # Policy improvement\n\n\\pi(s) \\leftarrow \\text{argmax}_a \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ]\n\nif \\pi has not changed: break\n\n\n\n\n\n\nValue iteration\nOne drawback of policy iteration is that it uses a full policy evaluation, which can be computationally exhaustive as the convergence of V_k is only at the limit and the number of states can be huge. The idea of value iteration is to interleave policy evaluation and policy improvement, so that the policy is improved after EACH iteration of policy evaluation, not after complete convergence.\nAs policy improvement returns a deterministic greedy policy, updating of the value of a state is then simpler:\n\n  V_{k+1}(s) = \\max_a \\sum_{s'} p(s' | s,a) [r(s, a, s') + \\gamma \\, V_k(s') ]\n\nNote that this is equivalent to turning the Bellman optimality equation into an update rule. Value iteration converges to V^*, faster than policy iteration, and should be stopped when the values do not change much anymore.\n\n\n\n\n\n\nValue iteration\n\n\n\n\nInitialize a deterministic policy \\pi(s) and set V(s)=0 \\; \\forall s \\in \\mathcal{S}.\nwhile not converged:\n\nfor all states s:\n\nV_\\text{target}(s) = \\max_a \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n\n\\delta = 0\nfor all states s:\n\n\\delta = \\max(\\delta, |V(s) - V_\\text{target}(s)|)\nV(s) = V_\\text{target}(s)\n\nif \\delta &lt; \\delta_\\text{threshold}:\n\nconverged = True\n\n\n\n\n\n\n\n\n\nSummary\nPolicy-iteration and value-iteration consist of alternations between policy evaluation and policy improvement, and converge to the optimal policy. This principle is called Generalized Policy Iteration (GPI). Solving the Bellman equations requires the following:\n\naccurate knowledge of environment dynamics p(s' | s, a) and r(s, a, s') for all transitions;\nenough memory and time to do the computations;\nthe Markov property.\n\nFinding an optimal policy is polynomial in the number of states and actions: \\mathcal{O}(N^2 \\, M) (N is the number of states, M the number of actions). The number of states is often astronomical (e.g., Go has about 10^{170} states), often growing exponentially with the number of state variables (what Bellman called “the curse of dimensionality”). In practice, classical DP can only be applied to problems with a few millions of states.\n\n\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press.",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Markov Decision Process</span>"
    ]
  },
  {
    "objectID": "src/1.4-MC.html",
    "href": "src/1.4-MC.html",
    "title": "Monte Carlo methods",
    "section": "",
    "text": "Monte Carlo policy evaluation\nIn dynamic programming, policy evaluation was done by explicitly solving the Bellman equations. In Monte Carlo (and temporal difference) methods, policy evaluation relies on sampling.\nWhen the environment is a priori unknown, it has to be explored in order to build estimates of the V or Q value functions. The key idea of Monte Carlo sampling (MC) is rather simple: the expected return in state s is approximated by sampling M trajectories \\tau_i starting from s and computing the sampling average of the obtained returns:\nV^{\\pi}(s) = \\mathbb{E}_{\\rho_\\pi} (R_t | s_t = s) \\approx \\frac{1}{M} \\sum_{i=1}^M R(\\tau_i)\nIf you have enough trajectories, the sampling average is an unbiased estimator of the value function. The advantage of Monte Carlo methods is that they require only experience, not the complete dynamics p(s' | s,a) and r(s, a, s'). The idea of MC policy evaluation is therefore to repeatedly sample episodes starting from each possible state s_0 and maintain a running average of the obtained returns for each state:\nQ-values can also be approximated using the same procedure:\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha (R_t - Q(s_t, a_t))\nThe two main drawbacks of MC methods are:\nThe second issue is linked to the exploration-exploitation dilemma already seen with bandits: the episode is generated using the current policy (or a policy derived from it, see later). If the policy always select the same actions from the beginning (exploitation), the agent will never discover better alternatives: the values will converge to a local minimum. If the policy always pick randomly actions (exploration), the policy which is evaluated is not the current policy \\pi, but the random policy. A trade-off between the two therefore has to be maintained: usually a lot of exploration at the beginning of learning to accumulate knowledge about the environment, less towards the end to actually use the knowledge and perform optimally.\nThere are two types of methods trying to cope with exploration:",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo methods</span>"
    ]
  },
  {
    "objectID": "src/1.4-MC.html#monte-carlo-policy-evaluation",
    "href": "src/1.4-MC.html#monte-carlo-policy-evaluation",
    "title": "Monte Carlo methods",
    "section": "",
    "text": "Monte Carlo policy evaluation\n\n\n\nwhile True:\n\nStart from an initial state s_0.\nGenerate a sequence of transitions according to the current policy \\pi until a terminal state s_T is reached.\n\n\n    \\tau = (s_o, a_o, r_ 1, s_1, a_1, \\ldots, s_T)\n\n\nCompute the return R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} for all encountered states s_0, s_1, \\ldots, s_T.\nUpdate the estimated state value V(s_t) of all encountered states using the obtained return:\n\n\n    V(s_t) \\leftarrow V(s_t) + \\alpha \\, (R_t - V(s_t))\n\n\n\n\n\n\n\nThe task must be episodic, i.e. stop after a finite amount of transitions. Updates are only applied at the end of an episode.\nA sufficient level of exploration has to be ensured to make sure the estimates converge to the optimal values.\n\n\n\n\nOn-policy methods generate the episodes using the learned policy \\pi, but it has to be \\epsilon-soft, i.e. stochastic: it has to let a probability of at least \\epsilon of selecting another action than the greedy action (the one with the highest estimated Q-value).\nOff-policy methods use a second policy called the behavior policy to generate the episodes, but learn a different policy for exploitation, which can even be deterministic.",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo methods</span>"
    ]
  },
  {
    "objectID": "src/1.4-MC.html#on-policy-monte-carlo-methods",
    "href": "src/1.4-MC.html#on-policy-monte-carlo-methods",
    "title": "Monte Carlo methods",
    "section": "On-policy Monte Carlo methods",
    "text": "On-policy Monte Carlo methods\n\\epsilon-soft policies are easy to create, and we have already seen them in Section Sampling and Bandits. The simplest one is the \\epsilon-greedy action selection method, which assigns a probability (1-\\epsilon) of selecting the greedy action (the one with the highest Q-value), and a probability \\epsilon of selecting any of the other available actions:\n\n    a_t = \\begin{cases} a_t^* \\quad \\text{with probability} \\quad (1 - \\epsilon) \\\\\n                       \\text{any other action with probability } \\epsilon \\end{cases}\n\nAnother solution is the Softmax (or Gibbs distribution) action selection method, which assigns to each action a probability of being selected depending on their relative Q-values:\n\n    P(s, a) = \\frac{\\exp Q(s, a) / \\tau}{ \\sum_b \\exp Q(s, b) / \\tau}\n\n\\tau is a positive parameter called the temperature: high temperatures make the actions nearly equiprobable (random policy), while low temperatures only select the actions with the highest Q-values (greedy policy).\nIn on-policy MC control, each sample episode is generated using the current policy, which ensures exploration, while the control method still converges towards the optimal \\epsilon-policy.\n\n\n\n\n\n\nOn-policy Monte Carlo control\n\n\n\nwhile True:\n\nGenerate an episode \\tau = (s_0, a_0, r_1, \\ldots, s_T) using the current stochastic policy \\pi.\nFor each state-action pair (s_t, a_t) in the episode, update the estimated Q-value:\n\n\n    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (R_t - Q(s_t, a_t))\n\n\nFor each state s_t in the episode, improve the policy (e.g. \\epsilon-greedy):\n\n\n    \\pi(s_t, a) = \\begin{cases}\n                    1 - \\epsilon \\; \\text{if} \\; a = \\text{argmax}\\, Q(s, a) \\\\\n                    \\frac{\\epsilon}{|\\mathcal{A(s_t)}-1|} \\; \\text{otherwise.} \\\\\n                    \\end{cases}",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo methods</span>"
    ]
  },
  {
    "objectID": "src/1.4-MC.html#off-policy-monte-carlo-methods",
    "href": "src/1.4-MC.html#off-policy-monte-carlo-methods",
    "title": "Monte Carlo methods",
    "section": "Off-policy Monte Carlo methods",
    "text": "Off-policy Monte Carlo methods\nAnother option to ensure exploration is to generate the sample episodes using a behavior policy b(s, a) different from the learned policy \\pi(s, a) of the agent. The behavior policy b(s, a) used to generate the episodes is only required to select at least occasionally the same actions as the learned policy \\pi(s, a) (coverage assumption).\n \\pi(s,a) &gt; 0 \\Rightarrow b(s,a) &gt; 0\nThere are mostly two choices regarding the behavior policy:\n\nAn \\epsilon-soft behavior policy over the Q-values as in on-policy MC is often enough, while a deterministic (greedy) policy can be learned implictly.\nThe behavior policy could also come from expert knowledge, i.e. known episodes from the MDP generated by somebody else (human demonstrator, classical algorithm).\n\nBut are we mathematically allowed to do this? We search for the optimal policy that maximizes in expectation the return of each trajectory (episode) possible under the learned policy \\pi:\n\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_\\pi} [R(\\tau)]\n\\rho_\\pi denotes the probability distribution of trajectories achievable using the policy \\pi. If we generate the trajectories from the behavior policy b(s, a), we end up maximizing something else:\n\\mathcal{J}'(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_b} [R(\\tau)]\nThe policy that maximizes \\mathcal{J}'(\\pi) is not the optimal policy of the MDP.\n\nIf you try to estimate a parameter of a random distribution \\pi using samples of another distribution b, the sample average will have a strong bias. We need to correct the samples from b in order to be able to estimate the parameters of \\pi correctly, through importance sampling (IS).\n\nImportance sampling\nWe want to estimate the expected return of the trajectories generated by the policy \\pi:\n\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_\\pi} [R(\\tau)]\nWe start by using the definition of the mathematical expectation:\n\\mathcal{J}(\\pi) = \\int_\\tau \\rho_\\pi(\\tau) \\, R(\\tau) \\, d\\tau\nThe expectation is the integral over all possible trajectories of their return R(\\tau), weighted by the likelihood \\rho_\\pi(\\tau) that a trajectory \\tau is generated by the policy \\pi.\n\n\n\nOnly certain trajectories are likely under a given policy.\n\n\nThe trick is to introduce the behavior policy b in what we want to estimate:\n\\mathcal{J}(\\pi) = \\int_\\tau \\frac{\\rho_b(\\tau)}{\\rho_b(\\tau)} \\, \\rho_\\pi(\\tau) \\, R(\\tau) \\, d\\tau\n\\rho_b(\\tau) is the likelihood that a trajectory \\tau is generated by the behavior policy b. We shuffle a bit the terms:\n\\mathcal{J}(\\pi) = \\int_\\tau \\rho_b(\\tau) \\, \\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} \\,  R(\\tau) \\, d\\tau\nand notice that it has the form of an expectation over trajectories generated by b:\n\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_b} [\\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} \\, R(\\tau)]\nThis means that we can sample trajectories from b, but we need to correct the observed return by the importance sampling weight \\dfrac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)}.\nThe importance sampling weight corrects the mismatch between \\pi and b.\n\nIf the two distributions are the same (on-policy), the IS weight is 1, no need to correct the return.\n\nIf a sample is likely under b but not under \\pi, we should not care about its return: \\dfrac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} &lt;&lt; 1\nIf a sample is likely under \\pi but not much under b, we increase its importance in estimating the return: \\dfrac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} &gt;&gt; 1\n\nThe sampling average of the corrected samples will be closer from the true estimate (unbiased).\nHow do we compute these probability distributions \\rho_\\pi(\\tau) and \\rho_b(\\tau) for a trajectory \\tau? A trajectory \\tau is a sequence of state-action transitions (s_0, a_0, s_1, a_1, \\ldots, s_T) whose probability depends on:\n\nthe probability of choosing an action a_t in state s_t: the policy \\pi(s, a).\nthe probability of arriving in the state s_{t+1} from the state s_t with the action a_t: the transition probability p(s_{t+1} | s_t, a_t).\n\n\nThe likelihood of a trajectory \\tau = (s_0, a_0, s_1, a_1, \\ldots, s_T) under a policy \\pi depends on the policy and the transition probabilities (Markov property):\n\n    \\rho_\\pi(\\tau) = p_\\pi(s_0, a_0, s_1, a_1, \\ldots, s_T) = p(s_0) \\, \\prod_{t=0}^{T-1} \\pi_\\theta(s_t, a_t) \\, p(s_{t+1} | s_t, a_t)\n\np(s_0) is the probability of starting an episode in s_0, we do not have control over it.\nWhat is interesting is that the transition probabilities disappear when calculating the importance sampling weight:\n\n    \\rho_{0:T-1} = \\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} = \\frac{p_0 (s_0) \\, \\prod_{t=0}^{T-1} \\pi(s_t, a_t) p(s_{t+1} | s_t, a_t)}{p_0 (s_0) \\, \\prod_{t=0}^T b(s_t, a_t) p(s_{t+1} | s_t, a_t)} = \\frac{\\prod_{t=0}^{T-1} \\pi(s_t, a_t)}{\\prod_{t=0}^T b(s_t, a_t)} = \\prod_{t=0}^{T-1} \\frac{\\pi(s_t, a_t)}{b(s_t, a_t)}\n\nThe importance sampling weight is simply the product over the length of the episode of the ratio between \\pi(s_t, a_t) and b(s_t, a_t).\n\n\nOff-policy Monte Carlo control\nIn off-policy MC control, we generate episodes using the behavior policy b and update greedily the learned policy \\pi. For the state s_t, the obtained returns just need to be weighted by the relative probability of occurrence of the rest of the episode following the policies \\pi and b:\n\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)}\nV^\\pi(s_t) = \\mathbb{E}_{\\tau \\sim \\rho_b} [\\rho_{t:T-1} \\, R_t]\nThis gives us the updates:\n\n    V(s_t) = V(s_t) + \\alpha  \\, \\rho_{t:T-1} \\, (R_t - V(s_t))\n\nand:\n\n    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha  \\, \\rho_{t:T-1} \\, (R_t - Q(s_t, a_t))\n\nUnlikely episodes under \\pi are barely used for learning, likely ones are used a lot.\n\n\n\n\n\n\nOff-policy Monte Carlo control\n\n\n\nwhile True:\n\nGenerate an episode \\tau = (s_0, a_0, r_1, \\ldots, s_T) using the behavior policy b.\nFor each state-action pair (s_t, a_t) in the episode, update the estimated Q-value:\n\n\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)}\n\n    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha  \\, \\rho_{t:T-1} \\, (R_t - Q(s_t, a_t))\n\n\nFor each state s_t in the episode, update the learned deterministic policy (greedy):\n\n\n    \\pi(s_t, a) = \\begin{cases}\n                    1\\; \\text{if} \\; a = \\text{argmax} \\, Q(s_t, a) \\\\\n                    0 \\; \\text{otherwise.} \\\\\n                    \\end{cases}\n\n\n\nProblem 1: if the learned policy is greedy, the IS weight becomes quickly 0 for a non-greedy action a_t:\n\\pi(s_t, a_t) = 0 \\rightarrow \\rho_{0:T-1} = \\prod_{k=0}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)} = 0\nOff-policy MC control only learns from the last greedy actions, what is slow at the beginning.\nSolution: \\pi and b should not be very different. Usually \\pi is greedy and b is a softmax (or \\epsilon-greedy) over it.\nProblem 2: if the learned policy is stochastic, the IS weights can quickly vanish to 0 or explode to infinity:\n\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)}\nIf \\dfrac{\\pi(s_k, a_k)}{b(s_k, a_k)} is smaller than 1, the products go to 0. If it is bigger than 1, it grows to infinity.\nSolution: one can normalize the IS weight between different episodes (see Sutton and Barto) or clip it (e.g. restrict it to [0.9, 1.1], see PPO later in this course).\n\n\nAdvantages of off-policy methods\nThe main advantage of off-policy strategies is that you can learn from other’s actions, you don’t have to rely on your initially wrong policies to discover the solution by chance. Example: learning to play chess by studying thousands/millions of plays by chess masters. In a given state, only a subset of the possible actions are actually executed by experts: the others may be too obviously wrong. The exploration is then guided by this expert knowledge, not randomly among all possible actions.\nOff-policy methods greatly reduce the number of transitions needed to learn a policy: very stupid actions are not even considered, but the estimation policy learns an optimal strategy from the “classical” moves. Drawback: if a good move is not explored by the behavior policy, the learned policy will never try it.",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo methods</span>"
    ]
  },
  {
    "objectID": "src/1.5-TD.html",
    "href": "src/1.5-TD.html",
    "title": "Temporal Difference learning",
    "section": "",
    "text": "Temporal difference\nThe main drawback of Monte Carlo methods is that the task must be composed of finite episodes. Not only is it not always possible, but value updates have to wait for the end of the episode, what slows learning down.\nTemporal difference methods simply replace the actual return obtained after a state or an action, by an estimation composed of the reward immediately received plus the value of the next state or action:\nR_t \\approx r(s, a, s') + \\gamma \\, V^\\pi(s')\nAs seen in Section Dynamic programming, this comes from the simple relationship R_t = r_{t+1}  + \\gamma \\, R_{t+1}.\nThis gives us the following update rule for the value of a state:\nV(s) \\leftarrow V(s) + \\alpha (r(s, a, s') + \\gamma \\, V(s') - V(s))\nThe quantity:\n\\delta = r(s, a, s') + \\gamma \\, V(s') - V(s)\nis called the reward-prediction error (RPE), TD error, or 1-step advantage: it defines the surprise between the current expected return (V(s)) and its sampled target value, estimated as the immediate reward plus the expected return in the next state.\nThe main advantage of this learning method is that the update of the V-value can be applied immediately after a transition: no need to wait until the end of an episode, or even to have episodes at all: this is called online learning and allows very fast learning from single transitions. The main drawback is that the updates depend on other estimates, which are initially wrong: it will take a while before all estimates are correct.\nA similar TD update rule can be defined for the Q-values:\nQ(s, a) \\leftarrow Q(s, a) + \\alpha (r(s, a, s') + \\gamma \\, Q(s', a') - Q(s, a))\nWhen learning Q-values directly, the question is which next action a' should be used in the update rule: the action that will actually be taken for the next transition (defined by \\pi(s', a')), or the greedy action (a^* = \\text{argmax}_a Q(s', a)).\nThis relates to the on-policy / off-policy distinction already seen for MC methods:\nQ(s, a) \\leftarrow Q(s, a) + \\alpha (r(s, a, s') + \\gamma \\, Q(s', \\pi(s')) - Q(s, a))\nQ(s, a) \\leftarrow Q(s, a) + \\alpha (r(s, a, s') + \\gamma \\, \\max_{a'} Q(s', a') - Q(s, a))\nIn Q-learning, the behavior policy has to ensure exploration, while this is achieved implicitly by the learned policy in SARSA, as it must be \\epsilon-soft. An easy way of building a behavior policy based on a deterministic learned policy is \\epsilon-greedy: the deterministic action \\mu(s_t) is chosen with probability 1 - \\epsilon, the other actions with probability \\epsilon. In continuous action spaces, additive noise (e.g. Ohrstein-Uhlenbeck) can be added to the action.\nAlternatively, domain knowledge can be used to create the behavior policy and restrict the search to meaningful actions: compilation of expert moves in games, approximate solutions, etc. Again, the risk is that the behavior policy never explores the actually optimal actions. See Section Off-policy Actor-Critic for more details on the difference between on-policy and off-policy methods.\nNote that, despite being off-policy, Q-learning does not necessitate importance sampling, as the update rule does not depend on the behavior policy:\nQ^\\pi(s, a) = \\mathbb{E}_{s_t \\sim \\rho_b, a_t \\sim b}[ r_{t+1} + \\gamma \\, \\max_a Q^\\pi(s_{t+1}, a) | s_t = s, a_t=a]\nbut:\nQ^\\pi(s, a) \\leftarrow Q^\\pi(s, a) + \\alpha (r(s, a, s') + \\gamma \\, \\max_{a'} Q^\\pi(s', a') - Q^\\pi(s, a))\nAs we only sample transitions using b and not episodes, there is no need to correct the returns. The returns use estimates Q^\\pi, which depend on \\pi and not b. The immediate reward r_{t+1} is stochastic, but is the same whether you sample a_t from \\pi or from b.",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Difference learning</span>"
    ]
  },
  {
    "objectID": "src/1.5-TD.html#temporal-difference",
    "href": "src/1.5-TD.html#temporal-difference",
    "title": "Temporal Difference learning",
    "section": "",
    "text": "If \\delta &gt; 0, the transition was positively surprising: one obtains more reward or lands in a better state than expected. The initial state or action was actually underrated, so its estimated value must be increased.\nIf \\delta &lt; 0, the transition was negatively surprising. The initial state or action was overrated, its value must be decreased.\nIf \\delta = 0, the transition was fully predicted: one obtains as much reward as expected, so the values should stay as they are.\n\n\n\n\n\n\n\n\nFigure 5.1: Temporal difference algorithms update values after a single transition. Source: Sutton and Barto (1998).\n\n\n\n\n\n\n\n\n\nTD(0) policy evaluation\n\n\n\nwhile True:\n\nStart from an initial state s_0.\nforeach step t of the episode:\n\nSelect a_t using the current policy \\pi in state s_t.\nApply a_t, observe r_{t+1} and s_{t+1}.\nCompute the TD error:\n\n\\delta_t = r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t)\n\nUpdate the state-value function of s_t:\n\n\n      V(s_t) = V(s_t) + \\alpha \\, \\delta_t\n  \n\nif s_{t+1} is terminal: break\n\n\n\n\n\n\n\n\n\n\nBias-variance trade-off of TD\n\n\n\nBy using an estimate of the return R_t instead of directly the return as in MC,\n\nwe increase the bias (estimates are always wrong, especially at the beginning of learning)\nbut we reduce the variance: only r(s, a, s') is stochastic, not the value function V^\\pi.\n\nWe can therefore expect less optimal solutions, but we will also need less samples. TD has a better sample efficiency than MC, but a worse convergence (suboptimal).\n\n\n\n\n\n\n\nOn-policy TD learning is called SARSA (state-action-reward-state-action). It uses the next action sampled from the policy \\pi(s', a') to update the current transition. This selected action could be noted \\pi(s') for simplicity. It is required that this next action will actually be performed for the next transition. The policy must be \\epsilon-soft, for example \\epsilon-greedy or softmax:\n\n\n\n\n\n\n\n\nSARSA\n\n\n\nwhile True:\n\nStart from an initial state s_0 and select a_0 using the current policy \\pi.\nforeach step t of the episode:\n\nApply a_{t}, observe r_{t+1} and s_{t+1}.\nSelect a_{t+1} using the current stochastic policy \\pi.\nUpdate the action-value function of (s_t, a_t):\n\n Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (r_{t+1} + \\gamma \\, Q(s_{t+1}, a_{t+1})  - Q(s_t, a_t)) \n\nImprove the stochastic policy, e.g:\n\n\n      \\pi(s_t, a) = \\begin{cases}\n                      1 - \\epsilon \\; \\text{if} \\; a = \\text{argmax} \\, Q(s_t, a) \\\\\n                      \\frac{\\epsilon}{|\\mathcal{A}(s_t) -1|} \\; \\text{otherwise.} \\\\\n                      \\end{cases}\n  \n\nif s_{t+1} is terminal: break\n\n\n\n\n\nOff-policy TD learning is called Q-learning (Watkins, 1989). The greedy action in the next state (the one with the highest Q-value) is used to update the current transition. It does not mean that the greedy action will actually have to be selected for the next transition. The learned policy can therefore also be deterministic:\n\n\n\n\n\n\n\n\nNote\n\n\n\nwhile True:\n\nStart from an initial state s_0.\nforeach step t of the episode:\n\nSelect a_{t} using the behavior policy b (e.g. derived from \\pi).\nApply a_t, observe r_{t+1} and s_{t+1}.\nUpdate the action-value function of (s_t, a_t):\n\nQ(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (r_{t+1} + \\gamma \\, \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\n\nImprove greedily the learned policy:\n\n\\pi(s_t, a) = \\begin{cases}\n                  1\\; \\text{if} \\; a = \\text{argmax} \\, Q(s_t, a) \\\\\n                  0 \\; \\text{otherwise.} \\\\\n                  \\end{cases}\n  \n\nif s_{t+1} is terminal: break",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Difference learning</span>"
    ]
  },
  {
    "objectID": "src/1.5-TD.html#actor-critic-methods",
    "href": "src/1.5-TD.html#actor-critic-methods",
    "title": "Temporal Difference learning",
    "section": "Actor-critic methods",
    "text": "Actor-critic methods\nThe TD error after each transition (s_t, a_t, r_{t+1}, s_{t+1}):\n \\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)\ntells us how good the action a_t was compared to our expectation V(s_t).\nWhen the advantage \\delta_t &gt; 0, this means that the action lead to a better reward or a better state than what was expected by V(s_t), which is a good surprise, so the action should be reinforced (selected again) and the value of that state increased. When \\delta_t &lt; 0, this means that the previous estimation of (s_t, a_t) was too high (bad surprise), so the action should be avoided in the future and the value of the state reduced.\n\n\n\n\n\n\nFigure 5.2: Actor-critic architecture. Source: Sutton and Barto (1998).\n\n\n\nActor-critic methods are TD methods that have a separate memory structure to explicitly represent the policy and the value function. The policy \\pi is implemented by the actor, because it is used to select actions. The estimated values V(s) are implemented by the critic, because it criticizes the actions made by the actor.\nThe critic computes the TD error or 1-step advantage:\n\\delta_t = r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t)\nThis scalar signal is the output of the critic and drives learning in both the actor and the critic. The critic is updated using this scalar signal:\n\n    V(s_t) \\leftarrow V(s_t) + \\alpha \\, \\delta_t\n\nThe actor is updated according to this TD error signal. For example a softmax actor over preferences:\n\n\\begin{cases}\np(s_t, a_t) \\leftarrow p(s_t, a_t) + \\beta \\, \\delta_t \\\\\n\\\\\n\\pi(s, a) = \\frac{\\exp{p(s, a)}}{\\sum_b \\exp{p(s, b)}} \\\\\n\\end{cases}\n\nWhen \\delta_t &gt;0, the preference is increased, so the probability of selecting it again increases. When \\delta_t &lt;0, the preference is decreased, so the probability of selecting it again decreases.\n\n\n\n\n\n\nActor-critic algorithm with preferences\n\n\n\n\nStart in s_0. Initialize the preferences p(s,a) for each state action pair and the critic V(s) for each state.\nforeach step t:\n\nSelect a_t using the actor \\pi in state s_t:\n\n\\pi(s_t, a) = \\frac{\\exp{p(s, a)}}{\\sum_b \\exp{p(s, b)}}\n\nApply a_t, observe r_{t+1} and s_{t+1}.\nCompute the TD error in s_t using the critic:\n\n\n      \\delta_t = r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t)\n  \n\nUpdate the actor:\n\n\n      p(s_t, a_t) \\leftarrow p(s_t, a_t) + \\beta \\, \\delta_t\n  \n\nUpdate the critic:\n\n\n      V(s_t) \\leftarrow V(s_t) + \\alpha \\, \\delta_t\n  \n\n\n\nThe advantage of the separation between the actor and the critic is that now the actor can take any form (preferences, linear approximation, deep networks). It requires minimal computation in order to select the actions, in particular when the action space is huge or even continuous. It can learn stochastic policies, which is particularly useful in non-Markov problems.\nHowever, it is obligatory to learn on-policy: the critic must evaluate the actions taken by the current actor, and the actor must learn from the current critic, not “old” V-values.\nClassical TD learning only learn a value function (V^\\pi(s) or Q^\\pi(s, a)): these methods are called value-based methods. Actor-critic architectures are particularly important in policy search methods.",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Difference learning</span>"
    ]
  },
  {
    "objectID": "src/1.5-TD.html#advantage-estimation",
    "href": "src/1.5-TD.html#advantage-estimation",
    "title": "Temporal Difference learning",
    "section": "Advantage estimation",
    "text": "Advantage estimation\n\nn-step advantages\n\n\n\n\n\n\nFigure 5.3: Bias-variance trade-off. Source: https://www.machinelearningplus.com/machine-learning/bias-variance-tradeoff/\n\n\n\nMC methods have high variance, low bias: Return estimates are correct on average, as we use real rewards from the environment, but each of them individually is wrong, because of the stochasticity of the policy/environment.\n\n    R_t^\\text{MC} = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}\n\nThe small bias ensures good convergence properties, as we are more likely to find the optimal policy with correct estimates. The high variance means that we will need many samples to converge. The updates are not very sensitive to initial estimates.\nOn the other hand, TD has low variance, high bias, as the target returns contain mostly estimates.\n\n    R_t^\\text{TD} = r_{t+1} + \\gamma \\, V^\\pi(s_{t+1})\n\nThe only stochasticity comes from the immediate rewards, which is low, so the targets will not vary much during learning. But because they use other estimates, which are initially wrong, they will always be off. These wrong updates can, more often than not, lead to suboptimal policies. However, convergence will be much faster than with MC methods.\nIn order to control the bias-variance trade-off, we would like an estimator for the return with intermediate properties between MC and TD. This is what the n-step return offers:\n\n    R^n_t = \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\,  V(s_{t+n})\n\nThe n-step return uses the next n real rewards, and completes the rest of the sequence with the value of the state reached n steps in the future. Because it uses more real rewards than TD, its bias is smaller, while its variance is lower than MC.\n\n\n\n\n\n\nFigure 5.4: n-step returns define a trade-off between TD and MC. Source: Sutton and Barto (1998)\n\n\n\nThe n-step advantage at time t is defined as the difference between the n-step return and the current estimate:\n\nA^n_t = \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\,  V(s_{t+n}) - V (s_t)\n\nIt is easy to check that the TD error is the 1-step advantage:\n\n    \\delta_t = A^1_t = r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t)\n\nn-step advantages are going to play an important role in deep RL, as the right choice of n will allow us to control the bias-variance trade-off: smaller values of n decrease the variance (smaller sample complexity) but may lead to suboptimal policies, while higher values of n converge to better policies, at the cost of necessitating more samples.\n\n\nEligibility traces\nThe main drawback of TD learning is that learning can be slow, especially when the problem provides sparse rewards (as opposed to dense rewards). For example in a game like chess, a reward is given only at the end of a game (+1 for winning, -1 for losing). All other actions receive a reward of 0, although they are as important as the last one in order to win.\n\n\n\n\n\n\nFigure 5.5: Principle of eligibility traces applied to the Gridworld problem using SARSA(\\lambda). Source: Sutton and Barto (1998).\n\n\n\nImagine you initialize all Q-values to 0 and apply Q-learning to the Gridworld problem of Figure 5.5. During the first episode, all actions but the last one will receive a reward of 0 and arrive in a state where the greedy action has a value Q^\\pi(s', a') of 0 (initially), so the TD error \\delta is 0 and their Q-value will not change. Only the very last action will receive a non-zero reward and update its value slightly (because of the learning rate \\alpha).\nWhen this episode is performed again, the last action will again be updated, but also the one just before: Q^\\pi(s', a') is now different from 0 for this action, so the TD error is now different from 0. It is straightforward to see that if the episode has a length of 100 moves, the agent will need at least 100 episodes to “backpropagate” the final sparse reward to the first action of the episode. In practice, this is even worse: the learning rate \\alpha and the discount rate \\gamma will slow learning down even more. MC methods suffer less from this problem, as the first action of the episode would be updated using the actual return, which contains the final reward (although it is discounted by \\gamma).\nEligibility traces can be seen a trick to mix the advantages of MC (faster updates) with the ones of TD (online learning, smaller variance). The idea is that the TD error at time t (\\delta_t) will be used not only to update the action taken at time t (\\Delta Q(s_t, a_t) = \\alpha \\, \\delta_t), but also all the preceding actions, which are also responsible for the success or failure of the action taken at time t.\n\n\n\n\n\n\nFigure 5.6: The decaying factor \\lambda controls how much future TD errors influence learning at the current time step. Source: Sutton and Barto (1998).\n\n\n\nA parameter \\lambda between 0 and 1 (decaying factor) controls how far back in time a single TD error influences past actions. This is important when the policy is mostly exploratory: initial actions may be mostly random and finally find the the reward by chance. They should learn less from the reward than the last one, otherwise they would be systematically reproduced. There are many possible implementations of eligibility traces (Watkin’s, Peng, Tree Backup, etc. See the Chapter 12 of Sutton and Barto (2017)). Generally, one distinguished a forward and a backward view of eligibility traces.\n\nThe forward view considers that one transition (s_t, a_t) gathers the TD errors made at future time steps t' and discounts them with the parameter \\lambda:\n\n\n    R_t^\\lambda =  \\sum_{k=0}^T (\\gamma \\lambda)^{k} \\delta_{t+k}\n\nFrom this equation, \\gamma and \\lambda seem to play a relatively similar role, but remember that \\gamma is also used inside the TD error, so they control different aspects of learning. The drawback of this approach is that the future transitions and their respective TD errors must be known when updating the transition, so this prevents online learning (the episode must be terminated to apply the updates, like in MC).\n\n\n\n\n\n\nFigure 5.7: Forward view of the eligibility trace. Source: Sutton and Barto (1998).\n\n\n\n\nThe backward view considers that the TD error made at time t is sent backwards in time to all transitions previously executed. The easiest way to implement this is to update an eligibility trace e(s,a) for each possible transition, which is incremented every time a transition is visited and otherwise decays exponentially with a speed controlled by \\lambda:\n\n\n    e(s, a) = \\begin{cases} e(s, a) + 1 \\quad \\text{if} \\quad s=s_t \\quad \\text{and} \\quad a=a_t \\\\\n                            \\lambda \\, e(s, a) \\quad \\text{otherwise.}\n              \\end{cases}\n\nThe Q-value of all transitions (s, a) (not only the one just executed) is then updated proportionally to the corresponding trace and the current TD error:\n\n    Q(s, a) \\leftarrow  Q(s, a) + \\alpha \\, e(s, a) \\, \\delta_{t} \\quad \\forall s, a\n\n\n\n\n\n\n\nFigure 5.8: Backward view of the eligibility trace. Source: Sutton and Barto (1998).\n\n\n\nThe forward and backward implementations are equivalent: the first requires to know the future, the second requires to update many transitions at each time step. The best solution will depend on the complexity of the problem.\nTD learning, SARSA and Q-learning can all be efficiently extended using eligibility traces. This gives the algorithms TD(\\lambda), SARSA(\\lambda) and Q(\\lambda), which can learn much faster than their 1-step equivalent, at the cost of more computations.\n\n\n\n\n\n\nTD(\\lambda) algorithm: policy evaluation\n\n\n\n\nforeach step t of the episode:\n\nSelect a_t using the current policy \\pi in state s_t, observe r_{t+1} and s_{t+1}.\nCompute the TD error in s_t:\n\n\n      \\delta_t = r_{t+1} + \\gamma \\, V_k(s_{t+1}) - V_k(s_t)\n  \n\nIncrement the trace of s_t:\n\n\n      e_{t+1}(s_t) = e_t(s_t) + 1\n  \n\nforeach state s \\in [s_o, \\ldots, s_t] in the episode:\n\nUpdate the state value function:\n\n\n      V_{k+1}(s) = V_k(s) + \\alpha \\, \\delta_t \\, e_t(s)\n  \n\nDecay the eligibility trace:\n\n\n      e_{t+1}(s) = \\lambda \\, \\gamma \\, e_t(s)\n  \nif s_{t+1} is terminal: break\n\n\n\n\n\n\nGeneralized Advantage Estimation (GAE)\nLet’s recall the n-step advantage:\n\n    A^{n}_t = \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V^\\pi(s_{t+n+1}) - V^\\pi(s_t)\n\nIt is easy to show recursively that it depends on the TD error \\delta_t = r_{t+1} + \\gamma \\, V^\\pi(s_{t+1}) - V^\\pi(s_t) of the n next steps:\n\n    A^{n}_t = \\sum_{l=0}^{n-1} \\gamma^l \\, \\delta_{t+l}\n\n\n\n\n\n\n\nProof with n=2:\n\n\n\n\\begin{aligned}\nA^2_t &= r_{t+1} + \\gamma \\, r_{t+2} + \\gamma^2 \\, V(s_{t+2}) - V(s_{t}) \\\\\n&\\\\\n&= (r_{t+1} - V(s_t)) + \\gamma \\, (r_{t+2} + \\gamma \\, V(s_{t+2}) ) \\\\\n&\\\\\n&= (r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t)) + \\gamma \\, (r_{t+2} + \\gamma \\, V(s_{t+2}) - V(s_{t+1})) \\\\\n&\\\\\n&= \\delta_t + \\gamma \\, \\delta_{t+1}\n\\end{aligned}\n\n\n\nIn other words, the prediction error over n steps is the (discounted) sum of the prediction errors between two successive steps. Now, what is the optimal value of n? GAE decides not to choose and to simply average all n-step advantages and to weight them with a discount parameter \\lambda.\nThis defines the Generalized Advantage Estimator A^{\\text{GAE}(\\gamma, \\lambda)}_t:\n\n    A^{\\text{GAE}(\\gamma, \\lambda)}_t = (1-\\lambda) \\, \\sum_{l=0}^\\infty \\lambda^l A^l_t = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}\n\nThe GAE is simply a forward eligibility trace over distant n-step advantages: the 1-step advantage is more important the the 1000-step advantage (too much variance).\n\nWhen \\lambda=0, we have A^{\\text{GAE}(\\gamma, 0)}_t = A^{0}_t = \\delta_t, i.e. the TD advantage (high bias, low variance).\nWhen \\lambda=1, we have (at the limit) A^{\\text{GAE}(\\gamma, 1)}_t = R_t, i.e. the MC advantage (low bias, high variance).\n\nChoosing the right value of \\lambda between 0 and 1 allows to control the bias/variance trade-off.\n\\gamma and \\lambda play different roles in GAE: \\gamma determines the scale or horizon of the value functions: how much future rewards rewards are to be taken into account. The higher \\gamma &lt;1, the smaller the bias, but the higher the variance. Empirically, Schulman et al. (2015) found that small \\lambda values introduce less bias than \\gamma, so \\lambda can be chosen smaller than \\gamma (which is typically 0.99).\nIn practice, GAE leads to a better estimation than n-step advantages, but is more computationally expensive. It is used in particular in PPO (Section Proximal Policy Optimization (PPO)).\n\n\n\n\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust Region Policy Optimization. in Proceedings of the 31 st International Conference on Machine Learning, 1889–1897. Available at: http://proceedings.mlr.press/v37/schulman15.html.\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press.\n\n\nSutton, R. S., and Barto, A. G. (2017). Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press Available at: http://incompleteideas.net/book/the-book-2nd.html.\n\n\nWatkins, C. J. (1989). Learning from delayed rewards.",
    "crumbs": [
      "**Tabular RL**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Difference learning</span>"
    ]
  },
  {
    "objectID": "src/2.1-FunctionApproximation.html",
    "href": "src/2.1-FunctionApproximation.html",
    "title": "Function approximation",
    "section": "",
    "text": "Value-based function approximation\nAll the methods presented before are tabular methods, as one needs to store one value per state-action pair: either the Q-value of the action or a preference for that action.\nIn most useful applications, the number of values to store would quickly become prohibitive: when working on raw images, the number of possible states alone is untractable. Moreover, these algorithms require that each state-action pair is visited a sufficient number of times to converge towards the optimal policy: if a single state-action pair is never visited, there is no guarantee that the optimal policy will be found. The problem becomes even more obvious when considering continuous state or action spaces.\nHowever, in a lot of applications, the optimal action to perform in two very close states is likely to be the same: changing one pixel in a video game does not change which action should be applied. It would therefore be very useful to be able to interpolate Q-values between different states: only a subset of all state-action pairs has to explored; the others will be “guessed” depending on the proximity between the states and/or the actions. The problem is now generalization, i.e. transferring acquired knowledge to unseen but similar situations.\nThis is where function approximation (FA) becomes useful: the V/Q-values or the policy are not stored in a table, but rather learned by a function approximator. The type of function approximator does not really matter here: in deep RL we are of course interested in deep neural networks, but any kind of regressor theoretically works (linear algorithms, radial-basis function network, SVR…).",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Function approximation</span>"
    ]
  },
  {
    "objectID": "src/2.1-FunctionApproximation.html#value-based-function-approximation",
    "href": "src/2.1-FunctionApproximation.html#value-based-function-approximation",
    "title": "Function approximation",
    "section": "",
    "text": "State value approximators\nLet’s represent a state s by a vector of d features \\phi(s) = [\\phi_1(s), \\phi_2(s), \\ldots, \\phi_d(s)]^T. For the cartpole, the feature vector would be:\n \\phi(s) = \\begin{bmatrix}x \\\\ \\dot{x} \\\\ \\theta \\\\ \\dot{\\theta} \\end{bmatrix}\nx is the position, \\theta the angle, \\dot{x} and \\dot{\\theta} their derivatives. We are able to represent any state s of the Cartpole using these four variables. If the state can be represented by an image, we only need to put its pixels into a single vector. For more complex problems, the feature vector should include all the necessary information (Markov property).\nIn state value approximation, we want to approximate the state value function V^\\pi(s) with a parameterized function V_\\varphi(s):\nV_\\varphi(s) \\approx V^\\pi(s)\n\n\n\n\n\n\nFigure 6.3: Parameterized model to approximate state values.\n\n\n\nThe parameterized function can have any form. It has a set of parameters \\varphi used to transform the feature vector \\phi(s) into an approximated value V_\\varphi(s).\nThe simplest function approximator (FA) is the linear approximator.\n\n\n\n\n\n\nFigure 6.4: Linear approximator for state values.\n\n\n\nThe approximated value is a linear combination of the features:\nV_\\varphi(s) = \\sum_{i=1}^d w_i \\, \\phi_i(s) = \\mathbf{w}^T \\times \\phi(s)\nThe weight vector \\mathbf{w} = [w_1, w_2, \\ldots, w_d]^Tis the set of parameters \\varphi of the function.\nRegardless the form of the function approximator, we want to find the parameters \\varphi making the approximated values V_\\varphi(s) as close as possible from the true values V^\\pi(s) for all states s. This is a regression problem. We want to minimize the mean square error between the two quantities:\n \\min_\\varphi \\mathcal{L}(\\varphi) = \\mathbb{E}_{s \\in \\mathcal{S}} [ (V^\\pi(s) - V_\\varphi(s))^2]\nThe loss function \\mathcal{L}(\\varphi) is minimal when the predicted values are close to the true ones on average for all states. Let’s suppose that we know the true state values V^\\pi(s) for all states and that the parameterized function is differentiable. We can find the minimum of the loss function by applying gradient descent (GD) iteratively:\n\n    \\Delta \\varphi = - \\eta \\, \\nabla_\\varphi \\mathcal{L}(\\varphi)\n\n\\nabla_\\varphi \\mathcal{L}(\\varphi) is the gradient of the loss function w.r.t to the parameters \\varphi:\n\n    \\nabla_\\varphi \\mathcal{L}(\\varphi) = \\begin{bmatrix}\n        \\dfrac{\\partial \\mathcal{L}(\\varphi)}{\\partial \\varphi_1} \\\\\n        \\dfrac{\\partial \\mathcal{L}(\\varphi)}{\\partial \\varphi_2} \\\\\n        \\ldots \\\\\n        \\dfrac{\\partial \\mathcal{L}(\\varphi)}{\\partial \\varphi_K} \\\\\n    \\end{bmatrix}\n\nWhen applied repeatedly, GD converges to a local minimum of the loss function. To minimize the mean square error, we just need to compute its gradient with respect to the parametsr \\varphi:\n\n\\begin{aligned}\n    \\mathcal{L}(\\varphi) &= \\nabla_\\varphi \\mathbb{E}_{s \\in \\mathcal{S}} [ (V^\\pi(s) - V_\\varphi(s))^2] \\\\\n    &\\\\\n    & = \\mathbb{E}_{s \\in \\mathcal{S}} [\\nabla_\\varphi  (V^\\pi(s) - V_\\varphi(s))^2] \\\\\n    &\\\\\n    & = \\mathbb{E}_{s \\in \\mathcal{S}} [ - (V^\\pi(s) - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)] \\\\\n\\end{aligned}\n\nAs it would be too slow to compute the expectation on the whole state space (batch algorithm), we will update the parameters with stochastic gradient descent (SGD):\n\n    \\Delta \\varphi = \\eta \\,  \\frac{1}{K} \\sum_{k=1}^K (V^\\pi(s_k) - V_\\varphi(s_k)) \\, \\nabla_\\varphi V_\\varphi(s_k)\n\nwhere K is the batch size. We can also sample a single state s (online algorithm):\n\n    \\Delta \\varphi = \\eta \\, (V^\\pi(s) - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)\n\nUnless stated otherwise, we will sample single states in this section, but the parameter updates will be noisy (high variance).\nThe obtained rule is the delta learning rule of linear regression and classification, with \\phi(s) being the input vector and V^\\pi(s) - V_\\varphi(s) the prediction error. The rule can be used with any function approximator, we only need to be able to differentiate it to get \\nabla_\\varphi V_\\varphi(s). The problem is that we do not know V^\\pi(s), as it is what we are trying to estimate. We can replace V^\\pi(s) by a sampled estimate using Monte Carlo or TD:\n\nMonte Carlo function approximation:\n\n\n    \\Delta \\varphi = \\eta \\, (R_t - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)\n\n\nTemporal Difference function approximation:\n\n\n    \\Delta \\varphi = \\eta \\, (r_{t+1} + \\gamma \\, V_\\varphi(s') - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)\n\nNote that for Temporal Difference, we actually want to minimize the TD reward-prediction error for all states, i.e. the surprise:\n\\mathcal{L}(\\varphi) = \\mathbb{E}_{s \\in \\mathcal{S}} [ (r_{t+1} + \\gamma \\, V_\\varphi(s') - V_\\varphi(s))^2]= \\mathbb{E}_{s \\in \\mathcal{S}} [ \\delta_t^2]\n\n\n\n\n\n\nGradient Monte Carlo Algorithm for value estimation\n\n\n\n\nAlgorithm:\n\nInitialize the parameter \\varphi to 0 or randomly.\nwhile not converged:\n\nGenerate an episode according to the current policy \\pi until a terminal state s_T is reached.\n\n\n      \\tau = (s_o, a_o, r_ 1, s_1, a_1, \\ldots, s_T)\n  \n\nFor all encountered states s_0, s_1, \\ldots, s_{T-1}:\n\nCompute the return R_t = \\sum_k \\gamma^k r_{t+k+1} .\nUpdate the parameters using function approximation:\n\n\n     \\Delta \\varphi = \\eta \\, (R_t - V_\\varphi(s_t)) \\, \\nabla_\\varphi V_\\varphi(s_t)\n\n\n\n\n\n\nGradient Monte Carlo has no bias (real returns) but a high variance.\n\n\n\n\n\n\nSemi-gradient Temporal Difference Algorithm for value estimation\n\n\n\n\nAlgorithm:\n\nInitialize the parameter \\varphi to 0 or randomly.\nwhile not converged:\n\nStart from an initial state s_0.\nforeach step t of the episode:\n\nSelect a_t using the current policy \\pi in state s_t.\nObserve r_{t+1} and s_{t+1}.\nUpdate the parameters using function approximation:\n\n\n      \\Delta \\varphi = \\eta \\, (r_{t+1} + \\gamma \\, V_\\varphi(s_{t+1}) - V_\\varphi(s_t)) \\, \\nabla_\\varphi V_\\varphi(s_t)\n  \n\nif s_{t+1} is terminal: break\n\n\n\n\n\n\nSemi-gradient TD has less variance, but a significant bias as V_\\varphi(s_{t+1}) is initially wrong. You can never trust these estimates completely.\n\n\nAction value approximators\nQ-values can be approximated by a parameterized function Q_\\theta(s, a) in the same manner. There are basically two options for the structure of the function approximator:\n\nThe FA takes a feature vector for both the state s and the action a (which can be continuous) as inputs, and outputs a single Q-value Q_\\theta(s ,a).\n\n\n\n\n\n\n\nFigure 6.5: Single Q-value approximation.\n\n\n\n\nThe FA takes a feature vector for the state s as input, and outputs one Q-value Q_\\theta(s ,a) per possible action (the action space must be discrete).\n\n\n\n\n\n\n\nFigure 6.6: Multiple Q-value approximation\n\n\n\nIn both cases, we minimize the mse between the true value Q^\\pi(s, a) and the approximated value Q_\\theta(s, a). The target can be approximated with SARSA or Q-learning:\n\n\n\n\n\n\nQ-learning with function approximation\n\n\n\n\nInitialize the parameters \\theta.\nwhile True:\n\nStart from an initial state s_0.\nforeach step t of the episode:\n\nSelect a_{t} using the behavior policy b (e.g. derived from \\pi).\nTake a_t, observe r_{t+1} and s_{t+1}.\nUpdate the parameters \\theta:\n\n\\Delta \\theta = \\eta \\, (r_{t+1} + \\gamma \\, \\max_a Q_\\theta(s_{t+1}, a) - Q_\\theta(s_t, a_t)) \\, \\nabla_\\theta Q_\\theta(s_t, a_t)\n\nImprove greedily the learned policy:\n\n\\pi(s_t, a) = \\text{Greedy}(Q_\\theta(s_t, a))\n\nif s_{t+1} is terminal: break",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Function approximation</span>"
    ]
  },
  {
    "objectID": "src/2.1-FunctionApproximation.html#policy-based-function-approximation",
    "href": "src/2.1-FunctionApproximation.html#policy-based-function-approximation",
    "title": "Function approximation",
    "section": "Policy-based function approximation",
    "text": "Policy-based function approximation\nIn policy-based function approximation, we want to directly learn a policy \\pi_\\theta(s, a) that maximizes the expected return of each possible transition, i.e. the ones which are selected by the policy. The objective function to be maximized is defined over all trajectories \\tau = (s_0, a_0, s_1, a_1, \\ldots, s_T, a_T) conditioned by the policy:\n\n    \\mathcal{J}(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} [R (\\tau)]\n\nIn short, the learned policy \\pi_\\theta should only produce trajectories \\tau where each state is associated to a high return R(\\tau) and avoid trajectories with low returns. Although this objective function leads to the desired behavior, it is not computationally tractable as we would need to integrate over all possible trajectories. The methods presented in Section Policy Gradient methods will provide estimates of the gradient of this objective function.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Function approximation</span>"
    ]
  },
  {
    "objectID": "src/2.2-DeepNetworks.html",
    "href": "src/2.2-DeepNetworks.html",
    "title": "Deep learning",
    "section": "",
    "text": "Feedforward neural networks\nDeep RL uses deep neural networks as function approximators, allowing complex representations of the value of state-action pairs to be learned. This section provides a very quick overview of deep learning. For additional details, refer to the excellent book of Goodfellow et al. (2016).\nA deep neural network (DNN) or multi-layer perceptron (MLP) consists of one input layer \\mathbf{x}, one or several hidden layers \\mathbf{h_1}, \\mathbf{h_2}, \\ldots, \\mathbf{h_n} and one output layer \\mathbf{y}.\nEach layer k (called fully-connected FC layer) transforms the activity of the previous layer (the vector \\mathbf{h_{k-1}}) into another vector \\mathbf{h_{k}} by multiplying it with a weight matrix W_k, adding a bias vector \\mathbf{b_k} and applying a non-linear activation function f.\n\\mathbf{h_{k}} = f(W_k \\times \\mathbf{h_{k-1}} + \\mathbf{b_k})\n\\tag{7.1}\nThe activation function can theoretically be of any type as long as it is non-linear (sigmoid, tanh…), but modern neural networks use preferentially the Rectified Linear Unit (ReLU) function f(x) = \\max(0, x) or its parameterized variants.\nThe goal of learning is to find the weights and biases \\theta minimizing a given loss function on a training set \\mathcal{D}.\n\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}} [||\\mathbf{t} - \\mathbf{y}||^2]\nwhere \\mathbf{x} is the input, \\mathbf{t} the true output (defined in the training set) and \\mathbf{y} the prediction of the NN for the input \\mathbf{x}. The closer the prediction from the true value, the smaller the mse.\n\\mathcal{L}(\\theta) = - \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}} [\\sum_i t_i \\log y_i]\nwhere the log-likelihood of the prediction \\mathbf{y} to match the data \\mathbf{t} is maximized over the training set. The mse could be used for classification problems too, but the output layer usually has a softmax activation function for classification problems, which works nicely with the cross entropy loss function. See https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss for the link between cross entropy and log-likelihood and https://deepnotes.io/softmax-crossentropy for the interplay between softmax and cross entropy.\nOnce the loss function is defined, it has to be minimized by searching optimal values for the free parameters \\theta. This optimization procedure is based on gradient descent, which is an iterative procedure modifying estimates of the free parameters in the opposite direction of the gradient of the loss function:\n\\Delta \\theta = -\\eta \\, \\nabla_\\theta \\mathcal{L}(\\theta) = -\\eta \\, \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta}\nThe learning rate \\eta is chosen very small to ensure a smooth convergence. Intuitively, the gradient (or partial derivative) represents how the loss function changes when each parameter is slightly increased. If the gradient w.r.t a single parameter (e.g. a weight w) is positive, increasing the weight increases the loss function (i.e. the error), so the weight should be slightly decreased instead. If the gradient is negative, one should increase the weight.\nThe question is now to compute the gradient of the loss function w.r.t all the parameters of the DNN, i.e. each single weight and bias. The solution is given by the backpropagation algorithm, which is simply an application of the chain rule to feedforward neural networks:\n\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial W_k} = \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{y}} \\times \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{h_n}} \\times \\frac{\\partial \\mathbf{h_n}}{\\partial \\mathbf{h_{n-1}}} \\times \\ldots \\times \\frac{\\partial \\mathbf{h_k}}{\\partial W_k}\nEach layer of the network adds a contribution to the gradient when going backwards from the loss function to the parameters. Importantly, all functions used in a NN are differentiable, i.e. those partial derivatives exist (and are easy to compute). For the fully connected layer represented by Equation 7.1, the partial derivative is given by:\n\\frac{\\partial \\mathbf{h_{k}}}{\\partial \\mathbf{h_{k-1}}} = f'(W_k \\times \\mathbf{h_{k-1}} + \\mathbf{b_k}) \\, W_k\nand its dependency on the parameters is:\n\\frac{\\partial \\mathbf{h_{k}}}{\\partial W_k} = f'(W_k \\times \\mathbf{h_{k-1}} + \\mathbf{b_k}) \\, \\mathbf{h_{k-1}}\n \n    \\frac{\\partial \\mathbf{h_{k}}}{\\partial \\mathbf{b_k}} = f'(W_k \\times \\mathbf{h_{k-1}} + \\mathbf{b_k})\nActivation functions are chosen to have an easy-to-compute derivative, such as the ReLU function:\nf'(x) = \\begin{cases} 1 \\quad \\text{if} \\quad x &gt; 0 \\\\ 0 \\quad \\text{otherwise.} \\end{cases}\nPartial derivatives are automatically computed by the underlying libraries, such as tensorflow, theano, pytorch, etc. The next step is choose an optimizer, i.e. a gradient-based optimization method allow to modify the free parameters using the gradients. Optimizers do not work on the whole training set, but use minibatches (a random sample of training examples: their number is called the batch size) to compute iteratively the loss function. The most popular optimizers are:\nSee this useful post for a comparison of the different optimizers: http://ruder.io/optimizing-gradient-descent (Ruder, 2016). The common wisdom is that SGD with Nesterov momentum works best (i.e. it finds a better minimum) but its meta-parameters (learning rate, momentum) are hard to find, while Adam works out-of-the-box, at the cost of a slightly worse minimum. For deep RL, Adam is usually preferred, as the goal is to quickly find a working solution, not to optimize it to the last decimal.\nAdditional regularization mechanisms are now typically part of DNNs in order to avoid overfitting (learning by heart the training set but failing to generalize): L1/L2 regularization, dropout, batch normalization, etc. Refer to Goodfellow et al. (2016) for further details.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Deep learning</span>"
    ]
  },
  {
    "objectID": "src/2.2-DeepNetworks.html#feedforward-neural-networks",
    "href": "src/2.2-DeepNetworks.html#feedforward-neural-networks",
    "title": "Deep learning",
    "section": "",
    "text": "Figure 7.1: Architecture of a deep neural network. Source: Nielsen (2015), CC-BY-NC.\n\n\n\n\n\n\n\n\nIn regression problems, the mean square error (mse) is minimized:\n\n\n\n\nIn classification problems, the cross entropy (or negative log-likelihood) is minimized:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSGD (stochastic gradient descent): vanilla gradient descent on random minibatches.\nSGD with momentum (Nesterov or not): additional momentum to avoid local minima of the loss function.\nAdagrad\nAdadelta\nRMSprop\nAdam\nMany others. Check the doc of keras to see what is available: https://keras.io/optimizers",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Deep learning</span>"
    ]
  },
  {
    "objectID": "src/2.2-DeepNetworks.html#convolutional-networks",
    "href": "src/2.2-DeepNetworks.html#convolutional-networks",
    "title": "Deep learning",
    "section": "Convolutional networks",
    "text": "Convolutional networks\nConvolutional Neural Networks (CNN) are an adaptation of DNNs to deal with highly dimensional input spaces such as images. The idea is that neurons in the hidden layer reuse (“share”) weights over the input image, as the features learned by early layers are probably local in visual classification tasks: in computer vision, an edge can be detected by the same filter all over the input image.\nA convolutional layer learns to extract a given number of features (typically 16, 32, 64, etc) represented by 3x3 or 5x5 matrices. These matrices are then convoluted over the whole input image (or the previous convolutional layer) to produce feature maps. If the input image has a size NxMx1 (grayscale) or NxMx3 (colored), the convolutional layer will be a tensor of size NxMxF, where F is the number of extracted features. Padding issues may reduce marginally the spatial dimensions. One important aspect is that the convolutional layer is fully differentiable, so backpropagation and the usual optimizers can be used to learn the filters.\n\n\n\n\n\n\nFigure 7.2: Convolutional layer. Source: https://github.com/vdumoulin/conv_arithmetic.\n\n\n\nAfter a convolutional layer, the spatial dimensions are preserved. In classification tasks, it does not matter where the object is in the image, the only thing that matters is what it is: classification requires spatial invariance in the learned representations. The max-pooling layer was introduced to downsample each feature map individually and increase their spatial invariance. Each feature map is divided into 2x2 blocks (generally): only the maximal feature activation in that block is preserved in the max-pooling layer. This reduces the spatial dimensions by a factor two in each direction, but keeps the number of features equal.\n\n\n\n\n\n\nFigure 7.3: Max-pooling layer. Source: Stanford’s CS231n course http://cs231n.github.io/convolutional-networks\n\n\n\nA convolutional neural network is simply a sequence of convolutional layers and max-pooling layers (sometime two convolutional layers are applied in a row before max-pooling, as in VGG (Simonyan and Zisserman, 2015)), followed by a couple of fully-connected layers and a softmax output layer. Figure 7.4 shows the architecture of AlexNet, the winning architecture of the ImageNet challenge in 2012 (Krizhevsky et al., 2012).\n\n\n\n\n\n\nFigure 7.4: Architecture of the AlexNet CNN. Source: Krizhevsky et al. (2012).\n\n\n\nMany improvements have been proposed since 2012 (e.g. ResNets (He et al., 2015)) but the idea stays similar. Generally, convolutional and max-pooling layers are alternated until the spatial dimensions are so reduced (around 10x10) that they can be put into a single vector and fed into a fully-connected layer. This is NOT the case in deep RL! Contrary to object classification, spatial information is crucial in deep RL: position of the ball, position of the body, etc. It matters whether the ball is to the right or to the left of your paddle when you decide how to move it. Max-pooling layers are therefore omitted and the CNNs only consist of convolutional and fully-connected layers. This greatly increases the number of weights in the networks, hence the number of training examples needed to train the network. This is still the main limitation of using CNNs in deep RL.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Deep learning</span>"
    ]
  },
  {
    "objectID": "src/2.2-DeepNetworks.html#recurrent-neural-networks",
    "href": "src/2.2-DeepNetworks.html#recurrent-neural-networks",
    "title": "Deep learning",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\nFeedforward neural networks learn to efficiently map static inputs \\mathbf{x} to outputs \\mathbf{y} but have no memory or context: the output at time t does not depend on the inputs at time t-1 or t-2, only the one at time t. This is problematic when dealing with video sequences for example: if the task is to classify videos into happy/sad, a frame by frame analysis is going to be inefficient (most frames a neutral). Concatenating all frames in a giant input vector would increase dramatically the complexity of the classifier and no generalization can be expected.\nRecurrent Neural Networks (RNN) are designed to deal with time-varying inputs, where the relevant information to take a decision at time t may have happened at different times in the past. The general structure of a RNN is depicted on Figure 7.5:\n\n\n\n\n\n\nFigure 7.5: Architecture of a RNN. Left: recurrent architecture. Right: unrolled network, showing that a RNN is equivalent to a deep network. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\n\nThe output \\mathbf{h}_t of the RNN at time t depends on its current input \\mathbf{x}_t, but also on its previous output \\mathbf{h}_{t-1}, which, by recursion, depends on the whole history of inputs (x_0, x_1, \\ldots, x_t).\n\n    \\mathbf{h}_t = f(W_x \\, \\mathbf{x}_{t} + W_h \\, \\mathbf{h}_{t-1} + \\mathbf{b})\n\nOnce unrolled, a RNN is equivalent to a deep network, with t layers of weights between the first input \\mathbf{x}_0 and the current output \\mathbf{h}_t. The only difference with a feedforward network is that weights are reused between two time steps / layers. Backpropagation though time (BPTT) can be used to propagate the gradient of the loss function backwards in time and learn the weights W_x and W_h using the usual optimizer (SGD, Adam…).\nHowever, this kind of RNN can only learn short-term dependencies because of the vanishing gradient problem (Hochreiter, 1991). When the gradient of the loss function travels backwards from \\mathbf{h}_t to \\mathbf{x}_0, it will be multiplied t times by the recurrent weights W_h. If |W_h| &gt; 1, the gradient will explode with increasing t, while if |W_h| &lt; 1, the gradient will vanish to 0.\nThe solution to this problem is provided by long short-term memory networks [LSTM;Hochreiter and Schmidhuber (1997)]. LSTM layers maintain additionally a state \\mathbf{C}_t (also called context or memory) which is manipulated by three learnable gates (input, forget and output gates). As in regular RNNs, a candidate state \\tilde{\\mathbf{C}_t} is computed based on the current input and the previous output:\n\n    \\tilde{\\mathbf{C}_t} = f(W_x \\, \\mathbf{x}_{t} + W_h \\, \\mathbf{h}_{t-1} + \\mathbf{b})\n\n\n\n\n\n\n\nFigure 7.6: Architecture of a LSTM layer. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\n\nThe activation function f is usually a tanh function. The input and forget learn to decide how the candidate state should be used to update the current state:\n\nThe input gate decides which part of the candidate state \\tilde{\\mathbf{C}_t} will be used to update the current state \\mathbf{C}_t:\n\n\n    \\mathbf{i}_t = \\sigma(W^i_x \\, \\mathbf{x}_{t} + W^i_h \\, \\mathbf{h}_{t-1} + \\mathbf{b}^i)\n\nThe sigmoid activation function \\sigma is used to output a number between 0 and 1 for each neuron: 0 means the candidate state will not be used at all, 1 means completely.\n\nThe forget gate decides which part of the current state should be kept or forgotten:\n\n\n    \\mathbf{f}_t = \\sigma(W^f_x \\, \\mathbf{x}_{t} + W^f_h \\, \\mathbf{h}_{t-1} + \\mathbf{b}^f)\n\nSimilarly, 0 means that the corresponding element of the current state will be erased, 1 that it will be kept.\nOnce the input and forget gates are computed, the current state can be updated based on its previous value and the candidate state:\n\n   \\mathbf{C}_t =  \\mathbf{i}_t \\odot \\tilde{\\mathbf{C}_t} + \\mathbf{f}_t \\odot \\mathbf{C}_{t-1}\n\nwhere \\odot is the element-wise multiplication.\n\nThe output gate finally learns to select which part of the current state \\mathbf{C}_t should be used to produce the current output \\mathbf{h}_t:\n\n\n    \\mathbf{o}_t = \\sigma(W^o_x \\, \\mathbf{x}_{t} + W^o_h \\, \\mathbf{h}_{t-1} + \\mathbf{b}^o)\n\n\n    \\mathbf{h}_t = \\mathbf{o}_t \\odot \\tanh \\mathbf{C}_t\n\nThe architecture may seem complex, but everything is differentiable: backpropagation though time can be used to learn not only the input and recurrent weights for the candidate state, but also the weights and and biases of the gates. The main advantage of LSTMs is that they solve the vanishing gradient problem: if the input at time t=0 is important to produce a response at time t, the input gate will learn to put it into the memory and the forget gate will learn to maintain in the current state until it is not needed anymore. During this “working memory” phase, the gradient is multiplied by exactly one as nothing changes: the dependency can be learned with arbitrary time delays!\nThere are alternatives to the classical LSTM layer such as the gated recurrent unit [GRU; Cho et al. (2014)] or peephole connections (Gers, 2001). See http://colah.github.io/posts/2015-08-Understanding-LSTMs, https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714 or http://blog.echen.me/2017/05/30/exploring-lstms/ for more visual explanations of LSTMs and their variants.\nRNNs are particularly useful for deep RL when considering POMDPs, i.e. partially observable problems. If an observation does not contain enough information about the underlying state (e.g. a single image does not contain speed information), LSTM can integrate these observations over time and learn to implicitly represent speed in its context vector, allowing efficient policies to be learned.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Deep learning</span>"
    ]
  },
  {
    "objectID": "src/2.2-DeepNetworks.html#transformers",
    "href": "src/2.2-DeepNetworks.html#transformers",
    "title": "Deep learning",
    "section": "Transformers",
    "text": "Transformers\nComing soon",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Deep learning</span>"
    ]
  },
  {
    "objectID": "src/2.2-DeepNetworks.html#diffusion-models",
    "href": "src/2.2-DeepNetworks.html#diffusion-models",
    "title": "Deep learning",
    "section": "Diffusion models",
    "text": "Diffusion models\nComing soon\n\n\n\n\nCho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., et al. (2014). Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Available at: http://arxiv.org/abs/1406.1078.\n\n\nGers, F. (2001). Long Short-Term Memory in Recurrent Neural Networks. Available at: http://www.felixgers.de/papers/phd.pdf.\n\n\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press Available at: http://www.deeplearningbook.org.\n\n\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. Available at: http://arxiv.org/abs/1512.03385.\n\n\nHochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Available at: http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf.\n\n\nHochreiter, S., and Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation 9, 1735–1780. doi:10.1162/neco.1997.9.8.1735.\n\n\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. in Advances in Neural Information Processing Systems (NIPS) Available at: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.\n\n\nNielsen, M. A. (2015). Neural Networks and Deep Learning. Determination Press Available at: http://neuralnetworksanddeeplearning.com/.\n\n\nRuder, S. (2016). An overview of gradient descent optimization algorithms. Available at: http://arxiv.org/abs/1609.04747.\n\n\nSimonyan, K., and Zisserman, A. (2015). Very Deep Convolutional Networks for Large-Scale Image Recognition. International Conference on Learning Representations (ICRL), 1–14. doi:10.1016/j.infsof.2008.09.005.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Deep learning</span>"
    ]
  },
  {
    "objectID": "src/2.3-DQN.html",
    "href": "src/2.3-DQN.html",
    "title": "Deep Q-network (DQN)",
    "section": "",
    "text": "Limitations of deep neural networks for function approximation\nThe goal of value-based deep RL is to approximate the Q-value of each possible state-action pair using a deep neural network. As shown on Figure 8.1, the network can either take a state-action pair as input and return a single output value, or take only the state as input and return the Q-value of all possible actions (only possible if the action space is discrete). In both cases, the goal is to learn estimates Q_\\theta(s, a) with a NN with parameters \\theta.\nWhen using Q-learning, we have already seen that the problem is a regression problem, where the following mse loss function has to be minimized:\n\\mathcal{L}(\\theta) = \\mathbb{E}_{(s, a, r ,s')}[(r(s, a, s') + \\gamma \\, \\max_{a'} Q_\\theta(s', a') - Q_\\theta(s, a))^2]\nIn short, we want to reduce the prediction error, i.e. the mismatch between the estimate of the value of an action Q_\\theta(s, a) and the real return Q^\\pi(s, a), here approximated with r(s, a, s') + \\gamma \\, \\text{max}_{a'} Q_\\theta(s', a').\nWe can compute this loss by gathering enough samples (s, a, r, s') (i.e. single transitions), concatenating them randomly in minibatches, and let the DNN learn to minimize the prediction error using backpropagation and SGD, indirectly improving the policy. The following pseudocode would describe the training procedure when gathering transitions online, i.e. when directly interacting with the environment:\nHowever, the definition of the loss function uses the mathematical expectation operator E over all transitions, which can only be approximated by randomly sampling the distribution (the MDP). This implies that the samples concatenated in a minibatch should be independent from each other (i.i.d).",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deep Q-network (DQN)</span>"
    ]
  },
  {
    "objectID": "src/2.3-DQN.html#limitations-of-deep-neural-networks-for-function-approximation",
    "href": "src/2.3-DQN.html#limitations-of-deep-neural-networks-for-function-approximation",
    "title": "Deep Q-network (DQN)",
    "section": "",
    "text": "Figure 8.1: Function approximators can either associate a state-action pair (s, a) to its Q-value (left), or associate a state s to the Q-values of all actions possible in that state (right).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: Value-based Q-learning agent.\n\n\n\n\n\n\n\n\n\n\nNaive Q-learning with function approximation\n\n\n\n\nInitialize value network Q_{\\theta} with random weights.\nInitialize empty minibatch \\mathcal{D} of maximal size n.\nObserve the initial state s_0.\nfor t \\in [0, T_\\text{total}]:\n\nSelect the action a_t based on the behavior policy derived from Q_\\theta(s_t, a) (e.g. softmax).\nPerform the action a_t and observe the next state s_{t+1} and the reward r_{t+1}.\nStore (s_t, a_t, r_{t+1}, s_{t+1}) in the minibatch.\nWhen minibatch \\mathcal{D} is full:\n\nTrain the value network Q_{\\theta} on \\mathcal{D} to minimize\n\n\n  \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D}[(r(s, a, s') + \\gamma \\, \\text{max}_{a'} Q_\\theta(s', a') - Q_\\theta(s, a))^2]\n  \n\nEmpty the minibatch \\mathcal{D}.\n\n\n\n\n\n\n\nCorrelated inputs\nWhen gathering transitions online, the samples are correlated: (s_t, a_t, r_{t+1}, s_{t+1}) will be followed by (s_{t+1}, a_{t+1}, r_{t+2}, s_{t+2}), etc. When playing video games, two successive frames will be very similar (a few pixels will change, or even none if the sampling rate is too high) and the optimal action will likely not change either (to catch the ball in pong, you will need to perform the same action - going left - many times in a row).\n\n\n\n\n\n\nFigure 8.3: Sucessive frames in a video game are highly correlated.\n\n\n\nCorrelated inputs/outputs are very bad for deep neural networks: the DNN will overfit and fall into a very bad local minimum. That is why stochastic gradient descent works so well: it randomly samples values from the training set to form minibatches and minimize the loss function on these uncorrelated samples (hopefully). If all samples of a minibatch were of the same class (e.g. zeros in MNIST), the network would converge poorly. This is the first problem preventing an easy use of deep neural networks as function approximators in RL.\n\n\nNon-stationary targets\nThe second major problem is the non-stationarity of the targets in the loss function. In classification or regression, the desired values \\mathbf{t} are fixed throughout learning: the class of an object does not change in the middle of the training phase.\n\n    \\mathcal{L}(\\theta) = - \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}}[ ||\\mathbf{t} - \\mathbf{y}||^2]\n\nIn Q-learning, the target :\n\n    t = r(s, a, s') + \\gamma \\, \\max_{a'} Q_\\theta(s', a')\n\nwill change during learning, as Q_\\theta(s', a') depends on the weights \\theta and will hopefully increase as the performance improves. This is the second problem of deep RL: deep NN are particularly bad on non-stationary problems, especially feedforward networks. They iteratively converge towards the desired value, but have troubles when the target also moves (like a dog chasing its tail).\n\n\n\n\n\n\nFigure 8.4: In supervised learning, the targets are stationary, leading to good convergence properties. In RL, the targets are non-stationary and depending on the network itself. This often leads to suboptimal convergence.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deep Q-network (DQN)</span>"
    ]
  },
  {
    "objectID": "src/2.3-DQN.html#deep-q-network-dqn",
    "href": "src/2.3-DQN.html#deep-q-network-dqn",
    "title": "Deep Q-network (DQN)",
    "section": "Deep Q-Network (DQN)",
    "text": "Deep Q-Network (DQN)\nMnih et al. (2015) (originally arXived in Mnih et al. (2013)) proposed an elegant solution to the problems of correlated inputs/outputs and non-stationarity inherent to RL. This article is a milestone of deep RL and it is fair to say that it started the hype for deep RL.\n\nExperience replay memory\nThe first idea proposed by Mnih et al. (2015) solves the problem of correlated input/outputs and is actually quite simple: instead of feeding successive transitions into a minibatch and immediately training the NN on it, transitions are stored in a huge buffer called experience replay memory (ERM) or replay buffer able to store 100000 transitions. When the buffer is full, new transitions replace the old ones. SGD can now randomly sample the ERM to form minibatches and train the NN.\n\n\n\n\n\n\nFigure 8.5: Experience replay memory. Interactions with the environment are stored in the ERM. Random minibatches are sampled from it to train the DQN value network.\n\n\n\nThe loss minimized by DQN is defined on a minibatch of size K:\n\n    \\mathcal{L}(\\theta) = \\dfrac{1}{K} \\, \\sum_{k=1}^K (r_k + \\gamma \\, \\text{max}_{a'} Q_\\theta(s'_k, a') - Q_\\theta(s_k, a_k))^2\n\nAre these K samples i.i.d? They are independent because they are randomly sampled from the ERM, but they do not come from the same distribution: some were generated by a very old policy, some much more recently… However, this does not matter, as Q-learning is off-policy: the different policies that populated the ERM are a behavior policy, different from the learned one. Off-policy methods do not mind if the samples come from the same distribution or not. It would be very different if we has used SARSA instead.\n→ It is only possible to use an experience replay memory with off-policy algorithms\n\n\nTarget networks\nThe second idea solves the non-stationarity of the targets r(s, a, s') + \\gamma \\, \\max_{a'} Q_\\theta(s', a'). Instead of computing it with the current parameters \\theta of the NN, they are computed with an old version of the NN called the target network with parameters \\theta'.\n\n    \\mathcal{L}(\\theta) = \\dfrac{1}{K} \\, \\sum_{k=1}^K (r_k + \\gamma \\, \\text{max}_{a'} Q_{\\theta'}(s'_k, a') - Q_\\theta(s_k, a_k))^2\n\nThe target network is updated only infrequently (every thousands of iterations or so) with the learned weights \\theta. As this target network does not change very often, the targets stay constant for a long period of time, and the problem becomes more stationary.\n\n\n\n\n\n\nFigure 8.6: The target network is used to compute the targets to train the value network. Its waits are regularly copied from the value network.\n\n\n\n\n\n\n\n\n\nFigure 8.7: By keeping the the targets constant for a while, the target network lets the value network catch up with them and converge optimally (in principle).\n\n\n\n\n\nDQN algorithm\nThe resulting algorithm is called Deep Q-Network (DQN). It is summarized by the following pseudocode:\n\n\n\n\n\n\nDQN algorithm\n\n\n\n\nInitialize value network Q_{\\theta} with random weights.\nCopy Q_{\\theta} to create the target network Q_{\\theta'}.\nInitialize experience replay memory \\mathcal{D} of maximal size N.\nObserve the initial state s_0.\nfor t \\in [0, T_\\text{total}]:\n\nSelect the action a_t based on the behavior policy derived from Q_\\theta(s_t, a) (e.g. softmax).\nPerform the action a_t and observe the next state s_{t+1} and the reward r_{t+1}.\nStore (s_t, a_t, r_{t+1}, s_{t+1}) in the experience replay memory.\nEvery T_\\text{train} steps:\n\nSample a minibatch \\mathcal{D}_s randomly from \\mathcal{D}.\nFor each transition (s, a, r, s') in the minibatch:\n\nPredict the Q-value of the greedy action in the next state \\max_{a'} Q_{\\theta'}(s', a') using the target network.\nCompute the target value t = r + \\gamma \\, \\max_{a'} Q_{\\theta'}(s', a').\n\nTrain the value network Q_{\\theta} on \\mathcal{D}_s to minimize \\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{D}_s}[(t - Q_\\theta(s, a))^2]\n\nEvery T_\\text{target} steps:\n\nUpdate the target network with the trained value network: \\theta' \\leftarrow \\theta\n\n\n\n\n\nThe first thing to notice is that experienced transitions are not immediately used for learning, but simply stored in the ERM to be sampled later. Due to the huge size of the ERM, it is even likely that the recently experienced transition will only be used for learning hundreds or thousands of steps later. Meanwhile, very old transitions, generated using an initially bad policy, can be used to train the network for a very long time.\nThe second thing is that the target network is not updated very often (T_\\text{target}=10000), so the target values are going to be wrong a long time. More recent algorithms such as DDPG use a smoothed version of the current weights, as proposed in Lillicrap et al. (2015):\n\n    \\theta' = \\tau \\, \\theta + (1-\\tau) \\, \\theta'\n\nIf this rule is applied after each step with a very small rate \\tau, the target network will slowly track the learned network, but never be the same. Modern implementations of DQN use this smoothed version.\nThese two facts make DQN extremely slow to learn: millions of transitions are needed to obtain a satisfying policy. This is called the sample complexity, i.e. the number of transitions needed to obtain a satisfying performance. DQN finds very good policies, but at the cost of a very long training time.\nDQN was initially applied to solve various Atari 2600 games. Video frames were used as observations and the set of possible discrete actions was limited (left/right/up/down, shoot, etc). The CNN used is depicted on Figure 8.8. It has two convolutional layers, no max-pooling, 2 fully-connected layer and one output layer representing the Q-value of all possible actions in the games.\n\n\n\n\n\n\nFigure 8.8: Architecture of the CNN used in the original DQN paper. Source: Mnih et al. (2015).\n\n\n\nThe problem of partial observability (a single frame does not hold the Markov property) is solved by concatenating the four last video frames into a single tensor used as input to the CNN. The convolutional layers become able through learning to extract the speed information from it. Some of the Atari games (Pinball, Breakout) were solved with a performance well above human level, especially when they are mostly reactive. Games necessitating more long-term planning (Montezuma’s Revenge) were still poorly learned, though.\n\n\n\n\n\n\nWhy no max-pooling?\n\n\n\nThe CNN used in deep RL agents (DQN or others) usually do not have many max-pooling layers (or strides, which are equivalent). The goal of a max-pooling layer is to achieve spatial invariance, i.e. being able to recognize an object whatever its position in the input image. A cat is a cat, whether it is on the left or the right of the image.\nHowever, we usually do not want spatial invariance in RL: the location of the ball in the frame in Breakout or Pinball is extremely important for the policy, we do not want to get rid of it.\nThe drawback of not having max-pooling layers is that the last convolutional layer (before the first FC layer) will still have a lot of elements, so the first FC matrix will likely be huge. This limits the ability of deep RL algorithms to work with big images.\n\n\nBeside being able to learn using delayed and sparse rewards in highly dimensional input spaces, the true tour de force of DQN is that it was able to learn the 49 Atari games using the same architecture and hyperparameters, showing the generality of the approach.\n\n\n\n\n\n\n\n\nFigure 8.9: Results on the Atari benchmark. Some games achieved super-human performance. Source: Mnih et al. (2015)\n\n\n\n\n\n\n\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., et al. (2015). Continuous control with deep reinforcement learning. CoRR. Available at: http://arxiv.org/abs/1509.02971.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., et al. (2013). Playing Atari with Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1312.5602.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). Human-level control through deep reinforcement learning. Nature 518, 529–533. doi:10.1038/nature14236.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deep Q-network (DQN)</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html",
    "href": "src/2.4-DQNvariants.html",
    "title": "DQN variants (Rainbow)",
    "section": "",
    "text": "Double DQN\nIn DQN, the experience replay memory and the target network were decisive in allowing the CNN to learn the tasks through RL. Their drawback is that they drastically slow down learning and increase the sample complexity. Additionally, DQN has stability issues: the same network may not converge the same way in different runs. One first improvement on DQN was proposed by van Hasselt et al. (2015) and called double DQN.\nThe idea is that the target value y = r(s, a, s') + \\gamma \\, \\max_{a'} Q_{\\theta'}(s', a') is frequently over-estimating the true return because of the maximum operator. Especially at the beginning of learning when Q-values are far from being correct, if an action is over-estimated (Q_{\\theta'}(s', a) is higher that its true value) and selected by the target network as the next greedy action, the learned Q-value Q_{\\theta}(s, a) will also become over-estimated, what will propagate to all previous actions on the long-term. van Hasselt (2010) showed that this over-estimation is inevitable in regular Q-learning and proposed double learning.\nThe idea is to train independently two value networks: one will be used to find the greedy action (the action with the maximal Q-value), the other to estimate the Q-value itself. Even if the first network choose an over-estimated action as the greedy action, the other might provide a less over-estimated value for it, solving the problem.\nApplying double learning to DQN is particularly straightforward: there are already two value networks, the trained network and the target network. Instead of using the target network to both select the greedy action in the next state and estimate its Q-value, here the trained network \\theta is used to select the greedy action a^* = \\text{argmax}_{a'} Q_\\theta (s', a') while the target network only estimates its Q-value. The target value becomes:\ny = r(s, a, s') + \\gamma \\, Q_{\\theta'}(s', \\text{argmax}_{a'} Q_\\theta (s', a'))\nThis induces only a small modification of the DQN algorithm and significantly improves its performance and stability:",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants (Rainbow)</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#sec-doubleqlearning",
    "href": "src/2.4-DQNvariants.html#sec-doubleqlearning",
    "title": "DQN variants (Rainbow)",
    "section": "",
    "text": "Figure 9.1: Overestimation of Q-values with DQN on a few Atari games. The true Q-value can be estimated by counting the rewards until the end of the episode. Source van Hasselt et al. (2015).\n\n\n\n\n\n\n\n\n\n\n\n\n\nDouble DQN algorithm\n\n\n\n\nEvery T_\\text{train} steps:\n\nSample a minibatch \\mathcal{D}_s randomly from \\mathcal{D}.\nFor each transition (s, a, r, s') in the minibatch:\n\nSelect the greedy action in the next state a^* = \\text{argmax}_{a'} Q_\\theta (s', a') using the trained network.\nPredict its Q-value Q_{\\theta'}(s', a^*) using the target network.\nCompute the target value y = r + \\gamma \\, Q_{\\theta'}(s', a*).",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants (Rainbow)</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#prioritized-experience-replay",
    "href": "src/2.4-DQNvariants.html#prioritized-experience-replay",
    "title": "DQN variants (Rainbow)",
    "section": "Prioritized experience replay",
    "text": "Prioritized experience replay\nAnother drawback of the original DQN is that the experience replay memory is sampled uniformly. Novel and interesting transitions are selected with the same probability as old well-predicted transitions, what slows down learning. The main idea of prioritized experience replay (Schaul et al., 2015) is to order the transitions in the experience replay memory in decreasing order of their TD error:\n\n    \\delta = r(s, a, s') + \\gamma \\, Q_{\\theta'}(s', \\text{argmax}_{a'} Q_\\theta (s', a')) - Q_\\theta(s, a)\n\nand sample with a higher probability those surprising transitions to form a minibatch:\n\n    P(k) = \\frac{(|\\delta_k| + \\epsilon)^\\alpha}{\\sum_k (|\\delta_k| + \\epsilon)^\\alpha}\n\nHowever, non-surprising transitions might become relevant again after enough training, as the Q_\\theta(s, a) change, so prioritized replay has a softmax function over the TD error to ensure “exploration” of memorized transitions. This data structure has of course a non-negligible computational cost, but accelerates learning so much that it is worth it. See https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/ for a presentation of double DQN with prioritized replay.\n\n\n\n\n\n\nFigure 9.2: Relative improvement on the Atari benchmark brought by PER. Source: Schaul et al. (2015)",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants (Rainbow)</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#duelling-network",
    "href": "src/2.4-DQNvariants.html#duelling-network",
    "title": "DQN variants (Rainbow)",
    "section": "Duelling network",
    "text": "Duelling network\nThe classical DQN architecture uses a single NN to predict directly the value of all possible actions Q_\\theta(s, a). The value of an action depends on two factors:\n\nthe value of the underlying state s: in some states, all actions are bad, you lose whatever you do.\nthe interest of that action: some actions are better than others for a given state.\n\n\n\n\n\n\n\nFigure 9.3: The value of an action depends on the value of the underlying state, plus its own advantage.\n\n\n\nHowever, the exact Q-values of all actions are not equally important.\n\nIn bad states (low V^\\pi(s)), you can do whatever you want, you will lose.\nIn neutral states, you can do whatever you want, nothing happens.\nIn good states (high V^\\pi(s)), you need to select the right action to get rewards, otherwise you lose.\n\nThe total variance of the Q-values (over all states) is quite high: it can be a problem for the underlying neural network, which has to output very negative and very positive number.\nThis leads to the definition of the advantage A^\\pi(s,a) of an action:\n\n    A^\\pi(s, a) = Q^\\pi(s, a) - V^\\pi(s)\n\\tag{9.1}\nThe advantage of the optimal action in s is equal to zero: the expected return in s is the same as the expected return when being in s and taking a, as the optimal policy will choose a in s anyway. The advantage of all other actions is negative: they bring less reward than the optimal action (by definition), so they are less advantageous. Note that this is only true if your estimate of V^\\pi(s) is correct.\nBaird (1993) has shown that it is advantageous to decompose the Q-value of an action into the value of the state and the advantage of the action (advantage updating):\n\n    Q^\\pi(s, a) = V^\\pi(s) + A^\\pi(s, a)\n\nIf you already know that the value of a state is very low, you do not need to bother exploring and learning the value of all actions in that state, they will not bring much. Moreover, the advantage function has less variance than the Q-values, which is a very good property when using neural networks for function approximation. The variance of the Q-values comes from the fact that they are estimated based on other estimates, which themselves evolve during learning (non-stationarity of the targets) and can drastically change during exploration (stochastic policies). The advantages only track the relative change of the value of an action compared to its state, what is going to be much more stable over time.\n\n\n\n\n\n\nFigure 9.4: The advantages have a much smaller variability than the Q-values.\n\n\n\nThe range of values taken by the advantages is also much smaller than the Q-values. Let’s suppose we have two states with values -10 and 10, and two actions with advantages 0 and -1 (it does not matter which one). The Q-values will vary between -11 (the worst action in the worst state) and 10 (the best action in the best state), while the advantage only varies between -1 and 0. It is therefore going to be much easier for a neural network to learn the advantages than the Q-values, which are theoretically not bounded.\n\n\n\n\n\n\nFigure 9.5: Duelling network architecture. Top: classical feedforward architecture to predict Q-values. Bottom: Duelling networks predicting state values and advantage functions to form the Q-values. Source: Wang et al. (2016).\n\n\n\nWang et al. (2016) incorporated the idea of advantage updating in a double DQN architecture with prioritized replay (Figure 9.5). As in DQN, the last layer represents the Q-values of the possible actions and has to minimize the mse loss:\n\n    \\mathcal{L}(\\theta) = \\mathbb{E}_\\pi([r(s, a, s') + \\gamma \\, Q_{\\theta', \\alpha', \\beta'}(s', \\text{argmax}_{a'} Q_{\\theta, \\alpha, \\beta} (s', a')) - Q_{\\theta, \\alpha, \\beta}(s, a)]^2)\n\nThe difference is that the previous fully-connected layer is forced to represent the value of the input state V_{\\theta, \\beta}(s) and the advantage of each action A_{\\theta, \\alpha}(s, a) separately. There are two separate sets of weights in the network, \\alpha and \\beta, to predict these two values, sharing representations from the early convolutional layers through weights \\theta. The output layer performs simply a parameter-less summation of both sub-networks:\n\n    Q_{\\theta, \\alpha, \\beta}(s, a) = V_{\\theta, \\beta}(s) + A_{\\theta, \\alpha}(s, a)\n\nThe issue with this formulation is that one could add a constant to V_{\\theta, \\beta}(s) and substract it from A_{\\theta, \\alpha}(s, a) while obtaining the same result. An easy way to constrain the summation is to normalize the advantages, so that the greedy action has an advantage of zero as expected:\n\n    Q_{\\theta, \\alpha, \\beta}(s, a) = V_{\\theta, \\beta}(s) + (A_{\\theta, \\alpha}(s, a) - \\max_a A_{\\theta, \\alpha}(s, a))\n\nBy doing this, the advantages are still free, but the state value will have to take the correct value. Wang et al. (2016) found that it is actually better to replace the \\max operator by the mean of the advantages. In this case, the advantages only need to change as fast as their mean, instead of having to compensate quickly for any change in the greedy action as the policy improves:\n\n    Q_{\\theta, \\alpha, \\beta}(s, a) = V_{\\theta, \\beta}(s) + (A_{\\theta, \\alpha}(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_a A_{\\theta, \\alpha}(s, a))\n\nApart from this specific output layer, everything works as usual, especially the gradient of the mse loss function can travel backwards using backpropagation to update the weights \\theta, \\alpha and \\beta. The resulting architecture outperforms double DQN with prioritized replay (DDQN-PER) on most Atari games, particularly games with repetitive actions.\n\n\n\n\n\n\nFigure 9.6: Relative improvement of Duelling DQN over DDQN-PER on Atari games. Source: Wang et al. (2016).",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants (Rainbow)</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#sec-distributionalrl",
    "href": "src/2.4-DQNvariants.html#sec-distributionalrl",
    "title": "DQN variants (Rainbow)",
    "section": "Categorical DQN",
    "text": "Categorical DQN\nAll RL methods based on the Bellman equations use the expectation operator to average returns and compute the values of states and actions:\n\n    Q^\\pi(s, a) = \\mathbb{E}_{s, a \\sim \\pi}[R(s, a)]\n\nThe variance of the returns is not considered in the action selection scheme, and most methods actually try to reduce this variance as it impairs the convergence of neural networks. Decision theory states that only the mean should matter on the long-term, but one can imagine tasks where the variance is an important factor for the decision. Imagine you are in a game where you have two actions available: the first one brings returns of 10 and 20, with a probability of 0.5 each (to simplify), while the second one brings returns of -10 and +40 with probability 0.5 too. Both actions have the same Q-value of 15 (a return which is actually never experienced), so one can theoretically pick whatever action, both are optimal in the Bellman’s sense.\nHowever, this is only true when playing long enough. If, after learning, one is only allowed one try on that game, it is obviously safer (but less fun) to choose the first action, as one wins at worse 10, while it is -10 with the second action. Knowing the distribution of the returns can allow to distinguish risky choices from safe ones more easily and adapt the behavior. Another advantage would be that by learning the distribution of the returns instead of just their mean, one actually gathers more information about the environment dynamics: it can only help the convergence of the algorithm towards the optimal policy.\nBellemare et al. (2017) proposed to learn the value distribution (the probability distribution of the returns) through a modification of the Bellman equation. They show that learning the complete distribution of rewards instead of their mean leads to performance improvements on Atari games over modern variants of DQN.\nTheir proposed categorical DQN (also called C51) has an architecture based on DQN, but where the output layer predicts the distribution of the returns for each action a in state s, instead of its mean Q^\\pi(s, a). In practice, each action a is represented by N output neurons, who encode the support of the distribution of returns. If the returns take values between V_\\text{min} and V_\\text{max}, one can represent their distribution \\mathcal{Z} by taking N discrete “bins” (called atoms in the paper) in that range. Figure 9.7 shows how the distribution of returns between -10 and 10 can be represented using 21 atoms.\n\n\n\n\n\n\nFigure 9.7: Example of a value distribution using 21 atoms between -10 and 10. The average return is 3, but its variance is explicitly represented.\n\n\n\nOf course, the main problem is to know in advance the range of returns [V_\\text{min}, V_\\text{max}] (it depends largely on the choice of the discount rate \\gamma), but you can infer it from training another algorithm such as DQN beforehand. Dabney et al. (2017) got rid of this problem with quantile regression. In the paper, the authors found out experimentally that 51 is the most efficient number of atoms (hence the name C51).\nLet’s note z_i these atoms with 1 \\leq i &lt; N. The atom probability that the return associated to a state-action pair (s, a) lies within the bin associated to the atom z_i is noted p_i(s, a). These probabilities can be predicted by a neural network, typically by using a softmax function over outputs f_i(s, a; \\theta):\n\n    p_i(s, a; \\theta) = \\frac{\\exp f_i(s, a; \\theta)}{\\sum_{j=1}^{N} \\exp f_j(s, a; \\theta)}\n\nThe distribution of the returns \\mathcal{Z} is simply a sum over the atoms (represented by the Dirac distribution \\delta_{z_i}):\n\n    \\mathcal{Z}_\\theta(s, a) = \\sum_{i=1}^{N} p_i(s, a; \\theta) \\, \\delta_{z_i}\n\nIf these probabilities are correctly estimated, the Q-value is easy to compute as the mean of the distribution:\n\n    Q_\\theta(s, a) = \\mathbb{E} [\\mathcal{Z}_\\theta(s, a)] = \\sum_{i=1}^{N} p_i(s, a; \\theta) \\, z_i\n\nThese Q-values can then be used for action selection as in the regular DQN. The problem is now to learn the value distribution \\mathcal{Z}_\\theta, i.e. to find a learning rule / loss function for the p_i(s, a; \\theta). Let’s consider a single transition (s, a, r, s') and select the greedy action a' in s' using the current policy \\pi_\\theta. The value distribution \\mathcal{Z}_\\theta can be evaluated by applying recursively the Bellman operator \\mathcal{T}:\n\n    \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) = \\mathcal{R}(s, a) + \\gamma \\, \\mathcal{Z}_\\theta(s', a')\n\nwhere \\mathcal{R}(s, a) is the distribution of immediate rewards after (s, a). This use of the Bellman operator is the same as in Q-learning:\n\n    \\mathcal{T} \\, \\mathcal{Q}_\\theta(s, a) = \\mathbb{E}[r(s, a)] + \\gamma \\, \\mathcal{Q}_\\theta(s', a')\n\nIn Q-learning, one minimizes the difference (mse) between \\mathcal{T} \\, \\mathcal{Q}_\\theta(s, a) and \\mathcal{Q}_\\theta(s, a), which are expectations (so we only manipulate scalars). Here, we will minimize the statistical distance between the distributions \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) and \\mathcal{Z}_\\theta(s, a) themselves, using for example the KL divergence, Wasserstein metric, total variation or whatnot.\nThe problem is mostly that the distributions \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) and \\mathcal{Z}_\\theta(s, a) do not have the same support: for a particular atom z_i, \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) can have a non-zero probability p_i(s, a), while \\mathcal{Z}_\\theta(s, a) has a zero probability. Besides, the probabilities must sum to 1, so one cannot update the z_i independently from one another.\nThe proposed method consists of three steps:\n\nComputation of the Bellman update \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a). They simply compute translated values for each z_i according to:\n\n\n    \\mathcal{T} \\, z_i = r + \\gamma \\, z_i\n\nand clip the obtained value to [V_\\text{min}, V_\\text{max}]. The reward r translates the distribution of atoms, while the discount rate \\gamma scales it. Figure 9.8 shows the distribution of \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) compared to \\mathcal{Z}_\\theta(s, a). Note that the atoms of the two distributions are not aligned.\n\n\n\n\n\n\nFigure 9.8: Computation of the Bellman update \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a). The atoms of the two distributions are not aligned.\n\n\n\n\nDistribution of the probabilities of \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) on the support of \\mathcal{Z}_\\theta(s, a). The projected atom \\mathcal{T} \\, z_i lie between two “real” atoms z_l and z_u, with a non-integer index b (for example b = 3.4, l = 3 and u=4). The corresponding probability p_{b}(s', a'; \\theta) of the next greedy action (s', a') is “spread” to its neighbors through a local interpolation depending on the distances between b, l and u:\n\n\n    \\Delta p_{l}(s', a'; \\theta) = p_{b}(s', a'; \\theta) \\, (b - u)\n \n    \\Delta p_{u}(s', a'; \\theta) = p_{b}(s', a'; \\theta) \\, (l - b)\n\nFigure 9.9 shows how the projected update distribution \\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) now matches the support of \\mathcal{Z}_\\theta(s, a)\n\n\n\n\n\n\nFigure 9.9: Projected update \\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) on the support of \\mathcal{Z}_\\theta(s, a). The atoms are now aligned, the statistical distance between the two distributions can be minimized.\n\n\n\nThe projection of the Bellman update onto an atom z_i can be summarized by the following equation:\n\n    (\\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a))_i = \\sum_{j=1}^N \\big [1 - \\frac{| [\\mathcal{T}\\, z_j]_{V_\\text{min}}^{V_\\text{max}} - z_i|}{\\Delta z} \\big ]_0^1 \\, p_j (s', a'; \\theta)\n\nwhere [\\cdot]_a^b bounds its argument in [a, b] and \\Delta z is the step size between two atoms.\n\nMinimizing the statistical distance between \\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) and \\mathcal{Z}_\\theta(s, a). Now that the Bellman update has the same support as the value distribution, we can minimize the KL divergence between the two for a single transition:\n\n\n    \\mathcal{L}(\\theta) = D_\\text{KL} (\\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_{\\theta'}(s, a) | \\mathcal{Z}_\\theta(s, a))\n\nusing a target network \\theta' for the target. It is to be noted that minimizing the KL divergence is the same as minimizing the cross-entropy between the two, as in classification tasks:\n\n    \\mathcal{L}(\\theta) =  - \\sum_i (\\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_{\\theta'}(s, a))_i \\log p_i (s, a; \\theta)\n\nThe projected Bellman update plays the role of the one-hot encoded target vector in classification (except that it is not one-hot encoded). DQN performs a regression on the Q-values (mse loss), while categorical DQN performs a classification (cross-entropy loss). Apart from the way the target is computed, categorical DQN is very similar to DQN: architecture, experience replay memory, target networks, etc.\nFigure 9.10 illustrates how the predicted value distribution changes when playing Space invaders (also have a look at the Youtube video at https://www.youtube.com/watch?v=yFBwyPuO2Vg). C51 outperforms DQN on most Atari games, both in terms of the achieved performance and the sample complexity.\n\n\n\n\n\n\nFigure 9.10: Evolution of the value distribution for the categorical DQN playing Space Invaders. Animation Source: https://deepmind.com/blog/going-beyond-average-reinforcement-learning/\n\n\n\n\n\nAdditional resources:\n\nhttps://deepmind.com/blog/going-beyond-average-reinforcement-learning\nhttps://physai.sciencesconf.org/data/pages/distributional_RL_Remi_Munos.pdf\nhttps://flyyufelix.github.io/2017/10/24/distributional-bellman.html, with keras code for C51.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants (Rainbow)</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#noisy-dqn",
    "href": "src/2.4-DQNvariants.html#noisy-dqn",
    "title": "DQN variants (Rainbow)",
    "section": "Noisy DQN",
    "text": "Noisy DQN\nDQN and its variants rely on \\epsilon-greedy action selection over the Q-values to explore. The exploration parameter \\epsilon is annealed during training to reach a final minimal value. It is preferred to softmax action selection, where \\tau scales with the unknown Q-values. The problem is that it is a global exploration mechanism: well-learned states do not need as much exploration as poorly explored ones.\n\\epsilon-greedy and softmax add exploratory noise to the output of DQN: The Q-values predict a greedy action, but another action is taken. What about adding noise to the parameters (weights and biases) of the DQN, what would change the greedy action everytime? Controlling the level of noise inside the neural network indirectly controls the exploration level.\n\n\n\n\n\n\nFigure 9.11: Parameter noise. Source: https://openai.com/blog/better-exploration-with-parameter-noise/\n\n\n\nParameter noise builds on the idea of Bayesian deep learning. Instead of learning a single value of the parameters:\ny = \\theta_1 \\, x + \\theta_0\nwe learn the distribution of the parameters, for example by assuming they come from a normal distribution:\n\\theta \\sim \\mathcal{N}(\\mu_\\theta, \\sigma_\\theta^2)\nFor each new input, we sample a value for the parameter:\n\\theta = \\mu_\\theta + \\sigma_\\theta \\, \\epsilon\nwith \\epsilon \\sim \\mathcal{N}(0, 1) a random variable.\nThe prediction y will vary for the same input depending on the variances:\ny = (\\mu_{\\theta_1} + \\sigma_{\\theta_1} \\, \\epsilon_1) \\, x + \\mu_{\\theta_0} + \\sigma_{\\theta_0} \\, \\epsilon_0\nThe mean and variance of each parameter can be learned through backpropagation! As the random variables \\epsilon_i  \\sim \\mathcal{N}(0, 1) are not correlated with anything, the variances \\sigma_\\theta^2 should decay to 0. The variances \\sigma_\\theta^2 represent the uncertainty about the prediction y.\nApplied to DQN, this means that a state which has not been visited very often will have a high uncertainty: The predicted Q-values will change a lot between two evaluations and the greedy action might change: exploration. Conversely, a well-explored state will have a low uncertainty: The greedy action stays the same: exploitation.\nNoisy DQN (Fortunato et al., 2017) uses greedy action selection over noisy Q-values. The level of exploration is learned by the network on a per-state basis. No need for scheduling! Parameter noise improves the performance of \\epsilon-greedy-based methods, including DQN, dueling DQN, A3C, DDPG (see later), etc.\n\n\n\n\n\n\nFigure 9.12: Results of Noisy DQN. Source: Fortunato et al. (2017)",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants (Rainbow)</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#rainbow-dqn",
    "href": "src/2.4-DQNvariants.html#rainbow-dqn",
    "title": "DQN variants (Rainbow)",
    "section": "Rainbow DQN",
    "text": "Rainbow DQN\nAs we have seen. the original formulation of DQN (Mnih et al., 2015) has seen many improvements over the years.\n\nDouble DQN (van Hasselt et al., 2015) separates the selection of the greedy action in the next state from its evaluation in order to prevent over-estimation of Q-values:\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(s´, \\text{argmax}_{a'} Q_{\\theta}(s', a')) - Q_\\theta(s, a))^2]\n\nPrioritized Experience Replay (Schaul et al., 2015) selects transitions from the ERM proportionally to their current TD error:\n\nP(k) = \\frac{(|\\delta_k| + \\epsilon)^\\alpha}{\\sum_k (|\\delta_k| + \\epsilon)^\\alpha}\n\nDueling DQN (Wang et al., 2016) splits learning of Q-values into learning of advantages and state values:\n\nQ_\\theta(s, a) = V_\\alpha(s) + A_\\beta(s, a)\n\nCategorical DQN (Bellemare et al., 2017) learns the distribution of returns instead of their expectation:\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{D}_s}[ - \\mathbf{t}_k \\, \\log Z_\\theta(s_k, a_k)]\n\nn-step returns (Sutton and Barto, 2017) reduce the bias of the estimation by taking the next n rewards into account, at the cost of a slightly higher variance.\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(\\sum_{k=1}^n r_{t+k} + \\gamma \\max_a Q_\\theta(s_{t+n+1}, a) - Q_\\theta(s_t, a_t))^2\n\nNoisy DQN (Fortunato et al., 2017) ensures exploration by adding noise to the parameters of the network instead of a softmax / \\epsilon-greedy action selection over the Q-values.\n\nAll these improvements exceed the performance of vanilla DQN on most if not all Atari game. But which ones are the most important?\nHessel et al. (2017) designed a Rainbow DQN integrating all these improvements. Not only does the combined network outperform all the DQN variants, but each of its components is important for its performance as shown by ablation studies (apart from double learning and duelling networks), see Figure 9.13.\n\n\n\n\n\n\nFigure 9.13: Performance of the Rainbow DQN compared to other DQN variants (left) and ablation studies. Figures Source: Hessel et al. (2017).",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants (Rainbow)</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#deep-recurrent-q-learning-drqn",
    "href": "src/2.4-DQNvariants.html#deep-recurrent-q-learning-drqn",
    "title": "DQN variants (Rainbow)",
    "section": "Deep Recurrent Q-learning (DRQN)",
    "text": "Deep Recurrent Q-learning (DRQN)\nThe Atari games used as a benchmark for value-based methods are partially observable MDPs (POMDP), i.e. a single frame does not contain enough information to predict what is going to happen next (e.g. the speed and direction of the ball on the screen is not known). In DQN, partial observability is solved by stacking four consecutive frames and using the resulting tensor as an input to the CNN. if this approach worked well for most Atari games, it has several limitations (as explained in https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc):\n\nIt increases the size of the experience replay memory, as four video frames have to be stored for each transition.\nIt solves only short-term dependencies (instantaneous speeds). If the partial observability has long-term dependencies (an object has been hidden a long time ago but now becomes useful), the input to the neural network will not have that information. This is the main explanation why the original DQN performed so poorly on games necessitating long-term planning like Montezuma’s revenge.\n\n\n\n\n\n\n\nFigure 9.14: Architecture of DRQN. Source: Hausknecht and Stone (2015).\n\n\n\nBuilding on previous ideas from the Schmidhuber’s group (Bakker, 2001; Wierstra et al., 2007), Hausknecht and Stone (2015) replaced one of the fully-connected layers of the DQN network by a LSTM layer while using single frames as inputs. The resulting deep recurrent q-learning (DRQN) network became able to solve POMDPs thanks to the learning abilities of LSTMs: the LSTM layer learn to remember which part of the sensory information will be useful to take decisions later.\nHowever, LSTMs are not a magical solution either. They are trained using truncated BPTT, i.e. on a limited history of states. Long-term dependencies exceeding the truncation horizon cannot be learned. Additionally, all states in that horizon (i.e. all frames) have to be stored in the ERM to train the network, increasing drastically its size. Firthermore, the training time (but not inference time) is orders of magnitude slower than with a comparable feedforward network. Despite these limitations, DRQN is a much more elegant solution to the partial observability problem, letting the network decide which horizon it needs to solve long-term dependencies.\n\n\n\n\n\n\nFigure 9.15: Performance of DRWN compared to DQN. Source: Hausknecht and Stone (2015).\n\n\n\n\n\n\n\nBaird, L. C. (1993). Advantage updating. Wright-Patterson Air Force Base Available at: http://leemon.com/papers/1993b.pdf.\n\n\nBakker, B. (2001). Reinforcement Learning with Long Short-Term Memory. in Advances in Neural Information Processing Systems 14 (NIPS 2001), 1475–1482. Available at: https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory.\n\n\nBellemare, M. G., Dabney, W., and Munos, R. (2017). A Distributional Perspective on Reinforcement Learning. Available at: http://arxiv.org/abs/1707.06887.\n\n\nDabney, W., Rowland, M., Bellemare, M. G., and Munos, R. (2017). Distributional Reinforcement Learning with Quantile Regression. Available at: http://arxiv.org/abs/1710.10044 [Accessed June 28, 2019].\n\n\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., et al. (2017). Noisy Networks for Exploration. Available at: http://arxiv.org/abs/1706.10295 [Accessed March 2, 2020].\n\n\nHausknecht, M., and Stone, P. (2015). Deep Recurrent Q-Learning for Partially Observable MDPs. Available at: http://arxiv.org/abs/1507.06527.\n\n\nHessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., et al. (2017). Rainbow: Combining Improvements in Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1710.02298.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). Human-level control through deep reinforcement learning. Nature 518, 529–533. doi:10.1038/nature14236.\n\n\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized Experience Replay. Available at: http://arxiv.org/abs/1511.05952.\n\n\nSutton, R. S., and Barto, A. G. (2017). Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press Available at: http://incompleteideas.net/book/the-book-2nd.html.\n\n\nvan Hasselt, H. (2010). Double Q-learning. in Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2 (Curran Associates Inc.), 2613–2621. Available at: https://dl.acm.org/citation.cfm?id=2997187.\n\n\nvan Hasselt, H., Guez, A., and Silver, D. (2015). Deep Reinforcement Learning with Double Q-learning. Available at: http://arxiv.org/abs/1509.06461.\n\n\nWang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de Freitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1511.06581 [Accessed November 21, 2019].\n\n\nWierstra, D., Foerster, A., Peters, J., and Schmidhuber, J. (2007). “Solving Deep Memory POMDPs with Recurrent Policy Gradients,” in (Springer, Berlin, Heidelberg), 697–706. doi:10.1007/978-3-540-74690-4_71.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants (Rainbow)</span>"
    ]
  },
  {
    "objectID": "src/2.5-DistributedLearning.html",
    "href": "src/2.5-DistributedLearning.html",
    "title": "Distributed learning",
    "section": "",
    "text": "Distributed DQN (GORILA)\nThe main limitation of deep RL is the slowness of learning, which is mainly influenced by two factors:\nThe second factor is particularly critical in real-world applications like robotics: physical robots evolve in real time, so the acquisition speed of transitions will be limited. Even in simulation (video games, robot emulators), the environment might turn out to be much slower than training the underlying neural network. In most settings, the value and target networks runs on a single GPu, while the environment is simulated on the CPU, as well as the ERM (there is not enough on the GPU to store it there). As the communication between the CPU and the GPU is rather slow, the GPU has to wait quite a long tme between two minibatches and is therefore idle most of the time.\nGoogle Deepmind proposed the GORILA (General Reinforcement Learning Architecture) framework to speed up the training of DQN networks using distributed actors and learners (Nair et al., 2015). The framework is quite general and the distribution granularity can change depending on the task.\nIn GORILA, multiple actors interact with the environment to gather transitions. Each actor has an independent copy of the environment, so they can gather N times more samples per second if there are N actors. This is possible in simulation (starting N instances of the same game in parallel) but much more complicated for real-world systems (but see Gu et al. (2017) for an example where multiple identical robots are used to gather experiences in parallel).\nThe experienced transitions are sent to the experience replay memory, which may be distributed or centralized. Multiple DQN learners will then sample a minibatch from the ERM and compute the DQN loss on this minibatch (also using a target network). All learners start with the same parameters \\theta and simply compute the gradient of the loss function \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta} on the minibatch. The gradients are sent to a parameter server (a master network) which uses the gradients to apply the optimizer (e.g. SGD) and find new values for the parameters \\theta. Weight updates can also be applied in a distributed manner. This distributed method to train a network using multiple learners is now quite standard in deep learning: on multiple GPU systems, each GPU has a copy of the network and computes gradients on a different minibatch, while a master network integrates these gradients and updates the slaves.\nThe parameter server regularly updates the actors (to gather samples with the new policy) and the learners (to compute gradients w.r.t the new parameter values). Such a distributed system can greatly accelerate learning, but it can be quite tricky to find the optimum number of actors and learners (too many learners might degrade the stability) or their update rate (if the learners are not updated frequently enough, the gradients might not be correct).\nThe final performance is not incredibly better than single-GPU DQN, but obtained much faster in wall-clock time (2 days instead of 12-14 days on a single GPU in 2015).",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributed learning</span>"
    ]
  },
  {
    "objectID": "src/2.5-DistributedLearning.html#distributed-dqn-gorila",
    "href": "src/2.5-DistributedLearning.html#distributed-dqn-gorila",
    "title": "Distributed learning",
    "section": "",
    "text": "the sample complexity, i.e. the number of transitions needed to learn a satisfying policy.\nthe online interaction with the environment (states are visited one after the other).\n\n\n\n\n\n\n\n\nFigure 10.1: Typical architecture of DQN using a single CPU and GPU. Source: Nair et al. (2015).\n\n\n\n\n\n\n\n\n\n\nFigure 10.2: GORILA architecture. Multiple actors interact with multiple copies of the environment and store their experiences in a (distributed) experience replay memory. Multiple DQN learners sample from the ERM and compute the gradient of the loss function w.r.t the parameters \\theta. A master network (parameter server, possibly distributed) gathers the gradients, apply weight updates and synchronizes regularly both the actors and the learners with new parameters. Source: Nair et al. (2015).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.3: Results of GORILA. Source: Nair et al. (2015).",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributed learning</span>"
    ]
  },
  {
    "objectID": "src/2.5-DistributedLearning.html#ape-x",
    "href": "src/2.5-DistributedLearning.html#ape-x",
    "title": "Distributed learning",
    "section": "Ape-X",
    "text": "Ape-X\nFurther variants of distributed DQN learning include Ape-X (Horgan et al., 2018) and IMPALA (Espeholt et al., 2018). In Ape-X, they realized that using a single learner and many many actors is actually more efficient. The ERM further uses prioritized experience replay to increase the efficiency. The learner uses n-step returns and the double dueling DQN network architecture, so it is not much different from Rainbow DQN internally.\nHowever, the multiple parallel workers can collect much more frames, leading to a much better performance in term of wall-clock time, but also pure performance (3x better than humans in only 20 hours of training, but using 360 CPU cores and one GPU).\n\n\n\n\n\n\n\nFigure 10.4: Results of Ape-X on the Atari benchmark. Source: Horgan et al. (2018)",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributed learning</span>"
    ]
  },
  {
    "objectID": "src/2.5-DistributedLearning.html#recurrent-replay-distributed-dqn-r2d2",
    "href": "src/2.5-DistributedLearning.html#recurrent-replay-distributed-dqn-r2d2",
    "title": "Distributed learning",
    "section": "Recurrent Replay Distributed DQN (R2D2)",
    "text": "Recurrent Replay Distributed DQN (R2D2)\nR2D2 (Kapturowski et al., 2019) builds on Ape-X and DRQN by combining:\n\na double dueling DQN with n-step returns (n=5) and prioritized experience replay.\n256 CPU actors, 1 GPU learner for distributed learning.\na LSTM layer after the convolutional stack to address POMDPs.\n\nAdditionally solving practical problems with LSTMs (choice of the initial state), it became for a moment the state of the art on the Atari-57 benchmark. The jump in performance from Ape-X is impressive. Distributed learning with multiple actor is now a standard technique, as it only necessitates a few more cores (or robots…).\n\n\n\n\n\n\nFigure 10.5: Results of R2D2 on the Atari benchmark. 20 times better than humans in only 120 hours of training… Source: Kapturowski et al. (2019).\n\n\n\n\n\n\n\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., et al. (2018). IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. doi:10.48550/arXiv.1802.01561.\n\n\nGu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates. in Proc. ICRA Available at: http://arxiv.org/abs/1610.00633.\n\n\nHorgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van Hasselt, H., et al. (2018). Distributed Prioritized Experience Replay. Available at: http://arxiv.org/abs/1803.00933 [Accessed December 14, 2019].\n\n\nKapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W. (2019). Recurrent experience replay in distributed reinforcement learning. in, 19. Available at: https://openreview.net/pdf?id=r1lyTjAqYX.\n\n\nNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., et al. (2015). Massively Parallel Methods for Deep Reinforcement Learning. Available at: https://arxiv.org/pdf/1507.04296.pdf.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributed learning</span>"
    ]
  },
  {
    "objectID": "src/2.6-Misc.html",
    "href": "src/2.6-Misc.html",
    "title": "Misc.",
    "section": "",
    "text": "Average-DQN (Anschel et al., 2016) proposes to increase the stability and performance of DQN by replacing the single target network (a copy of the trained network) by an average of the last parameter values, in other words an average of many past target networks.\nHe et al. (2016) proposed fast reward propagation through optimality tightening to speedup learning: when rewards are sparse, they require a lot of episodes to propagate these rare rewards to all actions leading to it. Their method combines immediate rewards (single steps) with actual returns (as in Monte Carlo) via a constrained optimization approach.\nNever Give Up: Learning Directed Exploration Strategies (Badia et al., 2020b)\nAgent57: Outperforming the Atari Human Benchmark (Badia et al., 2020a)\nHuman-level Atari 200x faster (Kapturowski et al., 2022)\n\n\n\n\n\nAnschel, O., Baram, N., and Shimkin, N. (2016). Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1611.01929.\n\n\nBadia, A. P., Piot, B., Kapturowski, S., Sprechmann, P., Vitvitskyi, A., Guo, D., et al. (2020a). Agent57: Outperforming the Atari Human Benchmark. Available at: http://arxiv.org/abs/2003.13350 [Accessed January 17, 2022].\n\n\nBadia, A. P., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot, B., Kapturowski, S., et al. (2020b). Never Give Up: Learning Directed Exploration Strategies. Available at: http://arxiv.org/abs/2002.06038 [Accessed January 17, 2022].\n\n\nHe, F. S., Liu, Y., Schwing, A. G., and Peng, J. (2016). Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening. Available at: http://arxiv.org/abs/1611.01606.\n\n\nKapturowski, S., Campos, V., Jiang, R., Rakićević, N., van Hasselt, H., Blundell, C., et al. (2022). Human-level Atari 200x faster. doi:10.48550/arXiv.2209.07550.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Misc.</span>"
    ]
  },
  {
    "objectID": "src/3.1-PolicyGradient.html",
    "href": "src/3.1-PolicyGradient.html",
    "title": "Policy Gradient methods",
    "section": "",
    "text": "Policy search\nApproximating directly the Q-values in value-based methods (DQN) suffers from many problems when using deep neural networks:\nInstead of learning the Q-values, one could approximate directly the policy \\pi_\\theta(s, a) with the neural network. \\pi_\\theta(s, a) is then called a parameterized policy, as it depends directly on the parameters \\theta of the NN. For discrete action spaces, the output of the NN can be a softmax layer, directly giving the probability of selecting an action. For continuous action spaces, the output layer can directly control the effector (e.g. the joint angles of a robotic arm). Parameterized policies can represent continuous policies and avoid the curse of dimensionality. See Section Continuous action spaces for more details.\nPolicy search methods directly learn to estimate the policy \\pi_\\theta with a parameterized function estimator. The goal of the neural network is to maximize an objective function representing the return (sum of rewards, noted R(\\tau) for simplicity) of the trajectories \\tau = (s_0, a_0, s_1, a_1, \\ldots, s_T, a_T) selected by the policy \\pi_\\theta:\nJ(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[R(\\tau)] = \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\sum_{t=0}^T \\gamma^t \\, r(s_t, a_t, s_{t+1}) ]\nTo maximize this objective function, the policy \\pi_\\theta should only generate trajectories \\tau associated with high returns R(\\tau) and avoid those with low return, which is exactly what we want.\nThe objective function uses the mathematical expectation of the return over all possible trajectories. The likelihood that a trajectory is generated by the policy \\pi_\\theta is noted \\rho_\\theta(\\tau) and given by:\n\\rho_\\theta(\\tau) = p_\\theta(s_0, a_0, \\ldots, s_T, a_T) = p_0 (s_0) \\, \\prod_{t=0}^T \\pi_\\theta(s_t, a_t) p(s_{t+1} | s_t, a_t)\np_0 (s_0) is the initial probability of starting in s_0 (independent from the policy) and p(s_{t+1} | s_t, a_t) is the transition probability defining the MDP. Having the probability distribution of the trajectories, we can expand the mathematical expectation in the objective function:\nJ(\\theta) = \\int_\\tau \\rho_\\theta (\\tau) \\, R(\\tau) \\, d\\tau\nMonte Carlo sampling could be used to estimate the objective function. One basically would have to sample multiple trajectories \\{\\tau_i\\} and average the obtained returns:\nJ(\\theta) \\approx \\frac{1}{N} \\, \\sum_{i=1}^N  R(\\tau_i)\nHowever, this approach would suffer from several problems:\nThe policy search methods presented in this section are called policy gradient methods. As we are going to apply gradient ascent on the weights \\theta in order to maximize J(\\theta), all we actually need is the gradient \\nabla_\\theta J(\\theta) of the objective function w.r.t the weights:\n\\nabla_\\theta J(\\theta) = \\frac{\\partial J(\\theta)}{\\partial \\theta}\nOnce a suitable estimation of this policy gradient is obtained, gradient ascent is straightforward:\n\\theta \\leftarrow \\theta + \\eta \\, \\nabla_\\theta J(\\theta)\nThe rest of this section basically presents methods allowing to estimate the policy gradient (REINFORCE, DPG) and to improve the sample complexity. See:\nThe article by Peters and Schaal (2008) is also a good overview of policy gradient methods.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Policy Gradient methods</span>"
    ]
  },
  {
    "objectID": "src/3.1-PolicyGradient.html#policy-search",
    "href": "src/3.1-PolicyGradient.html#policy-search",
    "title": "Policy Gradient methods",
    "section": "",
    "text": "Figure 12.2: We search for a policy that only generates trajectories associated wizh a high return.\n\n\n\n\n\n\n\n\n\n\n\n\nThe trajectory space is extremely huge, so one would need a lot of sampled trajectories to have a correct estimate of the objective function (high variance).\nFor stability reasons, only small changes can be made to the policy at each iteration, so it would necessitate a lot of episodes (sample complexity).\nFor continuing tasks (T = \\infty), the return can not be estimated as the episode never ends.\n\n\n\n\n\n\n\nhttp://www.scholarpedia.org/article/Policy_gradient_methods for an more detailed overview of policy gradient methods,\nhttps://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html and\nhttp://karpathy.github.io/2016/05/31/rl/ for excellent tutorials from Lilian Weng and Andrej Karpathy.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Policy Gradient methods</span>"
    ]
  },
  {
    "objectID": "src/3.1-PolicyGradient.html#reinforce",
    "href": "src/3.1-PolicyGradient.html#reinforce",
    "title": "Policy Gradient methods",
    "section": "REINFORCE",
    "text": "REINFORCE\n\nEstimating the policy gradient\nWilliams (1992) proposed a useful estimate of the policy gradient. Considering that the return R(\\tau) of a trajectory does not depend on the parameters \\theta, one can simplify the policy gradient in the following way:\n\n    \\nabla_\\theta J(\\theta) = \\nabla_\\theta \\int_\\tau \\rho_\\theta (\\tau) \\, R(\\tau) \\, d\\tau =  \\int_\\tau (\\nabla_\\theta \\rho_\\theta (\\tau)) \\, R(\\tau) \\, d\\tau\n\nWe now use the log-trick, a simple identity based on the fact that:\n\n    \\frac{d \\log f(x)}{dx} = \\frac{f'(x)}{f(x)}\n\nto rewrite the policy gradient of a single trajectory:\n\n    \\nabla_\\theta \\rho_\\theta (\\tau) = \\rho_\\theta (\\tau) \\, \\nabla_\\theta \\log \\rho_\\theta (\\tau)\n\nThe policy gradient becomes:\n\n    \\nabla_\\theta J(\\theta) =  \\int_\\tau \\rho_\\theta (\\tau) \\, \\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, R(\\tau) \\, d\\tau\n\nwhich now has the form of a mathematical expectation:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[ \\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, R(\\tau) ]\n\nThis means that we can obtain an estimate of the policy gradient by simply sampling different trajectories \\{\\tau_i\\} and averaging \\nabla_\\theta \\log \\rho_\\theta (\\tau_i) \\, R(\\tau_i) (Monte Carlo sampling).\nLet’s now look further at how the gradient of the log-likelihood of a trajectory \\log \\pi_\\theta (\\tau) look like. Through its definition, the log-likelihood of a trajectory is:\n\n    \\log \\rho_\\theta(\\tau) = \\log p_0 (s_0) + \\sum_{t=0}^T \\log \\pi_\\theta(s_t, a_t) + \\sum_{t=0}^T \\log p(s_{t+1} | s_t, a_t)\n\n\\log p_0 (s_0) and \\log p(s_{t+1} | s_t, a_t) do not depend on the parameters \\theta (they are defined by the MDP), so the gradient of the log-likelihood is simply:\n\n    \\nabla_\\theta \\log \\rho_\\theta(\\tau) = \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)\n\n\\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) is called the score function.\nThis is the main reason why policy gradient algorithms are used: the gradient is independent from the MDP dynamics, allowing model-free learning. The policy gradient is then given by:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, R(\\tau) ] =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, (\\sum_{t=0}^T \\gamma^t r_{t+1})]\n\nEstimating the policy gradient now becomes straightforward using Monte Carlo sampling. The resulting algorithm is called the REINFORCE algorithm (Williams, 1992):\n\n\n\n\n\n\nREINFORCE algorithm\n\n\n\nwhile not converged:\n\nSample N trajectories \\{\\tau_i\\} using the current policy \\pi_\\theta and observe the returns \\{R(\\tau_i)\\}.\nEstimate the policy gradient as an average over the trajectories:\n\n\n    \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, R(\\tau_i)\n\n\nUpdate the policy using gradient ascent:\n\n\n    \\theta \\leftarrow \\theta + \\eta \\, \\nabla_\\theta J(\\theta)\n\n\n\nWhile very simple, the REINFORCE algorithm does not work very well in practice:\n\nThe returns \\{R(\\tau_i)\\} have a very high variance (as the Q-values in value-based methods), which is problematic for NNs.\nIt requires a lot of episodes to converge (sample inefficient).\nIt only works with online learning: trajectories must be frequently sampled and immediately used to update the policy.\nThe problem must be episodic (T finite).\n\nHowever, it has two main advantages:\n\nIt is a model-free method, i.e. one does not need to know anything about the MDP.\nIt also works on partially observable problems (POMDP): as the return is computed over complete trajectories, it does not matter if the states are not Markovian.\n\nThe methods presented in this section basically try to solve the limitations of REINFORCE (high variance, sample efficiency, online learning) to produce efficient policy gradient algorithms.\n\n\nReducing the variance\nThe main problem with the REINFORCE algorithm is the high variance of the policy gradient. This variance comes from the fact that we learn stochastic policies (it is often unlikely to generate twice the exact same trajectory) in stochastic environments (rewards are stochastic, the same action in the same state may receive). Two trajectories which are identical at the beginning will be associated with different returns depending on the stochasticity of the policy, the transition probabilities and the probabilistic rewards.\nConsider playing a game like chess with always the same opening, and then following a random policy. You may end up winning (R=1) or losing (R=-1) with some probability. The initial actions of the opening will receive a policy gradient which is sometimes positive, sometimes negative: were these actions good or bad? Should they be reinforced? In supervised learning, this would mean that the same image of a cat will be randomly associated to the labels “cat” or “dog” during training: the NN will not like it.\nIn supervised learning, there is no problem of variance in the outputs, as training sets are fixed. This is in contrary very hard to ensure in deep RL and constitutes one of its main limitations. The only direct solution is to sample enough trajectories and hope that the average will be able to smooth the variance. The problem is even worse in the following conditions:\n\nHigh-dimensional action spaces: it becomes difficult to sample the environment densely enough if many actions are possible.\nLong horizons: the longer the trajectory, the more likely it will be unique.\nFinite samples: if we cannot sample enough trajectories, the high variance can introduce a bias in the gradient, leading to poor convergence.\n\nSee https://medium.com/mlreview/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565 for a nice explanation of the bias/variance trade-off in deep RL.\nAnother related problem is that the REINFORCE gradient is sensitive to reward scaling. Let’s consider a simple MDP where only two trajectories \\tau_1 and \\tau_2 are possible. Depending on the choice of the reward function, the returns may be different:\n\nR(\\tau_1) = 1 and R(\\tau_2) = -1\nR(\\tau_1) = 3 and R(\\tau_2) = 1\n\nIn both cases, the policy should select the trajectory \\tau_1. However, the policy gradient for \\tau_2 will change its sign between the two cases, although the problem is the same! What we want to do is to maximize the returns, regardless the absolute value of the rewards, but the returns are unbounded. Because of the non-stationarity of the problem (the agent becomes better with training, so the returns of the sampled trajectories will increase), the policy gradients will increase over time, what is linked to the variance problem. Value-based methods addressed this problem by using target networks, but it is not a perfect solution (the gradients become biased).\nA first simple but effective idea to solve both problems would be to subtract the mean of the sampled returns from the returns:\n\n\n\n\n\n\nREINFORCE algorithm with baseline\n\n\n\nwhile not converged:\n\nSample N trajectories \\{\\tau_i\\} using the current policy \\pi_\\theta and observe the returns \\{R(\\tau_i)\\}.\nCompute the mean return: \n  \\hat{R} = \\frac{1}{N} \\sum_{i=1}^N R(\\tau_i)\n\nEstimate the policy gradient as an average over the trajectories: \n  \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, ( R(\\tau_i) - \\hat{R})\n\nUpdate the policy using gradient ascent: \n  \\theta \\leftarrow \\theta + \\eta \\, \\nabla_\\theta J(\\theta)\n\n\n\n\nThis obviously solves the reward scaling problem, and reduces the variance of the gradients. But are we allowed to do this (i.e. does it introduce a bias to the gradient)? Williams (1992) showed that subtracting a constant b from the returns still leads to an unbiased estimate of the gradient:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, (R(\\tau) -b) ]\n\nThe proof is actually quite simple:\n\n\\begin{aligned}\n    \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, b ] & = \\int_\\tau \\rho_\\theta (\\tau) \\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, b \\, d\\tau \\\\\n    & = \\int_\\tau \\nabla_\\theta  \\rho_\\theta (\\tau) \\, b \\, d\\tau \\\\\n    &= b \\, \\nabla_\\theta \\int_\\tau \\rho_\\theta (\\tau) \\, d\\tau \\\\\n    &=  b \\, \\nabla_\\theta 1 \\\\\n    &= 0\n\\end{aligned}\n\nAs long as the constant b does not depend on \\theta, the estimator is unbiased. The resulting algorithm is called REINFORCE with baseline. Williams (1992) has actually showed that the best baseline (the one which also reduces the variance) is the mean return weighted by the square of the gradient of the log-likelihood:\n\n    b = \\frac{\\mathbb{E}_{\\tau \\sim \\rho_\\theta}[(\\nabla_\\theta \\log \\rho_\\theta (\\tau))^2 \\, R(\\tau)]}{\\mathbb{E}_{\\tau \\sim \\rho_\\theta}[(\\nabla_\\theta \\log \\rho_\\theta (\\tau))^2]}\n\nbut the mean reward actually work quite well. Advantage actor-critic methods replace the constant b with an estimate of the value of each state \\hat{V}(s_t).",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Policy Gradient methods</span>"
    ]
  },
  {
    "objectID": "src/3.1-PolicyGradient.html#policy-gradient-theorem",
    "href": "src/3.1-PolicyGradient.html#policy-gradient-theorem",
    "title": "Policy Gradient methods",
    "section": "Policy Gradient theorem",
    "text": "Policy Gradient theorem\nLet’s have another look at the REINFORCE estimate of the policy gradient after sampling:\n\n   \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, R(\\tau_i) = \\frac{1}{N} \\sum_{i=1}^N (\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) ) \\, (\\sum_{t'=0}^T \\gamma^{t'} \\, r(s_{t'}, a_{t'}, s_{t'+1}) )\n\nFor each transition (s_t, a_t), the gradient of its log-likelihood (score function) \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) ) is multiplied by the return of the whole episode R(\\tau) = \\sum_{t'=0}^T \\gamma^{t'} \\, r(s_{t'}, a_{t'}, s_{t'+1}). However, the causality principle dictates that the reward received at t=0 does not depend on actions taken in the future, so we can simplify the return for each transition:\n\n\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N (\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)  \\, \\sum_{t'=t}^T \\gamma^{t'-t} \\, r(s_{t'}, a_{t'}, s_{t'+1}) )\n\nThe quantity R_t = \\sum_{t'=t}^T \\gamma^{t'-t} \\, r(s_{t'}, a_{t'}, s_{t'+1}) is the return (or reward to-go) after the transition (s_t, a_t), i.e. the discounted sum of future rewards. Quite obviously, the Q-value of that action is the mathematical expectation of this quantity.\n\n\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, R_t\n\n\n\n\n\n\n\nFigure 12.3: The reward to-go is the sum of rewards gathered during a single trajectory after a transition (s, a). The Q-value of the action (s, a) is the expectation of the reward to-go. Source: S. Levine’s lecture http://rll.berkeley.edu/deeprlcourse/.\n\n\n\nSutton et al. (1999) showed that the policy gradient can be estimated by replacing the return of the sampled trajectory with the Q-value of each action, what leads to the policy gradient theorem:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)]\n\nwhere \\rho_\\theta is the distribution of states reachable under the policy \\pi_\\theta. Because the actual return R(\\tau) is replaced by its expectation Q^{\\pi_\\theta}(s, a), the policy gradient is now a mathematical expectation over single transitions instead of complete trajectories, allowing bootstrapping as in temporal difference methods.\nOne clearly sees that REINFORCE is actually a special case of the policy gradient theorem, where the Q-value of an action replaces the return obtained during the corresponding trajectory.\nThe problem is of course that the true Q-value of the actions is as unknown as the policy. However, Sutton et al. (1999) showed that it is possible to estimate the Q-values with a function approximator Q_\\varphi(s, a) with parameters \\varphi and obtain an unbiased estimation:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q_\\varphi(s, a))]\n\nFormally, the Q-value approximator must respect the Compatible Function Approximation Theorem, which states that the value approximator must be compatible with the policy (\\nabla_\\varphi Q_\\varphi(s, a) = \\nabla_\\theta \\log \\pi_\\theta(s, a)) and minimize the mean-square error with the true Q-values \\mathbb{E}_{s \\sim \\rho^\\pi, a \\sim \\pi_\\theta} [(Q^{\\pi_\\theta}(s, a) - Q_\\varphi(s, a))^2]. In the algorithms presented in this section, these conditions are either met or neglected.\nThe resulting algorithm belongs to the actor-critic class, in the sense that:\n\nThe actor \\pi_\\theta(s, a) learns to approximate the policy by using the policy gradient.\nThe critic Q_\\varphi(s, a) learns to estimate the Q-values of the actions generated by the policy.\n\nFigure 1 shows the architecture of the algorithm. The only problem left is to provide the critic with the true Q-values (Bellman targets).\n\n\n\n\n\n\nFigure 12.4: Architecture of the policy gradient (PG) method.\n\n\n\nThe critic can be trained with any advantage estimator, including Q-learning. It is common to use the DQN loss (or any variant of it: double DQN, n-step, etc) for the critic.\n\n\\mathcal{L}(\\varphi) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta} [(r(s, a, s') + \\gamma \\, Q_{\\varphi'}(s', \\text{argmax}_{a'} Q_\\varphi (s', a')) - Q_\\varphi (s, a) )^2]\n\n\n\n\n\nPeters, J., and Schaal, S. (2008). Reinforcement learning of motor skills with policy gradients. Neural Networks 21, 682–697. doi:10.1016/j.neunet.2008.02.003.\n\n\nSutton, R. S., McAllester, D., Singh, S., and Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. in Proceedings of the 12th International Conference on Neural Information Processing Systems (MIT Press), 1057–1063. Available at: https://dl.acm.org/citation.cfm?id=3009806.\n\n\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8, 229–256.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Policy Gradient methods</span>"
    ]
  },
  {
    "objectID": "src/3.2-ActorCritic.html",
    "href": "src/3.2-ActorCritic.html",
    "title": "Advantage Actor-Critic (A3C)",
    "section": "",
    "text": "Actor-critic algorithms\nThe policy gradient theorem provides an actor-critic arhictecture that allow to estimate the PG from single transitions:\n\\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q_\\varphi(s, a))]\nThe critic can be trained with any advantage estimator, including Q-learning. It is common to use the DQN loss (or any variant of it: double DQN, n-step, etc) for the critic.\n\\mathcal{L}(\\varphi) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta} [(r(s, a, s') + \\gamma \\, Q_{\\varphi'}(s', \\text{argmax}_{a'} Q_\\varphi (s', a')) - Q_\\varphi (s, a) )^2]\nMost policy-gradient algorithms in this section are actor-critic architectures. The different versions of the policy gradient take the form:\n\\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho^\\pi, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, \\psi_t ]\nwhere:\nGenerally speaking:\nThis is the classical bias/variance trade-off in machine learning. n-step advantages are an attempt to mitigate between these extrema. Schulman et al. (2015) proposed the Generalized Advantage Estimate (GAE, see Section Generalized Advantage Estimation (GAE)) to further control the bias/variance trade-off.\nNote: A2C is actually derived from the A3C algorithm presented later, but it is simpler to explain it first. See https://openai.com/index/openai-baselines-acktr-a2c/ for an explanation of the reasons. A good explanation of A2C and A3C with Python code is available at https://cgnicholls.github.io/reinforcement-learning/2017/03/27/a3c.html.",
    "crumbs": [
      "**Policy-gradient methods**",
      "Advantage Actor-Critic (A3C)"
    ]
  },
  {
    "objectID": "src/3.2-ActorCritic.html#actor-critic-algorithms",
    "href": "src/3.2-ActorCritic.html#actor-critic-algorithms",
    "title": "Advantage Actor-Critic (A3C)",
    "section": "",
    "text": "Figure 1: Architecture of the policy gradient (PG) method.\n\n\n\n\n\n\n\n\n\n\\psi_t = R_t is the REINFORCE algorithm (MC sampling).\n\\psi_t = R_t - b is the REINFORCE with baseline algorithm.\n\\psi_t = Q^\\pi(s_t, a_t) is the policy gradient theorem.\n\\psi_t = A^\\pi(s_t, a_t) is the advantage actor-critic.\n\\psi_t = r_{t+1} + \\gamma \\, V^\\pi(s_{t+1}) - V^\\pi(s_t) is the TD actor-critic.\n\\psi_t = \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V^\\pi(s_{t+n+1}) - V^\\pi(s_t) is the n-step advanatge actor-critic (A2C).\n\n\n\nthe more \\psi_t relies on real rewards (e.g. R_t), the more the gradient will be correct on average (small bias), but the more it will vary (high variance). This increases the sample complexity: we need to average more samples to correctly estimate the gradient.\nthe more \\psi_t relies on estimations (e.g. the TD error), the more stable the gradient (small variance), but the more incorrect it is (high bias). This can lead to suboptimal policies, i.e. local optima of the objective function.",
    "crumbs": [
      "**Policy-gradient methods**",
      "Advantage Actor-Critic (A3C)"
    ]
  },
  {
    "objectID": "src/3.2-ActorCritic.html#advantage-actor-critic-a2c",
    "href": "src/3.2-ActorCritic.html#advantage-actor-critic-a2c",
    "title": "Advantage Actor-Critic (A3C)",
    "section": "Advantage Actor-Critic (A2C)",
    "text": "Advantage Actor-Critic (A2C)\nThe first aspect of A2C is that it relies on n-step updating, which is a trade-off between MC and TD:\n\nMC waits until the end of an episode to update the value of an action using the reward to-go (sum of obtained rewards) R(s, a).\nTD updates immediately the action using the immediate reward r(s, a, s') and approximates the rest with the value of the next state V^\\pi(s).\nn-step uses the n next immediate rewards and approximates the rest with the value of the state visited n steps later.\n\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho^\\pi, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, ( \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n+1}) - V_\\varphi(s_t))]\n\nA2C has an actor-critic architecture:\n\nThe actor outputs the policy \\pi_\\theta for a state s, i.e. a vector of probabilities for each action.\nThe critic outputs the value V_\\varphi(s) of a state s.\n\n\n\n\n\n\n\nFigure 2: Advantage actor-critic architecture.\n\n\n\nHaving a computable formula for the policy gradient, the algorithm is rather simple:\n\nAcquire a batch of transitions (s, a, r, s') using the current policy \\pi_\\theta (either a finite episode or a truncated one).\nFor each state encountered, compute the discounted sum of the next n rewards \\sum_{k=0}^{n} \\gamma^{k} \\, r_{t+k+1} and use the critic to estimate the value of the state encountered n steps later V_\\varphi(s_{t+n+1}).\n\n\n    R_t = \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n+1})\n\n\nUpdate the actor.\n\n\n    \\nabla_\\theta J(\\theta) =  \\sum_t \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, (R_t - V_\\varphi(s_t))\n\n\nUpdate the critic to minimize the TD error between the estimated value of a state and its true value.\n\n\n    \\mathcal{L}(\\varphi) = \\sum_t (R_t - V_\\varphi(s_t))^2\n\n\nRepeat.\n\nThis is not very different in essence from REINFORCE (sample transitions, compute the return, update the policy), apart from the facts that episodes do not need to be finite and that a critic has to be learned in parallel. A more detailed pseudo-algorithm for a single A2C learner is the following:\n\n\n\n\n\n\nA2C algorithm for a single worker/learner\n\n\n\n\nInitialize the actor \\pi_\\theta and the critic V_\\varphi with random weights.\nObserve the initial state s_0.\nwhile not converged:\n\nInitialize empty episode minibatch.\nfor k \\in [0, n]: # Sample episode\n\nSelect a action a_k using the actor \\pi_\\theta.\nPerform the action a_k and observe the next state s_{k+1} and the reward r_{k+1}.\nStore (s_k, a_k, r_{k+1}) in the episode minibatch.\n\nif s_n is not terminal: set R = V_\\varphi(s_n) with the critic, else R=0.\nReset gradient d\\theta and d\\varphi to 0.\nfor k \\in [n-1, 0]: # Backwards iteration over the episode\n\nUpdate the discounted sum of rewards R = r_k + \\gamma \\, R\nAccumulate the policy gradient using the critic:\n\n\n      d\\theta \\leftarrow d\\theta + \\nabla_\\theta \\log \\pi_\\theta(s_k, a_k) \\, (R - V_\\varphi(s_k))\n  \n\nAccumulate the critic gradient:\n\n\n      d\\varphi \\leftarrow d\\varphi + \\nabla_\\varphi (R - V_\\varphi(s_k))^2\n  \nUpdate the actor and the critic with the accumulated gradients using gradient descent or similar:\n\n\n      \\theta \\leftarrow \\theta + \\eta \\, d\\theta \\qquad \\varphi \\leftarrow \\varphi + \\eta \\, d\\varphi\n  \n\n\n\nNote that not all states are updated with the same horizon n: the last action encountered in the sampled episode will only use the last reward and the value of the final state (TD learning), while the very first action will use the n accumulated rewards. In practice it does not really matter, but the choice of the discount rate \\gamma will have a significant influence on the results.\nAs many actor-critic methods, A2C performs online learning: a couple of transitions are explored using the current policy, which is immediately updated. As for value-based networks (e.g. DQN), the underlying NN will be affected by the correlated inputs and outputs: a single batch contains similar states and action (e.g. consecutive frames of a video game). The solution retained in A2C and A3C does not depend on an experience replay memory as DQN, but rather on the use of multiple parallel actors and learners (see Section Distributed learning).\nThe idea is depicted on Figure 3 (actually for A3C, but works with A2C). The actor and critic are stored in a global network. Multiple instances of the environment are created in different parallel threads (the workers). At the beginning of an episode, each worker receives a copy of the actor and critic weights from the global network. Each worker samples an episode (starting from different initial states, so the episodes are uncorrelated), computes the accumulated gradients and sends them back to the global network. The global networks merges the gradients and uses them to update the parameters of the policy and critic networks. The new parameters are send to each worker again, until it converges.\n\n\n\n\n\n\nDistributed A2C algorithm\n\n\n\n\nInitialize the actor \\pi_\\theta and the critic V_\\varphi in the global network.\nwhile not converged:\n\nfor each worker i in parallel:\n\nGet a copy of the global actor \\pi_\\theta and critic V_\\varphi.\nSample an episode of n steps.\nReturn the accumulated gradients d\\theta_i and d\\varphi_i.\n\nWait for all workers to terminate.\nMerge all accumulated gradients into d\\theta and d\\varphi.\nUpdate the global actor and critic networks.\n\n\n\n\nThis solves the problem of correlated inputs and outputs, as each worker explores different regions of the environment (one can set different initial states in each worker, vary the exploration rate, etc), so the final batch of transitions used for training the global networks is much less correlated. The only drawback of this approach is that it has to be possible to explore multiple environments in parallel. This is easy to achieve in simulated environments (e.g. video games) but much harder in real-world systems like robots. A brute-force solution for robotics is simply to buy enough robots and let them learn in parallel (Gu et al., 2017).",
    "crumbs": [
      "**Policy-gradient methods**",
      "Advantage Actor-Critic (A3C)"
    ]
  },
  {
    "objectID": "src/3.2-ActorCritic.html#asynchronous-advantage-actor-critic-a3c",
    "href": "src/3.2-ActorCritic.html#asynchronous-advantage-actor-critic-a3c",
    "title": "Advantage Actor-Critic (A3C)",
    "section": "Asynchronous Advantage Actor-Critic (A3C)",
    "text": "Asynchronous Advantage Actor-Critic (A3C)\n\n\n\n\n\n\nFigure 3: Distributed architecture of A3C. A global network interacts asynchronously with several workers, each having a copy of the network and interacting with a separate environment. At the end of an episode, the accumulated gradients are sent back to the master network, and a new value of the parameters is sent to the workers.\n\n\n\nAsynchronous Advantage Actor-Critic (A3C, Mnih et al., 2016) extends the approach of A2C by removing the need of synchronization between the workers at the end of each episode before applying the gradients. The rationale behind this is that each worker may need different times to complete its task, so they need to be synchronized. Some workers might then be idle most of the time, what is a waste of resources. Gradient merging and parameter updates are sequential operations, so no significant speedup is to be expected even if one increases the number of workers.\nThe solution retained in A3C is to simply skip the synchronization step: each worker reads and writes the network parameters whenever it wants. Without synchronization barriers, there is of course a risk that one worker tries to read the network parameters while another writes them: the obtained parameters would be a mix of two different networks. Surprisingly, it does not matter: if the learning rate is small enough, there is anyway not a big difference between two successive versions of the network parameters. This kind of “dirty” parameter sharing is called HogWild! updating (Niu et al., 2011) and has been proven to work under certain conditions which are met here.\nThe resulting A3C pseudocode is summarized here:\n\n\n\n\n\n\nDistributed A3C algorithm\n\n\n\n\nInitialize the actor \\pi_\\theta and the critic V_\\varphi in the global network.\nfor each worker i in parallel:\n\nrepeat:\n\nGet a copy of the global actor \\pi_\\theta and critic V_\\varphi.\nSample an episode of n steps.\nCompute the accumulated gradients d\\theta_i and d\\varphi_i.\nUpdate the global actor and critic networks asynchronously (HogWild!).\n\n\n\n\n\nThe workers are fully independent: their only communication is through the asynchronous updating of the global networks. This can lead to very efficient parallel implementations: in the original A3C paper (Mnih et al., 2016), they solved the same Atari games than DQN using 16 CPU cores instead of a powerful GPU, while achieving a better performance in less training time (1 day instead of 8). The speedup is almost linear: the more workers, the faster the computations, the better the performance (as the policy updates are less correlated).\n\nEntropy regularization\nAn interesting addition in A3C is the way they enforce exploration during learning. In actor-critic methods, exploration classically relies on the fact that the learned policies are stochastic (on-policy): \\pi(s, a) describes the probability of taking the action a in the state s. In discrete action spaces, the output of the actor can be a softmax layer, ensuring that all actions get a non-zero probability of being selected during training. In continuous action spaces, the executed action is sampled from the output probability distribution. However, this is often not sufficient and hard to control.\nIn A3C, the authors added an entropy regularization term (Williams and Peng, 1991) to the policy gradient update:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho^\\pi, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, ( R_t - V_\\varphi(s_t)) + \\beta \\, \\nabla_\\theta H(\\pi_\\theta(s_t))]\n\nFor discrete actions, the entropy of the policy for a state s_t is simple to compute:\n\n    H(\\pi_\\theta(s_t)) = - \\sum_a \\pi_\\theta(s_t, a) \\, \\log \\pi_\\theta(s_t, a)\n\nIt measures the “randomness” of the policy: if the policy is fully deterministic (the same action is systematically selected), the entropy is zero as it carries no information. If the policy is completely random, the entropy is maximal. Maximizing the entropy at the same time as the returns improves exploration by forcing the policy to be as non-deterministic as possible.\nSee Section Maximum Entropy RL (SAC) for more details on using the entropy for exploration.",
    "crumbs": [
      "**Policy-gradient methods**",
      "Advantage Actor-Critic (A3C)"
    ]
  },
  {
    "objectID": "src/3.2-ActorCritic.html#actor-critic-neural-architectures",
    "href": "src/3.2-ActorCritic.html#actor-critic-neural-architectures",
    "title": "Advantage Actor-Critic (A3C)",
    "section": "Actor-critic neural architectures",
    "text": "Actor-critic neural architectures\nWe have considered that actor-critic architectures consist of two separate neural networks, the actor \\pi(s, a) and the critic Q(s, a) both taking the state s (or observation o) as an input and outputing one value per action. Each of these networks have their own loss function. They share nothing except the “data”. Is it really the best option?\nWhen working on images, the first few layers of the CNNs are likely to learn the same visual features (edges, contours). It would be more efficient to share some of the extracted features. Actor-critic architectures can share layers between the actor and the critic, sometimes up to the output layer. A compound loss sums the losses for the actor and the critic. Tensorflow/pytorch know which parameters influence which part of the loss.\n\n    \\mathcal{L}(\\theta) = \\mathcal{L}_\\text{actor}(\\theta) + \\mathcal{L}_\\text{critic}(\\theta)\n\nFor pixel-based environments (Atari), the networks often share the convolutional layers. For continuous environments (Mujoco), separate networks sometimes work better than two-headed networks.\n\n\n\n\n\n\nFigure 4: The actor and the critic can share no/some/most layers, depending on the algorithm and the application.",
    "crumbs": [
      "**Policy-gradient methods**",
      "Advantage Actor-Critic (A3C)"
    ]
  },
  {
    "objectID": "src/3.2-ActorCritic.html#sec-continuousspaces",
    "href": "src/3.2-ActorCritic.html#sec-continuousspaces",
    "title": "Advantage Actor-Critic (A3C)",
    "section": "Continuous action spaces",
    "text": "Continuous action spaces\nThe actor-critic methods presented above use stochastic policies \\pi_\\theta(s, a) assigning parameterized probabilities of being selecting to each (s, a) pair.\n\nWhen the action space is discrete, the output layer of the actor is simply a softmax layer with as many neurons as possible actions in each state, making sure the probabilities sum to one. It is then straightforward to sample an action from this layer.\nWhen the action space is continuous, one has to make an assumption on the underlying distribution. The actor learns the parameters of the distribution and the executed action is simply sampled from the parameterized distribution.\n\nSuppose that we want to control a robotic arm with n degrees of freedom. An action \\mathbf{a} could be a vector of joint displacements:\n\\mathbf{a} = \\begin{bmatrix} \\Delta \\theta_1 & \\Delta \\theta_2 & \\ldots \\, \\Delta \\theta_n\\end{bmatrix}^T\nThe output layer of the policy network can very well represent this vector, but how would we implement exploration? \\epsilon-greedy and softmax action selection would not work, as all output neurons are useful.\nThe most common solution is to use a stochastic Gaussian policy, based on the Gaussian distribution. In this case, the output of the actor is a mean vector \\mu_\\theta(s) and a variance vector \\sigma_\\theta(s), providing the parameters of the normal distribution. The policy \\pi_\\theta(s, a) = \\mathcal{N}(\\mu_\\theta(s), \\sigma^2_\\theta(s)) is then simply defined as:\n\n    \\pi_\\theta(s, a) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_\\theta(s)}} \\, \\exp -\\frac{(a - \\mu_\\theta(s))^2}{2\\sigma_\\theta(s)^2}\n\n\n\n\n\n\n\nFigure 5: Reparameterization trick to implement continuous stochastic Gaussian policies.\n\n\n\nIn order to use backpropagation on the policy gradient (i.e. getting an analytical form of the score function \\nabla_\\theta \\log \\pi_\\theta (s, a)), one can use the reparameterization trick (Heess et al., 2015) by rewriting the policy as:\n\n    a = \\mu_\\theta(s) + \\sigma_\\theta(s) \\times \\xi \\qquad \\text{where} \\qquad \\xi \\sim \\mathcal{N}(0,1)\n\nTo select an action, we only need to sample \\xi from the unit normal distribution, multiply it by the standard deviation and add the mean. To compute the score function, we use the following partial derivatives:\n\n    \\nabla_\\mu \\log \\pi_\\theta (s, a) = \\frac{a - \\mu_\\theta(s)}{\\sigma_\\theta(s)^2} \\qquad \\nabla_\\sigma \\log \\pi_\\theta (s, a) = \\frac{(a - \\mu_\\theta(s))^2}{\\sigma_\\theta(s)^3} - \\frac{1}{\\sigma_\\theta(s)}\n\nand use the chain rule to obtain the score function. The reparameterization trick is a cool trick to apply backpropagation on stochastic problems: it is for example used in the variational auto-encoders [VAE; Kingma and Welling (2013)].\nDepending on the problem, one could use: 1) a fixed \\sigma for the whole action space, 2) a fixed \\sigma per DoF, 3) a learnable \\sigma per DoF (assuming all action dimensions to be mutually independent) or even 4) a covariance matrix \\Sigma when the action dimensions are dependent.\nOne limitation of Gaussian policies is that their support is infinite: even with a small variance, samples actions can deviate a lot (albeit rarely) from the mean. This is particularly a problem when action must have a limited range: the torque of an effector, the linear or angular speed of a car, etc. Clipping the sampled action to minimal and maximal values introduces a bias which can impair learning. Chou et al. (2017) proposed to use beta-distributions instead of Gaussian ones in the actor. Sampled values have a [0,1] support, which can rescaled to [v_\\text{min},v_\\text{max}] easily. They show that beta policies have less bias than Gaussian policies in most continuous problems.\n\n\n\n\nChou, P.-W., Maturana, D., and Scherer, S. (2017). Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution. in International Conference on Machine Learning Available at: http://proceedings.mlr.press/v70/chou17a/chou17a.pdf.\n\n\nGu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates. in Proc. ICRA Available at: http://arxiv.org/abs/1610.00633.\n\n\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T. (2015). Learning continuous control policies by stochastic value gradients. Proc. International Conference on Neural Information Processing Systems, 2944–2952. Available at: http://dl.acm.org/citation.cfm?id=2969569.\n\n\nKingma, D. P., and Welling, M. (2013). Auto-Encoding Variational Bayes. Available at: http://arxiv.org/abs/1312.6114.\n\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., et al. (2016). Asynchronous Methods for Deep Reinforcement Learning. in Proc. ICML Available at: http://arxiv.org/abs/1602.01783.\n\n\nNiu, F., Recht, B., Re, C., and Wright, S. J. (2011). HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. in Proc. Advances in Neural Information Processing Systems, 21–21. Available at: http://arxiv.org/abs/1106.5730.\n\n\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust Region Policy Optimization. in Proceedings of the 31 st International Conference on Machine Learning, 1889–1897. Available at: http://proceedings.mlr.press/v37/schulman15.html.\n\n\nWilliams, R. J., and Peng, J. (1991). Function optimization using connectionist reinforcement learning algorithms. Connection Science 3, 241–268.",
    "crumbs": [
      "**Policy-gradient methods**",
      "Advantage Actor-Critic (A3C)"
    ]
  },
  {
    "objectID": "src/3.3-ImportanceSampling.html",
    "href": "src/3.3-ImportanceSampling.html",
    "title": "Off-policy Actor-Critic",
    "section": "",
    "text": "On-policy vs. off-policy\nActor-critic architectures are generally on-policy algorithms: the actions used to explore the environment must have been generated by the actor, otherwise the feedback provided by the critic (the advantage) will introduce a huge bias (i.e. an error) in the policy gradient. This comes from the definition of the policy gradient theorem:\n\\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho^\\pi, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)]\nThe state distribution \\rho^\\pi defines the ensemble of states that can be visited using the actor policy \\pi_\\theta. If, during Monte Carlo sampling of the policy gradient, the states s do not come from this state distribution, the approximated policy gradient will be wrong (high bias) and the resulting policy will be suboptimal.\nThe major drawback of on-policy methods is their sample complexity: it is difficult to ensure that the “interesting” regions of the policy are actually discovered by the actor (see Figure 13.1). If the actor is initialized in a flat region of the reward space (where there is not a lot of rewards), policy gradient updates will only change slightly the policy and it may take a lot of iterations until interesting policies are discovered and fine-tuned.\nThe problem becomes even worse when the state or action spaces are highly dimensional, or when rewards are sparse. Imagine the scenario where you are searching for your lost keys at home (a sparse reward is delivered only once you find them): you could spend hours trying randomly each action at your disposal (looking in your jackets, on your counter, but also jumping around, cooking something, watching TV…) until finally you explore the action “look behind the curtains” and find them. (Note: with deep RL, you would even have to do that one million times in order to allow gradient descent to train your brain…). If you had somebody telling you “if I were you, I would first search in your jackets, then on your counter and finally behind the curtains, but forget about watching TV, you will never find anything by doing that”, this would certainly reduce your exploration time.\nThis is somehow the idea behind off-policy algorithms: they use a behavior policy b(s, a) to explore the environment and train the target policy \\pi(s, a) to reproduce the best ones by estimating how good they are. This does not come without caveats: if the behavior policy does not explore the optimal actions, the target policy will likely not be able to find it by itself, except by chance. But if the behavior policy is good enough, this can drastically reduce the amount of exploration needed to obtain a satisfying policy. Sutton and Barto (2017) noted that:\nThe most famous off-policy method is Q-learning. The reason why it is off-policy is that it does not use the next executed action (a_{t+1}) to update the value of an action, but the greedy action in the next state, which is independent from exploration:\n\\delta = r(s, a, s') + \\gamma \\, \\max_{a'} Q^\\pi(s', a') - Q^\\pi(s, a)\nThe only condition for Q-learning to work (in the tabular case) is that the behavior policy b(s,a) must be able to explore actions which are selected by the target policy:\n\\pi(s, a) &gt; 0 \\rightarrow b(s, a) &gt; 0\nActions which would be selected by the target policy should be selected at least from time to time by the behavior policy in order to allow their update: if the target policy thinks this action should be executed, the behavior policy should try it to confirm or infirm this assumption. In mathematical terms, there is an assumption of coverage of \\pi by b (the support of b includes the one of \\pi).\nThere are mostly two ways to create the behavior policy:\nThe second option allows to control the level of exploration during learning (by controlling \\epsilon or the softmax temperature) while making sure that the target policy (the one used in production) is deterministic and optimal. It furthermore makes sure that the coverage assumption is respected: the greedy action of the target policy always has a non-zero probability of being selected by an \\epsilon-greedy or softmax action selection. This is harder to ensure using expert knowledge.\nQ-learning methods such as DQN use this second option. The target policy in DQN is actually a greedy policy with respect to the Q-values (i.e. the action with the maximum Q-value will be deterministically chosen), but an \\epsilon-soft behavior policy is derived from it to ensure exploration. This explains now the following comment in the description of the DQN algorithm:\nOff-policy learning furthermore allows the use of an experience replay memory: in this case, the transitions used for training the target policy were generated by an older version of it (sometimes much older). Only off-policy methods can work with replay buffers. A3C is for example on-policy: it relies on multiple parallel learners to fight against the correlation of inputs and outputs.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Off-policy Actor-Critic</span>"
    ]
  },
  {
    "objectID": "src/3.3-ImportanceSampling.html#on-policy-vs.-off-policy",
    "href": "src/3.3-ImportanceSampling.html#on-policy-vs.-off-policy",
    "title": "Off-policy Actor-Critic",
    "section": "",
    "text": "Figure 13.1: Illustration of the sample complexity inherent to on-policy methods, where the actor has only two parameters \\theta_1 and \\theta_2. If only very small regions of the actor parameters are associated with high rewards, the policy might wander randomly for a very long time before “hitting”the interesting regions.\n\n\n\n\n\n\nOn-policy methods are generally simpler and are considered first. Off-policy methods require additional concepts and notation, and because the data is due to a different policy, off-policy methods are often of greater variance and are slower to converge.\n\n\n\n\n\n\n\n\nUse expert knowledge / human demonstrations. Not all available actions should be explored: the programmer already knows they do not belong to the optimal policy. When an agent learns to play chess, for example, the behavior policy could consist of the moves typically played by human experts: if chess masters play this move, it is likely to be a good action, so it should be tried out, valued and possibly incorporated into the target policy (if it is indeed a good action, experts might be wrong). A similar idea was used to bootstrap early versions of AlphaGo (Silver et al., 2016). In robotics, one could for example use “classical” engineering methods to control the exploration of the robot, while learning (hopefully) a better policy. It is also possible to perform imitation learning, where the agent learns from human demonstrations (e.g. Levine and Koltun (2013)).\nDerive it from the target policy. In Q-learning, the target policy can be deterministic, i.e. always select the greedy action (with the maximum Q-value). The behavior policy can be derived from the target policy by making it \\epsilon-soft, for example using a \\epsilon-greedy or softmax action selection scheme on the Q-values learned by the target policy.\n\n\n\n\nSelect the action a_t based on the behavior policy derived from Q_\\theta(s_t, a) (e.g. softmax).",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Off-policy Actor-Critic</span>"
    ]
  },
  {
    "objectID": "src/3.3-ImportanceSampling.html#importance-sampling",
    "href": "src/3.3-ImportanceSampling.html#importance-sampling",
    "title": "Off-policy Actor-Critic",
    "section": "Importance sampling",
    "text": "Importance sampling\nOff-policy methods learn a target policy \\pi(s,a) while exploring with a behavior policy b(s,a). The environment is sampled using the behavior policy to form estimates of the state or action values (for value-based methods) or of the policy gradient (for policy gradient methods). But is it mathematically correct?\nIn policy gradient methods, we want to maximize the expected return of trajectories:\n\n    J(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[R(\\tau)] = \\int_\\tau \\rho_\\theta(\\tau) \\, R(\\tau) \\, d\\tau \\approx \\frac{1}{N} \\sum_{i=1}^N R(\\tau_i)\n\nwhere \\rho_\\theta is the distribution of trajectories \\tau generated by the target policy \\pi_\\theta. Mathematical expectations can be approximating by an average of enough samples of the estimator (Monte Carlo). In policy gradient, we estimate the gradient, but let’s consider we sample the objective function for now. If we use a behavior policy to generate the trajectories, what we are actually estimating is:\n\n    \\hat{J}(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_b}[R(\\tau)] = \\int_\\tau \\rho_b(\\tau) \\, R(\\tau) \\, d\\tau\n\nwhere \\rho_b is the distribution of trajectories generated by the behavior policy. In the general case, there is no reason why \\hat{J}(\\theta) should be close from J(\\theta), even when taking their gradient.\nAs seen in Section Importance sampling, importance sampling is a classical statistical method used to estimate properties of a distribution (here the expected return of the trajectories of the target policy) while only having samples generated from a different distribution (here the trajectories of the behavior policy). See for example https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf and http://timvieira.github.io/blog/post/2014/12/21/importance-sampling for more generic explanations.\nThe trick is simply to rewrite the objective function as:\n\n\\begin{aligned}\n    J(\\theta) & = \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[R(\\tau)]  \\\\\n              & = \\int_\\tau \\rho_\\theta(\\tau) \\, R(\\tau) \\, d\\tau \\\\\n              & = \\int_\\tau \\frac{\\rho_b(\\tau)}{\\rho_b(\\tau)} \\, \\rho_\\theta(\\tau) \\, R(\\tau) \\, d\\tau \\\\\n              & = \\int_\\tau \\rho_b(\\tau) \\frac{\\rho_\\theta(\\tau)}{\\rho_b(\\tau)} \\, R(\\tau) \\, d\\tau \\\\\n              & = \\mathbb{E}_{\\tau \\sim \\rho_b}[\\frac{\\rho_\\theta(\\tau)}{\\rho_b(\\tau)} \\, R(\\tau)]  \\\\\n\\end{aligned}\n\nThe ratio \\frac{\\rho_\\theta(\\tau)}{\\rho_b(\\tau)} is called the importance sampling weight for the trajectory. If a trajectory generated by b is associated with a lot of rewards R(\\tau) (with \\rho_b(\\tau) significantly high), the actor should learn to reproduce that trajectory with a high probability \\rho_\\theta(\\tau), as its goal is to maximize J(\\theta). Conversely, if the associated reward is low (R(\\tau)\\approx 0), the target policy can forget about it (by setting \\rho_\\theta(\\tau) = 0), even though the behavior policy still generates it!\nThe problem is now to estimate the importance sampling weight. Using the definition of the likelihood of a trajectory, the importance sampling weight only depends on the policies, not the dynamics of the environment (they cancel out):\n\n    \\frac{\\rho_\\theta(\\tau)}{\\rho_b(\\tau)} = \\frac{p_0 (s_0) \\, \\prod_{t=0}^T \\pi_\\theta(s_t, a_t) p(s_{t+1} | s_t, a_t)}{p_0 (s_0) \\, \\prod_{t=0}^T b(s_t, a_t) p(s_{t+1} | s_t, a_t)} = \\frac{\\prod_{t=0}^T \\pi_\\theta(s_t, a_t)}{\\prod_{t=0}^T b(s_t, a_t)} = \\prod_{t=0}^T \\frac{\\pi_\\theta(s_t, a_t)}{b(s_t, a_t)}\n\nThis allows to estimate the objective function J(\\theta) using Monte Carlo sampling (Meuleau et al., 2000; Peshkin and Shelton, 2002):\n\n  J(\\theta) \\approx \\frac{1}{m} \\, \\sum_{i=1}^m \\frac{\\rho_\\theta(\\tau_i)}{\\rho_b(\\tau_i)} \\, R(\\tau_i)\n\nAll one needs to do is to repeatedly apply the following algorithm:\n\n\n\n\n\n\nOff-policy Monte-Carlo Policy search\n\n\n\n\nGenerate m trajectories \\tau_i using the behavior policy:\n\nFor each transition (s_t, a_t, s_{t+1}) of each trajectory, store:\n\nThe received reward r_{t+1}.\nThe probability b(s_t, a_t) that the behavior policy generates this transition.\nThe probability \\pi_\\theta(s_t, a_t) that the target policy generates this transition.\n\n\nEstimate the objective function with:\n\n\n  \\hat{J}(\\theta) = \\frac{1}{m} \\, \\sum_{i=1}^m \\left(\\prod_{t=0}^T \\frac{\\pi_\\theta(s_t, a_t)}{b(s_t, a_t)} \\right) \\, \\left(\\sum_{t=0}^T \\gamma^t \\, r_{t+1} \\right)\n\n\nUpdate the target policy to maximize \\hat{J}(\\theta).\n\n\n\nTang and Abbeel (2010) showed that the same idea can be applied to the policy gradient, under assumptions often met in practice:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_b}[ \\nabla_\\theta \\log \\rho_\\theta(\\tau) \\, \\frac{\\rho_\\theta(\\tau)}{\\rho_b(\\tau)} \\, R(\\tau)]\n\nWhen decomposing the policy gradient for each state encountered, one can also use the causality principle to simplify the terms:\n\nThe return after being in a state s_t only depends on future states.\nThe importance sampling weight (relative probability of arriving in s_t using the behavior and target policies) only depends on the past weights.\n\nThis gives the following approximation of the policy gradient, used for example in Guided policy search (Levine and Koltun, 2013):\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_b}[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, \\left(\\prod_{t'=0}^t \\frac{\\pi_\\theta(s_{t'}, a_{t'})}{b(s_{t'}, a_{t'})} \\right) \\, \\left(\\sum_{t'=t}^T \\gamma^{t'-t} \\, r(s_{t'}, a_{t'}) \\right)]",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Off-policy Actor-Critic</span>"
    ]
  },
  {
    "objectID": "src/3.3-ImportanceSampling.html#linear-off-policy-actor-critic-off-pac",
    "href": "src/3.3-ImportanceSampling.html#linear-off-policy-actor-critic-off-pac",
    "title": "Off-policy Actor-Critic",
    "section": "Linear Off-Policy Actor-Critic (Off-PAC)",
    "text": "Linear Off-Policy Actor-Critic (Off-PAC)\nThe first off-policy actor-critic method was proposed by Degris et al. (2012) for linear approximators. Another way to express the objective function in policy search is by using the Bellman equation (here in the off-policy setting):\n\n    J(\\theta) = \\mathbb{E}_{s \\sim \\rho_b} [V^{\\pi_\\theta}(s)] = \\mathbb{E}_{s \\sim \\rho_b} [\\sum_{a\\in\\mathcal{A}} \\pi(s, a) \\, Q^{\\pi_\\theta}(s, a)]\n\nMaximizing the value of all states reachable by the policy is the same as finding the optimal policy: the encoutered states bring the maximum return. The policy gradient becomes:\n\n    \\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho_b} [\\sum_{a\\in\\mathcal{A}} \\nabla_\\theta  (\\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a))]\n\nBecause both \\pi(s, a) and Q^\\pi(s, a) depend on the target policy \\pi_\\theta (hence its parameters \\theta), one should normally write:\n\n    \\nabla_\\theta  (\\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)) = Q^{\\pi_\\theta}(s, a) \\, \\nabla_\\theta  \\pi_\\theta(s, a) + \\pi_\\theta(s, a) \\, \\nabla_\\theta Q^{\\pi_\\theta}(s, a)\n\nThe second term depends on \\nabla_\\theta Q^{\\pi_\\theta}(s, a), which is very difficult to estimate. Degris et al. (2012) showed that when the Q-values are estimated by an unbiased critic Q_\\varphi(s, a), this second term can be omitted. Using the log-trick and importance sampling, the policy gradient can be expressed as:\n\n\\begin{aligned}\n    \\nabla_\\theta J(\\theta) & = \\mathbb{E}_{s \\sim \\rho_b} [\\sum_{a\\in\\mathcal{A}} Q_\\varphi(s, a) \\, \\nabla_\\theta \\pi_\\theta(s, a)] \\\\\n                            & = \\mathbb{E}_{s \\sim \\rho_b} [\\sum_{a\\in\\mathcal{A}} b(s, a) \\, \\frac{\\pi_\\theta(s, a)}{b(s, a)} \\, Q_\\varphi(s, a) \\, \\frac{\\nabla_\\theta \\pi_\\theta(s, a)}{\\pi_\\theta(s, a)}] \\\\\n                            & = \\mathbb{E}_{s,a \\sim \\rho_b} [\\frac{\\pi_\\theta(s, a)}{b(s, a)} \\, Q_\\varphi(s, a) \\, \\nabla_\\theta \\log \\pi_\\theta(s, a)] \\\\\n\\end{aligned}\n\nWe now have an actor-critic architecture (actor \\pi_\\theta(s, a), critic Q_\\varphi(s, a)) able to learn from single transitions (s,a) (online update instead of complete trajectories) generated off-policy (behavior policy b(s,a) and importance sampling weight \\frac{\\pi_\\theta(s, a)}{b(s, a)}). The off-policy actor-critic (Off-PAC) algorithm of Degris et al. (2012) furthermore uses eligibility traces to stabilize learning. However, it was limited to linear function approximators because its variance is too high to train deep neural networks.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Off-policy Actor-Critic</span>"
    ]
  },
  {
    "objectID": "src/3.3-ImportanceSampling.html#retrace",
    "href": "src/3.3-ImportanceSampling.html#retrace",
    "title": "Off-policy Actor-Critic",
    "section": "Retrace",
    "text": "Retrace\nFor a good deep RL algorithm, we need the two following properties:\n\nOff-policy learning: it allows to learn from transitions stored in a replay buffer (i.e. generated with an older policy). As NN need many iterations to converge, it is important to be able to re-use old transitions for its training, instead of constantly sampling new ones (sample complexity). Multiple parallel actors as in A3C allow to mitigate this problem, but it is still too complex.\nMulti-step returns: the two extremes of RL are TD (using a single “real” reward for the update, the rest is estimated) and Monte Carlo (use only “real” rewards, no estimation). TD has a smaller variance, but a high bias (errors in estimates propagate to all other values), while MC has a small bias but a high variance (learns from many real rewards, but the returns may vary a lot between two almost identical episodes). Eligibility traces and n-step returns (used in A3C) are the most common trade-off between TD and MC.\n\nThe Retrace algorithm (Munos et al., 2016) is designed to exhibit both properties when learning Q-values. It can therefore be used to train the critic (instead of classical Q-learning) and provide the actor with safe, efficient and low-variance values.\nIn the generic form, Q-learning updates the Q-value of a transition (s_t, a_t) using the TD error:\n\n    \\Delta Q^\\pi(s_t, a_t) = \\alpha \\, \\delta_t = \\alpha \\, (r_{t+1} + \\gamma \\, \\max_a Q^\\pi(s_{t+1}, a_{t+1}) - Q^\\pi(s_t, a_t))\n\nWhen using eligibility traces in the forward view, the change in Q-value depends also on the TD error of future transitions at times t' &gt; t. A parameter \\lambda ensures the stability of the update:\n\n    \\Delta Q^\\pi(s_t, a_t) = \\alpha \\, \\sum_{t'=t}^T (\\gamma \\lambda)^{t'-t} \\delta_{t'}\n\nThe Retrace algorithm proposes to generalize this formula using a parameter c_s for each time step between t and t':\n\n    \\Delta Q^\\pi(s_t, a_t) = \\alpha \\, \\sum_{t'=t}^T (\\gamma)^{t'-t} \\left(\\prod_{s=t+1}^{t'} c_s \\right) \\, \\delta_{t'}\n\nDepending on the choice of c_s, the formula covers different existing methods:\n\nc_s = \\lambda is the classical eligibility trace mechanism (Q(\\lambda)) in its forward view, which is not safe: the behavior policy b must be very close from the target policy \\tau:\n\n\n    || \\pi - b ||_1 \\leq \\frac{1 - \\gamma}{\\lambda \\gamma}\n\nAs \\gamma is typically chosen very close from 1 (e.g. 0.99), this does not leave much room for the target policy to differ from the behavior policy (see Harutyunyan et al., 2016 for the proof).\n\nc_s = \\frac{\\pi(s_s, a_s)}{b(s_s, a_s)} is the importance sampling weight. Importance sampling is unbiased in off-policy settings, but can have a very large variance: the product of ratios \\prod_{s=t+1}^{t'} \\frac{\\pi(s_s, a_s)}{b(s_s, a_s)} can quickly vary between two episodes.\nc_s = \\pi(s_s, a_s) corresponds to the tree-backup algorithm TB(\\lambda) (Precup et al., 2000). It has the advantage to work for arbitrary policies \\pi and b, but the product of such probabilities decays very fast to zero when the time difference t' - t increases: TD errors will be efficiently shared over a couple of steps only.\n\nFor Retrace, Munos et al. (2016) showed that a much better value for c_s is:\n\n    c_s = \\lambda \\min (1, \\frac{\\pi(s_s, a_s)}{b(s_s, a_s)})\n\nThe importance sampling weight is clipped to 1, and decays exponentially with the parameter \\lambda. It can be seen as a trade-off between importance sampling and eligibility traces. The authors showed that Retrace(\\lambda) has a low variance (as it uses multiple returns), is safe (works for all \\pi and b) and efficient (it can propagate rewards over many time steps). They used retrace to learn Atari games and compared it positively with DQN, both in terms of optimality and speed of learning. These properties make Retrace particularly suited for deep RL and actor-critic architectures: it is for example used in ACER and the Reactor.\n\n\n\n\n\n\nAdditional resources\n\n\n\n\nRémi Munos uploaded some slides explaining Retrace in a simpler manner than in the original paper: https://ewrl.files.wordpress.com/2016/12/munos.pdf.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Off-policy Actor-Critic</span>"
    ]
  },
  {
    "objectID": "src/3.3-ImportanceSampling.html#sec-SIL",
    "href": "src/3.3-ImportanceSampling.html#sec-SIL",
    "title": "Off-policy Actor-Critic",
    "section": "Self-Imitation Learning (SIL)",
    "text": "Self-Imitation Learning (SIL)\nWe have discussed so far only strictly on-policy or off-policy methods. Off-policy methods are much more stable and efficient, but they learn generally a deterministic policy, what can be problematic in stochastic environments (e.g. two players games: being predictable is clearly an issue). Hybrid methods combining on- and off-policy mechanisms have clearly a great potential.\nOh et al. (2018) proposed a Self-Imitation Learning (SIL) method that can extend on-policy actor-critic algorithms (e.g. A2C) with a replay buffer able to feed past good experiences to the NN to speed up learning.\nThe main idea is to use prioritized experience replay (Schaul et al. (2015)) to select only transitions whose actual return is higher than their current expected value. This defines two additional losses for the actor and the critic:\n\n    \\mathcal{L}^\\text{SIL}_\\text{actor}(\\theta) = \\mathbb{E}_{s, a \\in \\mathcal{D}}[\\log \\pi_\\theta(s, a) \\, (R(s, a) - V_\\varphi(s))^+]\n \n    \\mathcal{L}^\\text{SIL}_\\text{critic}(\\varphi) = \\mathbb{E}_{s, a \\in \\mathcal{D}}[((R(s, a) - V_\\varphi(s))^+)^2]\n\nwhere (x)^+ = \\max(0, x) is the positive function. Transitions sampled from the replay buffer will participate to the off-policy learning only if their return is higher that the current value of the state, i.e. if they are good experiences compared to what is currently known (V_\\varphi(s)). The pseudo-algorithm is actually quite simple and simply extends the A2C procedure:\n\n\n\n\n\n\nSelf-imitation learning\n\n\n\n\nInitialize the actor \\pi_\\theta and the critic V_\\varphi with random weights.\nInitialize the prioritized experience replay buffer \\mathcal{D}.\nObserve the initial state s_0.\nfor t \\in [0, T_\\text{total}]:\n\nInitialize empty episode minibatch.\nfor k \\in [0, n]: # Sample episode\n\nSelect a action a_k using the actor \\pi_\\theta.\nPerform the action a_k and observe the next state s_{k+1} and the reward r_{k+1}.\nStore (s_k, a_k, r_{k+1}) in the episode minibatch.\n\nif s_n is not terminal: set R_n = V_\\varphi(s_n) with the critic, else R_n=0.\nfor k \\in [n-1, 0]: # Backwards iteration over the episode\n\nUpdate the discounted sum of rewards R_k = r_k + \\gamma \\, R_{k+1} and store it in the replay buffer \\mathcal{D}.\n\nUpdate the actor and the critic on-policy with the episode:\n\n\n      \\theta \\leftarrow \\theta + \\eta \\, \\sum_k \\nabla_\\theta \\log \\pi_\\theta(s_k, a_k) \\, (R_k - V_\\varphi(s_k))\n  \n\n      \\varphi \\leftarrow \\varphi + \\eta \\, \\sum_k \\nabla_\\varphi (R - V_\\varphi(s_k))^2\n  \n\nfor m \\in [0, M]:\n\nSample a minibatch of K transitions (s_k, a_k, R_k) from the replay buffer \\mathcal{D} prioritized with high (R_k - V_\\varphi(s_k)).\nUpdate the actor and the critic off-policy with self-imitation.\n\n\n      \\theta \\leftarrow \\theta + \\eta \\, \\sum_k \\nabla_\\theta \\log \\pi_\\theta(s_k, a_k) \\, (R_k - V_\\varphi(s_k))^+\n  \n\n      \\varphi \\leftarrow \\varphi + \\eta \\, \\sum_k \\nabla_\\varphi ((R_k - V_\\varphi(s_k))^+)^2\n  \n\n\n\n\nIn the paper, they furthermore used entropy regularization as in A3C. They showed that A2C+SIL has a better performance both on Atari games and continuous control problems (Mujoco) than state-of-the art methods (A3C, TRPO, Reactor, PPO). It shows that self-imitation learning can be very useful in problems where exploration is hard: a proper level of exploitation of past experiences actually fosters a deeper exploration of environment.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Off-policy Actor-Critic</span>"
    ]
  },
  {
    "objectID": "src/3.3-ImportanceSampling.html#the-reactor",
    "href": "src/3.3-ImportanceSampling.html#the-reactor",
    "title": "Off-policy Actor-Critic",
    "section": "The Reactor",
    "text": "The Reactor\nThe Reactor (Retrace Actor) of Gruslys et al. (2017) combines many architectural and algorithmic contributions in order to provide an algorithm that is both sample efficient and with a good run-time performance. A3C has for example a better run-time performance (smaller wall-clock time for the training) than DQN or categorical DQN thanks to the use of multiple actor-learners in parallel, but its sample complexity is actually higher (as it is on-policy).\nThe Reactor combines and improves on:\n\nAn actor-critic architecture using policy gradient with importance sampling,\nOff-policy corrected returns computed by the Retrace algorithm,\nDistributional learning of the Q-values in the critic,\nPrioritized experience replay for sequences.\n\nOne could consider REACTOR as the distributional version of ACER. We will not go into all the details here, but simply outline the main novelties.\nThe Reactor is composed of an actor \\pi_\\theta(s, a) and a critic Q_\\varphi(s, a). The actor is trained using policy gradient with importance sampling, as in Off-PAC. For a single state s and an action \\hat{a} sampled by the behavior policy b, the gradient of the objective is defined as:\n\n\\begin{aligned}\n    \\nabla_\\theta J(\\theta) = \\frac{\\pi_\\theta(s, \\hat{a})}{b(s, \\hat{a})} & \\, (R(s, \\hat{a}) - Q_\\varphi(s, \\hat{a})) \\, \\nabla_\\theta \\log \\pi_\\theta(s, \\hat{a}) \\\\\n    & + \\sum_a Q_\\varphi(s, a) \\, \\nabla_\\theta \\pi_\\theta(s, a) \\\\\n\\end{aligned}\n\nThe first term comes from Off-PAC and only concerns the chosen action \\hat{a} from the behavior policy. The actual return R(s, a) is compared to its estimate Q_\\varphi(s, \\hat{a}) in order to reduce its variance. The second term \\sum_a Q_\\varphi(s, a) \\, \\nabla_\\theta \\pi_\\theta(s, a) depends on all available actions in s. Its role is to reduce the bias of the first term, without adding any variance as it is only based on estimates. As the value of the state is defined by V^\\pi(s) = \\sum_a \\pi(s, a) \\, Q^\\pi(s, a), maximizing this term maximizes the value of the state, i.e. the associated returns. This rule is called leave-one-out (LOO), as one action is left out from the sum and estimated from actual returns instead of other estimates.\nFor a better control on the variance, the behavior probability b(s, a) is replaced by a parameter \\beta:\n\n    \\nabla_\\theta J(\\theta) = \\beta \\, (R(s, \\hat{a}) - Q_\\varphi(s, \\hat{a})) \\, \\nabla_\\theta \\pi_\\theta(s, \\hat{a}) + \\sum_a Q_\\varphi(s, a) \\, \\nabla_\\theta \\pi_\\theta(s, a)\n\n\\beta is defined as \\min (c, \\frac{1}{b(s, \\hat{a})}), where c&gt;1 is a constant. This truncated term is similar to what was used in ACER. The rule is now called \\beta-LOO and is a novel proposition of the Reactor.\nThe second importance contribution of the Reactor is how to combine the Retrace algorithm (Munos et al. (2016)) for estimating the return R(s, \\hat{a}) on multiple steps, with the distributional learning method of Categorical DQN. As Retrace uses n-steps returns iteratively, the n-step distributional Bellman target can updated using the n future rewards:\n\n    z_i^n = \\mathcal{T}^n \\, z_i = \\sum_{k=t}^{t+n} \\gamma^{k-t} r_k + \\gamma^n \\, z_i\n\nWe leave out the details on how Retrace is combined with these distributional Bellman updates: the notation is complicated but the idea is simple. The last importance contribution of the paper is the use of prioritized sequence replay. Prioritized experience replay allows to select in priority transitions from the replay buffer which are the most surprising, i.e. where the TD error is the highest. These transitions are the ones carrying the most information. A similar principle can be applied to sequences of transitions, which are needed by the n-step updates. They devised a specific sampling algorithm in order to achieve this and reduce the variance of the samples.\nThe last particularities of the Reactor is that it uses a LSTM layer to make the problem Markovian (instead of stacking four frames as in DQN) and train multiple actor-learners as in A3C. The algorithm is trained on CPU, with 10 or 20 actor-learners. The Reactor outperforms DQN and its variants, A3C and ACER on Atari games. Importantly, Reactor only needs one day of training on CPU, compared to the 8 days of GPU training needed by DQN.\n\n\n\n\nDegris, T., White, M., and Sutton, R. S. (2012). Linear Off-Policy Actor-Critic. in Proceedings of the 2012 International Conference on Machine Learning Available at: http://arxiv.org/abs/1205.4839.\n\n\nGruslys, A., Dabney, W., Azar, M. G., Piot, B., Bellemare, M., and Munos, R. (2017). The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning. Available at: http://arxiv.org/abs/1704.04651.\n\n\nHarutyunyan, A., Bellemare, M. G., Stepleton, T., and Munos, R. (2016). Q(λ) with off-policy corrections. Available at: http://arxiv.org/abs/1602.04951.\n\n\nLevine, S., and Koltun, V. (2013). Guided Policy Search. in Proceedings of Machine Learning Research, 1–9. Available at: http://proceedings.mlr.press/v28/levine13.html.\n\n\nMeuleau, N., Peshkin, L., Kaelbling, L. P., and Kim, K. (2000). Off-Policy Policy Search. Available at: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894.\n\n\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016). Safe and Efficient Off-Policy Reinforcement Learning. Available at: http://arxiv.org/abs/1606.02647.\n\n\nOh, J., Guo, Y., Singh, S., and Lee, H. (2018). Self-Imitation Learning. Available at: http://arxiv.org/abs/1806.05635.\n\n\nPeshkin, L., and Shelton, C. R. (2002). Learning from Scarce Experience. Available at: http://arxiv.org/abs/cs/0204043.\n\n\nPrecup, D., Sutton, R. S., and Singh, S. (2000). Eligibility traces for off-policy policy evaluation. in Proceedings of the Seventeenth International Conference on Machine Learning.\n\n\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized Experience Replay. Available at: http://arxiv.org/abs/1511.05952.\n\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature 529, 484–489. doi:10.1038/nature16961.\n\n\nSutton, R. S., and Barto, A. G. (2017). Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press Available at: http://incompleteideas.net/book/the-book-2nd.html.\n\n\nTang, J., and Abbeel, P. (2010). On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient. in Adv. Neural inf. Process. Syst. Available at: http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Off-policy Actor-Critic</span>"
    ]
  },
  {
    "objectID": "src/3.4-DPG.html",
    "href": "src/3.4-DPG.html",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "",
    "text": "Deterministic policy gradient theorem\nSo far, the actor produces a stochastic policy \\pi_\\theta(s) assigning probabilities to each discrete action or necessitating sampling in some distribution for continuous actions. The main advantage is that stochastic policies ensure exploration of the state-action space: as most actions have a non-zero probability of being selected, we should not miss any important reward which should be ignored if the greedy action is always selected (exploration/exploitation dilemma).\nThere are however two drawbacks:\nSuccessful value-based methods such as DQN produce a deterministic policy, where the action to be executed after learning is simply the greedy action a^*_t = \\text{argmax}_a Q_\\theta(s_t, a). Exploration is enforced by forcing the behavior policy (the one used to generate the samples) to be stochastic (\\epsilon-greedy), but the learned policy is itself deterministic. This is off-policy learning, allowing to use a different policy than the learned one to explore. When using an experience replay memory, the behavior policy is simply an older version of the learning policy (as samples stored in the ERM were generated by an older version of the actor).\nIn this section, we will see the now state-of-the-art method DDPG (Deep Deterministic Policy Gradient), which tries to combine the advantages of policy gradient methods (actor-critic, continuous or highly dimensional outputs, stability) with those of value-based methods (sample efficiency, off-policy).\nWe now assume that we want to learn a parameterized deterministic policy \\mu_\\theta(s). As for the stochastic policy gradient theorem, the goal is to maximize the expectation over all states reachable by the policy of the reward to-go (return) after each action:\nJ(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\mu}[R(s, \\mu_\\theta(s))]\nAs in the stochastic case, the distribution of states reachable by the policy \\rho_\\mu is impossible to estimate, so we will have to perform approximations. Building on Hafner and Riedmiller (2011), Silver et al. (2014) showed how to obtain a usable gradient for the objective function when the policy is deterministic.\nConsidering that the Q-value of an action is the expectation of the reward to-go after that action Q^\\pi(s, a) = \\mathbb{E}_\\pi[R(s, a)], maximizing the returns or maximizing the true Q-value of all actions leads to the same optimal policy. This is the basic idea behind dynamic programming, where policy evaluation first finds the true Q-value of all state-action pairs and policy improvement changes the policy by selecting the action with the maximal Q-value a^*_t = \\text{argmax}_a Q_\\theta(s_t, a).\nIn the continuous case, we will simply state that the gradient of the objective function is the same as the gradient of the Q-value. Supposing we have an unbiased estimate Q^\\mu(s, a) of the value of any action in s, changing the policy \\mu_\\theta(s) in the direction of \\nabla_\\theta Q^\\mu(s, a) leads to an action with a higher Q-value, therefore with a higher associated return:\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\mu}[\\nabla_\\theta Q^\\mu(s, a) |_{a = \\mu_\\theta(s)}]\nThis notation means that the gradient w.r.t a of the Q-value is taken at a = \\mu_\\theta(s). We now use the chain rule to expand the gradient of the Q-value:\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\mu}[\\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q^\\mu(s, a) |_{a = \\mu_\\theta(s)}]\nIt is perhaps clearer using partial derivatives and simplifying the notations:\n\\frac{\\partial Q(s,a)}{\\partial \\theta} = \\frac{\\partial Q(s,a)}{\\partial a} \\times \\frac{\\partial a}{\\partial \\theta}\nThe first term defines of the Q-value of an action changes when one varies slightly the action (if I move my joint a bit more to the right, do I get a higher Q-value, hence more reward?), the second term defines how the action changes when the parameters \\theta of the actor change (which weights should be changed in order to produce that action with a slightly higher Q-value?).\nWe already see an actor-critic architecture emerging from this equation: \\nabla_\\theta \\mu_\\theta(s) only depends on the parameterized actor, while \\nabla_a Q^\\mu(s, a) is a sort of critic, telling the actor in which direction to change its policy: towards actions associated with more reward.\nAs in the stochastic policy gradient theorem, the question is now how to obtain an unbiased estimate of the Q-value of any action and compute its gradient. Silver et al. (2014) showed that it is possible to use a function approximator Q_\\varphi(s, a) as long as it is compatible and minimize the quadratic error with the true Q-values:\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\mu}[\\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_\\varphi(s, a) |_{a = \\mu_\\theta(s)}]\n \n    J(\\varphi) = \\mathbb{E}_{s \\sim \\rho_\\mu}[(Q^\\mu(s, \\mu_\\theta(s)) - Q_\\varphi(s, \\mu_\\theta(s)))^2]\nFigure 14.2 outlines the actor-critic architecture of the DPG (deterministic policy gradient) method, to compare with the actor-critic architecture of the stochastic policy gradient.\nSilver et al. (2014) investigated the performance of DPG using linear function approximators and showed that it compared positively to stochastic algorithms in high-dimensional or continuous action spaces. However, non-linear function approximators such as deep NN would not work yet.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Deep Deterministic Policy Gradient (DDPG)</span>"
    ]
  },
  {
    "objectID": "src/3.4-DPG.html#deterministic-policy-gradient-theorem",
    "href": "src/3.4-DPG.html#deterministic-policy-gradient-theorem",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "",
    "text": "Figure 14.1: The chain rule can be applied on the deterministic policy gradient, as it is a composition of deterministic functions..\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.2: Architecture of the DPG (deterministic policy gradient) method.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Deep Deterministic Policy Gradient (DDPG)</span>"
    ]
  },
  {
    "objectID": "src/3.4-DPG.html#deep-deterministic-policy-gradient-ddpg",
    "href": "src/3.4-DPG.html#deep-deterministic-policy-gradient-ddpg",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "Deep Deterministic Policy Gradient (DDPG)",
    "text": "Deep Deterministic Policy Gradient (DDPG)\nLillicrap et al. (2015) extended the DPG approach to work with non-linear function approximators. In fact, they combined ideas from DQN and DPG to create a very successful algorithm able to solve continuous problems off-policy, the deep deterministic policy gradient (DDPG) algorithm..\nThe key ideas borrowed from DQN are:\n\nUsing an experience replay memory to store past transitions and learn off-policy.\nUsing target networks to stabilize learning.\n\nThey modified the update frequency of the target networks originally used in DQN. In DQN, the target networks are updated with the parameters of the trained networks every couple of thousands of steps. The target networks therefore change a lot between two updates, but not very often. Lillicrap et al. (2015) found that it is actually better to make the target networks slowly track the trained networks, by updating their parameters after each update of the trained network using a sliding average for both the actor and the critic:\n\n    \\theta' = \\tau \\, \\theta + (1-\\tau) \\, \\theta'\n\nwith \\tau &lt;&lt;1. Using this update rule, the target networks are always “late” with respect to the trained networks, providing more stability to the learning of Q-values.\nThe key idea borrowed from DPG is the policy gradient for the actor. The critic is learned using regular Q-learning and target networks:\n\n    J(\\varphi) = \\mathbb{E}_{s \\sim \\rho_\\mu}[(r(s, a, s') + \\gamma \\, Q_{\\varphi'}(s', \\mu_{\\theta'}(s')) - Q_\\varphi(s, a))^2]\n\nOne remaining issue is exploration: as the policy is deterministic, it can very quickly produce always the same actions, missing perhaps more rewarding options. Some environments are naturally noisy, enforcing exploration by itself, but this cannot be assumed in the general case. The solution retained in DDPG is an additive noise added to the deterministic action to explore the environment:\n\n    a_t = \\mu_\\theta(s_t) + \\xi\n\nThis additive noise could be anything, but the most practical choice is to use an Ornstein-Uhlenbeck process (Uhlenbeck and Ornstein, 1930) to generate temporally correlated noise with zero mean. Ornstein-Uhlenbeck processes are used in physics to model the velocity of Brownian particles with friction. It updates a variable x_t using a stochastic differential equation (SDE):\n dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t \\qquad \\text{with} \\qquad dW_t = \\mathcal{N}(0, dt)\n\\mu is the mean of the process (usually 0), \\theta is the friction (how fast it varies with noise) and \\sigma controls the amount of noise. Figure 14.3 shows three independent runs of a Ornstein-Uhlenbeck process: successive values of the variable x_t vary randomly but coherently over time.\n\n\n\n\n\n\nFigure 14.3: Three independent runs of an Ornstein-Uhlenbeck process with \\mu=0, \\sigma=0.3, \\theta=0.15 and dt=0.1.\n\n\n\nThe architecture of the DDPG algorithm is depicted on Figure 14.4.\n\n\n\n\n\n\nFigure 14.4: Architecture of the DDPG (deep deterministic policy gradient) algorithm.\n\n\n\nThe pseudo-algorithm is as follows:\n\n\n\n\n\n\nDDPG algorithm\n\n\n\n\nInitialize actor network \\mu_{\\theta} and critic Q_\\varphi with random weights.\nCreate the target networks \\mu_{\\theta'} and Q_{\\varphi'}.\nInitialize experience replay memory \\mathcal{D} of maximal size N.\nfor episode \\in [1, M]:\n\nInitialize random process \\xi.\nObserve the initial state s_0.\nfor t \\in [0, T_\\text{max}]:\n\nSelect the action a_t = \\mu_\\theta(s_t) + \\xi according to the current policy and the noise.\nPerform the action a_t and observe the next state s_{t+1} and the reward r_{t+1}.\nStore (s_t, a_t, r_{t+1}, s_{t+1}) in the experience replay memory.\nSample a minibatch of N transitions randomly from \\mathcal{D}.\nFor each transition (s_k, a_k, r_k, s'_k) in the minibatch:\n\nCompute the target value using target networks y_k = r_k + \\gamma \\, Q_{\\varphi'}(s'_k, \\mu_{\\theta'}(s'_k)).\n\nUpdate the critic by minimizing:\n\n\n      \\mathcal{L}(\\varphi) = \\frac{1}{N} \\sum_k (y_k - Q_\\varphi(s_k, a_k))^2\n  \n\nUpdate the actor using the sampled policy gradient:\n\n\n      \\nabla_\\theta J(\\theta) = \\frac{1}{N} \\sum_k \\nabla_\\theta \\mu_\\theta(s_k) \\times \\nabla_a Q_\\varphi(s_k, a) |_{a = \\mu_\\theta(s_k)}\n  \n\nUpdate the target networks:\n\n\\theta' \\leftarrow \\tau \\theta + (1-\\tau) \\, \\theta' \\varphi' \\leftarrow \\tau \\varphi + (1-\\tau) \\, \\varphi'\n\n\n\n\nThe question that arises is how to obtain the gradient of the Q-value w.r.t the action \\nabla_a Q_\\varphi(s, a), when the critic only outputs the Q-value Q_\\varphi(s, a). Fortunately, deep neural networks are simulated using automatic differentiation libraries such as tensorflow or pytorch, which can automatically output this gradient.\nNote that the DDPG algorithm is off-policy: the samples used to train the actor come from the replay buffer, i.e. were generated by an older version of the target policy. DDPG does not rely on importance sampling: as the policy is deterministic (we maximize \\mathbb{E}_{s}[Q(s, \\mu_\\theta(s))]), there is no need to balance the probabilities of the behavior and target policies (with stochastic policies, one should maximize \\mathbb{E}_{s}[\\sum_{a\\in\\mathcal{A}} \\pi(s, a) Q(s, a)]). In other words, the importance sampling weight can safely be set to 1 for deterministic policies.\nDDPG has rapidly become the state-of-the-art model-free method for continuous action spaces (although now PPO is preferred). It is able to learn efficent policies on most contiuous problems, either pixel-based or using individual state variables. In the original DDPG paper, they showed that batch normalization (Ioffe and Szegedy, 2015) is crucial in stabilizing the training of deep networks on such problems. Its main limitation is its high sample complexity. Distributed versions of DDPG have been proposed to speed up learning, similarly to the parallel actor learners of A3C (Barth-Maron et al., 2018; Lötzsch et al., 2017; Popov et al., 2017).\n\n\n\n\n\n\n\nAdditional resources\n\n\n\n\nhttp://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html for additional explanations and step-by-step tensorflow code.\nhttps://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html for contextual explanations.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Deep Deterministic Policy Gradient (DDPG)</span>"
    ]
  },
  {
    "objectID": "src/3.4-DPG.html#application-learning-to-drive-in-a-day",
    "href": "src/3.4-DPG.html#application-learning-to-drive-in-a-day",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "Application: learning to drive in a day",
    "text": "Application: learning to drive in a day\n\nThe startup Wayve made a very exciting demonstration of the utility of DDPG in autonomous driving in 2018: https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning.\n\n\n\n\n\n\nFigure 14.5: Structure of the DDPG architecture. Source: Kendall et al. (2018)\n\n\n\nThe algorithm is simply DDPG with prioritized experience replay. The actor and critic share the convolutional layers (pretrained in simulation using a variation autoencoder). Training is live in the car, with an on-board NVIDIA Drive PX2 GPU. A simulated environment is first used to find the hyperparameters.\n\n\n\n\n\n\nFigure 14.6: Simulator used to find the hyperparameters of the DDPG algorithm.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Deep Deterministic Policy Gradient (DDPG)</span>"
    ]
  },
  {
    "objectID": "src/3.4-DPG.html#td3---twin-delayed-deep-deterministic-policy-gradient",
    "href": "src/3.4-DPG.html#td3---twin-delayed-deep-deterministic-policy-gradient",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "TD3 - Twin Delayed Deep Deterministic policy gradient",
    "text": "TD3 - Twin Delayed Deep Deterministic policy gradient\n\nTwin critics\nAs any Q-learning-based method, DDPG overestimates Q-values. The Bellman target t = r + \\gamma \\, \\max_{a'} Q(s', a') uses a maximum over other values, so it is increasingly overestimated during learning. After a while, the overestimated Q-values disrupt training in the actor.\n\n\n\n\n\n\nFigure 14.7: Overestimation of Q-values by DDPG.\n\n\n\nDouble Q-learning (see Section Double DQN) solves the problem by using the target network \\theta' to estimate Q-values, but the value network \\theta to select the greedy action in the next state:\n\n    \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(s´, \\text{argmax}_{a'} Q_{\\theta}(s', a')) - Q_\\theta(s, a))^2]\n\nThe idea is to use two different independent networks to reduce overestimation. This does not work well with DDPG, as the Bellman target t = r + \\gamma \\, Q_{\\varphi'}(s', \\mu_{\\theta'}(s')) will use a target actor network that is not very different from the trained deterministic actor, as the greedy action is likely to be the same for both.\nTo solve this, TD3 (Fujimoto et al., 2018) uses two critics \\varphi_1 and \\varphi_2 (and target critics): the Q-value used to train the actor will be the lesser of two evils, i.e. the minimum Q-value:\nt = r + \\gamma \\, \\min(Q_{\\varphi'_1}(s', \\mu_{\\theta'}(s')), Q_{\\varphi'_2}(s', \\mu_{\\theta'}(s')))\nOne of the critic will always be less over-estimating than the other. Better than nothing… Using twin critics is called clipped double learning.\nBoth critics learn in parallel using the same target:\n\\mathcal{L}(\\varphi_1) = \\mathbb{E}[(t - Q_{\\varphi_1}(s, a))^2] \\qquad ; \\qquad \\mathcal{L}(\\varphi_2) = \\mathbb{E}[ (t - Q_{\\varphi_2}(s, a))^2]\nThe actor is trained using the first critic only:\n\\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}[ \\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_{\\varphi_1}(s, a) |_{a = \\mu_\\theta(s)} ]\n\n\nDelayed actor updates\nAnother issue with actor-critic architecture in general is that the critic is always biased during training, what can impact the actor and ultimately collapse the policy:\n\\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}[ \\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_{\\varphi_1}(s, a) |_{a = \\mu_\\theta(s)} ]\nQ_{\\varphi_1}(s, a) \\approx Q^{\\mu_\\theta}(s, a)\nThe critic should learn much faster than the actor in order to provide unbiased gradients.\n\n\n\n\n\n\nFigure 14.8: Non-stationarity of the Bellman targets impairs learning.\n\n\n\nIncreasing the learning rate in the critic creates instability, reducing the learning rate in the actor slows down learning. The solution proposed by TD3 is to delay the update of the actor, i.e. update it only every d minibatches:\n\n\n\n\n\n\nDelayed actor updates\n\n\n\n\nTrain the critics \\varphi_1 and \\varphi_2 on the minibatch.\nevery d steps:\n\nTrain the actor \\theta on the minibatch.\n\n\n\n\nThis leaves enough time to the critics to improve their prediction and provides less biased gradients to the actor.\n\n\nNoisy Bellman targets\nA last problem with deterministic policies is that they tend to always select the same actions \\mu_\\theta(s) (overfitting). For exploration, some additive noise is added to the selected action:\na = \\mu_\\theta(s) + \\xi\nBut this is not true for the Bellman targets, which use the deterministic action:\nt = r + \\gamma \\, Q_{\\varphi}(s', \\mu_{\\theta}(s'))\nTD3 proposes to also use additive noise in the Bellman targets:\nt = r + \\gamma \\, Q_{\\varphi}(s', \\mu_{\\theta}(s') + \\xi)\nIf the additive noise is zero on average, the Bellman targets will be correct on average (unbiased) but will prevent overfitting of particular actions. The additive noise does not have to be an Ornstein-Uhlenbeck stochastic process, but could simply be a random variable:\n\\xi \\sim \\mathcal{N}(0, 1)\n\n\n\n\n\n\nTD3 algorithm\n\n\n\n\nInitialize actor \\mu_{\\theta}, critics Q_{\\varphi_1}, Q_{\\varphi_2}, target networks \\mu_{\\theta'}, Q_{\\varphi_1'},Q_{\\varphi_2'}, ERM \\mathcal{D}, random processes \\xi_1, \\xi_2.\nfor t \\in [0, T_\\text{max}]:\n\nSelect the action a_t = \\mu_\\theta(s_t) + \\xi_1 and store (s_t, a_t, r_{t+1}, s_{t+1}) in the ERM.\nFor each transition (s_k, a_k, r_k, s'_k) in a minibatch sampled from \\mathcal{D}:\n\nCompute the target t_k = r_k + \\gamma \\, \\min(Q_{\\varphi_1'}(s'_k, \\mu_{\\theta'}(s'_k) + \\xi_2), Q_{\\varphi_2'}(s'_k, \\mu_{\\theta'}(s'_k) + \\xi_2)).\n\nUpdate the critics by minimizing:\n\n\n      \\mathcal{L}(\\varphi_1) = \\frac{1}{K} \\sum_k (t_k - Q_{\\varphi_1}(s_k, a_k))^2 \\qquad ; \\qquad \\mathcal{L}(\\varphi_2) = \\frac{1}{K} \\sum_k (t_k - Q_{\\varphi_2}(s_k, a_k))^2\n  \n\nevery d steps:\n\nUpdate the actor by applying the DPG using Q_{\\varphi_1}:\n\n\n  \\nabla_\\theta \\mathcal{J}(\\theta) = \\frac{1}{K} \\sum_k \\nabla_\\theta \\mu_\\theta(s_k) \\times \\nabla_a Q_{\\varphi_1}(s_k, a) |_{a = \\mu_\\theta(s_k)}\n  \n\nUpdate the target networks:\n\n\n      \\theta' \\leftarrow \\tau \\theta + (1-\\tau) \\, \\theta' \\; ; \\; \\varphi_1' \\leftarrow \\tau \\varphi_1 + (1-\\tau) \\, \\varphi_1' \\; ; \\; \\varphi_2' \\leftarrow \\tau \\varphi_2 + (1-\\tau) \\, \\varphi_2'\n  \n\n\n\n\nTD3 outperformed DDPG (but also PPO and SAC) on continuous control tasks.\n\n\n\n\n\n\nFigure 14.9: Performance of TD3 on Mujoco tasks compared to DDPG, PPO and SAC. Source: Fujimoto et al. (2018)",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Deep Deterministic Policy Gradient (DDPG)</span>"
    ]
  },
  {
    "objectID": "src/3.4-DPG.html#d4pg-distributed-distributional-ddpg",
    "href": "src/3.4-DPG.html#d4pg-distributed-distributional-ddpg",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "D4PG: Distributed Distributional DDPG",
    "text": "D4PG: Distributed Distributional DDPG\nD4PG (Distributed Distributional DDPG; Barth-Maron et al. (2018)) combined several recent improvements:\n\nDeterministic policy gradient as in DDPG: \\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}_{s \\sim \\rho_b}[\\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a \\mathbb{E} [\\mathcal{Z}_\\varphi(s, a)] |_{a = \\mu_\\theta(s)}]\nDistributional critic: The critic does not predict single Q-values Q_\\varphi(s, a), but the distribution of returns \\mathcal{Z}_\\varphi(s, a) (as in Categorical DQN): \\mathcal{L}(\\varphi) = \\mathbb{E}_{s \\in \\rho_b} [ \\text{KL}(\\mathcal{T} \\, \\mathcal{Z}_\\varphi(s, a) || \\mathcal{Z}_\\varphi(s, a))]\nn-step returns (as in A3C): \\mathcal{T} \\, \\mathcal{Z}_\\varphi(s_t, a_t)= \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, \\mathcal{Z}_\\varphi(s_{t+n}, \\mu_\\theta(s_{t+n}))\nDistributed workers: D4PG uses K=32 or 64 copies of the actor to fill the ERM in parallel.\nPrioritized Experience Replay (PER): P(k) = \\frac{(|\\delta_k| + \\epsilon)^\\alpha}{\\sum_k (|\\delta_k| + \\epsilon)^\\alpha}\n\n\n\n\n\n\n\nFigure 14.10: D4PG results. Source: Barth-Maron et al. (2018)\n\n\n\n\n\n\n\n\nBarth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., TB, D., et al. (2018). Distributed Distributional Deterministic Policy Gradients. Available at: http://arxiv.org/abs/1804.08617.\n\n\nFujimoto, S., van Hoof, H., and Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods. Available at: http://arxiv.org/abs/1802.09477 [Accessed March 1, 2020].\n\n\nHafner, R., and Riedmiller, M. (2011). Reinforcement learning in feedback control. Machine Learning 84, 137–169. doi:10.1007/s10994-011-5235-x.\n\n\nIoffe, S., and Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Available at: http://arxiv.org/abs/1502.03167.\n\n\nKendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen, J.-M., et al. (2018). Learning to Drive in a Day. Available at: http://arxiv.org/abs/1807.00412 [Accessed December 19, 2018].\n\n\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., et al. (2015). Continuous control with deep reinforcement learning. CoRR. Available at: http://arxiv.org/abs/1509.02971.\n\n\nLötzsch, W., Vitay, J., and Hamker, F. H. (2017). Training a deep policy gradient-based neural network with asynchronous learners on a simulated robotic problem. in INFORMATIK 2017. Gesellschaft für Informatik, eds. M. Eibl and M. Gaedke (Gesellschaft für Informatik, Bonn), 2143–2154. Available at: https://dl.gi.de/handle/20.500.12116/3986.\n\n\nPopov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G., Vecerik, M., et al. (2017). Data-efficient Deep Reinforcement Learning for Dexterous Manipulation. Available at: http://arxiv.org/abs/1704.03073.\n\n\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic Policy Gradient Algorithms. in Proc. ICML Proceedings of Machine Learning Research., eds. E. P. Xing and T. Jebara (PMLR), 387–395. Available at: http://proceedings.mlr.press/v32/silver14.html.\n\n\nUhlenbeck, G. E., and Ornstein, L. S. (1930). On the Theory of the Brownian Motion. Physical Review 36. doi:10.1103/PhysRev.36.823.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Deep Deterministic Policy Gradient (DDPG)</span>"
    ]
  },
  {
    "objectID": "src/3.5-NaturalGradient.html",
    "href": "src/3.5-NaturalGradient.html",
    "title": "Natural gradients",
    "section": "",
    "text": "Learning stability\nThe deep networks used as function approximators in the methods presented until now were all optimized (trained) using stochastic gradient descent (SGD) or any of its variants (RMSProp, Adam, etc). The basic idea is to change the parameters \\theta in the opposite direction of the gradient of the loss function (or the same direction as the policy gradient, in which case it is called gradient ascent), proportionally to a small learning rate \\eta:\n\\Delta \\theta = - \\eta \\, \\nabla_\\theta \\mathcal{L}(\\theta)\nSGD is also called a steepest descent method: one searches for the smallest parameter change \\Delta \\theta inducing the biggest negative change of the loss function. In classical supervised learning, this is what we want: we want to minimize the loss function as fast as possible, while keeping weight changes as small as possible, otherwise learning might become unstable (weight changes computed for a single minibatch might erase the changes made on previous minibatches). The main difficulty of supervised learning is to choose the right value for the learning rate: too high and learning is unstable; too low and learning takes forever.\nIn deep RL, we have an additional problem: the problem is not stationary. In Q-learning, the target r(s, a, s') + \\gamma \\, \\max_{a'} Q_\\theta(S', a') is changing with \\theta. If the Q-values change a lot between two minibatches, the network will not get any stable target signal to learn from, and the policy will end up suboptimal. The trick is to use target networks to compute the target, which can be either an old copy of the current network (vanilla DQN), or a smoothed version of it (DDPG). Obviously, this introduces a bias (the targets are always wrong during training), but this bias converges to zero (after sufficient training, the targets will be almost correct), at the cost of a huge sample complexity.\nTarget networks cannot be used in on-policy methods, especially actor-critic architectures.The critic must learn from transitions recently generated by the actor (although importance sampling and the Retrace algorithm might help). The problem with on-policy methods is that they waste a lot of data: they always need fresh samples to learn from and never reuse past experiences. The policy gradient theorem shows why:\n\\begin{aligned}\n    \\nabla_\\theta J(\\theta) & =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)] \\\\\n    & \\approx  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q_\\varphi(s, a)]\n\\end{aligned}\nIf the policy \\pi_\\theta changes a lot between two updates, the estimated Q-value Q_\\varphi(s, a) will represent the value of the action for a totally different policy, not the true Q-value Q^{\\pi_\\theta}(s, a). The estimated policy gradient will then be strongly biased and learning will be suboptimal. In other words, the actor should not change much faster than the critic, and vice versa. A naive solution would be to use a very small learning rate for the actor, but this just slows down learning (adding to the sample complexity) without solving the problem.\nTo solve the problem, we should actually do the opposite of the steepest descent:\nIf the parameter change is high, the actor will learn a lot internally from each experience. But if the policy change is small between two updates (although the parameters have changed a lot), we might be able to reuse past experiences, as the targets will not be that wrong.\nThis is where natural gradients come into play, which are originally a statistical method to optimize over spaces of probability distributions, for example for variational inference. The idea to use natural gradients to train neural networks comes from Amari (1998). Kakade (2001) applied natural gradients to policy gradient methods, while Peters and Schaal (2008) proposed a natural actor-critic algorithm for linear function approximators. The idea was adapted to deep RL by Schulman and colleagues, with Trust Region Policy Optimization (TRPO, Schulman et al., 2015) and Proximal Policy Optimization (PPO, Schulman et al., 2017), the latter having replaced DDPG as the go-to method for continuous RL problems, particularly because of its smaller sample complexity and its robustness to hyperparameters.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Natural gradients</span>"
    ]
  },
  {
    "objectID": "src/3.5-NaturalGradient.html#learning-stability",
    "href": "src/3.5-NaturalGradient.html#learning-stability",
    "title": "Natural gradients",
    "section": "",
    "text": "search for the biggest parameter change \\Delta \\theta inducing the smallest change in the policy, but in the right direction.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Natural gradients</span>"
    ]
  },
  {
    "objectID": "src/3.5-NaturalGradient.html#principle-of-natural-gradients",
    "href": "src/3.5-NaturalGradient.html#principle-of-natural-gradients",
    "title": "Natural gradients",
    "section": "Principle of natural gradients",
    "text": "Principle of natural gradients\n\n\n\n\n\n\nFigure 15.1: Euclidian distances in the parameter space do not represent well the statistical distance between probability distributions. The two Gaussians on the left (\\mathcal{N}(0, 0.2) and \\mathcal{N}(1, 0.2)) have the same Euclidian distance in the parameter space (d = \\sqrt{(\\mu_0 - \\mu_1)^2+(\\sigma_0 - \\sigma_1)^2}) than the two Gaussians on the right (\\mathcal{N}(0, 10) and \\mathcal{N}(1, 10)). However, the Gaussians on the right are much more similar than the two on the left: if you have a single sample, you could not say from which distribution it comes for the Gaussians on the right, while it is obvious for the Gaussians on the left.\n\n\n\nConsider the two Gaussian distributions in the left part of Figure 15.1 (\\mathcal{N}(0, 0.2) and \\mathcal{N}(1, 0.2)) and the two on the right (\\mathcal{N}(0, 10) and \\mathcal{N}(1, 10)). In both cases, the distance in the Euclidian space of parameters d = \\sqrt{(\\mu_0 - \\mu_1)^2+(\\sigma_0 - \\sigma_1)^2} is the same between the two Gaussians. Obviously, the two distributions on the left are however further away from each other than the two on the the right. This indicates that the Euclidian distance in the parameter space (which is what regular gradients act on) is not a correct measurement of the statistical distance between two distributions (which what we want to minimize between two iterations of PG).\nIn statistics, a common measurement of the statistical distance between two distributions p and q is the Kullback-Leibler (KL) divergence D_{KL}(p||q), also called relative entropy or information gain. It is defined as:\n\n    D_{KL}(p || q) = \\mathbb{E}_{x \\sim p} [\\log \\frac{p(x)}{q(x)}]  = \\int p(x) \\, \\log \\frac{p(x)}{q(x)} \\, dx\n\nIts minimum is 0 when p=q (as \\log \\frac{p(x)}{q(x)} is then 0) and is positive otherwise. Minimizing the KL divergence is equivalent to “matching” two distributions. Note that supervised methods in machine learning can all be interpreted as a minimization of the KL divergence: if p(x) represents the distribution of the data (the label of a sample x) and q(x) the one of the model (the prediction of a neural network for the same sample x), supervised methods want the output distribution of the model to match the distribution of the data, i.e. make predictions that are the same as the labels. For generative models, this is for example at the core of generative adversarial networks (Arjovsky et al., 2017; Goodfellow et al., 2014) or variational autoencoders (Kingma and Welling, 2013).\nThe KL divergence is however not symmetrical (D_{KL}(p || q) \\neq D_{KL}(q || p)), so a more useful divergence is the symmetric KL divergence, also known as Jensen-Shannon (JS) divergence:\n\n    D_{JS}(p || q) = \\frac{D_{KL}(p || q) + D_{KL}(q || p)}{2}\n\nOther forms of divergence measurements exist, such as the Wasserstein distance which improves generative adversarial networks (Arjovsky et al., 2017), but they are not relevant here. See https://www.alexirpan.com/2017/02/22/wasserstein-gan.html for more explanations.\nWe now have a global measurement of the similarity between two distributions on the whole input space, but which is hard to compute. How can we use it anyway in our optimization problem? As mentioned above, we search for the biggest parameter change \\Delta \\theta inducing the smallest change in the policy. We need a metric linking changes in the parameters of the distribution (the weights of the network) to changes in the distribution itself. In other terms, we will apply gradient descent on the statistical manifold defined by the parameters rather than on the parameters themselves.\n\n\n\n\n\n\nFigure 15.2: Naive illustration of the Riemannian metric. The Euclidian distance between p(x; \\theta) and p(x; \\theta + \\Delta \\theta) depends on the Euclidian distance between \\theta and \\theta + \\Delta\\theta, i.e. \\Delta \\theta. Riemannian metrics follow the geometry of the manifold to compute that distance, depending on its curvature. This figure is only for illustration: Riemannian metrics are purely local, \\Delta \\theta should be much smaller.\n\n\n\nLet’s consider a parameterized distribution p(x; \\theta) and its new value p(x; \\theta + \\Delta \\theta) after applying a small parameter change \\Delta \\theta. As depicted on Figure 15.2, the Euclidian metric in the parameter space (||\\theta + \\Delta \\theta - \\theta||^2) does not take the structure of the statistical manifold into account. We need to define a Riemannian metric which accounts locally for the curvature of the manifold between \\theta and \\theta + \\Delta \\theta. The Riemannian distance is defined by the dot product:\n\n    ||\\Delta \\theta||^2 = &lt; \\Delta \\theta , F(\\theta) \\, \\Delta \\theta &gt;\n\nwhere F(\\theta) is called the Riemannian metric tensor and is an inner product on the tangent space of the manifold at the point \\theta.\nWhen using the symmetric KL divergence to measure the distance between two distributions, the corresponding Riemannian metric is the Fisher Information Matrix (FIM), defined as the Hessian matrix of the KL divergence around \\theta, i.e. the matrix of second order derivatives w.r.t the elements of \\theta. See https://stats.stackexchange.com/questions/51185/connection-between-fisher-metric-and-the-relative-entropy and https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/ for an explanation of the link between the Fisher matrix and KL divergence.\nThe Fisher information matrix is defined as the Hessian of the KL divergence around \\theta, i.e. how the manifold locally changes around \\theta:\n\n    F(\\theta) = \\nabla^2 D_{JS}(p(x; \\theta) || p(x; \\theta + \\Delta \\theta))|_{\\Delta \\theta = 0}\n\nwhich necessitates to compute second order derivatives which are very complex and slow to obtain, especially when there are many parameters \\theta (the weights of the NN). Fortunately, it also has a simpler form which only depends on the outer product between the gradients of the log-likelihoods:\n\n    F(\\theta) = \\mathbb{E}_{x \\sim p(x, \\theta)}[ \\nabla \\log p(x; \\theta)  (\\nabla \\log p(x; \\theta))^T]\n\nwhich is something we can easily sample and compute.\nWhy is it useful? The Fisher Information matrix allows to locally approximate (for small \\Delta \\theta) the KL divergence between the two close distributions (using a second-order Taylor series expansion):\n\n    D_{JS}(p(x; \\theta) || p(x; \\theta + \\Delta \\theta)) \\approx \\Delta \\theta^T \\, F(\\theta) \\, \\Delta \\theta\n\nThe KL divergence is then locally quadratic, which means that the update rules obtained when minimizing the KL divergence with gradient descent will be linear. Suppose we want to minimize a loss function L parameterized by \\theta and depending on the distribution p. Natural gradient descent (Amari, 1998) attempts to move along the statistical manifold defined by p by correcting the gradient of L(\\theta) using the local curvature of the KL-divergence surface, i.e. moving some given distance in the direction \\tilde{\\nabla_\\theta} L(\\theta):\n\n    \\tilde{\\nabla_\\theta} L(\\theta) = F(\\theta)^{-1} \\, \\nabla_\\theta L(\\theta)\n\n\\tilde{\\nabla_\\theta} L(\\theta) is the natural gradient of L(\\theta). Natural gradient descent simply takes steps in this direction:\n\n    \\Delta \\theta = - \\eta \\, \\tilde{\\nabla_\\theta} L(\\theta)\n\nWhen the manifold is not curved (F(\\theta) is the identity matrix), natural gradient descent is the regular gradient descent.\nBut what is the advantage of natural gradients? The problem with regular gradient descent is that it relies on a fixed learning rate. In regions where the loss function is flat (a plateau), the gradient will be almost zero, leading to very slow improvements. Because the natural gradient depends on the inverse of the curvature (Fisher), the magnitude of the gradient will be higher in flat regions, leading to bigger steps, and smaller in very steep regions (around minima). Natural GD therefore converges faster and better than regular GD.\nNatural gradient descent is a generic optimization method, it can for example be used to train more efficiently deep networks in supervised learning. Its main drawback is the necessity to inverse the Fisher information matrix, whose size depends on the number of free parameters (if you have N weights in the NN, you need to inverse a NxN matrix). Several approximations allows to remediate to this problem, for example Conjugate Gradients or Kronecker-Factored Approximate Curvature (K-FAC).\n\n\n\n\n\n\nAdditional resources\n\n\n\n\nhttp://andymiller.github.io/2016/10/02/natural_gradient_bbvi.html\nhttps://hips.seas.harvard.edu/blog/2013/01/25/the-natural-gradient/\nhttp://kvfrans.com/what-is-the-natural-gradient-and-where-does-it-appear-in-trust-region-policy-optimization\nhttps://wiseodd.github.io/techblog/2018/03/14/natural-gradient/\nA tutorial by John Schulman (OpenAI) https://www.youtube.com/watch?v=xvRrgxcpaHY\nA blog post on the related Hessian-free optimization and conjuguate gradients http://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/\nK-FAC: https://syncedreview.com/2017/03/25/optimizing-neural-networks-using-structured-probabilistic-models/\nConjugate gradients: https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\n\nNote: Natural gradients can also be used to train DQN architectures, resulting in more efficient and stable learning behaviors (Knight and Lerner, 2018).",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Natural gradients</span>"
    ]
  },
  {
    "objectID": "src/3.5-NaturalGradient.html#natural-policy-gradient-and-natural-actor-critic-nac",
    "href": "src/3.5-NaturalGradient.html#natural-policy-gradient-and-natural-actor-critic-nac",
    "title": "Natural gradients",
    "section": "Natural policy gradient and Natural Actor Critic (NAC)",
    "text": "Natural policy gradient and Natural Actor Critic (NAC)\nKakade (2001) applied the principle of natural gradients proposed by Amari (1998) to the policy gradient theorem:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)]\n\nThis regular gradient does not take into account the underlying structure of the policy distribution \\pi(s, a). The Fisher information matrix for the policy is defined by:\n\n    F(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[ \\nabla \\log \\pi_\\theta(s, a)  (\\nabla \\log \\pi_\\theta(s, a))^T]\n\nThe natural policy gradient is simply:\n\n    \\tilde{\\nabla}_\\theta J(\\theta) = F(\\theta)^{-1} \\, \\nabla_\\theta J(\\theta)  = \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[ F(\\theta)^{-1} \\,  \\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)]\n\nKakade (2001) also shows that you can replace the true Q-value Q^{\\pi_\\theta}(s, a) with a compatible approximation Q_\\varphi(s, a) (as long as it minimizes the quadratic error) and still obtained an unbiased natural gradient. An important theoretical result is that policy improvement is guaranteed with natural gradients: the new policy after an update is always better (more expected returns) than before. He experimented this new rule on various simple MDPs and observed drastic improvements over vanilla PG.\nPeters and Schaal (2008) extended on the work of Kakade (2001) to propose the natural actor-critic (NAC). The exact derivations would be too complex to summarize here, but the article is an interesting read. He particularly reviews the progress at that time on policy gradient for its use in robotics. He showed that the F(\\theta)) is a true Fisher information matrix even when using sampled episodes, and derived a baseline b to reduce the variance of the natural policy gradient. He demonstrated the power of this algorithm by letting a robot learning motor primitives for baseball.\n\n\n\n\nAmari, S.-I. (1998). Natural gradient works efficiently in learning. Neural Computation 10, 251–276.\n\n\nArjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein GAN. Available at: http://arxiv.org/abs/1701.07875.\n\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al. (2014). Generative Adversarial Networks. Available at: http://arxiv.org/abs/1406.2661.\n\n\nKakade, S. (2001). A Natural Policy Gradient. in Advances in Neural Information Processing Systems 14 Available at: https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf.\n\n\nKingma, D. P., and Welling, M. (2013). Auto-Encoding Variational Bayes. Available at: http://arxiv.org/abs/1312.6114.\n\n\nKnight, E., and Lerner, O. (2018). Natural Gradient Deep Q-learning. Available at: http://arxiv.org/abs/1803.07482.\n\n\nPeters, J., and Schaal, S. (2008). Reinforcement learning of motor skills with policy gradients. Neural Networks 21, 682–697. doi:10.1016/j.neunet.2008.02.003.\n\n\nSchulman, J., Chen, X., and Abbeel, P. (2017). Equivalence Between Policy Gradients and Soft Q-Learning. Available at: http://arxiv.org/abs/1704.06440 [Accessed June 12, 2019].\n\n\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2015). High-Dimensional Continuous Control Using Generalized Advantage Estimation. Available at: http://arxiv.org/abs/1506.02438.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Natural gradients</span>"
    ]
  },
  {
    "objectID": "src/3.6-PPO.html",
    "href": "src/3.6-PPO.html",
    "title": "Policy optimization (TRPO, PPO)",
    "section": "",
    "text": "Trust Region Policy Optimization (TRPO)",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Policy optimization (TRPO, PPO)</span>"
    ]
  },
  {
    "objectID": "src/3.6-PPO.html#trust-region-policy-optimization-trpo",
    "href": "src/3.6-PPO.html#trust-region-policy-optimization-trpo",
    "title": "Policy optimization (TRPO, PPO)",
    "section": "",
    "text": "Principle\nSchulman et al. (2015) extended the idea of natural gradients to allow their use for non-linear function approximators (e.g. deep networks), as the previous algorithms only worked efficiently for linear approximators. The proposed algorithm, Trust Region Policy Optimization (TRPO), has now been replaced in practice by Proximal Policy Optimization (PPO, see next section) but its novel ideas are important to understand already.\nLet’s note the expected return of a policy \\pi as:\n\n    \\eta(\\pi) = \\mathbb{E}_{s \\sim \\rho_\\pi, a \\sim \\pi}[\\sum_{t=0}^\\infty \\gamma^t \\, r(s_t, a_t, s_{t+1})]\n\nwhere \\rho_\\pi is the discounted visitation frequency distribution (the probability that a state s will be visited at some point in time by the policy \\pi):\n\n    \\rho_\\pi(s) = P(s_0=s) + \\gamma \\, P(s_1=s) + \\gamma^2 \\, P(s_2=s) + \\ldots\n\nKakade and Langford (2002) had shown that it is possible to relate the expected return of two policies \\pi_\\theta and \\pi_{\\theta_\\text{old}} using advantages (omitting \\pi in the notations):\n\n    \\eta(\\theta) = \\eta(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\pi_\\theta}, a \\sim \\pi_\\theta} [A_{\\pi_{\\theta_\\text{old}}}(s, a)]\n\nThe advantage A_{\\pi_{\\theta_\\text{old}}}(s, a) denotes the change in the expected return obtained after (s, a) when using the new policy \\pi_\\theta, in comparison to the one obtained with the old policy \\pi_{\\theta_\\text{old}}. While this formula seems interesting (it measures how good the new policy is with regard to the average performance of the old policy, so we could optimize directly), it is difficult to estimate as the mathematical expectation depends on state-action pairs generated by the new policy : s \\sim \\rho_{\\pi_\\theta}, a \\sim \\pi_\\theta.\nSchulman et al. (2015) propose an approximation to this formula, by considering that if the two policies \\pi_\\theta and \\pi_{\\theta_\\text{old}} are not very different from another, one can sample the states from the old distribution:\n\n    \\eta(\\theta) \\approx \\eta(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_\\text{old}}}, a \\sim \\pi_\\theta} [A_{\\pi_{\\theta_\\text{old}}}(s, a)]\n\nOne can already recognize the main motivation behind natural gradients: finding a weight update that moves the policy in the right direction (getting more rewards) while keeping the change in the policy distribution as small as possible (to keep the assumption correct).\nLet’s now define the following objective function:\n\n    J_{\\theta_\\text{old}}(\\theta) = \\eta(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_\\text{old}}}, a \\sim \\pi_\\theta} [A_{\\pi_{\\theta_\\text{old}}}(s, a)]\n\nIt is easy to see that J_{\\theta_\\text{old}}(\\theta_\\text{old}) = \\eta(\\theta_\\text{old}) by definition of the advantage, and that its gradient w.r.t to \\theta taken in \\theta_\\text{old} is the same as the one of \\eta(\\theta_\\text{old}):\n\n    \\nabla_\\theta J_{\\theta_\\text{old}}(\\theta)|_{\\theta = \\theta_\\text{old}} = \\nabla_\\theta \\eta(\\theta)|_{\\theta = \\theta_\\text{old}}\n\nThis means that, at least locally, one maximization step of J_{\\theta_\\text{old}}(\\theta) goes in the same direction as maximizing \\eta(\\theta) if we do not go too far. J is called a surrogate objective function: it is not what we want to optimize, but it leads to the same result. TRPO belongs to the class of minorization-majorization algorithms (MM, we first find a local lower bound and then maximize it, iteratively).\nLet’s now suppose that we can find its maximum, i.e. a policy \\pi' that maximizes the advantage of each state-action pair over \\pi_{\\theta_\\text{old}}. There would be no guarantee that \\pi' and \\pi_{\\theta_\\text{old}} are close enough so that the assumption stands. We could therefore make only a small step in its direction and hope for the best:\n\n    \\pi_\\theta(s, a) = (1-\\alpha) \\, \\pi_{\\theta_\\text{old}}(s, a) + \\alpha \\, \\pi'(s,a)\n\\tag{16.1}\nThis is the conservative policy iteration method of Kakade and Langford (2002), where a bound on the difference between \\eta(\\pi_{\\theta_\\text{old}}) and J_{\\theta_\\text{old}}(\\theta) can be derived.\nSchulman et al. (2015) propose to penalize instead the objective function by the KL divergence between the new and old policies. There are basically two ways to penalize an optimization problem:\n\nAdding a hard constraint on the KL divergence, leading to a constrained optimization problem (where Lagrange methods can be applied):\n\n\n    \\text{maximize}_\\theta \\qquad J_{\\theta_\\text{old}}(\\theta) \\\\\n    \\qquad \\text{subject to} \\qquad D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) \\leq \\delta\n\n\nRegularizing the objective function with the KL divergence:\n\n\n    \\text{maximize}_\\theta \\qquad L(\\theta) = J_{\\theta_\\text{old}}(\\pi_\\theta) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta)\n\nIn the first case, we force the KL divergence to stay below a certain threshold. In the second case, we penalize solutions that would maximize J_{\\theta_\\text{old}}(\\theta) but would be too different from the previous policy. In both cases, we want to find a policy \\pi_\\theta maximizing the expected return (the objective), but which is still close (in terms of KL divergence) from the current one. Both methods are however sensible to the choice of the parameters \\delta and C.\nFormally, the KL divergence D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) should be the maximum KL divergence over the state space:\n\n    D^\\text{max}_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) = \\max_s D_{KL}(\\pi_{\\theta_\\text{old}}(s, .) || \\pi_\\theta(s, .))\n\nThis maximum KL divergence over the state space would be very hard to compute. Empirical evaluations showed however that it is safe to use the mean KL divergence, or even to sample it:\n\n    \\bar{D}_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) = \\mathbb{E}_s [D_{KL}(\\pi_{\\theta_\\text{old}}(s, .) || \\pi_\\theta(s, .))] \\approx \\frac{1}{N} \\sum_{i=1}^N D_{KL}(\\pi_{\\theta_\\text{old}}(s_i, .) || \\pi_\\theta(s_i, .))\n\n\n\nTrust regions\n\n\n\n\n\n\nFigure 16.1: Graphical illustration of trust regions. From the current parameters \\theta_\\text{old}, we search for the maximum \\theta^* of the real objective \\eta(\\theta). The unconstrained objective J_{\\theta_\\text{old}}(\\theta) is locally similar to \\eta(\\theta) but quickly diverge as \\pi_\\theta and \\pi_{\\theta_\\text{old}} become very different. The surrogate objective L(\\theta) = J_{\\theta_\\text{old}} (\\theta) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) is always smaller than \\eta(\\theta) and has a maximum close to \\theta^* which keeps \\pi_\\theta and \\pi_{\\theta_\\text{old}} close from each other in terms of KL divergence. The region around \\theta_\\text{old} where big optimization steps can be taken without changing the policy too much is called the trust region.\n\n\n\nBefore diving further into how these optimization problems can be solved, let’s wonder why the algorithm is called trust region policy optimization using the regularized objective. Figure 16.1 illustrates the idea. The “real” objective function \\eta(\\theta) should be maximized (with gradient descent or similar) starting from the parameters \\theta_\\text{old}. We cannot estimate the objective function directly, so we build a surrogate objective function L(\\theta) = J_{\\theta_\\text{old}} (\\theta) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta). We know that:\n\nThe two objectives have the same value in \\theta_\\text{old}: \nL(\\theta_\\text{old}) = J_{\\theta_\\text{old}}(\\theta_\\text{old}) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_{\\theta_\\text{old}}) = \\eta(\\theta_\\text{old})\n\nTheir gradient w.r.t \\theta are the same in \\theta_\\text{old}: \n\\nabla_\\theta L(\\theta)|_{\\theta = \\theta_\\text{old}} = \\nabla_\\theta \\eta(\\theta)|_{\\theta = \\theta_\\text{old}}\n\nThe surrogate objective is always smaller than the real objective, as the KL divergence is positive: \n\\eta(\\theta) \\geq J_{\\theta_\\text{old}}(\\theta) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta)\n\n\nUnder these conditions, the surrogate objective is also called a lower bound of the primary objective. The interesting fact is that the value of \\theta that maximizes L(\\theta) is at the same time:\n\nA big step in the parameter space towards the maximum of \\eta(\\theta), as \\theta and \\theta_\\text{old} can be very different.\nA small step in the policy distribution space, as the KL divergence between the previous and the new policies is kept small.\n\nExactly what we needed! The parameter region around \\theta_\\text{old} where the KL divergence is kept small is called the trust region. This means that we can safely take big optimization steps (e.g. with a high learning rate or even analytically) without risking to violate the initial assumptions.\n\n\nSample-based formulation\nAlthough the theoretical proofs in Schulman et al. (2015) used the regularized optimization method, the practical implementation uses the constrained optimization problem:\n\n    \\text{maximize}_\\theta \\qquad J_{\\theta_\\text{old}}(\\theta) = \\eta(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_\\text{old}}}, a \\sim \\pi_\\theta} [A_{\\pi_{\\theta_\\text{old}}}(s, a)] \\\\\n    \\qquad \\text{subject to} \\qquad D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) \\leq \\delta\n\nThe first thing to notice is that \\eta(\\theta_\\text{old}) does not depend on \\theta, so it is constant in the optimization problem. We only need to maximize the advantage of the actions taken by \\pi_\\theta in each state visited by \\pi_{\\theta_\\text{old}}. The problem is that \\pi_\\theta is what we search, so we can not sample actions from it. The solution is to use importance sampling to allow sampling actions from \\pi_{\\theta_\\text{old}}. This is possible as long as we correct the objective with the importance sampling weight:\n\n    \\text{maximize}_\\theta \\qquad \\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_\\text{old}}}, a \\sim \\pi_{\\theta_\\text{old}}} [\\frac{\\pi_\\theta(s, a)}{\\pi_{\\theta_\\text{old}}(s, a)} \\, A_{\\pi_{\\theta_\\text{old}}}(s, a)] \\\\\n    \\qquad \\text{subject to} \\qquad D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) \\leq \\delta\n\nNow that states and actions are generated by the old policy, we can safely sample many trajectories using \\pi_{\\theta_\\text{old}} (Schulman et al. (2015) proposes two methods called single path and Vine, but we ignore it here), compute the advantages of all state-action pairs (using real rewards along the trajectories), form the surrogate objective function and optimize it using second-order optimization methods.\nOne last thing to notice is that the advantages A_{\\pi_{\\theta_\\text{old}}}(s, a) = Q_{\\pi_{\\theta_\\text{old}}}(s, a) - V_{\\pi_{\\theta_\\text{old}}}(s) depend on the value of the states encountered by \\pi_{\\theta_\\text{old}}. The state values do not depend on the policies, they are constant for each optimization step, so they can also be safely removed:\n\n    \\text{maximize}_\\theta \\qquad \\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_\\text{old}}}, a \\sim \\pi_{\\theta_\\text{old}}} [\\frac{\\pi_\\theta(s, a)}{\\pi_{\\theta_\\text{old}}(s, a)} \\, Q_{\\pi_{\\theta_\\text{old}}}(s, a)] \\\\\n    \\qquad \\text{subject to} \\qquad D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) \\leq \\delta\n\nHere we go, that’s TRPO. It could seem a bit disappointing to come up with such a simple formulation (find a policy which maximizes the Q-value of sampled actions while being not too different from the previous one) after so many mathematical steps, but that is also the beauty of it: not only it works, but it is guaranteed to work. With TRPO, each optimization step brings the policy closer from an optimum, what is called monotonic improvement.\n\n\nPractical implementation\nNow, how do we solve the constrained optimization problem? And what is the link with natural gradients?\nTo solve constrained optimization problems, we can form a Lagrange function with an additional parameter \\lambda and search for its maximum:\n\n    \\mathcal{L}(\\theta, \\lambda) = J_{\\theta_\\text{old}}(\\theta)  - \\lambda \\, (D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) - \\delta)\n\nNotice how close the Lagrange function is from the regularized problem used in the theory. We can form a first-order approximation of J_{\\theta_\\text{old}}(\\theta):\n\n    J_{\\theta_\\text{old}}(\\theta) = \\nabla_\\theta J_{\\theta_\\text{old}}(\\theta) \\, (\\theta- \\theta_\\text{old})\n\nas J_{\\theta_\\text{old}}(\\theta_\\text{old}) = 0. g = \\nabla_\\theta J_{\\theta_\\text{old}}(\\theta) is the now familiar policy gradient with importance sampling. Higher-order terms do not matter, as they are going to be dominated by the KL divergence term.\nWe will use a second-order approximation of the KL divergence term using the Fisher Information Matrix:\n\n    D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) = (\\theta- \\theta_\\text{old})^T \\, F(\\theta_\\text{old}) \\,  (\\theta- \\theta_\\text{old})\n\nWe get the following Lagrangian function:\n\n    \\mathcal{L}(\\theta, \\lambda) = \\nabla_\\theta J_{\\theta_\\text{old}}(\\theta) \\, (\\theta- \\theta_\\text{old})  - \\lambda \\, (\\theta- \\theta_\\text{old})^T \\, F(\\theta_\\text{old}) \\,  (\\theta- \\theta_\\text{old})\n\nwhich is quadratic in \\Delta \\theta = \\theta- \\theta_\\text{old}. It has therefore a unique maximum, characterized by a first-order derivative equal to 0:\n\n    \\nabla_\\theta J_{\\theta_\\text{old}}(\\theta) = \\lambda \\, F(\\theta_\\text{old}) \\,  \\Delta \\theta\n\nor:\n\n    \\Delta \\theta  = \\frac{1}{\\lambda} \\, F(\\theta_\\text{old})^{-1} \\,  \\nabla_\\theta J_{\\theta_\\text{old}}(\\theta)\n\nwhich is the natural gradient descent! The size of the step \\frac{1}{\\lambda} still has to be determined, but it can also be replaced by a fixed hyperparameter.\nThe main problem is now to compute and inverse the Fisher information matrix, which is quadratic with the number of parameters \\theta, i.e. with the number of weights in the NN. Schulman et al. (2015) proposes to used conjugate gradients to iteratively approximate the Fisher, a second-order method which will not be presented here (see https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf for a detailed introduction). After the conjugate gradient optimization step, the constraint D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) \\leq \\delta is however not ensured anymore, so a line search is made as in Equation 16.1 until that criteria is met.\n\n\nSummary\nTRPO is a policy gradient method using natural gradients to monotonically improve the expected return associated to the policy. As a minorization-maximization (MM) method, it uses a surrogate objective function (a lower bound on the expected return) to iteratively change the parameters of the policy using large steps, but without changing the policy too much (as measured by the KL divergence). Its main advantage over DDPG is that it is much less sensible to the choice of the learning rate.\nHowever, it has several limitations:\n\nIt is hard to use with neural networks having multiple outputs (e.g. the policy and the value function, as in actor-critic methods) as natural gradients are dependent on the policy distribution and its relationship with the parameters.\nIt works well when the NN has only fully-connected layers, but empirically performs poorly on tasks requiring convolutional layers or recurrent layers.\nThe use of conjugate gradients makes the implementation much more complicated and less flexible than regular SGD.\n\n\n\n\n\n\n\nAdditional resources\n\n\n\n\nhttp://178.79.149.207/posts/trpo.html\nhttps://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9\nhttps://medium.com/@sanketgujar95/trust-region-policy-optimization-trpo-and-proximal-policy-optimization-ppo-e6e7075f39ed\nhttps://www.depthfirstlearning.com/2018/TRPO\nhttp://rll.berkeley.edu/deeprlcoursesp17/docs/lec5.pdf",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Policy optimization (TRPO, PPO)</span>"
    ]
  },
  {
    "objectID": "src/3.6-PPO.html#sec-PPO",
    "href": "src/3.6-PPO.html#sec-PPO",
    "title": "Policy optimization (TRPO, PPO)",
    "section": "Proximal Policy Optimization (PPO)",
    "text": "Proximal Policy Optimization (PPO)\nProximal Policy Optimization (PPO) was proposed by Schulman et al. (2017) to overcome the problems of TRPO (complexity, inability to share parameters or to use complex NN architectures) while increasing the range of tasks learnable by the system (compared to DQN) and improving the sample complexity (compared to online PG methods, which perform only one update per step).\nFor that, they investigated various surrogate objectives (lower bounds) that could be solved using first-order optimization techniques (gradient descent). Let’s rewrite the surrogate loss of TRPO in the following manner:\n\n    L^\\text{CPI}(\\theta) = \\mathbb{E}_{t} [\\frac{\\pi_\\theta(s_t, a_t)}{\\pi_{\\theta_\\text{old}}(s_t, a_t)} \\, A_{\\pi_{\\theta_\\text{old}}}(s_t, a_t)] = \\mathbb{E}_{t} [\\rho_t(\\theta) \\, A_{\\pi_{\\theta_\\text{old}}}(s_t, a_t)]\n\nby making the dependency over time explicit and noting the importance sampling weight \\rho_t(\\theta). The superscript CPI refers to conservative policy iteration (Kakade and Langford, 2002). Without a constraint, the maximization of L^\\text{CPI} would lead to an excessively large policy updates. The authors searched how to modify the objective, in order to penalize changes to the policy that make \\rho_t(\\theta) very different from 1, i.e. where the KL divergence between the new and old policies would become high. They ended up with the following surrogate loss:\n\n    L^\\text{CLIP}(\\theta) = \\mathbb{E}_{t} [ \\min (\\rho_t(\\theta) \\, A_{\\pi_{\\theta_\\text{old}}}(s_t, a_t), \\text{clip}(\\rho_t(\\theta) , 1- \\epsilon, 1+\\epsilon) \\,  A_{\\pi_{\\theta_\\text{old}}}(s_t, a_t)]\n\nThe left part of the min operator is the surrogate objective of TRPO L^\\text{CPI}(\\theta). The right part restricts the importance sampling weight between 1-\\epsilon and 1 +\\epsilon. Let’s consider two cases (depicted on Figure 16.2):\n\n\n\n\n\n\nFigure 16.2: Illustration of the effect of clipping the importance sampling weight. Source: Schulman et al. (2017).\n\n\n\n\nthe transition s_t, a_t has a positive advantage, i.e. it is a better action than expected. The probability of selecting that action again should be increased, i.e. \\pi_\\theta(s_t, a_t) &gt; \\pi_{\\theta_\\text{old}}(s_t, a_t). However, the importance sampling weight could become very high (a change from 0.01 to 0.05 is a ration of \\rho_t(\\theta) = 5). In that case, \\rho_t(\\theta) will be clipped to 1+\\epsilon, for example 1.2. As a consequence, the parameters \\theta will move in the right direction, but the distance between the new and the old policies will stay small.\nthe transition s_t, a_t has a negative advantage, i.e. it is a worse action than expected. Its probability will be decreased and the importance sampling weight might become much smaller than 1. Clipping it to 1-\\epsilon avoids drastic changes to the policy, while still going in the right direction.\n\nFinally, they take the minimum of the clipped and unclipped objective, so that the final objective is a lower bound of the unclipped objective. In the original paper, they use generalized advantage estimation (GAE, section Generalized Advantage Estimation (GAE)) to estimate A_{\\pi_{\\theta_\\text{old}}}(s_t, a_t), but anything could be used (n-steps, etc). Transitions are sampled by multiple actors in parallel, as in A2C.\nThe pseudo-algorithm of PPO is as follows:\n\n\n\n\n\n\nPPO algorithm\n\n\n\n\nInitialize an actor \\pi_\\theta and a critic V_\\varphi with random weights.\nwhile not converged :\n\nfor N actors in parallel:\n\nCollect T transitions using \\pi_\\text{old}.\nCompute the generalized advantage of each transition using the critic.\n\nfor K epochs:\n\nSample M transitions from the ones previously collected.\nTrain the actor to maximize the clipped surrogate objective.\n\n\n      L^\\text{CLIP}(\\theta) = \\mathbb{E}_{t} [ \\min (\\rho_t(\\theta) \\, A_{\\pi_{\\theta_\\text{old}}}(s_t, a_t), \\text{clip}(\\rho_t(\\theta) , 1- \\epsilon, 1+\\epsilon) \\,  A_{\\pi_{\\theta_\\text{old}}}(s_t, a_t)]\n  \n\nTrain the critic to minimize the mse using TD learning.\n\n\\theta_\\text{old} \\leftarrow \\theta\n\n\n\n\nThe main advantage of PPO with respect to TRPO is its simplicity: the clipped objective can be directly maximized using first-order methods like stochastic gradient descent or Adam. It does not depend on assumptions about the parameter space: CNNs and RNNs can be used for the policy. It is sample-efficient, as several epochs of parameter updates are performed between two transition samplings: the policy network therefore needs less fresh samples that strictly on-policy algorithms to converge.\nThe only drawbacks of PPO is that there no convergence guarantee (although in practice it converges more often than other state-of-the-art methods) and that the right value for \\epsilon has to be determined. PPO has improved the state-of-the-art on Atari games and Mujoco robotic tasks. It has become the go-to method for continuous control problems.\n\n\n\n\n\n\nAdditional resources\n\n\n\n\nMore explanations and demos from OpenAI: https://blog.openai.com/openai-baselines-ppo\n\n\n\n\n\n\n\n\nKakade, S., and Langford, J. (2002). Approximately Optimal Approximate Reinforcement Learning. Proc. 19th International Conference on Machine Learning, 267–274. Available at: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601.\n\n\nSchulman, J., Chen, X., and Abbeel, P. (2017). Equivalence Between Policy Gradients and Soft Q-Learning. Available at: http://arxiv.org/abs/1704.06440 [Accessed June 12, 2019].\n\n\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2015). High-Dimensional Continuous Control Using Generalized Advantage Estimation. Available at: http://arxiv.org/abs/1506.02438.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Policy optimization (TRPO, PPO)</span>"
    ]
  },
  {
    "objectID": "src/3.7-ACER.html",
    "href": "src/3.7-ACER.html",
    "title": "Actor-Critic with Experience Replay (ACER)",
    "section": "",
    "text": "The natural gradient methods presented previously (TRPO, PPO) are stochastic actor-critic methods, therefore strictly on-policy. Off-policy methods such as DQN or DDPG allow to reuse past transitions through the usage of an experience replay memory, potentially reducing the sample complexity at the cost of a higher variance and worse stability. Wang et al. (2017) proposed an off-policy actor-critic architecture using variance reduction techniques, the off-policy Retrace algorithm (Munos et al., 2016), parallel training of multiple actor-learners (Mnih et al., 2016), truncated importance sampling with bias correction, stochastic duelling network architectures (Wang et al., 2016), and efficient trust region policy optimization. It can be seen as the off-policy counterpart to A3C.\nThe first aspect of ACER is that it interleaves on-policy learning with off-policy: the agent samples a trajectory \\tau, learns from it on-policy, stores it in the replay buffer, samples n trajectories from the replay buffer and learns off-policy from each of them:\n\n\n\n\n\n\nACER algorithm\n\n\n\n\nSample a trajectory \\tau using the current policy.\nApply ACER on-policy on \\tau.\nStore \\tau in the replay buffer.\nSample n trajectories from the replay buffer.\nfor each sampled trajectory \\tau_k:\n\nApply ACER off-policy on \\tau_k.\n\n\n\n\nMixing on-policy learning with off-policy is quite similar to the Self-Imitation Learning approach of (Oh et al., 2018) (see Section Self-Imitation Learning (SIL)).\n\nRetrace evaluation\nACER comes in two flavors: one for discrete action spaces, one for continuous spaces. The discrete version is simpler, so let’s focus on this one. As any policy-gradient method, ACER tries to estimate the policy gradient for each transition of a trajectory, but using importance sampling (Degris et al., 2012):\n\n    \\nabla_\\theta J(\\theta)  = \\mathbb{E}_{s_t, a_t \\sim \\rho_b} [\\frac{\\pi_\\theta(s_t, a_t)}{b(s_t, a_t)} \\, Q_\\varphi(s_t, a_t) \\, \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)]\n\nThe problem is now to train the critic Q_\\varphi(s_t, a_t) by computing the correct target. ACER learning builds on the Retrace algorithm (Munos et al., 2016):\n\n    \\Delta Q_\\varphi(s_t, a_t) = \\alpha \\, \\sum_{t'=t}^T (\\gamma)^{t'-t} \\left(\\prod_{s=t+1}^{t'} c_s \\right) \\, \\delta_{t'}\n\nwith c_s = \\lambda \\min (1, \\frac{\\pi_\\theta(s_s, a_s)}{b(s_s, a_s)}) being the clipped importance sampling weight and \\delta_{t'} is the TD error at time t'&gt;t:\n\n    \\delta_{t'} = r_{t'+1} + \\gamma \\, V(s_{t'+1}) - V(s_{t'})\n\nBy noting Q^\\text{ret} the target value for the update of the critic (neglecting the learning rate \\alpha):\n\n    Q^\\text{ret}(s_t, a_t) = Q_\\varphi(s_t, a_t) +  \\sum_{t'=t}^T (\\gamma)^{t'-t} \\left(\\prod_{s=t+1}^{t'} c_s \\right) \\, \\delta_{t'}\n\nwe can transform the Retrace formula into a recurrent one:\n\n\\begin{aligned}\n    Q^\\text{ret}(s_t, a_t) & = Q_\\varphi(s_t, a_t) + \\delta_t + \\sum_{t'=t+1}^T (\\gamma)^{t'-t} \\left(\\prod_{s=t+1}^{t'} c_s \\right) \\, \\delta_{t'} \\\\\n    & = Q_\\varphi(s_t, a_t) + \\delta_t + \\gamma \\, c_{t+1} \\, (Q^\\text{ret}(s_{t+1}, a_{t+1}) - Q_\\varphi(s_{t+1}, a_{t+1})) \\\\\n\\end{aligned}\n\nQ_\\varphi(s_t, a_t) + \\delta_t = Q_\\varphi(s_t, a_t) + r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t) can furthermore be reduced to r_{t+1} + \\gamma \\, V(s_{t+1}) by considering that Q_\\varphi(s_t, a_t) \\approx V(s_t) (the paper does not justify this assumption, but it should be true in expectation).\nThis gives us the following target value for the Q-values:\n\n    Q^\\text{ret}(s_t, a_t) = r_{t+1} + \\gamma \\, c_{t+1} \\, (Q^\\text{ret}(s_{t+1}, a_{t+1}) - Q_\\varphi(s_{t+1}, a_{t+1})) + \\gamma \\, V(s_{t+1})\n\nOne remaining issue is that the critic would also need to output the value of each state V(s_{t+1}), in addition to the Q-values Q_\\varphi(s_t, a_t). In the discrete case, this is not necessary, as the value of a state is the expectation of the value of the available actions under the current policy:\n\n    V(s_{t+1}) = \\mathbb{E}_{a_{t+1} \\sim \\pi_\\theta} [Q_\\varphi(s_{t+1}, a_{t+1})] = \\sum_a \\pi_\\theta(s_{t+1}, a) \\, Q_\\varphi(s_{t+1}, a))\n\nThe value of the next state can be easily computed when we have the policy \\pi_\\theta(s, a) (actor) and the Q-value Q_\\varphi(s, a) (critic) of each action a in a state s.\nThe actor-critic architecture needed for ACER is therefore the following:\n\nThe actor \\pi_\\theta takes a state s as input and outputs a vector of probabilities \\pi_\\theta for each available action.\nThe critic Q_\\varphi takes a state s as input and outputs a vector of Q-values.\n\nThis is different from the architecture of A3C, where the critic “only” had to output the value of a state V_\\varphi(s): it is now a vector of Q-values. Note that the actor and the critic can share most of their parameters: the network only needs to output two different vectors \\pi_\\theta(s) and Q_\\varphi(s) for each input state s. This makes a “two heads” NN, similar to the duelling architecture of (Wang et al., 2016).\nThe target Q-value Q^\\text{ret}(s, a) can be found recursively by iterating backwards over the episode:\n\n\n\n\n\n\nRetrace evaluation\n\n\n\n\nInitialize Q^\\text{ret}(s_T, a_T), Q_\\varphi(s_T, a_T) and V(s_T) to 0, as the terminal state has no value.\nfor t \\in [T-1, \\ldots, 0]:\n\nUpdate the target Q-value using the received reward, the critic and the previous target value:\n\n\n      Q^\\text{ret}(s_t, a_t) = r_{t+1} + \\gamma \\, c_{t+1} \\, (Q^\\text{ret}(s_{t+1}, a_{t+1}) - Q_\\varphi(s_{t+1}, a_{t+1})) + \\gamma \\, V(s_{t+1})\n  \n\nApply the critic on the current action:\n\n\n      Q_\\varphi(s_t, a_t)\n  \n\nEstimate the value of the state using the critic:\n\n\n      V(s_t) = \\sum_a \\pi_\\theta(s_t) \\, Q_\\varphi(s_t, a)\n  \n\n\n\nAs the target value Q^\\text{ret}(s, a) use multiple “real” rewards r_{t+1}, it is actually less biased than the critic Q_\\varphi(s, a). It is then better to use it to update the actor:\n\n    \\nabla_\\theta J(\\theta)  = \\mathbb{E}_{s_t, a_t \\sim \\rho_b} [\\frac{\\pi_\\theta(s_t, a_t)}{b(s_t, a_t)} \\, Q^\\text{ret}(s_t, a_t) \\, \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)]\n\nThe critic just has to minimize the mse with the target value:\n\n    \\mathcal{L}(\\varphi) = \\mathbb{E}_{s_t, a_t \\sim \\rho_b} [(Q^\\text{ret}(s, a) - Q_\\varphi(s, a))^2]\n\n\n\nImportance weight truncation with bias correction\nWhen updating the actor, we rely on the importance sampling weight \\rho_t = \\frac{\\pi_\\theta(s_t, a_t)}{b(s_t, a_t)} which can vary a lot and destabilize learning.\n\n    \\nabla_\\theta J(\\theta)  = \\mathbb{E}_{s_t, a_t \\sim \\rho_b} [\\rho_t \\, Q^\\text{ret}(s_t, a_t) \\, \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)]\n\nPPO solved this problem by clipping the importance sampling weight between 1- \\epsilon and 1+\\epsilon. ACER uses a similar strategy, but only using an upper bound c = 10 on the weight:\n\n    \\bar{\\rho}_t = \\min(c, \\rho_t)\n\nUsing \\bar{\\rho}_t in the policy gradient directly would introduce a bias: actions whose importance sampling weight \\rho_t is higher than c would contribute to the policy gradient with a smaller value than they should, introducing a bias.\nThe solution in ACER is to add a bias correcting term, that corrects the policy gradient when an action has a weight higher than c:\n\n\\begin{aligned}\n    \\nabla_\\theta J(\\theta)  = & \\mathbb{E}_{s_t \\sim \\rho_b} [\\mathbb{E}_{a_t \\sim b} [\\bar{\\rho}_t \\, Q^\\text{ret}(s_t, a_t) \\, \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)] \\\\\n    & + \\mathbb{E}_{a \\sim \\pi_\\theta}[(\\frac{\\rho_t(a) - c}{\\rho_t(a)})^+ \\, Q_\\varphi(s_t, a) \\, \\nabla_\\theta \\log \\pi_\\theta(s_t, a)]] \\\\\n\\end{aligned}\n\nThe left part of that equation is the same policy gradient as before, but using a clipped importance sampling weight.\nThe right part requires to integrate over all possible actions in the state s_t according to the learned policy \\pi_\\theta, although only the action a_t was selected by the behavior policy b. The term (\\frac{\\rho_t(a) - c}{\\rho_t(a)})^+ is zero for all actions having an importance sampling weight smaller than c, and has a maximum of 1. In practice, this correction term can be computed using the vectors \\pi_\\theta(s, a) and Q_\\varphi(s, a), which are the outputs of the actor and the critic, respectively.\nFinally, the Q-values Q^\\text{ret}(s_t, a_t) and Q_\\varphi(s_t, a) are transformed into advantages Q^\\text{ret}(s_t, a_t)  - V_\\varphi(s_t) and Q_\\varphi(s_t, a) - V_\\varphi(s_t) by substracting the value of the state in order to reduce the variance of the policy gradient.\nIn short, we now have an estimator of the policy gradient which is unbiased and of smaller variance.\n\n\nEfficient trust region policy optimization\nHowever, the variance is still too high. The last important step of ACER is an efficient TRPO update for the parameters of the actor.\nA first component of their TRPO update is they use a target actor network \\theta' (called averaged policy network in the paper) slowly tracking the actor \\theta after each update:\n\n    \\theta' \\leftarrow \\alpha \\, \\theta' + (1 - \\alpha) \\, \\theta\n\nA second component is that the actor is decomposed into two components:\n\na distribution f.\nthe statistics \\Phi_\\theta(x) of this distribution.\n\nThis is what you do when you apply the softmax action selection on Q-values: the distribution is the Gibbs (or Boltzmann) distribution and the Q-values are its statistics. In the discrete case, they take a categorical (or multinouilli) distribution: \\Phi_\\theta(s) is the probability for each action to be taken and the distribution selects one of them. Think of a dice with one side per action and probabilities governed by the policy. In the continuous case, it could be anything, for example a normal distribution.\nLet’s rewrite the policy gradient with that formulation (we omit here the bias correction, but ACER uses it), but only w.r.t the output of the actor \\Phi_\\theta(s_t) for a state s_t:\n\n    \\hat{g_t}^\\text{ACER} = \\nabla_{\\Phi_\\theta(s_t)} J(\\theta)  = \\bar{\\rho}_t \\, (Q^\\text{ret}(s_t, a_t) - V_\\phi(s_t) ) \\, \\nabla_{\\Phi_\\theta(s_t)} \\log f(a_t | \\Phi_\\theta(s_t))\n\nTo compute the policy gradient, we would only need to apply the chain rule:\n\n    \\nabla_\\theta J(\\theta) = \\mathbb{E}_{s_t, a_t \\sim \\rho_b} [ \\hat{g_t}^\\text{ACER} \\, \\nabla_\\theta \\Phi_\\theta(s_t) ]\n\nThe variance of \\hat{g_t}^\\text{ACER} is too high. ACER defines the following TRPO problem: we search for a gradient z solution of:\n\n    \\min_z ||\\hat{g_t}^\\text{ACER} - z ||^2 \\\\\n    \\text{s.t.} \\quad \\nabla_{\\Phi_\\theta(s_t)} D_{KL}( f(\\cdot | \\Phi_{\\theta'}(s_t) ) || f(\\cdot | \\Phi_{\\theta'}(s_t)) )^T \\times z &lt; \\delta\n\nThe exact meaning of the constraint is hard to grasp, but here some intuition: the change of policy z (remember that \\hat{g_t}^\\text{ACER} is defined w.r.t the output of the actor) should be as orthogonal as possible (within a margin \\delta) to the change of the Kullback-Leibler divergence between the policy defined by the actor (\\theta) and the one defined by the target actor (\\theta'). In other words, we want to update the actor, but without making the new policy too different from its past values (the target actor).\nThe advantage of this formulation is that the objective function is quadratic in z and the constraint is linear. We can therefore very easily find its solution using KKT optimization:\n\n    z^* = \\hat{g_t}^\\text{ACER} - \\max(0, \\frac{k^T \\, \\hat{g_t}^\\text{ACER} - \\delta}{||k||^2}) \\, k\n\nwhere k = \\nabla_{\\Phi_\\theta(s_t)} D_{KL}( f(\\cdot | \\Phi_{\\theta'}(s_t) ) || f(\\cdot | \\Phi_{\\theta'}(s_t)) ).\nHaving obtained z^*, we can safely update the parameters of the actor in the direction of:\n\n    \\nabla_\\theta J(\\theta) = \\mathbb{E}_{s_t, a_t \\sim \\rho_b} [ z^* \\, \\nabla_\\theta \\Phi_\\theta(s_t) ]\n\nAs noted in the paper:\n\n“The trust region step is carried out in the space of the statistics of the distribution f , and not in the space of the policy parameters. This is done deliberately so as to avoid an additional back-propagation step through the policy network”.\n\nWe indeed need only one network update per transition. If the KL divergence was computed with respect to \\pi_\\theta directly, one would need to apply backpropagation on the target network too.\nThe target network \\theta' is furthermore used as the behavior policy b(s, a). here is also a target critic network \\varphi', which is primarily used to compute the value of the states V_{\\varphi'}(s) for variance reduction.\nFor a complete description of the algorithm, refer to the paper… To summarize, ACER is an actor-critic architecture using Retrace estimated values, importance weight truncation with bias correction and efficient TRPO. Its variant for continuous action spaces furthermore uses a Stochastic Dueling Network (SDN) in order estimate both Q_\\varphi(s, a) and V_\\varphi(s). It is straightforward in the discrete case (multiply the policy with the Q-values and take the average) but hard in the continuous case.\nACER improved the performance and/or the sample efficiency of the state-of-the-art (A3C, DDPG, etc) on a variety of tasks (Atari, Mujoco). Apart from truncation with bias correction, all aspects of the algorithm are essential to obtain this improvement, as shown by ablation studies.\n\n\n\n\nDegris, T., White, M., and Sutton, R. S. (2012). Linear Off-Policy Actor-Critic. in Proceedings of the 2012 International Conference on Machine Learning Available at: http://arxiv.org/abs/1205.4839.\n\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., et al. (2016). Asynchronous Methods for Deep Reinforcement Learning. in Proc. ICML Available at: http://arxiv.org/abs/1602.01783.\n\n\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016). Safe and Efficient Off-Policy Reinforcement Learning. Available at: http://arxiv.org/abs/1606.02647.\n\n\nOh, J., Guo, Y., Singh, S., and Lee, H. (2018). Self-Imitation Learning. Available at: http://arxiv.org/abs/1806.05635.\n\n\nWang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., et al. (2017). Learning to reinforcement learn. Available at: http://arxiv.org/abs/1611.05763 [Accessed February 5, 2021].\n\n\nWang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de Freitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1511.06581 [Accessed November 21, 2019].",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Actor-Critic with Experience Replay (ACER)</span>"
    ]
  },
  {
    "objectID": "src/3.8-EntropyRL.html",
    "href": "src/3.8-EntropyRL.html",
    "title": "Maximum Entropy RL (SAC)",
    "section": "",
    "text": "Entropy regularization\nAll the methods seen sofar focus on finding a policy (or value functions) that maximizes the obtained return. This corresponds to the exploitation part of RL: we care only about the optimal policy. The exploration is ensured by external mechanisms, such as \\epsilon-greedy or softmax policies in value based methods, or adding exploratory noise to the actions as in DDPG. Exploration mechanisms typically add yet another free parameter (\\epsilon, softmax temperature, etc) that additionally need to be scheduled (more exploration at the beginning of learning than at the end).\nThe idea behind maximum entropy RL is to let the algorithm learn by itself how much exploration it needs to learn appropriately. There are several approaches to this problem (see for example (Machado et al., 2018) for an approach using successor representations), we focus first on methods using entropy regularization, a concept already seen briefly in A3C, before looking at soft methods such as Soft Q-learning and SAC.\nEntropy regularization (Williams and Peng, 1991) adds a regularization term to the objective function:\nJ(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho^\\pi, a_t \\sim \\pi_\\theta}[ R(s_t, a_t) + \\beta \\,  H(\\pi_\\theta(s_t))]\nWe will neglect here how the objective function is sampled (policy gradient, etc.) and focus on the second part.\nThe entropy of a discrete policy \\pi_\\theta in a state s_t is defined by the expected value of the self-information of each action :\nH(\\pi_\\theta(s_t)) = \\mathbb{E}_{a \\sim \\pi_\\theta(s_t, a)} [- \\log \\pi_\\theta(s_t, a)] = - \\sum_a \\pi_\\theta(s_t, a) \\, \\log \\pi_\\theta(s_t, a)\nThe entropy of the policy measures its “randomness”:\nBy adding the entropy as a regularization term directly to the objective function, we force the policy to be as non-deterministic as possible, i.e. to explore as much as possible, while still getting as many rewards as possible. The parameter \\beta controls the level of regularization: we do not want the entropy to dominate either, as a purely random policy does not bring much reward. If \\beta is chosen too low, the entropy won’t play a significant role in the optimization and we may obtain a suboptimal deterministic policy early during training as there was not enough exploration. If \\beta is too high, the policy will be random and suboptimal.\nBesides exploration, why would we want to learn a stochastic policy, while the solution to the Bellman equations is deterministic by definition? A first answer is that we rarely have a MDP: most interesting problems are POMDP, where the states are indirectly inferred through observations, which can be probabilistic. Todorov (2008) showed that a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference (see also Toussaint, 2009).\nConsider a two-opponents game like chess: if you have a deterministic policy, you will always play the same moves in the same configuration. In particular, you will always play the same opening moves. Your game strategy becomes predictable for your opponent, who can adapt accordingly. Having a variety of opening moves (as long as they are not too stupid) is obviously a better strategy on the long term. This is due to the fact that chess is actually a POMDP: the opponent’s strategy and beliefs are not accessible.\nAnother way to view the interest of entropy regularization is to realize that learning a deterministic policy only leads to a single optimal solution to the problem. Learning a stochastic policy forces the agent to learn many optimal solutions to the same problem: the agent is somehow forced to learn as much information as possible for the experienced transitions, potentially reducing the sample complexity.\nEntropy regularization is nowadays used very commonly used in deep RL networks (e.g. O’Donoghue et al., 2016), as it is “only” an additional term to set in the objective function passed to the NN, adding a single hyperparameter \\beta.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Maximum Entropy RL (SAC)</span>"
    ]
  },
  {
    "objectID": "src/3.8-EntropyRL.html#entropy-regularization",
    "href": "src/3.8-EntropyRL.html#entropy-regularization",
    "title": "Maximum Entropy RL (SAC)",
    "section": "",
    "text": "Figure 18.1: The self-information - \\log P(X=x) of an outcome x decreases with its probability: when an outcome is very likely (P close to 1), observing it is not surprising and does not provide much information. When an outcome is rare (P close to 0), observing it is very surprising and carries a lot of information.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nComputing the entropy of a continuous policy distribution is more complex in the general case, but using a Gaussian policy (see Section Continuous action spaces) allows to compute it easily. If the policy is a normal distribution \\pi_\\theta(s, a) = \\mathcal{N}(\\mu_\\theta(s), \\sigma^2_\\theta(s)) parameterized by the mean vector \\mu_\\theta(s) and the variance vector \\sigma_\\theta(s) of size n, the pdf is given by:\n\n    \\pi_\\theta(s, a) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_\\theta(s)}} \\, \\exp -\\frac{(a - \\mu_\\theta(s))^2}{2\\sigma_\\theta(s)^2}\n\nThe differential entropy of the Gaussian policy only depends on the variance of the distribution:\n\n    H(\\pi_\\theta(s_t)) = \\mathbb{E}_{a \\sim \\pi_\\theta(s_t, a)} [- \\log \\pi_\\theta(s_t, a)] = \\dfrac{n}{2} \\, (1 + \\ln 2\\pi) + \\dfrac{1}{2} \\ln |\\sigma_\\theta(s)|\n\n\n\n\n\nif the policy is fully deterministic (the same action is systematically selected), the entropy is zero as it carries no information.\nif the policy is completely random (all actions are equally surprising), the entropy is maximal.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Maximum Entropy RL (SAC)</span>"
    ]
  },
  {
    "objectID": "src/3.8-EntropyRL.html#soft-q-learning",
    "href": "src/3.8-EntropyRL.html#soft-q-learning",
    "title": "Maximum Entropy RL (SAC)",
    "section": "Soft Q-learning",
    "text": "Soft Q-learning\nEntropy regularization greedily maximizes the entropy of the policy in each state (the objective is the return plus the entropy in the current state). Building on the maximum entropy RL framework (Nachum et al., 2017; Schulman et al., 2017; Ziebart et al., 2008), Haarnoja et al. (2017) proposed a version of soft-Q-learning by extending the definition of the objective:\n\n    J(\\theta) =  \\sum_t \\mathbb{E}_{s_t \\sim \\rho^\\pi, a_t \\sim \\pi_\\theta}[ r(s_t, a_t) + \\beta \\,  H(\\pi_\\theta(s_t))]\n\nIn this formulation based on trajectories, the agent seeks a policy that maximizes the entropy of the complete trajectories rather than the entropy of the policy in each state. This is a very important distinction: the agent does not only search a policy with a high entropy, but a policy that brings into states with a high entropy, i.e. where the agent is the most uncertain. This allows for very efficient exploration strategies, where the agent will try to reduce its uncertainty about the world and gather a lot more information than when simply searching for a good policy.\nNote that it is always possible to fall back to classical Q-learning by setting \\beta=0 and that it is possible to omit this hyperparameter by scaling the rewards with \\frac{1}{\\beta}. The discount rate \\gamma is omitted here for simplicity, but it should be added back when the task has an infinite horizon.\nIn soft Q-learning, the policy can be defined by a softmax over the soft Q-values Q_\\text{soft}(s, a), where \\beta plays the role of the temperature parameter:\n\n    \\pi(s, a) \\propto \\exp(Q_\\text{soft}(s_t, a_t) / \\beta)\n\nNote that -Q_\\text{soft}(s_t, a_t) / \\beta plays the role of the energy of the policy (as in Boltzmann machines), hence the name of the paper (Reinforcement Learning with Deep Energy-Based Policies). We will ignore this analogy here. The normalization term of the softmax (the log-partition function in energy-based models) is also omitted as it later disappears from the equations anyway.\nThe soft Q-values are defined by the following Bellman-like equation:\n\n    Q_\\text{soft}(s_t, a_t) = r(s_t, a_t) + \\gamma \\, \\mathbb{E}_{s_{t+1} \\in \\rho} [V_\\text{soft}(s_{t+1})]\n\nThis is the regular Bellman equation that can be turned into an update rule for the soft Q-values (minimizing the mse between the l.h.s and the r.h.s). The soft value of a state is given by:\n\n    V_\\text{soft}(s_t) = \\mathbb{E}_{a_{t} \\in \\pi} [Q_\\text{soft}(s_{t}, a_{t}) - \\log \\, \\pi(s_t, a_t)]\n\nThe notation in Haarnoja et al. (2017) is much more complex than that (the paper includes the theoretical proofs), but it boils down to this in Haarnoja et al. (2018). When definition of the soft Q-value is applied repeatedly with the definition of the soft V-value, it converges to the optimal solution of the soft Q-learning objective, at least in the tabular case.\nThe soft V-value of a state is the expectation of the Q-values in that state (as in regular RL) minus the log probability of each action. This last term measures the entropy of the policy in each state (when expanding the expectation over the policy, we obtain - \\pi \\log \\pi, which is the entropy).\nIn a nutshell, the soft Q-learning algorithm is:\n\n\n\n\n\n\nSoft Q-learning\n\n\n\n\nSample transitions (s, a, r, s') and store them in a replay memory.\nFor each transition (s, a, r, s') in a minibatch of the replay memory:\n\nEstimate V_\\text{soft}(s') by sampling several actions: \n  V_\\text{soft}(s_t) = \\mathbb{E}_{a_{t} \\in \\pi} [Q_\\text{soft}(s_{t}, a_{t}) - \\log \\, \\pi(s_t, a_t)]\n  \nUpdate the soft Q-value of (s,a): \n  Q_\\text{soft}(s_t, a_t) = r(s_t, a_t) + \\gamma \\, \\mathbb{E}_{s_{t+1} \\in \\rho} [V_\\text{soft}(s_{t+1})]\n  \nUpdate the policy (if not using the softmax over soft Q-values directly).\n\n\n\n\nThe main drawback of this approach is that several actions have to be sampled in the next state in order to estimate its current soft V-value, what makes it hard to implement in practice. The policy also has to be sampled from the Q-values, what is not practical for continuous action spaces.\nBut the real interesting thing is the policies that are learned in multi-goal settings, as in Figure 18.2. The agent starts in the middle of the environment and can obtain one of the four rewards (north, south, west, east). A regular RL agent would very quickly select only one of the rewards and stick to it. With soft Q-learning, the policy stays stochastic and the four rewards can be obtained even after convergence. This indicates that the soft agent has learned much more about its environment than its hard equivalent, thanks to its maximum entropy formulation.\n\n\n\n\n\n\nFigure 18.2: Policy learned by Soft Q-learning in a multi-goal setting. Source: Haarnoja et al. (2017).",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Maximum Entropy RL (SAC)</span>"
    ]
  },
  {
    "objectID": "src/3.8-EntropyRL.html#soft-actor-critic-sac",
    "href": "src/3.8-EntropyRL.html#soft-actor-critic-sac",
    "title": "Maximum Entropy RL (SAC)",
    "section": "Soft Actor-Critic (SAC)",
    "text": "Soft Actor-Critic (SAC)\nHaarnoja et al. (2018) proposed the Soft Actor-Critic (SAC), an off-policy actor-critic which allows to have a stochastic actor (contrary to DDPG) while being more optimal and sample efficient than on-policy methods such as A3C or PPO. It is also less sensible to hyperparameters than all these methods.\nSAC builds on soft Q-learning to achieve these improvements. It relies on three different function approximators:\n\na soft state value function V_\\varphi(s).\na soft Q-value function Q_\\psi(s,a).\na stochastic policy \\pi_\\theta(s, a).\n\nThe paper uses a different notation for the parameters \\theta, \\varphi, \\psi, but I choose to be consistent with the rest of this document.\nThe soft state-value function V_\\varphi(s) is learned vy turning the definition of the soft V-value into a loss function:\n\n    \\mathcal{L}(\\varphi) = \\mathbb{E}_{s_t \\in \\mathcal{D}} [\\mathbb{E}_{a_{t} \\in \\pi} [(Q_\\psi(s_{t}, a_{t}) - \\log \\, \\pi_\\theta(s_t, a_t)] - V_\\varphi(s_t) )^2]\n\nIn practice, we only need the gradient of this loss function to train the corresponding neural network. The expectation over the policy inside the loss function can be replaced by a single sample action a using the current policy \\pi_\\theta (but not a_{t+1} in the replay memory \\mathcal{D}, which is only used for the states s_t).\n\n    \\nabla_\\varphi \\mathcal{L}(\\varphi) = \\nabla_\\varphi V_\\varphi(s_t) \\, (V_\\varphi(s_t) - Q_\\psi(s_{t}, a) + \\log \\, \\pi_\\theta(s_t, a) )\n\nThe soft Q-values Q_\\psi(s_{t}, a_{t}) can be trained from the replay memory \\mathcal{D} on (s_t, a_t, r_{t+1} , s_{t+1}) transitions by minimizing the mse:\n\n    \\mathcal{L}(\\psi) = \\mathbb{E}_{s_t, a_t \\in \\mathcal{D}} [(r_{t+1} + \\gamma \\, V_\\varphi(s_{t+1}) - Q_\\psi(s_t, a_t))^2]\n\nFinally, the policy \\pi_\\theta can be trained to maximize the obtained returns. There are many ways to do that, but Haarnoja et al. (2018) proposes to minimize the Kullback-Leibler (KL) divergence between the current policy \\pi_\\theta and a softmax function over the soft Q-values:\n\n    \\mathcal{L}(\\theta) = \\mathbb{E}_{s_t \\in \\mathcal{D}} [D_\\text{KL}(\\pi_\\theta(s, \\cdot) | \\frac{\\exp Q_\\psi(s_t, \\cdot)}{Z(s_t)})]\n\nwhere Z is the partition function to normalize the softmax. Fortunately, it disappears when using the reparameterization trick and taking the gradient of this loss (see the paper for details).\nThere are additional tricks to make it more efficient and robust, such as target networks or the use of two independent function approximators for the soft Q-values in order to reduce the bias, but the gist of the algorithm is the following:\n\n\n\n\n\n\nSAC algorithm\n\n\n\n\nSample a transition (s_t, a_t, r_{t+1}, a_{t+1}) using the current policy \\pi_\\theta and store it in the replay memory \\mathcal{D}.\nFor each transition (s_t, a_t, r_{t+1}, a_{t+1}) of a minibatch of \\mathcal{D}:\n\nSample an action a \\in \\pi_\\theta(s_t, \\cdot) from the current policy.\nUpdate the soft state-value function V_\\varphi(s_t):\n\n\n      \\nabla_\\varphi \\mathcal{L}(\\varphi) = \\nabla_\\varphi V_\\varphi(s_t) \\, (V_\\varphi(s_t) - Q_\\psi(s_{t}, a) + \\log \\, \\pi_\\theta(s_t, a) )\n  \n\nUpdate the soft Q-value function Q_\\psi(s_t, a_t):\n\n\n      \\nabla_\\psi \\mathcal{L}(\\psi) = - \\nabla_\\psi Q_\\psi(s_t, a_t) \\, (r_{t+1} + \\gamma \\, V_\\varphi(s_{t+1}) - Q_\\psi(s_t, a_t))\n  \n\nUpdate the policy \\pi_\\theta(s_t, \\cdot):\n\n\n      \\nabla_\\theta \\mathcal{L}(\\theta) = \\nabla_\\theta D_\\text{KL}(\\pi_\\theta(s, \\cdot) | \\frac{\\exp Q_\\psi(s_t, \\cdot)}{Z(s_t)})\n  \n\n\n\nSAC was compared to DDPG, PPO, soft Q-learning and others on a set of gym and humanoid robotics tasks (with 21 joints!). It outperforms all these methods in both the final performance and the sample complexity, the difference being even more obvious for the complex tasks. The exploration bonus given by the maximum entropy allows the agent to discover better policies than its counterparts. SAC is an actor-critic architecture (the critic computing both V and Q) working off-policy (using an experience replay memory, so re-using past experiences) allowing to learn stochastic policies, even in high dimensional spaces.\n\n\n\n\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement Learning with Deep Energy-Based Policies. Available at: http://arxiv.org/abs/1702.08165 [Accessed February 13, 2019].\n\n\nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., et al. (2018). Soft Actor-Critic Algorithms and Applications. Available at: http://arxiv.org/abs/1812.05905 [Accessed February 5, 2019].\n\n\nMachado, M. C., Bellemare, M. G., and Bowling, M. (2018). Count-Based Exploration with the Successor Representation. Available at: http://arxiv.org/abs/1807.11622 [Accessed February 23, 2019].\n\n\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017). Bridging the Gap Between Value and Policy Based Reinforcement Learning. Available at: http://arxiv.org/abs/1702.08892 [Accessed June 12, 2019].\n\n\nO’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2016). Combining policy gradient and Q-learning. Available at: http://arxiv.org/abs/1611.01626 [Accessed February 13, 2019].\n\n\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal Policy Optimization Algorithms. Available at: http://arxiv.org/abs/1707.06347.\n\n\nTodorov, E. (2008). General duality between optimal control and estimation. in 2008 47th IEEE Conference on Decision and Control, 4286–4292. doi:10.1109/CDC.2008.4739438.\n\n\nToussaint, M. (2009). Robot Trajectory Optimization Using Approximate Inference. in Proceedings of the 26th Annual International Conference on Machine Learning ICML ’09. (New York, NY, USA: ACM), 1049–1056. doi:10.1145/1553374.1553508.\n\n\nWilliams, R. J., and Peng, J. (1991). Function optimization using connectionist reinforcement learning algorithms. Connection Science 3, 241–268.\n\n\nZiebart, B. D., Maas, A., Bagnell, J. A., and Dey, A. K. (2008). Maximum Entropy Inverse Reinforcement Learning. in, 6.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Maximum Entropy RL (SAC)</span>"
    ]
  },
  {
    "objectID": "src/3.9-Misc.html",
    "href": "src/3.9-Misc.html",
    "title": "Misc.",
    "section": "",
    "text": "Stochastic Value Gradient (SVG) (Heess et al., 2015)\nQ-Prop (Gu et al., 2016b)\nNormalized Advantage Function (NAF) (Gu et al., 2016a)\nFictitious Self-Play (FSP) (Heinrich et al., 2015; Heinrich and Silver, 2016)\n\n\n\n\n\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and Levine, S. (2016a). Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic. Available at: http://arxiv.org/abs/1611.02247.\n\n\nGu, S., Lillicrap, T., Sutskever, I., and Levine, S. (2016b). Continuous Deep Q-Learning with Model-based Acceleration. Available at: http://arxiv.org/abs/1603.00748.\n\n\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T. (2015). Learning continuous control policies by stochastic value gradients. Proc. International Conference on Neural Information Processing Systems, 2944–2952. Available at: http://dl.acm.org/citation.cfm?id=2969569.\n\n\nHeinrich, J., Lanctot, M., and Silver, D. (2015). Fictitious Self-Play in Extensive-Form Games. 805–813. Available at: http://proceedings.mlr.press/v37/heinrich15.html.\n\n\nHeinrich, J., and Silver, D. (2016). Deep Reinforcement Learning from Self-Play in Imperfect-Information Games. Available at: http://arxiv.org/abs/1603.01121.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Misc.</span>"
    ]
  },
  {
    "objectID": "src/4.1-ModelBased.html",
    "href": "src/4.1-ModelBased.html",
    "title": "Model-based RL",
    "section": "",
    "text": "Model-free vs. model-based RL\nIn the model-free (MF) RL methods seen sofar, we did not need to know anything about the dynamics of the environment to start learning a policy:\np(s' | s, a) \\; \\; r(s, a, s')\nWe just sampled transitions (s, a, r, s') and update the value / policy network. The main advantage is that the agent does not need to “think” when acting: it just select the action with highest Q-value or the one selected by the policy network (reflexive behavior). The other advantage is that you can use MF methods on any MDP: you do not need to know anything about them before applying MF as a blackbox optimizer.\nHowever, MF methods are very slow (sample complexity): as they make no assumption, they have to learn everything by trial-and-error from scratch. If you had a model of the environment, you could plan ahead (what would happen if I did that?) and speed up learning (do not explore obviously stupid ideas): such a behavior is called model-based RL (MB). Dynamic programming (Section Dynamic programming) is for example a model-based method, as it requires the knowledge of p(s' | s, a) and r(s, a, s') to solve the Bellman equations.\nIn chess, for example, players plan ahead the possible moves up to a certain horizon and evaluate moves based on their emulated consequences. In real-time strategy games, learning the environment (world model) is part of the strategy: you do not attack right away.\nThis chapter presents several MB algorithms, including MPC planning algorithms, World models and the different variants of AlphaGo. We first present the main distinction in MB RL, planning algorithms (MPC) versus MB-augmented MF (Dyna) methods. Another useful dichotomy that we will see in the AlphaGo section is about learned models vs. given models.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model-based RL</span>"
    ]
  },
  {
    "objectID": "src/4.1-ModelBased.html#model-free-vs.-model-based-rl",
    "href": "src/4.1-ModelBased.html#model-free-vs.-model-based-rl",
    "title": "Model-based RL",
    "section": "",
    "text": "Figure 20.1: Model-based RL uses planning to find the optimal action to perform, while model-free RL caches the future and leads to more reflexive behavior. Source: Dayan and Niv (2008)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.2: Source: https://github.com/avillemin/RL-Personnal-Notebook",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model-based RL</span>"
    ]
  },
  {
    "objectID": "src/4.1-ModelBased.html#learning-a-dynamics-model",
    "href": "src/4.1-ModelBased.html#learning-a-dynamics-model",
    "title": "Model-based RL",
    "section": "Learning a dynamics model",
    "text": "Learning a dynamics model\nLearning the world model is not complicated in theory. We just need to collect enough transitions (s, a, r , s') using a random agent (or an expert) and train a supervised model to predict the next state and the corresponding reward.\n\n\n     M(s, a) = (s', r )\n\nSuch a model is called the dynamics model, the transition model or the forward model, and basically answers the question:\n\nWhat would happen if I did that?\n\nThe model can be deterministic (in which case we should use neural networks) or stochastic (in which case we should use Gaussian processes, mixture density networks or recurrent state space models). Any kind of supervised learning method can be used in principle.\nOnce you have trained a good transition model, you can generate rollouts, i.e. imaginary trajectories / episodes \\tau using the model. Given an initial state s_0 and a policy \\pi, you can unroll the future using the model s', r = M(s, a).\n\n    s_0  \\xrightarrow[\\pi]{} a_0 \\xrightarrow[M]{} s_1  \\xrightarrow[\\pi]{} a_1 \\xrightarrow[\\pi]{} s_2 \\xrightarrow[]{} \\ldots \\xrightarrow[M]{} s_T\n\nGiven the model, you can also compute the return R(\\tau) of the emulated trajectory. Everything is as if you were interacting with the environment, but you actually do not need it anymore: the model becomes the environment. You can now search for an optimal policy on these emulated trajectories:\n\n\n\n\n\n\nTraining in imagination\n\n\n\n\nCollect transitions (s, a, r, s') using a (random/expert) policy b and create a dataset \\mathcal{D} = \\{(s_k, a_k, r_, s'_k\\}_{k}.\nTrain the model M(s, a) = (s', r) on \\mathcal{D} using supervised learning.\nOptimize the policy \\pi on rollouts \\tau generated by the model.\n\n\n\nAny method can be used to optimize the policy. We can obviously use a model-free algorithm to maximize the expected return of the trajectories:\n\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_\\pi}[R(\\tau)]\nThe only sample complexity is the one needed to train the model: the rest is emulated. For problems where a physical step (t \\rightarrow t+1) is very expensive compared to an inference step of the model (neural networks can predict very fast), this can even allow to use inefficient but optimal methods to find the policy. Brute-force optimization becomes possible if using the model is much faster that the real environment. However, this approach has two major drawbacks:\n\nThe model can only be as good as the data, and errors are going to accumulate, especially for long trajectories or probabilistic MDPs.\nIf the dataset does not contain the important transitions (for example where there are sparse rewards), the policy will likely be sub-optimal. In extreme cases, training the model up to a sufficient precision might necessitate more samples than learning the policy directly with MF methods.\n\n\n\n\n\nDayan, P., and Niv, Y. (2008). Reinforcement learning: The Good, The Bad and The Ugly. Current Opinion in Neurobiology 18, 185–196. doi:10.1016/j.conb.2008.08.003.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model-based RL</span>"
    ]
  },
  {
    "objectID": "src/4.2-MBMF.html",
    "href": "src/4.2-MBMF.html",
    "title": "Model-based-augmented model-free RL (Dyna-Q, I2A)",
    "section": "",
    "text": "Dyna-Q\nOnce a model of the environment is learned, it is possible to augment MF algorithms with MB transitions. The MF algorithm (e.g. Q-learning) learns from transitions (s, a, r, s') sampled either with:\nIf the simulated transitions are realistic enough, the MF algorithm can converge using much less real transitions, thereby reducing its sample complexity.\nThe Dyna-Q algorithm (Sutton, 1990) is an extension of Q-learning to integrate a model M(s, a) = (s', r'). It alternates between online updates of the agent using the real environment and (possible multiple) offline updates using the model.\nIt is interesting to notice that Dyna-Q is the inspiration for DQN and its experience replay memory. In DQN, the ERM stores real transitions generated in the past and recovers them later intact, while in Dyna-Q, the model generates imagined transitions approximated based on past real transitions. Interleaving on-policy and off-policy updates is also the core idea of ACER (section Actor-Critic with Experience Replay (ACER)).",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Model-based-augmented model-free RL (Dyna-Q, I2A)</span>"
    ]
  },
  {
    "objectID": "src/4.2-MBMF.html#dyna-q",
    "href": "src/4.2-MBMF.html#dyna-q",
    "title": "Model-based-augmented model-free RL (Dyna-Q, I2A)",
    "section": "",
    "text": "real experience: online interaction with the environment.\nsimulated experience: simulated transitions by the model.\n\n\n\n\n\n\n\nFigure 21.1: Dyna-Q alternates between online updates and simulated “offline” updates from the model.\n\n\n\n\n\n\n\n\n\n\n\nDyna-Q (Sutton, 1990)\n\n\n\n\nInitialize values Q(s, a) and model M(s, a).\nfor t \\in [0, T_\\text{total}]:\n\nSelect a_t using Q, take it on the real environment and observe s_{t+1} and r_{t+1}.\nUpdate the Q-value of the real action:\n\n\\Delta Q(s_t, a_t) = \\alpha \\, (r_{t+1} + \\gamma \\, \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\n\nUpdate the model:\n\nM(s_t, a_t) \\leftarrow (s_{t+1}, r_{t+1})\n\nfor K steps:\n\nSample a state s_k from a list of visited states.\nSelect a_k using Q, predict s_{k+1} and r_{k+1} using the model M(s_k, a_k).\nUpdate the Q-value of the imagined action:\n\n\\Delta Q(s_k, a_k) = \\alpha \\, (r_{k+1} + \\gamma \\, \\max_a Q(s_{k+1}, a) - Q(s_k, a_k))",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Model-based-augmented model-free RL (Dyna-Q, I2A)</span>"
    ]
  },
  {
    "objectID": "src/4.2-MBMF.html#sec-i2a",
    "href": "src/4.2-MBMF.html#sec-i2a",
    "title": "Model-based-augmented model-free RL (Dyna-Q, I2A)",
    "section": "I2A - Imagination-augmented agents",
    "text": "I2A - Imagination-augmented agents\nI2A (Weber et al., 2017) is a model-based augmented model-free method: it trains a MF algorithm (A3C) with the help of rollouts generated by a MB model. The authors showcase their algorithm on the puzzle environment Sokoban, where you need to move boxes to specified locations.\n\n\n\n\n\n\nFigure 21.2: Game of Sokoban used to showcase the abilities of I2A. Source: Weber et al. (2017)\n\n\n\nSokoban is a quite hard game, as actions are irreversible (you can get stuck) and the solution requires many actions (sparse rewards). MF methods are bad at this game as they learn through trials-and-(many)-errors.\n\nI2A is composed of several different modules. We will now have a look at them one by one.\n\nEnvironment model\n\n\n\n\n\n\nFigure 21.3: Environment model of I2A. Source: Weber et al. (2017)\n\n\n\nThe environment model learns to predict the next frame and the next reward based on the four last frames and the chosen action:\n\n    (o_{t-3}, o_{t-2}, o_{t-1}, o_{t}, a_t) \\rightarrow (o_{t+1}, r_{t+1})\n\nAs Sokoban is a POMDP (partially observable), the notation uses observations o_t instead of states s_t, but it does not really matter here.\nThe neural network is a sort of convolutional autoencoder, taking additionally an action a as input and predicting the next reward. Formally, the output “image” being different from the input, the neural network is not an autoencoder but belongs to the family of segmentation networks such as SegNet (Badrinarayanan et al., 2016) or U-net (Ronneberger et al., 2015). It can be pretrained using a random policy, and later fine-tuned during training.\n\n\nImagination core\n\n\n\n\n\n\nFigure 21.4: Imagination core. Source: Weber et al. (2017)\n\n\n\nThe imagination core is composed of the environment model M(s, a) and a rollout policy \\hat{\\pi}. The rollout policy \\hat{\\pi} is a simple and fast policy. It does not have to be the trained policy \\pi: It could even be a random policy, or a pretrained policy using for example A3C directly. In I2A, the rollout policy \\hat{\\pi} is obtained through policy distillation of the bigger policy network \\pi.\n\n\n\n\n\n\nPolicy distillation (Rusu et al., 2016)\n\n\n\nThe small rollout policy network \\hat{\\pi} tries to copy the outputs \\pi(s, a) of the whole model. This is a supervised learning task: we just need minimize the KL divergence between the two policies:\n\\mathcal{L}(\\hat{\\theta}) = \\mathbb{E}_{s, a} [D_\\text{KL}(\\hat{\\pi}(s, a) || \\pi(s, a))]\nAs the network is smaller, it won’t be quite as good as \\pi (although not dramatically), but its learning objective is simpler: supervised learning is much easier than RL, especially when the rewards are sparse. A very small network (up to 90% of the original parameters) is often enough for the same functionality.\n\n\n\n\n\n\nFigure 21.5: Policy distillation. The student model learns to imitate the teacher model through supervised learning, which is much easier than RL.\n\n\n\nIn general, policy distillation can be used to ensure generalization over different environments, as in Distral (Teh et al., 2017). Each learning algorithms learns its own task, but tries not to diverge too much from a shared policy, which turns out to be good at all tasks.\n\n\n\n\n\n\nFigure 21.6: Distral architecture. Each sub-policy \\pi_i learns a specific environment. The central policy \\pi_0 distills knowledge from the sub-policies, while forcing them through regularization not to diverge too much. Ultimately, the central policy becomes able to solve all the environments. Source: Teh et al. (2017)\n\n\n\n\n\n\n\nImagination rollout module\n\n\n\n\n\n\nFigure 21.7: Imagination rollout module. Source: Weber et al. (2017)\n\n\n\nThe imagination rollout module uses the imagination core to predict iteratively the next \\tau frames and rewards using the current frame o_t and the rollout policy:\n(o_{t-3}, o_{t-2}, o_{t-1}, o_{t}) \\rightarrow \\hat{o}_{t+1} \\rightarrow \\hat{o}_{t+2} \\rightarrow \\ldots \\rightarrow \\hat{o}_{t+\\tau}\nThe \\tau frames and rewards are passed backwards to a convolutional LSTM (from t+\\tau to t) which produces an embedding / encoding of the rollout. The output of the imagination rollout module is a vector e_i (the final state of the LSTM) representing the whole rollout, including the (virtually) obtained rewards. Note that because of the stochasticity of the rollout policy \\hat{\\pi}, different rollouts can lead to different encoding vectors.\n\n\nModel-free path\n\n\n\n\n\n\nFigure 21.8: Model-free path Source: Weber et al. (2017)\n\n\n\nFor the current observation o_t (and the three frames before), we then generate one rollout per possible action (5 in Sokoban):\n\nWhat would happen if I do action 1?\nWhat would happen if I do action 2?\netc.\n\nThe resulting vectors are concatenated to the output of model-free path (a convolutional neural network taking the current observation as input).\n\n\nFull model\nAltogether, we have a huge NN with weights \\theta (model, rollout encoder, MF path) between the input observation o_t and the output policy \\pi (plus the critic V).\n\n\n\n\n\n\nFigure 21.9: Complete architecture of I2A. Source: Weber et al. (2017)\n\n\n\nWe can then learn the policy \\pi and value function V using the n-step advantage actor-critic (A3C) :\n\\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, (\\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n}) - V_\\varphi(s_t)) ]\n\\mathcal{L}(\\varphi) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[(\\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n}) - V_\\varphi(s_t))^2]\nThe complete architecture may seem complex, but everything is differentiable so we can apply backpropagation and train the network end-to-end using multiple workers. It is simply the A3C algorithm (MF), but augmented by MB rollouts, i.e. with explicit information about the (emulated) future.\n\n\nResults\nUnsurprisingly, I2A performs better than A3C on Sokoban. The deeper the rollout, the better:\n\n\n\n\n\n\nFigure 21.10: Performance of I2A. Source: Weber et al. (2017)\n\n\n\nThe model does not even have to be perfect: the MF path can compensate for imperfections:\n\n\n\n\n\n\nFigure 21.11: Influence of noise. Source: Weber et al. (2017)\n\n\n\n\n\n\n\n\nBadrinarayanan, V., Kendall, A., and Cipolla, R. (2016). SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation. Available at: http://arxiv.org/abs/1511.00561 [Accessed November 29, 2020].\n\n\nRonneberger, O., Fischer, P., and Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. Available at: http://arxiv.org/abs/1505.04597 [Accessed November 29, 2020].\n\n\nRusu, A. A., Colmenarejo, S. G., Gulcehre, C., Desjardins, G., Kirkpatrick, J., Pascanu, R., et al. (2016). Policy Distillation. Available at: http://arxiv.org/abs/1511.06295 [Accessed January 26, 2020].\n\n\nSutton, R. S. (1990). Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming. Machine Learning Proceedings 1990, 216–224. doi:10.1016/B978-1-55860-141-3.50030-4.\n\n\nTeh, Y. W., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., et al. (2017). Distral: Robust Multitask Reinforcement Learning. Available at: http://arxiv.org/abs/1707.04175 [Accessed January 26, 2020].\n\n\nWeber, T., Racanière, S., Reichert, D. P., Buesing, L., Guez, A., Rezende, D. J., et al. (2017). Imagination-Augmented Agents for Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1707.06203.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Model-based-augmented model-free RL (Dyna-Q, I2A)</span>"
    ]
  },
  {
    "objectID": "src/4.3-Planning.html",
    "href": "src/4.3-Planning.html",
    "title": "Planning (MPC, TDM)",
    "section": "",
    "text": "Model Predictive Control\nModel-based learning is not only used to augment MF methods with imaginary rollouts. It can also be use directly for planning, as the model s', r = M(s, a) can emulate complete trajectories and estimate their return.\ns_0  \\xrightarrow[\\pi]{} a_0 \\xrightarrow[M]{} s_1  \\xrightarrow[\\pi]{} a_1 \\xrightarrow[\\pi]{} s_2 \\xrightarrow[]{} \\ldots \\xrightarrow[M]{} s_T\nA non-linear optimization procedure can then learn to select a policy that maximizes the return. It can either be:\nFor long horizons, the slightest imperfection in the model can accumulate over time (drift) and lead to completely wrong trajectories.\nThe emulated trajectories will have a biased return, and the optimization algorithm will not converge to the optimal policy. If you have a perfect model at your disposal, you should not be using RL anyway, as classical control methods would be much faster in principle (but see AlphaGo).\nThe solution is to replan at each time step and execute only the first planned action in the real environment. After this step is performed, we plan again.\nModel Predictive Control iteratively plans complete trajectories, but only selects the first action. This can be computationally expensive, but prediction errors do not accumulate.\nThe frequency at which the dynamics model is retrained or fine-tune may vary from implementation to implementation, it does not have to be between each trajectory. See http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_9_model_based_rl.pdf for more details on MPC.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Planning (MPC, TDM)</span>"
    ]
  },
  {
    "objectID": "src/4.3-Planning.html#model-predictive-control",
    "href": "src/4.3-Planning.html#model-predictive-control",
    "title": "Planning (MPC, TDM)",
    "section": "",
    "text": "Figure 22.2: Replanning after each step allows to compensate for model errors. Source: https://medium.com/@jonathan_hui/rl-model-based-reinforcement-learning-3c2b6f0aa323\n\n\n\n\n\n\n\n\n\n\nModel Predictive Control\n\n\n\n\nCollect transitions (s, a, r, s') using a (random/expert) policy b and create an initial dataset \\mathcal{D} = \\{(s_k, a_k, r_, s'_k\\}_{k}.\nwhile not converged:\n\n(Re)Train the dynamics model M(s, a) = (s', r) on \\mathcal{D} using supervised learning.\nforeach step t in the trajectory:\n\nPlan a trajectory from the current state s_t using the model M, returning a sequence of planned actions:\n\na_{t}, a_{t+1}, \\ldots, a_T\n\nTake the first action a_t, observe the next state s_{t+1}.\nAppend the transition (s_t, a_t, s_{t+1}) to the dataset.\n\n\n\n\n\n\n\nExample with a neural dynamics model\n\n\n\nModel-predictive control using a learned model. Source: Nagabandi et al. (2017).\n\n\nIn Nagabandi et al. (2017), the dynamics model is a neural network predicting the next state and the associated reward. The controller uses random-sampling shooting:\n\nIn the current state, a set of possible actions is selected.\nRollouts are generated from these actions using the model and their return is computed.\nThe initial action of the rollout with the highest return is selected.\nRepeat.\n\n\n\n\n\n\n\nFigure 22.3: Random-sampling shooting. Using different initial actions, several imaginary rollouts are performed. The one leading to the maximal return is chosen and executed. Source: https://bair.berkeley.edu/blog/2017/11/30/model-based-rl/\n\n\n\nThe main advantage of MPC is that you can change the reward function (the goal) on the fly: what you learn is the model, but planning is just an optimization procedure. You can set intermediary goals to the agent very flexibly: no need for a well-defined reward function. Model imperfection is not a problem as you replan all the time. As seen below, the model can adapt to changes in the environment (slippery terrain, simulation to real-world).\n\n\n\n\n\n\nFigure 22.4: Source: https://bair.berkeley.edu/blog/2017/11/30/model-based-rl/\n\n\n\n\n\n\n\n\n\nFigure 22.5: Source: https://bair.berkeley.edu/blog/2017/11/30/model-based-rl/",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Planning (MPC, TDM)</span>"
    ]
  },
  {
    "objectID": "src/4.3-Planning.html#temporal-difference-models---tdm",
    "href": "src/4.3-Planning.html#temporal-difference-models---tdm",
    "title": "Planning (MPC, TDM)",
    "section": "Temporal difference models - TDM",
    "text": "Temporal difference models - TDM\n\nPlanning over long horizon\nOne problem with model-based planning is the discretization time step (difference between t and t+1). It is determined by the action rate: how often a different action a_t has to be taken. In robotics, it could be below the millisecond, leading to very long trajectories in terms of steps.\n\n\n\n\n\n\nFigure 22.6: Example of going from Berkeley to the Golden Gate bridge with a bike. Source: Pong et al. (2018) and https://bairblog.github.io/2018/04/26/tdm/\n\n\n\nIf you want to go from Berkeley to the Golden Gate bridge with your bike, planning over leg movements will be very expensive (long horizon). A solution is to use multiple steps ahead planning. Instead of learning a one-step model:\ns_{t+1} = f_\\theta(s_t, a_t)\none learns to predict the state achieved in T steps using the current policy:\ns_{t+ T} = f_\\theta(s_t, a_t, \\pi)\nPlanning and acting can occur at different time scales: as in MPC, you plan for a significant number of steps in the future, but you only take the first step. If you learn to predict directly T steps into the future, you do not even need the intermediary steps.\n\n\nGoal-conditioned RL\nAnother problem with RL in general is how to define the reward function. If you goal is to travel from Berkeley to the Golden State bridge, which reward function should you use?\n\n+1 at the bridge, 0 otherwise (sparse).\n+100 at the bridge, -1 otherwise (sparse).\nminus the distance to the bridge (dense).\n\nMoreover, do you have to re-learn everything if you want to go somewhere else?\nGoal-conditioned RL defines the reward function using the distance between the achieved state s_{t+1} and a goal state s_g:\nr(s_t, a_t, s_{t+1}) = - || s_{t+1} - s_g ||\nAn action will reinforced if it brings the agent closer to its goal. The Euclidean distance works well for the biking example (e.g. using a GPS), but the metric can be adapted to the task. One advantage is that you can learn multiple “tasks” at the same time with a single policy, not the only one hard-coded in the reward function.\nAnother advantage is that it makes a better use of exploration by learning from mistakes, as in hindsight experience replay (HER, Andrychowicz et al. (2017)). If your goal is to reach s_g but the agent generates a trajectory landing in s_{g'}, you can learn that this trajectory is a good way to reach s_{g'}! In football, if you try to score a goal but end up doing a pass to a teammate, you can learn that this was a bad shot and a good pass. HER is a model-based method: you implicitly learn a model of the environment by knowing how to reach any position.\n\n\n\n\n\n\nFigure 22.7: Hindsight experience replay. Source: https://openai.com/blog/ingredients-for-robotics-research/\n\n\n\nExploration never fails: you always learn to do something, even if this was not your original goal. The principle of HER can be combined with all model-free methods: DQN, DDPG, etc. Your value / policy network only has to take inputs from s and g instead of only s.\n\n\nTDM\nUsing the goal-conditioned reward function r(s_t, a_t, s_{t+1}) = - || s_{t+1} - s_g ||, how can we learn? TDM (Pong et al., 2018) introduces goal-conditioned Q-values with a horizon T: Q(s, a, s_g, T). The Q-value of an action should denote how close we will be from the goal s_g in T steps. If we can estimate these Q-values, we can use a planning algorithm such as MPC to find the action that will bring us closer to the goal easily:\na^* = \\text{arg}\\max_{a_t} \\, r(s_{t+T}, a_{t+T}, s_{t+T + 1})\nThis corresponds to planning T steps ahead: which action should I do now in order to be close to the goal in T steps? If the horizon T is well chosen, we only need to plan over a small number of intermediary positions, not over each possible action. TDM is model-free on each subgoal, but model-based on the whole trajectory.\nHow can we learn the goal-conditioned Q-values Q(s, a, s_g, T) with a model? TDM introduces a recursive relationship for the Q-values:\n\\begin{aligned}\n    Q(s, a, s_g, T) &= \\begin{cases}\n        \\mathbb{E}_{s'} [r(s, a, s')] \\; \\text{if} \\; T=0\\\\\n        &\\\\\n        \\mathbb{E}_{s'} [\\max_a \\, Q(s', a, s_g, T-1)] \\; \\text{otherwise.}\\\\\n        \\end{cases} \\\\\n        &\\\\\n        &= \\mathbb{E}_{s'} [r(s, a, s') \\, \\mathbb{1}(T=0) + \\max_a \\, Q(s', a, s_g, T-1) \\, \\mathbb{1}(T\\neq 0)]\\\\\n\\end{aligned}\n\nIf we plan over T=0 steps, i.e. immediately after the action (s, a), the Q-value is the remaining distance to the goal from the next state s'. Otherwise, it is the Q-value of the greedy action in the next state s' with an horizon T-1 (one step shorter). This allows to learn the Q-values from single transitions (s_t, a_t, s_{t+1}):\n\nwith T=0, the target is the remaining distance to the goal.\nwith T&gt;0, the target is the Q-value of the next action at a shorter horizon.\n\nThe critic learns to minimize the prediction error off-policy:\n\n\\begin{aligned}\n\\mathcal{L}(\\theta) = & \\mathbb{E}_{s_t, a_t, s_{t+1} \\in \\mathcal{D}} [  & \\\\\n    & (r(s_t, a_t, s_{t+1}) \\, \\mathbb{1}(T=0) + \\max_a \\, Q(s_{t+1}, a, s_g, T-1) \\, \\mathbb{1}(T\\neq 0) - Q(s_t, a_t, s_g, T))^2 & \\\\\n&& ]\n\\end{aligned}\n\nThis is a model-free Q-learning-like update rule, that can be learned by any off-policy value-based algorithm (DQN, DDPG) and an experience replay memory. The cool trick is that, with a single transition (s_t, a_t, s_{t+1}), you can train the critic with:\n\ndifferent horizons T, e.g. between 0 and T_\\text{max}.\ndifferent goals s_g. You can sample any achievable state as a goal, including the “true” s_{t+T} (hindsight).\n\nYou do not only learn to reach s_g, but any state! TDM learns a lot of information from a single transition, so it has a very good sample complexity. TDM learns to break long trajectories into finite horizons (model-based planning) by learning model-free (Q-learning) updates. TDM is a model-based method in disguise: it does predict the next state directly, but how much closer it will be to the goal via Q-learning.\nFor problems where the model is easy to learn, the performance of TDM is on par with model-based methods (MPC).\n\n\n\nSource: https://bairblog.github.io/2018/04/26/tdm/\n\n\nModel-free methods have a much higher sample complexity, while TDM learns much more from single transitions.\n\n\n\nSource: https://bairblog.github.io/2018/04/26/tdm/\n\n\nFor problems where the model is complex to learn, the performance of TDM is on par with model-free methods (DDPG).\n\n\n\nSource: https://bairblog.github.io/2018/04/26/tdm/\n\n\nModel-based methods suffer from model imprecision on long horizons, while TDM plans over shorter horizons T.\n\n\n\nSource: https://bairblog.github.io/2018/04/26/tdm/\n\n\n\n\n\n\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., et al. (2017). Hindsight Experience Replay. Available at: http://arxiv.org/abs/1707.01495.\n\n\nNagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. (2017). Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. Available at: http://arxiv.org/abs/1708.02596 [Accessed March 3, 2019].\n\n\nPong, V., Gu, S., Dalal, M., and Levine, S. (2018). Temporal Difference Models: Model-Free Deep RL for Model-Based Control. Available at: http://arxiv.org/abs/1802.09081.\n\n\nSalimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017). Evolution Strategies as a Scalable Alternative to Reinforcement Learning. Available at: http://arxiv.org/abs/1703.03864.\n\n\nSzita, I., and Lörincz, A. (2006). Learning Tetris Using the Noisy Cross-Entropy Method. Neural Computation 18, 2936–2941. doi:10.1162/neco.2006.18.12.2936.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Planning (MPC, TDM)</span>"
    ]
  },
  {
    "objectID": "src/4.4-WorldModels.html",
    "href": "src/4.4-WorldModels.html",
    "title": "World models, Dreamer",
    "section": "",
    "text": "World models\nThe core idea of world models (Ha and Schmidhuber, 2018) is to explicitly separate the world model (what will happen next) from the controller (how to act). The neural networks used in deep RL are usually small, as rewards do not contain enough information to train huge networks. However, unsupervised data (without any label nor reward) is plenty and could be leveraged to learn useful representations. A huge world model can be efficiently trained by self-supervised / unsupervised methods, while a small controller should not need too many trials if its input representations are good.\nHa and Schmidhuber (2018) used the Vizdoom Take Cover environment (http://vizdoom.cs.put.edu.pl/) to demonstrate the power of world models, as well as a car racing environment.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>World models, Dreamer</span>"
    ]
  },
  {
    "objectID": "src/4.4-WorldModels.html#world-models",
    "href": "src/4.4-WorldModels.html#world-models",
    "title": "World models, Dreamer",
    "section": "",
    "text": "Tip\n\n\n\nFor a detailed explanation of world models, refer to:\nhttps://worldmodels.github.io/\nThe videos embedded here come from this page.\n\n\n\nArchitecture\nThe architecture of World Models is composed of three modules trained in succession:\n\nThe Vision module V,\nThe Memory module M,\nThe Controller module C.\n\n\n\n\n\n\n\nFigure 23.1: Architecture of world models. Source: https://worldmodels.github.io/\n\n\n\nVision module\nThe vision module V is the encoder of a variational autoencoder (VAE), trained on single frames of the game (obtained using a random policy). The resulting latent vector \\mathbf{z}_t contains a compressed representation of the frame \\mathbf{o}_t.\n\n\n\n\n\n\nFigure 23.2: Vision module. Source: https://worldmodels.github.io/\n\n\n\nMemory module\nThe sequence of latent representations \\mathbf{z}_0, \\ldots \\mathbf{z}_t in a game is fed to a LSTM layer (RNN) together with the actions a_t to compress what happens over time.\nA Mixture Density Network (MDN, Bishop (1994)) is used to predict the distribution of the next latent representations P(\\mathbf{z}_{t+1} | a_t, \\mathbf{h}_t, \\ldots \\mathbf{z}_t). In short, MDN allows to perform probabilistic regression, but predicting both the mean and the variance of the data, instead of just its mean as in vanilla least sqaures regression. Most MDN methods use a mixture of Gaussian distributions to model the target distribution.\n\n\n\n\n\n\nFigure 23.3: Memory module. Source: https://worldmodels.github.io/\n\n\n\n\n\n\n\n\n\nRNN-MDN\n\n\n\nThe RNN-MDN architecture has been used successfully in the past for sequence generation problems such as generating handwriting and sketches (Sketch-RNN, Ha and Eck (2017)).\nCheck a demo here: https://magenta.tensorflow.org/sketch-rnn-demo\n\n\n\nController module\nThe last step is the controller. It takes a latent representation \\mathbf{z}_t and the current hidden state of the LSTM \\mathbf{h}_t as inputs and selects an action linearly:\na_t = \\text{tanh}(W \\, [\\mathbf{z}_t, \\mathbf{h}_t ] + b)\nA RL actor cannot get simpler as that…\n\n\n\n\n\n\nFigure 23.4: Controller module. Source: https://worldmodels.github.io/\n\n\n\nThe controller is not even trained with RL: it uses a genetic algorithm, the Covariance-Matrix Adaptation Evolution Strategy (CMA-ES, Hansen and Ostermeier (2001)), to find the output weights that maximize the returns. The world model is trained by classical self-supervised learning using a random agent before learning, while the controller is simply evolved using a black-box optimizer.\n\n\n\n\n\n\nWorld models\n\n\n\nAlgorithm:\n\nCollect 10,000 rollouts from a random policy.\nTrain VAE (V) to encode each frame into a latent vector \\mathbf{z} \\in \\mathcal{R}^{32}.\nTrain MDN-RNN (M) to model P(\\mathbf{z}_{t+1} | a_t, \\mathbf{h}_t, \\ldots \\mathbf{z}_t).\nEvolve Controller (C) to maximize the expected cumulative reward of a rollout.\n\n\n\nFor the car racing environment, the repartition of the number of weights clearly shows that the complexity of the model lies in the world model, not the controller:\nParameters for car racing:\n\n\n\nModel\nParameter Count\n\n\n\n\nVAE\n4,348,547\n\n\nMDN-RNN\n422,368\n\n\nController\n867\n\n\n\n\n\nResults\nPerformance in car racing:\n\nBelow is the input of the VAE and the reconstruction. The reconstruction does not have to be perfect as long as the latent space is informative.\n\nHaving access to a full rollout of the future leads to more stable driving:\n\n\n\n\n\n\nFigure 23.5: Performance of World models on the car racing environemnt. Source: https://worldmodels.github.io/\n\n\n\nIn summary, the world model V+M is learned offline with a random agent, using self-supervised learning, while the controller C has few weights (1000) and can be trained by evolutionary algorithms, not even RL. The network can even learn by playing entirely in its own imagination, as the world model can be applied on itself and predict all future frames. It just needs to additionally predict the reward. After that, the learned policy can be transferred to the real environment.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>World models, Dreamer</span>"
    ]
  },
  {
    "objectID": "src/4.4-WorldModels.html#deep-planning-network---planet",
    "href": "src/4.4-WorldModels.html#deep-planning-network---planet",
    "title": "World models, Dreamer",
    "section": "Deep Planning Network - PlaNet",
    "text": "Deep Planning Network - PlaNet\nPlaNet (Hafner et al., 2019) extends the idea of World models by learning the model together with the policy (end-to-end). It learns a latent dynamics model that takes the past observations o_t into account (needed for POMDPs):\ns_{t}, r_{t+1}, \\hat{o}_t = f(o_t, a_t, s_{t-1})\nand plans in the latent space using multiple rollouts:\na_t = \\text{arg}\\max_a \\mathbb{E}[R(s_t, a, s_{t+1}, \\ldots)]\nTraining\n\n\n\n\n\n\nFigure 23.6: Latent dynamics model of PlaNet. Source: https://planetrl.github.io/\n\n\n\nThe latent dynamics model is a sequential variational autoencoder learning concurrently:\n\nAn encoder from the observation o_t to the latent space s_t.\n\nq(s_t | o_t)\n\nA decoder from the latent space to the reconstructed observation \\hat{o}_t.\n\np(\\hat{o}_t | s_t)\n\nA transition model to predict the next latent representation given an action.\n\np(s_{t+1} | s_t, a_t)\n\nA reward model predicting the immediate reward.\n\np(r_t | s_t)\nTraining sequences (o_1, a_1, o_2, \\ldots, o_T) can be generated off-policy (e.g. from demonstrations) or on-policy. The loss function to train this recurrent state-space model (RSSM), which has a stochastic component in the encoder (VAE), and has to compensate for latent overshooting (i.e. to enforce consistency between one-step and multi-step predictions in the latent space), is slightly complicated and is not explained here.\n\n\n\n\n\n\nFigure 23.7: Training the latent dynamics model of PlaNet. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html\n\n\n\nInference\nFrom a single observation o_t encoded into s_t, we can generate 10000 rollouts using random sampling. In these rollouts, the action sequences are varied randomly, generating as many random sequences as needed. The return of each rollout can be estimated using the reward model. A belief over the action sequences is updated using the cross-entropy method (CEM, Szita and Lörincz (2006)) in order to restrict the search.\nAfter the 10000 rollouts are executed (in imagination), the sequence with the highest return is selected and its first action is executed. At the next time step, planning starts from scratch: this is the key idea of Model Predictive Control. There is no actor in PlaNet, only a transition model used for planning. The reason PlaNet works is that planning is done in the latent space, hich has a much lower dimensionality than the observations (e.g. images).\n\n\n\nSource: https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html\n\n\nResults\nPlanet learns continuous Mujoco image-based control problems in 2000 episodes, where D4PG needs 50 times more.\n\nThe latent dynamics model can learn 6 control tasks at the same time. As there is no actor, but only a planner, the same network can control all agents!\n\n\n\n\n\n\nFigure 23.8: Top row: agent behavior on the 6 Mujoco tasks. Bottom row: predcited frames by the agent. Source: https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>World models, Dreamer</span>"
    ]
  },
  {
    "objectID": "src/4.4-WorldModels.html#dreamer",
    "href": "src/4.4-WorldModels.html#dreamer",
    "title": "World models, Dreamer",
    "section": "Dreamer",
    "text": "Dreamer\nDreamer (Hafner et al., 2020) extends the idea of PlaNet by additionally training an actor instead of using a MPC planner. The latent dynamics model is the same RSSM architecture. Training a “model-free” actor on imaginary rollouts instead of MPC planning should reduce the computational cost at inference time.\n\n\n\n\n\n\nFigure 23.9: Dreamer first learns the World model (RSSM), then trains a model-free agent in its imagination to maximize the rewards, before being used to interact with the environment. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html\n\n\n\nThe latent dynamics model is the same as in PlaNet, learning from past experiences.\n\n\n\n\n\n\nFigure 23.10: Latent dynamics model of Dreamer, exactly the same as PlaNet. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html\n\n\n\nThe behavior module learns to predict the value of a state V_\\varphi(s) and the policy \\pi_\\theta(s) (actor-critic). It is trained in imagination in the latent space using the reward model for the immediate rewards (to compute returns) and the transition model for the next states.\n\n\n\n\n\n\nFigure 23.11: The actor-critic agent is trained in imagination using rollouts generated by the RSSM model. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html\n\n\n\nThe current observation o_t is encoded into a state s_t, the actor selects an action a_t, the transition model predicts s_{t+1}, the reward model predicts r_{t+1}, the critic predicts V_\\varphi(s_t). At the end of the sequence, we apply backpropagation-through-time to train the actor and the critic.\nThe critic V_\\varphi(s_t) is trained on the imaginary sequence (s_t, a_t, r_{t+1}, s_{t+1}, \\ldots, s_T) to minimize the prediction error with the \\lambda-return:\nR^\\lambda_t = (1  - \\lambda) \\, \\sum_{n=1}^{T-t-1} \\lambda^{n-1} \\, R^n_t + \\lambda^{T-t-1} \\, R_t\nThe actor \\pi_\\theta(s_t, a_t) is trained on the sequence to maximize the sum of the value of the future states:\n\\mathcal{J}(\\theta) = \\mathbb{E}_{s_t, a_t \\sim \\pi_\\theta} [\\sum_{t'=t}^T V_\\varphi(s_{t'})]\nThe main advantage of training an actor is that we need only one rollout when training it: backpropagation maximizes the expected returns. When acting, we just need to encode the history of the episode in the latent space, and the actor becomes model-free!\n\n\n\n\n\n\nFigure 23.12: Complete Dreamer architecture. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html\n\n\n\nDreamer beats model-free and model-based methods on 20 continuous control tasks.\n\n\n\n\n\n\n\nFigure 23.13: Results of Dreamer on various Mujoco tasks, compared to SotA control methods. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html\n\n\n\nIt also learns Atari and Deepmind lab video games, sometimes on par with Rainbow or IMPALA!\n\n\n\n\n\n\n\nFigure 23.14: Results of Dreamer on Atari gameSource: https://dreamrl.github.io/\n\n\n\nA recent extension of Dreamer, DayDreamer (Wu et al., 2022), allows physical robots to learn complex tasks in a few hours.\n\n\n\nDayDreamer. Source: Wu et al. (2022) and https://danijar.com/daydreamer\n\n\n\n\n\n\n\nBishop, C. M. (1994). Mixture Density Networks. Birmingham, UK: Neural Computing Research Group, Aston University Available at: https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf [Accessed November 12, 2024].\n\n\nHa, D., and Eck, D. (2017). A Neural Representation of Sketch Drawings. Available at: http://arxiv.org/abs/1704.03477 [Accessed January 17, 2021].\n\n\nHa, D., and Schmidhuber, J. (2018). World Models. doi:10.5281/zenodo.1207631.\n\n\nHafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2020). Dream to Control: Learning Behaviors by Latent Imagination. Available at: http://arxiv.org/abs/1912.01603 [Accessed March 24, 2020].\n\n\nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., et al. (2019). Learning Latent Dynamics for Planning from Pixels. Available at: http://arxiv.org/abs/1811.04551 [Accessed January 24, 2020].\n\n\nHansen, N., and Ostermeier, A. (2001). Completely Derandomized Self-Adaptation in Evolution Strategies. Evolutionary Computation 9, 159–195. doi:10.1162/106365601750190398.\n\n\nSzita, I., and Lörincz, A. (2006). Learning Tetris Using the Noisy Cross-Entropy Method. Neural Computation 18, 2936–2941. doi:10.1162/neco.2006.18.12.2936.\n\n\nWu, P., Escontrela, A., Hafner, D., Goldberg, K., and Abbeel, P. (2022). DayDreamer: World Models for Physical Robot Learning. doi:10.48550/arXiv.2206.14176.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>World models, Dreamer</span>"
    ]
  },
  {
    "objectID": "src/4.5-AlphaGo.html",
    "href": "src/4.5-AlphaGo.html",
    "title": "AlphaGo",
    "section": "",
    "text": "Minimax and Alpha-Beta\nGo is an ancient two-opponents board game, where each player successively places stones on a 19x19 grid. When a stone is surrounded by four opponents, it dies. The goal is to ensure strategical position in order to cover the biggest territory. There are around 10^{170} possible states and 250 actions available at each turn (10^{761} possible games), making it a much harder game than chess for a computer (35 possible actions, 10^{120} possible games). A game lasts 150 moves on average (80 in chess). Up until 2015 and AlphaGo, Go AIs could not compete with world-class experts, and people usually considered AI would need at least another 20 years to solve it. They were wrong.\nThe Minimax algorithm is a classical algorithm to find the optimal strategy in zero sum games, where what one player wins is lost by the other (basically all games except collaborative games). Minimax expands the whole game tree, simulating the aternating moves of the MAX (you) and MIN (your opponent) players. The final outcome (win = +1 or lose = -1, but it could be any number) is assigned to the leaves. By supposing that MIN plays optimally (i.e. in his own interest), it is possible to infer what the optimal strategy for MAX is..\nThe value of the leaves is propagated backwards to the starting position: MAX chooses the action leading to the state with the highest value, MIN does the opposite. MAX just has to play first the action with the highest value and let his opponent play. It might be necessary to re-plan if MIN is not optimal.\nFor most games, the tree becomes too huge for such a systematic search: The value of all states further than a couple of moves away are approximated by a heuristic function: the value V(s) of these states. Obviously useless parts of the tree can be pruned: This is the Alpha-Beta algorithm. Alpha-Beta methods work well for simple problems where the complete game tree can be manipulated: Tic-Tac-Toe has only a couple of possible states and actions (3^9 = 19000 states). It also works when precise heuristics can be derived in a reasonable time. This is the principle of IBM DeepBlue which was the first Chess AI to beat a world champion (Garry Kasparov) in 1995. Carefully engineered heuristics (with the help of chess masters) allowed DeepBlue to search 6 moves away what is the best situation it can arrive in.\nThis approach does not work in Go because its branching factor (250 actions possible from each state) is too huge: the tree explodes very soon. 250^{6} \\approx 10^{15}, so even if your processor evaluates 1 billion nodes per second, it would need 11 days to evaluate a single position 6 moves away…",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>AlphaGo</span>"
    ]
  },
  {
    "objectID": "src/4.5-AlphaGo.html#minimax-and-alpha-beta",
    "href": "src/4.5-AlphaGo.html#minimax-and-alpha-beta",
    "title": "AlphaGo",
    "section": "",
    "text": "Figure 24.2: Zero-sum game.\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.3: Solution found by Minimax\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.4: Game tree of Go. Source: &lt;https://www.quora.com/What-does-it-mean-that-AlphaGo-relied-on-Monte Carlo-tree-search/answer/Kostis-Gourgoulias&gt;",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>AlphaGo</span>"
    ]
  },
  {
    "objectID": "src/4.5-AlphaGo.html#alphago",
    "href": "src/4.5-AlphaGo.html#alphago",
    "title": "AlphaGo",
    "section": "AlphaGo",
    "text": "AlphaGo\n\nTraining the neural networks\nAlphaGo (Silver et al., 2016) uses four different neural networks:\n\nThe rollout policy and the SL policy network use supervised learning to predict expert human moves in any state.\nThe RL policy network uses self-play and reinforcement learning to learn new strategies.\nThe value network learns to predict the outcome of a game (win/lose) from the current state.\n\nThe rollout policy and the value network are used to guide stochastic tree exploration in Monte Carlo Tree Search (MCTS) (MPC-like planning algorithm).\n\n\n\n\n\n\nFigure 24.5: Four neural networks are used in AlphaGo. Source: Silver et al. (2016)\n\n\n\nSupervised learning is used for bootstrapping the policy network \\rho_\\sigma, learning to predict human expert moves. 30M expert games have been gathered: the input is the 19x19 board configuration, the output is the move played by the expert. The CNN has 13 convolutional layers (5x5) and no max-pooling. The accuracy at the end of learning is 57% (not bad, but not sufficient to beat experts).\nA faster rollout policy network \\rho_\\pi is also trained: It has only one layer and views only part of the state (around the last opponent’s move). Its prediction accuracy is only 24%, but its inference time is only 2 \\mus, instead of 3 ms for the policy network \\rho_\\sigma. The idea of having a smaller rollout network has been reused later in I2A (see Section I2A - Imagination-augmented agents)\nThe SL policy network \\rho_\\sigma is used to initialize the weights of the RL policy network \\rho_\\rho, so it can start exploring from a decent policy. The RL policy network then plays against an older version of itself (\\approx target network) to improve its policy, updating the weights using Policy Gradient (REINFORCE):\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, R ]\n\nwhere R = +1 when the game is won, -1 otherwise.\nThe idea of playing against an older version of the same network (self-play) allows to learn offline, bypassing the need for (slow) human opponents. The RL policy network already wins 85% of the time against the strongest AI at the time (Pachi), but not against expert humans.\nA value network \\nu_\\theta finally learns to predict the outcome of a game (+1 when winning, -1 when losing) based on the self-play positions generated by the RL policy network.\n\n\nMonte Carlo Tree Search\n\n\n\n\n\n\nFigure 24.6: Monte Carlo Tree Search. Source: Silver et al. (2016)\n\n\n\nThe final AlphaGo player uses Monte Carlo Tree Search (MCTS), which is an incremental tree search (depth-limited), biased by the Q-value of known transitions. The game tree is traversed depth-first from the current state, but the order of the visits depends on the value of the transition. MCTS was previously the standard approach for Go AIs, but based on expert moves only, not deep networks.\nIn the selection phase, a path is found in the tree of possible actions using Upper Confidence Bound (UCB). The probability of selecting an action when sampling the tree depends on:\n\nIts Q-value Q(s, a) (as learned by MCTS): how likely this action leads to winning.\nIts prior probability: how often human players would play it, given by the SL policy network \\rho_\\sigma.\nIts number of visits N(s, a): this ensures exploration during the sampling.\n\na_t = \\text{argmax}_a \\, Q(s, a) + K \\cdot \\frac{P(s, a)}{1 + N(s, a)}\nIn the expansion phase, a leaf state s_L of the game tree is reached. The leaf is expanded, and the possible successors of that state are added to the tree. One requires a model to know which states are possible successors, but this is very easy in Go.\ns_{t+1} = f(s_t, a_t)\nThe tree therefore grows every time a Monte Carlo sampling (“episode”) is done.\nIn the evaluation phase, the leaf s_L is evaluated both by:\n\nthe RL value network \\nu_\\theta (how likely can we win from that state)\na random rollout until the end of the game using the fast rollout policy \\rho_\\pi.\n\nThe random rollout consists in “emulating” the end of the game using the fast rollout policy network. The rollout is of course imperfect, but complements the value network: they are more accurate together than alone!\nV(s_L) = (1 - \\lambda)  \\, \\nu_\\theta(s_L) + \\lambda \\, R_\\text{rollout} \nThis combination solves the bias/variance trade-off.\nIn the backup phase, the Q-values of all actions taken when descending the tree are updated with the value of the leaf node:\nQ(s, a) = \\frac{1}{N(s, a)} \\sum_{i=1}^{n} V(s_L^i) \nThis is a Monte Carlo method: perform one episode and update the Q-value of all taken actions. However, it never uses real rewards, only value estimates. The Q-values are learned by using both the learned value of future states (value network) and internal simulations (rollout).\nThe four phases are then repeated as long as possible (time is limited in Go), to expand the game tree as efficiently as possible. The game tree is repeatedly sampled and grows after each sample. When the time is up, the greedy action (highest Q-value) in the initial state is chosen and played. For the next move, the tree is reset and expanded again (MPC replanning).\nIn the end, during MCTS, only the value network \\nu_\\theta, the SL policy network \\rho_\\sigma and the fast rollout policy \\rho_\\pi are used. The RL policy network \\rho_\\rho is only used to train the value network \\nu_\\theta. i.e. to predict which positions are interesting or not. However, the RL policy network can discover new strategies by playing many times against itself, without relying on averaging expert moves like the previous approaches.\nAlphaGo was able to beat Lee Sedol in 2016, 19 times World champion. It relies on human knowledge to bootstrap a RL agent (supervised learning). The RL agent discovers new strategies by using self-play: during the games against Lee Sedol, it was able to use novel moves which were never played before and surprised its opponent. The neural networks are only used to guide random search using MCTS: the policy network alone is not able to beat grandmasters. However, the computational cost of AlphaGo is huge: training took several weeks on 1202 CPUs and 176 GPUs…",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>AlphaGo</span>"
    ]
  },
  {
    "objectID": "src/4.5-AlphaGo.html#alphazero",
    "href": "src/4.5-AlphaGo.html#alphazero",
    "title": "AlphaGo",
    "section": "AlphaZero",
    "text": "AlphaZero\nAlphaZero (Silver et al., 2018) totally skips the supervised learning part: the RL policy network starts self-play from scratch! The RL policy network uses MCTS to select moves, not a softmax-like selection as in AlphaGo. The policy and value networks are merged into a two-headed monster: the convolutional residual layers are shared to predict both:\n\nThe policy \\pi_\\theta(s), which is only used to guide MCTS (prior of UCB).\n\na_t = \\text{argmax}_a \\, Q(s, a) + K \\cdot \\frac{\\pi_\\theta(s, a)}{1 + N(s, a)}\n\nThe state value V_\\varphi(s) for the value of the leaves (no fast rollout).\n\n\n\n\n\n\n\nFigure 24.7: Architecture of AlphaZero. Source: Silver et al. (2018)\n\n\n\nThe loss function used to train the network is a compound loss:\n\n    \\mathcal{L}(\\theta) = (R − V_\\varphi(s))^2 - \\pi_\\text{MCTS}(s) \\, \\log \\pi_\\theta(s) + c ||\\theta||^2\n\nThe policy head \\pi_\\theta(s) learns to mimic the actions selected by MCTS by minimizing the cross-entropy (or KL divergence): policy distillation (see Section I2A - Imagination-augmented agents). The value network V_\\varphi(s) learns to predict the return using Q-learning.\n\n\n\n\n\n\nFigure 24.8: Source: https://deepmind.com/blog/alphago-zero-learning-scratch/\n\n\n\nBy using a single network instead of four and learning faster, AlphaZero also greatly reduces the energy consumption.\n\n\n\nSource: https://deepmind.com/blog/alphago-zero-learning-scratch/\n\n\nMore impressively, the same algorithm can also play Chess and Shogi!\n\n\n\nSource: https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go\n\n\nThe network weights are reset for each game, but it uses the same architecture and hyperparameters. After only 8 hours of training, AlphaZero beats Stockfish with 28-72-00, the best Chess AI at the time, which itself beats any human. This proves that the algorithm is generic and can be applied to any board game.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>AlphaGo</span>"
    ]
  },
  {
    "objectID": "src/4.5-AlphaGo.html#muzero",
    "href": "src/4.5-AlphaGo.html#muzero",
    "title": "AlphaGo",
    "section": "MuZero",
    "text": "MuZero\nMuZero (Schrittwieser et al., 2019) is the latest extension of AlphaZero (but see EfficientZero (Ye et al., 2021)). Instead of relying on a perfect simulator for the MCTS, it learns the dynamics model instead.\ns_{t+1}, r_{t+1} = f_\\theta(s_t, a_t)\n\n\n\n\n\n\nFigure 24.9: MuZero. Source: Schrittwieser et al. (2019)\n\n\n\nMuZero is composed of three neural networks:\n\nThe representation network s= h(o_1, \\ldots, o_t) (encoder) transforming the history of observations into a state representation (latent space).\nThe dynamics model s', r = g(s, a) used to generate rollouts for MCTS.\nThe policy and value network \\pi, V = f(s) learning the policy with PG.\n\nThe dynamics model s', r = g(s, a) replaces the perfect simulator in MCTS. It is used in the expansion phase of MCTS to add new nodes. Importantly, nodes are latent representations of the observations, not observations directly. This is a similar idea to World Models and PlaNet/Dreamer, which plan in the latent space of a VAE. Selection in MCTS still follows an upper confidence bound using the learned policy \\pi:\n\nThe actions taking during self-play are taken from the MCTS search as in AlphaZero. Note that the network plays each turn: there is additional information about whether the network is playing white or black. Self-played games are stored in a huge experience replay memory. Finally, complete games sampled from the ERM are used to learn simultaneously the three networks f, g and h.\nMuZero beats AlphaZero on Chess, Go and Shogi, but also R2D2 on Atari games. The representation network h allows to encode the Atari frames in a compressed manner that allows planning over raw images.\n\n\n\n\n\n\nFigure 24.10: Results of MuZero. Source: Schrittwieser et al. (2019)\n\n\n\n\n\n\n\n\n\nAdditional resources\n\n\n\nA nice series of blog posts by David Foster explaining how to implement MuZero:\nhttps://medium.com/applied-data-science/how-to-build-your-own-muzero-in-python-f77d5718061a\nhttps://medium.com/applied-data-science/how-to-build-your-own-deepmind-muzero-in-python-part-2-3-f99dad7a7ad\nhttps://medium.com/applied-data-science/how-to-build-your-own-deepmind-muzero-in-python-part-3-3-ccea6b03538b\n\n\n\n\n\n\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., et al. (2019). Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. Available at: http://arxiv.org/abs/1911.08265 [Accessed November 24, 2019].\n\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature 529, 484–489. doi:10.1038/nature16961.\n\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., et al. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science 362, 1140–1144. doi:10.1126/science.aar6404.\n\n\nYe, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y. (2021). Mastering Atari Games with Limited Data. doi:10.48550/arXiv.2111.00210.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>AlphaGo</span>"
    ]
  },
  {
    "objectID": "src/4.6-Misc.html",
    "href": "src/4.6-Misc.html",
    "title": "Misc.",
    "section": "",
    "text": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images (Watter et al., 2015)\nEfficient Model-Based Deep Reinforcement Learning with Variational State Tabulation (Corneil et al., 2018)\nModel-Based Value Estimation for Efficient Model-Free Reinforcement Learning (Feinberg et al., 2018)\nLearning to Adapt: Meta-Learning for Model-Based Control, (Clavera et al., 2018)\nThe Predictron: End-To-End Learning and Planning (Silver et al., 2016)\nModel-Based Planning with Discrete and Continuous Actions (Henaff et al., 2017)\nSchema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics (Kansky et al., 2017)\nUniversal Planning Networks (Srinivas et al., 2018)\nRecall Traces: Backtracking Models for Efficient Reinforcement Learning (Goyal et al., 2018)\nDeep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning (Peng et al., 2018)\nQ-map: a Convolutional Approach for Goal-Oriented Reinforcement Learning (Pardo et al., 2018)\n\n\n\n\n\nClavera, I., Nagabandi, A., Fearing, R. S., Abbeel, P., Levine, S., and Finn, C. (2018). Learning to Adapt: Meta-Learning for Model-Based Control. Available at: http://arxiv.org/abs/1803.11347.\n\n\nCorneil, D., Gerstner, W., and Brea, J. (2018). Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation. Available at: http://arxiv.org/abs/1802.04325.\n\n\nFeinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez, J. E., and Levine, S. (2018). Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning. Available at: http://arxiv.org/abs/1803.00101.\n\n\nGoyal, A., Brakel, P., Fedus, W., Lillicrap, T., Levine, S., Larochelle, H., et al. (2018). Recall Traces: Backtracking Models for Efficient Reinforcement Learning. Available at: http://arxiv.org/abs/1804.00379.\n\n\nHenaff, M., Whitney, W. F., and LeCun, Y. (2017). Model-Based Planning with Discrete and Continuous Actions. Available at: http://arxiv.org/abs/1705.07177.\n\n\nKansky, K., Silver, T., Mély, D. A., Eldawy, M., Lázaro-Gredilla, M., Lou, X., et al. (2017). Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics. Available at: http://arxiv.org/abs/1706.04317 [Accessed January 10, 2019].\n\n\nPardo, F., Levdik, V., and Kormushev, P. (2018). Q-map: A Convolutional Approach for Goal-Oriented Reinforcement Learning. Available at: http://arxiv.org/abs/1810.02927.\n\n\nPeng, B., Li, X., Gao, J., Liu, J., Wong, K.-F., and Su, S.-Y. (2018). Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning. Available at: http://arxiv.org/abs/1801.06176.\n\n\nSilver, D., van Hasselt, H., Hessel, M., Schaul, T., Guez, A., Harley, T., et al. (2016). The Predictron: End-To-End Learning and Planning. Available at: http://arxiv.org/abs/1612.08810.\n\n\nSrinivas, A., Jabri, A., Abbeel, P., Levine, S., and Finn, C. (2018). Universal Planning Networks. Available at: http://arxiv.org/abs/1804.00645.\n\n\nWatter, M., Springenberg, J. T., Boedecker, J., and Riedmiller, M. (2015). Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images. Available at: https://arxiv.org/pdf/1506.07365.pdf.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Misc.</span>"
    ]
  },
  {
    "objectID": "src/5.1-Intrinsic.html",
    "href": "src/5.1-Intrinsic.html",
    "title": "Intrinsic motivation",
    "section": "",
    "text": "work in progress\n\n(Pathak et al., 2017) Curiosity-driven Exploration by Self-supervised Prediction\n(Burda et al., 2018) Exploration by Random Network Distillation\n(Sekar et al., 2020) Planning to Explore via Self-Supervised World Models\n\n\n\n\n\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. (2018). Exploration by Random Network Distillation. doi:10.48550/arXiv.1810.12894.\n\n\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). Curiosity-driven Exploration by Self-supervised Prediction. Available at: http://arxiv.org/abs/1705.05363 [Accessed February 6, 2021].\n\n\nSekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., and Pathak, D. (2020). Planning to Explore via Self-Supervised World Models. Available at: http://arxiv.org/abs/2005.05960 [Accessed January 29, 2024].",
    "crumbs": [
      "**Advanced topics**",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Intrinsic motivation</span>"
    ]
  },
  {
    "objectID": "src/5.2-Inverse.html",
    "href": "src/5.2-Inverse.html",
    "title": "Inverse Reinforcement Learning",
    "section": "",
    "text": "work in progress\nHindsight experience replay: Andrychowicz et al. (2017)\nGoal-conditioned inverse RL: Ding et al. (2019)\n\n\n\n\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., et al. (2017). Hindsight Experience Replay. Available at: http://arxiv.org/abs/1707.01495.\n\n\nDing, Y., Florensa, C., Phielipp, M., and Abbeel, P. (2019). Goal-conditioned Imitation Learning. in (Long Beach, California: PMLR), 8. Available at: https://openreview.net/pdf?id=HkglHcSj2N.",
    "crumbs": [
      "**Advanced topics**",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Inverse Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "src/5.3-OfflineRL.html",
    "href": "src/5.3-OfflineRL.html",
    "title": "Offline RL",
    "section": "",
    "text": "Diffusion policies\nwork in progress\nChi et al. (2023) Diffusion Policy: Visuomotor Policy Learning via Action Diffusion Wang et al. (2024) One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion Distillation",
    "crumbs": [
      "**Advanced topics**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Offline RL</span>"
    ]
  },
  {
    "objectID": "src/5.3-OfflineRL.html#diffusion-policies",
    "href": "src/5.3-OfflineRL.html#diffusion-policies",
    "title": "Offline RL",
    "section": "",
    "text": "Chi, C., Xu, Z., Feng, S., Cousineau, E., Du, Y., Burchfiel, B., et al. (2023). Diffusion Policy: Visuomotor Policy Learning via Action Diffusion. Available at: https://arxiv.org/abs/2303.04137v5 [Accessed October 9, 2024].\n\n\nWang, Z., Li, Z., Mandlekar, A., Xu, Z., Fan, J., Narang, Y., et al. (2024). One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion Distillation. doi:10.48550/arXiv.2410.21257.",
    "crumbs": [
      "**Advanced topics**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Offline RL</span>"
    ]
  },
  {
    "objectID": "src/5.4-Meta.html",
    "href": "src/5.4-Meta.html",
    "title": "Meta learning",
    "section": "",
    "text": "work in progress",
    "crumbs": [
      "**Advanced topics**",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Meta learning</span>"
    ]
  },
  {
    "objectID": "src/5.5-Hierarchical.html",
    "href": "src/5.5-Hierarchical.html",
    "title": "Hierarchical Reinforcement Learning",
    "section": "",
    "text": "work in progress\nVery good link: https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/\nNachum et al. (2018)\nHaarnoja et al. (2018)\nCo-Reyes et al. (2018)\n\n\n\n\nCo-Reyes, J. D., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., and Levine, S. (2018). Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings.\n\n\nHaarnoja, T., Hartikainen, K., Abbeel, P., and Levine, S. (2018). Latent Space Policies for Hierarchical Reinforcement Learning. Available at: http://arxiv.org/abs/1804.02808.\n\n\nNachum, O., Gu, S., Lee, H., and Levine, S. (2018). Data-Efficient Hierarchical Reinforcement Learning. Available at: http://arxiv.org/abs/1805.08296.",
    "crumbs": [
      "**Advanced topics**",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Hierarchical Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "src/references.html",
    "href": "src/references.html",
    "title": "References",
    "section": "",
    "text": "Amari, S.-I. (1998). Natural gradient works efficiently in learning.\nNeural Computation 10, 251–276.\n\n\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R.,\nWelinder, P., et al. (2017). Hindsight Experience Replay.\nAvailable at: http://arxiv.org/abs/1707.01495.\n\n\nAnschel, O., Baram, N., and Shimkin, N. (2016).\nAveraged-DQN: Variance Reduction and\nStabilization for Deep Reinforcement Learning.\nAvailable at: http://arxiv.org/abs/1611.01929.\n\n\nArjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein\nGAN. Available at: http://arxiv.org/abs/1701.07875.\n\n\nBadia, A. P., Piot, B., Kapturowski, S., Sprechmann, P., Vitvitskyi, A.,\nGuo, D., et al. (2020a). Agent57: Outperforming the\nAtari Human Benchmark. Available at: http://arxiv.org/abs/2003.13350\n[Accessed January 17, 2022].\n\n\nBadia, A. P., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot, B.,\nKapturowski, S., et al. (2020b). Never Give Up:\nLearning Directed Exploration Strategies. Available at: http://arxiv.org/abs/2002.06038\n[Accessed January 17, 2022].\n\n\nBadrinarayanan, V., Kendall, A., and Cipolla, R. (2016).\nSegNet: A Deep Convolutional Encoder-Decoder\nArchitecture for Image Segmentation. Available at:\nhttp://arxiv.org/abs/1511.00561\n[Accessed November 29, 2020].\n\n\nBaird, L. C. (1993). Advantage updating. Wright-Patterson Air Force Base\nAvailable at: http://leemon.com/papers/1993b.pdf.\n\n\nBakker, B. (2001). Reinforcement Learning with Long\nShort-Term Memory. in Advances in Neural Information\nProcessing Systems 14 (NIPS 2001), 1475–1482.\nAvailable at: https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory.\n\n\nBarth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., TB,\nD., et al. (2018). Distributed Distributional Deterministic Policy\nGradients. Available at: http://arxiv.org/abs/1804.08617.\n\n\nBellemare, M. G., Dabney, W., and Munos, R. (2017). A\nDistributional Perspective on Reinforcement\nLearning. Available at: http://arxiv.org/abs/1707.06887.\n\n\nBishop, C. M. (1994). Mixture Density Networks. Birmingham,\nUK: Neural Computing Research Group, Aston University Available at: https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf\n[Accessed November 12, 2024].\n\n\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. (2018). Exploration\nby Random Network Distillation. doi:10.48550/arXiv.1810.12894.\n\n\nChi, C., Xu, Z., Feng, S., Cousineau, E., Du, Y., Burchfiel, B., et al.\n(2023). Diffusion Policy: Visuomotor Policy\nLearning via Action Diffusion. Available at: https://arxiv.org/abs/2303.04137v5\n[Accessed October 9, 2024].\n\n\nCho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,\nSchwenk, H., et al. (2014). Learning Phrase Representations\nusing RNN Encoder-Decoder for Statistical Machine\nTranslation. Available at: http://arxiv.org/abs/1406.1078.\n\n\nChou, P.-W., Maturana, D., and Scherer, S. (2017). Improving\nStochastic Policy Gradients in Continuous\nControl with Deep Reinforcement Learning using the\nBeta Distribution. in International\nConference on Machine Learning Available\nat: http://proceedings.mlr.press/v70/chou17a/chou17a.pdf.\n\n\nClavera, I., Nagabandi, A., Fearing, R. S., Abbeel, P., Levine, S., and\nFinn, C. (2018). Learning to Adapt:\nMeta-Learning for Model-Based Control.\nAvailable at: http://arxiv.org/abs/1803.11347.\n\n\nCo-Reyes, J. D., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., and\nLevine, S. (2018). Self-Consistent Trajectory Autoencoder:\nHierarchical Reinforcement Learning with Trajectory\nEmbeddings.\n\n\nCorneil, D., Gerstner, W., and Brea, J. (2018). Efficient\nModel-Based Deep Reinforcement Learning with\nVariational State Tabulation. Available at: http://arxiv.org/abs/1802.04325.\n\n\nDabney, W., Rowland, M., Bellemare, M. G., and Munos, R. (2017).\nDistributional Reinforcement Learning with Quantile\nRegression. Available at: http://arxiv.org/abs/1710.10044\n[Accessed June 28, 2019].\n\n\nDayan, P., and Niv, Y. (2008). Reinforcement learning: The\nGood, The Bad and The Ugly. Current\nOpinion in Neurobiology 18, 185–196. doi:10.1016/j.conb.2008.08.003.\n\n\nDegrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese,\nF., et al. (2022). Magnetic control of tokamak plasmas through deep\nreinforcement learning. Nature 602, 414–419. doi:10.1038/s41586-021-04301-9.\n\n\nDegris, T., White, M., and Sutton, R. S. (2012). Linear Off-Policy\nActor-Critic. in Proceedings of the 2012 International\nConference on Machine Learning Available at: http://arxiv.org/abs/1205.4839.\n\n\nDing, Y., Florensa, C., Phielipp, M., and Abbeel, P. (2019).\nGoal-conditioned Imitation Learning. in (Long Beach,\nCalifornia: PMLR), 8. Available at: https://openreview.net/pdf?id=HkglHcSj2N.\n\n\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., et\nal. (2018). IMPALA: Scalable Distributed\nDeep-RL with Importance Weighted Actor-Learner\nArchitectures. doi:10.48550/arXiv.1802.01561.\n\n\nFeinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez, J. E., and\nLevine, S. (2018). Model-Based Value Estimation for\nEfficient Model-Free Reinforcement Learning. Available at:\nhttp://arxiv.org/abs/1803.00101.\n\n\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves,\nA., et al. (2017). Noisy Networks for\nExploration. Available at: http://arxiv.org/abs/1706.10295\n[Accessed March 2, 2020].\n\n\nFujimoto, S., van Hoof, H., and Meger, D. (2018). Addressing\nFunction Approximation Error in Actor-Critic\nMethods. Available at: http://arxiv.org/abs/1802.09477\n[Accessed March 1, 2020].\n\n\nGers, F. (2001). Long Short-Term Memory in Recurrent\nNeural Networks. Available at: http://www.felixgers.de/papers/phd.pdf.\n\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,\nD., Ozair, S., et al. (2014). Generative Adversarial\nNetworks. Available at: http://arxiv.org/abs/1406.2661.\n\n\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep\nLearning. MIT Press Available at: http://www.deeplearningbook.org.\n\n\nGoyal, A., Brakel, P., Fedus, W., Lillicrap, T., Levine, S., Larochelle,\nH., et al. (2018). Recall Traces: Backtracking\nModels for Efficient Reinforcement Learning.\nAvailable at: http://arxiv.org/abs/1804.00379.\n\n\nGruslys, A., Dabney, W., Azar, M. G., Piot, B., Bellemare, M., and\nMunos, R. (2017). The Reactor: A fast and\nsample-efficient Actor-Critic agent for Reinforcement\nLearning. Available at: http://arxiv.org/abs/1704.04651.\n\n\nGu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep\nReinforcement Learning for Robotic\nManipulation with Asynchronous Off-Policy Updates.\nin Proc. ICRA Available at: http://arxiv.org/abs/1610.00633.\n\n\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and Levine, S.\n(2016a). Q-Prop: Sample-Efficient Policy\nGradient with An Off-Policy Critic. Available at: http://arxiv.org/abs/1611.02247.\n\n\nGu, S., Lillicrap, T., Sutskever, I., and Levine, S. (2016b). Continuous\nDeep Q-Learning with Model-based\nAcceleration. Available at: http://arxiv.org/abs/1603.00748.\n\n\nHa, D., and Eck, D. (2017). A Neural Representation of\nSketch Drawings. Available at: http://arxiv.org/abs/1704.03477\n[Accessed January 17, 2021].\n\n\nHa, D., and Schmidhuber, J. (2018). World Models. doi:10.5281/zenodo.1207631.\n\n\nHaarnoja, T., Hartikainen, K., Abbeel, P., and Levine, S. (2018a).\nLatent Space Policies for Hierarchical Reinforcement\nLearning. Available at: http://arxiv.org/abs/1804.02808.\n\n\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement\nLearning with Deep Energy-Based Policies.\nAvailable at: http://arxiv.org/abs/1702.08165\n[Accessed February 13, 2019].\n\n\nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., et\nal. (2018b). Soft Actor-Critic Algorithms and\nApplications. Available at: http://arxiv.org/abs/1812.05905\n[Accessed February 5, 2019].\n\n\nHafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2020). Dream to\nControl: Learning Behaviors by Latent\nImagination. Available at: http://arxiv.org/abs/1912.01603\n[Accessed March 24, 2020].\n\n\nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H.,\net al. (2019). Learning Latent Dynamics for\nPlanning from Pixels. Available at: http://arxiv.org/abs/1811.04551\n[Accessed January 24, 2020].\n\n\nHafner, R., and Riedmiller, M. (2011). Reinforcement learning in\nfeedback control. Machine Learning 84, 137–169. doi:10.1007/s10994-011-5235-x.\n\n\nHansen, N., and Ostermeier, A. (2001). Completely Derandomized\nSelf-Adaptation in Evolution Strategies.\nEvolutionary Computation 9, 159–195. doi:10.1162/106365601750190398.\n\n\nHarutyunyan, A., Bellemare, M. G., Stepleton, T., and Munos, R. (2016).\nQ(λ) with off-policy corrections. Available at: http://arxiv.org/abs/1602.04951.\n\n\nHausknecht, M., and Stone, P. (2015). Deep Recurrent\nQ-Learning for Partially Observable MDPs. Available\nat: http://arxiv.org/abs/1507.06527.\n\n\nHe, F. S., Liu, Y., Schwing, A. G., and Peng, J. (2016). Learning to\nPlay in a Day: Faster Deep Reinforcement\nLearning by Optimality Tightening. Available at: http://arxiv.org/abs/1611.01606.\n\n\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual\nLearning for Image Recognition. Available at: http://arxiv.org/abs/1512.03385.\n\n\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T.\n(2015). Learning continuous control policies by stochastic value\ngradients. Proc. International Conference on Neural Information\nProcessing Systems, 2944–2952. Available at: http://dl.acm.org/citation.cfm?id=2969569.\n\n\nHeinrich, J., Lanctot, M., and Silver, D. (2015). Fictitious\nSelf-Play in Extensive-Form Games. 805–813.\nAvailable at: http://proceedings.mlr.press/v37/heinrich15.html.\n\n\nHeinrich, J., and Silver, D. (2016). Deep Reinforcement\nLearning from Self-Play in\nImperfect-Information Games. Available at: http://arxiv.org/abs/1603.01121.\n\n\nHenaff, M., Whitney, W. F., and LeCun, Y. (2017). Model-Based\nPlanning with Discrete and Continuous\nActions. Available at: http://arxiv.org/abs/1705.07177.\n\n\nHessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G.,\nDabney, W., et al. (2017). Rainbow: Combining Improvements\nin Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1710.02298.\n\n\nHochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen\nNetzen. Available at: http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf.\n\n\nHochreiter, S., and Schmidhuber, J. (1997). Long Short-Term\nMemory. Neural Computation 9, 1735–1780. doi:10.1162/neco.1997.9.8.1735.\n\n\nHorgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van\nHasselt, H., et al. (2018). Distributed Prioritized Experience\nReplay. Available at: http://arxiv.org/abs/1803.00933\n[Accessed December 14, 2019].\n\n\nIoffe, S., and Szegedy, C. (2015). Batch Normalization:\nAccelerating Deep Network Training by Reducing\nInternal Covariate Shift. Available at: http://arxiv.org/abs/1502.03167.\n\n\nKakade, S. (2001). A Natural Policy Gradient. in\nAdvances in Neural Information Processing Systems\n14 Available at: https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf.\n\n\nKakade, S., and Langford, J. (2002). Approximately Optimal\nApproximate Reinforcement Learning. Proc. 19th International\nConference on Machine Learning, 267–274. Available at: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601.\n\n\nKansky, K., Silver, T., Mély, D. A., Eldawy, M., Lázaro-Gredilla, M.,\nLou, X., et al. (2017). Schema Networks: Zero-shot Transfer with a Generative Causal\nModel of Intuitive Physics. Available at: http://arxiv.org/abs/1706.04317\n[Accessed January 10, 2019].\n\n\nKapturowski, S., Campos, V., Jiang, R., Rakićević, N., van Hasselt, H.,\nBlundell, C., et al. (2022). Human-level Atari 200x faster.\ndoi:10.48550/arXiv.2209.07550.\n\n\nKapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W.\n(2019). Recurrent experience replay in distributed reinforcement\nlearning. in, 19. Available at: https://openreview.net/pdf?id=r1lyTjAqYX.\n\n\nKaufmann, E., Bauersfeld, L., Loquercio, A., Müller, M., Koltun, V., and\nScaramuzza, D. (2023). Champion-level drone racing using deep\nreinforcement learning. Nature 620, 982–987. doi:10.1038/s41586-023-06419-4.\n\n\nKendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen, J.-M., et\nal. (2018). Learning to Drive in a Day.\nAvailable at: http://arxiv.org/abs/1807.00412\n[Accessed December 19, 2018].\n\n\nKingma, D. P., and Welling, M. (2013). Auto-Encoding Variational\nBayes. Available at: http://arxiv.org/abs/1312.6114.\n\n\nKnight, E., and Lerner, O. (2018). Natural Gradient\nDeep Q-learning. Available at: http://arxiv.org/abs/1803.07482.\n\n\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet\nClassification with Deep Convolutional Neural\nNetworks. in Advances in Neural Information Processing\nSystems (NIPS) Available at: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.\n\n\nLevine, S., and Koltun, V. (2013). Guided Policy Search. in\nProceedings of Machine Learning Research, 1–9.\nAvailable at: http://proceedings.mlr.press/v28/levine13.html.\n\n\nLi, W., Zhu, Y., and Zhao, D. (2022). Missile guidance with assisted\ndeep reinforcement learning for head-on interception of maneuvering\ntarget. Complex Intell. Syst. 8, 1205–1216. doi:10.1007/s40747-021-00577-6.\n\n\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa,\nY., et al. (2015). Continuous control with deep reinforcement learning.\nCoRR. Available at: http://arxiv.org/abs/1509.02971.\n\n\nLötzsch, W., Vitay, J., and Hamker, F. H. (2017). Training a deep policy\ngradient-based neural network with asynchronous learners on a simulated\nrobotic problem. in INFORMATIK 2017.\nGesellschaft für Informatik, eds. M. Eibl\nand M. Gaedke (Gesellschaft für Informatik, Bonn), 2143–2154. Available\nat: https://dl.gi.de/handle/20.500.12116/3986.\n\n\nLuo, J., Paduraru, C., Voicu, O., Chervonyi, Y., Munns, S., Li, J., et\nal. (2022). Controlling Commercial Cooling Systems Using\nReinforcement Learning. doi:10.48550/arXiv.2211.07357.\n\n\nMachado, M. C., Bellemare, M. G., and Bowling, M. (2018).\nCount-Based Exploration with the Successor\nRepresentation. Available at: http://arxiv.org/abs/1807.11622\n[Accessed February 23, 2019].\n\n\nMadeka, D., Torkkola, K., Eisenach, C., Luo, A., Foster, D. P., and\nKakade, S. M. (2022). Deep Inventory Management. doi:10.48550/arXiv.2210.03137.\n\n\nMalibari, N., Katib, I., and Mehmood, R. (2023). Systematic\nReview on Reinforcement Learning in the\nField of Fintech. doi:10.48550/arXiv.2305.07466.\n\n\nMeuleau, N., Peshkin, L., Kaelbling, L. P., and Kim, K. (2000).\nOff-Policy Policy Search. Available at: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894.\n\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley,\nT., et al. (2016). Asynchronous Methods for Deep\nReinforcement Learning. in Proc. ICML\nAvailable at: http://arxiv.org/abs/1602.01783.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I.,\nWierstra, D., et al. (2013). Playing Atari with Deep\nReinforcement Learning. Available at: http://arxiv.org/abs/1312.5602.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J.,\nBellemare, M. G., et al. (2015). Human-level control through deep\nreinforcement learning. Nature 518, 529–533. doi:10.1038/nature14236.\n\n\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016).\nSafe and Efficient Off-Policy Reinforcement Learning.\nAvailable at: http://arxiv.org/abs/1606.02647.\n\n\nNachum, O., Gu, S., Lee, H., and Levine, S. (2018). Data-Efficient\nHierarchical Reinforcement Learning. Available at: http://arxiv.org/abs/1805.08296.\n\n\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017). Bridging the\nGap Between Value and Policy Based Reinforcement\nLearning. Available at: http://arxiv.org/abs/1702.08892\n[Accessed June 12, 2019].\n\n\nNagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. (2017). Neural\nNetwork Dynamics for Model-Based Deep Reinforcement\nLearning with Model-Free Fine-Tuning. Available at:\nhttp://arxiv.org/abs/1708.02596\n[Accessed March 3, 2019].\n\n\nNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De\nMaria, A., et al. (2015). Massively Parallel Methods for\nDeep Reinforcement Learning. Available at: https://arxiv.org/pdf/1507.04296.pdf.\n\n\nNielsen, M. A. (2015). Neural Networks and Deep\nLearning. Determination Press Available at: http://neuralnetworksanddeeplearning.com/.\n\n\nNiu, F., Recht, B., Re, C., and Wright, S. J. (2011).\nHOGWILD!: A Lock-Free Approach to\nParallelizing Stochastic Gradient Descent. in Proc.\nAdvances in Neural Information Processing\nSystems, 21–21. Available at: http://arxiv.org/abs/1106.5730.\n\n\nO’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2016).\nCombining policy gradient and Q-learning.\nAvailable at: http://arxiv.org/abs/1611.01626\n[Accessed February 13, 2019].\n\n\nOh, J., Guo, Y., Singh, S., and Lee, H. (2018). Self-Imitation\nLearning. Available at: http://arxiv.org/abs/1806.05635.\n\n\nPardo, F., Levdik, V., and Kormushev, P. (2018). Q-map: A\nConvolutional Approach for Goal-Oriented\nReinforcement Learning. Available at: http://arxiv.org/abs/1810.02927.\n\n\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017).\nCuriosity-driven Exploration by Self-supervised Prediction. Available at: http://arxiv.org/abs/1705.05363\n[Accessed February 6, 2021].\n\n\nPeng, B., Li, X., Gao, J., Liu, J., Wong, K.-F., and Su, S.-Y. (2018).\nDeep Dyna-Q: Integrating Planning for\nTask-Completion Dialogue Policy Learning. Available at: http://arxiv.org/abs/1801.06176.\n\n\nPeshkin, L., and Shelton, C. R. (2002). Learning from Scarce\nExperience. Available at: http://arxiv.org/abs/cs/0204043.\n\n\nPeters, J., and Schaal, S. (2008). Reinforcement learning of motor\nskills with policy gradients. Neural Networks 21, 682–697.\ndoi:10.1016/j.neunet.2008.02.003.\n\n\nPong, V., Gu, S., Dalal, M., and Levine, S. (2018). Temporal\nDifference Models: Model-Free Deep RL for\nModel-Based Control. Available at: http://arxiv.org/abs/1802.09081.\n\n\nPopov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G.,\nVecerik, M., et al. (2017). Data-efficient Deep Reinforcement\nLearning for Dexterous Manipulation. Available at:\nhttp://arxiv.org/abs/1704.03073.\n\n\nPrecup, D., Sutton, R. S., and Singh, S. (2000). Eligibility traces for\noff-policy policy evaluation. in Proceedings of the\nSeventeenth International Conference on Machine\nLearning.\n\n\nRonneberger, O., Fischer, P., and Brox, T. (2015). U-Net:\nConvolutional Networks for Biomedical Image\nSegmentation. Available at: http://arxiv.org/abs/1505.04597\n[Accessed November 29, 2020].\n\n\nRoy, R., Raiman, J., Kant, N., Elkin, I., Kirby, R., Siu, M., et al.\n(2022). PrefixRL: Optimization of\nParallel Prefix Circuits using Deep Reinforcement\nLearning. doi:10.1109/DAC18074.2021.9586094.\n\n\nRuder, S. (2016). An overview of gradient descent optimization\nalgorithms. Available at: http://arxiv.org/abs/1609.04747.\n\n\nRusu, A. A., Colmenarejo, S. G., Gulcehre, C., Desjardins, G.,\nKirkpatrick, J., Pascanu, R., et al. (2016). Policy\nDistillation. Available at: http://arxiv.org/abs/1511.06295\n[Accessed January 26, 2020].\n\n\nSalimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017).\nEvolution Strategies as a Scalable Alternative\nto Reinforcement Learning. Available at: http://arxiv.org/abs/1703.03864.\n\n\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized\nExperience Replay. Available at: http://arxiv.org/abs/1511.05952.\n\n\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L.,\nSchmitt, S., et al. (2019). Mastering Atari,\nGo, Chess and Shogi by\nPlanning with a Learned Model. Available at:\nhttp://arxiv.org/abs/1911.08265\n[Accessed November 24, 2019].\n\n\nSchulman, J., Chen, X., and Abbeel, P. (2017a). Equivalence\nBetween Policy Gradients and Soft Q-Learning.\nAvailable at: http://arxiv.org/abs/1704.06440\n[Accessed June 12, 2019].\n\n\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.\n(2015a). Trust Region Policy Optimization. in\nProceedings of the 31 st International Conference on\nMachine Learning, 1889–1897. Available at: http://proceedings.mlr.press/v37/schulman15.html.\n\n\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.\n(2015b). High-Dimensional Continuous Control Using Generalized\nAdvantage Estimation. Available at: http://arxiv.org/abs/1506.02438.\n\n\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.\n(2017b). Proximal Policy Optimization Algorithms. Available\nat: http://arxiv.org/abs/1707.06347.\n\n\nSekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., and\nPathak, D. (2020). Planning to Explore via\nSelf-Supervised World Models. Available at: http://arxiv.org/abs/2005.05960\n[Accessed January 29, 2024].\n\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den\nDriessche, G., et al. (2016a). Mastering the game of Go\nwith deep neural networks and tree search. Nature 529, 484–489.\ndoi:10.1038/nature16961.\n\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M.,\nGuez, A., et al. (2018). A general reinforcement learning algorithm that\nmasters chess, shogi, and Go through self-play.\nScience 362, 1140–1144. doi:10.1126/science.aar6404.\n\n\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and\nRiedmiller, M. (2014). Deterministic Policy Gradient\nAlgorithms. in Proc. ICML Proceedings of\nMachine Learning Research., eds. E. P. Xing and T. Jebara\n(PMLR), 387–395. Available at: http://proceedings.mlr.press/v32/silver14.html.\n\n\nSilver, D., van Hasselt, H., Hessel, M., Schaul, T., Guez, A., Harley,\nT., et al. (2016b). The Predictron: End-To-End\nLearning and Planning. Available at: http://arxiv.org/abs/1612.08810.\n\n\nSimonyan, K., and Zisserman, A. (2015). Very Deep Convolutional\nNetworks for Large-Scale Image Recognition.\nInternational Conference on Learning Representations (ICRL),\n1–14. doi:10.1016/j.infsof.2008.09.005.\n\n\nSrinivas, A., Jabri, A., Abbeel, P., Levine, S., and Finn, C. (2018).\nUniversal Planning Networks. Available at: http://arxiv.org/abs/1804.00645.\n\n\nSutton, R. S. (1990). Integrated Architectures for\nLearning, Planning, and Reacting\nBased on Approximating Dynamic Programming.\nMachine Learning Proceedings 1990, 216–224. doi:10.1016/B978-1-55860-141-3.50030-4.\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement\nLearning: An introduction. Cambridge, MA:\nMIT press.\n\n\nSutton, R. S., and Barto, A. G. (2017). Reinforcement\nLearning: An Introduction. 2nd ed.\nCambridge, MA: MIT Press Available at: http://incompleteideas.net/book/the-book-2nd.html.\n\n\nSutton, R. S., McAllester, D., Singh, S., and Mansour, Y. (1999). Policy\ngradient methods for reinforcement learning with function approximation.\nin Proceedings of the 12th International Conference on\nNeural Information Processing Systems (MIT Press),\n1057–1063. Available at: https://dl.acm.org/citation.cfm?id=3009806.\n\n\nSzita, I., and Lörincz, A. (2006). Learning Tetris Using\nthe Noisy Cross-Entropy Method. Neural Computation\n18, 2936–2941. doi:10.1162/neco.2006.18.12.2936.\n\n\nTang, J., and Abbeel, P. (2010). On a Connection between\nImportance Sampling and the Likelihood Ratio Policy\nGradient. in Adv. Neural inf.\nProcess. Syst. Available at: http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf.\n\n\nTeh, Y. W., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J.,\nHadsell, R., et al. (2017). Distral: Robust Multitask\nReinforcement Learning. Available at: http://arxiv.org/abs/1707.04175\n[Accessed January 26, 2020].\n\n\nTodorov, E. (2008). General duality between optimal control and\nestimation. in 2008 47th IEEE Conference on\nDecision and Control, 4286–4292. doi:10.1109/CDC.2008.4739438.\n\n\nToussaint, M. (2009). Robot Trajectory Optimization Using\nApproximate Inference. in Proceedings of the 26th\nAnnual International Conference on Machine\nLearning ICML ’09. (New York, NY, USA: ACM),\n1049–1056. doi:10.1145/1553374.1553508.\n\n\nUhlenbeck, G. E., and Ornstein, L. S. (1930). On the Theory\nof the Brownian Motion. Physical Review 36. doi:10.1103/PhysRev.36.823.\n\n\nvan Hasselt, H. (2010). Double Q-learning.\nin Proceedings of the 23rd International Conference on\nNeural Information Processing Systems - Volume\n2 (Curran Associates Inc.), 2613–2621. Available at: https://dl.acm.org/citation.cfm?id=2997187.\n\n\nvan Hasselt, H., Guez, A., and Silver, D. (2015). Deep\nReinforcement Learning with Double\nQ-learning. Available at: http://arxiv.org/abs/1509.06461.\n\n\nWang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z.,\nMunos, R., et al. (2017). Learning to reinforcement learn. Available at:\nhttp://arxiv.org/abs/1611.05763\n[Accessed February 5, 2021].\n\n\nWang, Z., Li, Z., Mandlekar, A., Xu, Z., Fan, J., Narang, Y., et al.\n(2024). One-Step Diffusion Policy: Fast Visuomotor\nPolicies via Diffusion Distillation. doi:10.48550/arXiv.2410.21257.\n\n\nWang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de\nFreitas, N. (2016). Dueling Network Architectures for\nDeep Reinforcement Learning. Available at: http://arxiv.org/abs/1511.06581\n[Accessed November 21, 2019].\n\n\nWatkins, C. J. (1989). Learning from delayed rewards.\n\n\nWatter, M., Springenberg, J. T., Boedecker, J., and Riedmiller, M.\n(2015). Embed to Control: A Locally Linear Latent\nDynamics Model for Control from Raw\nImages. Available at: https://arxiv.org/pdf/1506.07365.pdf.\n\n\nWeber, T., Racanière, S., Reichert, D. P., Buesing, L., Guez, A.,\nRezende, D. J., et al. (2017). Imagination-Augmented Agents\nfor Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1707.06203.\n\n\nWierstra, D., Foerster, A., Peters, J., and Schmidhuber, J. (2007).\n“Solving Deep Memory POMDPs with Recurrent\nPolicy Gradients,” in (Springer, Berlin, Heidelberg),\n697–706. doi:10.1007/978-3-540-74690-4_71.\n\n\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms\nfor connectionist reinforcement learning. Machine Learning 8,\n229–256.\n\n\nWilliams, R. J., and Peng, J. (1991). Function optimization using\nconnectionist reinforcement learning algorithms. Connection\nScience 3, 241–268.\n\n\nWu, P., Escontrela, A., Hafner, D., Goldberg, K., and Abbeel, P. (2022).\nDayDreamer: World Models for Physical\nRobot Learning. doi:10.48550/arXiv.2206.14176.\n\n\nYe, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y. (2021). Mastering\nAtari Games with Limited Data. doi:10.48550/arXiv.2111.00210.\n\n\nYu, C., Liu, J., and Nemati, S. (2020). Reinforcement\nLearning in Healthcare: A Survey.\ndoi:10.48550/arXiv.1908.08796.\n\n\nZiebart, B. D., Maas, A., Bagnell, J. A., and Dey, A. K. (2008). Maximum\nEntropy Inverse Reinforcement Learning. in, 6.",
    "crumbs": [
      "References"
    ]
  }
]