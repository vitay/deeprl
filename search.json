[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "The goal of this webbook is to keep track of the state-of-the-art in deep reinforcement learning. It starts with basics in reinforcement learning and deep learning to introduce the notations. It then covers different classes of deep RL methods, value-based or policy-based, model-free or model-based, etc. Later sections focus on more advanced topics.\nThis document is meant to stay work in progress forever, as new algorithms will be added as they are published. Feel free to comment, correct, suggest, pull request by writing to julien.vitay@gmail.com.\nSome figures are taken from the original publication (“Source:” in the caption). Their copyright stays to the respective authors, naturally. The rest is my own work and can be distributed, reproduced and modified under CC-BY-SA-NC 4.0.\n\n\n\n\n\n\nLicense\n\n\n\n\nExcept where otherwise noted, this work is licensed under a Creative Commons Attribution-Non Commercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "src/0-Introduction.html",
    "href": "src/0-Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "What is Reinforcement Learning?\nSupervised learning (SL) trains a discriminative model (classification or regression) by comparing the correct answer (ground truth) available in a training set to compute a prediction error. For neural networks, the prediction error (typically the difference between the ground truth and the predicted output) is used by the backpropagation algorithm to adapt the parameters of the model so that the prediction error is iteratively reduced. Typical examples are convolutional neural networks (CNN) predicting the label associated to an image, a recurrent neural network (RNN) predicting autoregressively the future of a time series, or a fully-connected network performing credit scoring. The major drawback of SL methods is that they typically require a lot of annotated data, which are very expensive to produce.\nUnsupervised learning (UL) only deals with raw data, trying to extract statistical properties without any additional information. One application of UL is dimensionality reduction, which searches how to project highly dimensional data (e.g. images) onto smaller spaces without losing too much information. Algorithms like PCA (Principal Components Analysis) or neural architectures like autoencoders are typically used. Another approach to UL is generative modelling, i.e. learning a model of the distribution of the data allowing to generate new samples. Generative AI (ChatGPT, Midjourney, etc) relies on learning the distribution of vast amounts of text or images in order to generate novel high-quality samples. Self-supervised learning relies on using supervised learning algorithms on raw data by using self-generated pretext tasks, such as masking part of the data in the input and learning to predict it, or guessing the next item in a sequence.\nReinforcement learning (RL) lies somehow in between: the model makes a prediction (an action), but there is no ground truth to compare with. The only feedback it gets from the environment is a unidimensional reward signal that informs how good (or bad) the action was. In the extreme case, this partial feedback can be binary, like winning or losing a game after a sequence of dozens of actions.\nRL setups follow the agent-environment interface (Sutton and Barto, 1998). The agent (for example a robot) is in a given state s_t at time t. This state represents the perception of the robot (camera, internal sensors) but also its position inside the environment, and generally anything relevant information for the task. The agent selects an action a_t according to its policy (or strategy). This action modifies the environment (or world), what brings the agent in a new state s_{t+1}. Furthermore, a reward r_{t+1} is delivered to the agent to valuate the executed action. This interaction loop continues over time, leading to episodes or trajectories of various lengths until a terminal state is reached. The goal of the agent is to find a policy that maximizes the sum of the rewards received over time (more on that later).\nThe key concept in RL is trial and error learning: trying out actions until their outcome is good. The agent selects an action from its repertoire and observes the outcome. If the outcome is positive (reward), the action is reinforced (it becomes more likely to occur again). If the outcome is negative (punishment), the action will be avoided in the future. After enough interactions, the agent has learned which action to perform in a given situation.\nThe agent has to explore its environment via trial-and-error in order to gain knowledge. The agent’s behavior then is roughly divided into two phases:\nThe biggest issue with this approach is that exploring large action spaces might necessitate a lot of trials (a problem referred to as sample complexity). The modern techniques we will see in this book try to reduce the sample complexity.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "src/0-Introduction.html#what-is-reinforcement-learning",
    "href": "src/0-Introduction.html#what-is-reinforcement-learning",
    "title": "Introduction",
    "section": "",
    "text": "Figure 1: Three types of machine learning: Supervised learning uses a ground truth to compute a prediction error that drives learning. Unsupervised learning extracts statistical properties from raw data. Reinforcement learning uses a reward signal from the environment to assess the correctness of an action.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Agent-environment interface. Images generated by ChatGPT.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Trial and error learning. Top: classical trial and error learning, where the agent tries different actions until the outcome is satisfying. Bottom: the outcome of the trial influences via learning whether the corresponding action will be reinforced or avoided in the future.\n\n\n\n\n\nThe exploration phase, where it gathers knowledge about its environment.\nThe exploitation phase, where this knowledge is used to collect as many rewards as possible.\n\n\n\n\n\n\n\n\nFigure 4: Sample complexity: learning to control a modern plane through trial and error might be too long and dangerous. Image generated by ChatGPT.\n\n\n\n\n\n\n\n\n\nSutton and Barto. Reinforcement learning: An Introduction\n\n\n\nThe book “Reinforcement learning: An introduction” (1st and 2nd editions) (Sutton and Barto, 1998, 2017) contains everything you need to know about the basics of RL. The second edition can be found here:\nhttp://incompleteideas.net/book/the-book-2nd.html",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "src/0-Introduction.html#applications-of-reinforcement-learning",
    "href": "src/0-Introduction.html#applications-of-reinforcement-learning",
    "title": "Introduction",
    "section": "Applications of Reinforcement Learning",
    "text": "Applications of Reinforcement Learning\nRL can be used in many control problems, ranging from simple problems to complex robotics, video games or even plasma control.\n\nOptimal control\nThe most basic problems on which RL can be applied are simple control environments with a few degrees of freedom. The gymnasium library (formerly gym) maintained by the Farama foundation provides an API for RL environments as well as the reference implementation of the most popular ones, including control problems, Mujoco robotic simulations and Atari games:\nhttps://gymnasium.farama.org\nSome examples:\n\n\n\n\n\n\nPendulum\n\n\n\nGoal: maintaining the pendulum vertical by changing the applied torque.\n\n\nBefore\n\n\n\nSource: https://keras.io/examples/rl/ddpg_pendulum/\n\n\n\nAfter\n\n\n\n\n\n\n\n\n\n\n\nCartpole\n\n\n\nGoal: maintaining the pole vertical by moving the cart.\n\n\nBefore\n\n\nAfter\n\n\n\nSource: https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288\nSee the Cartpole learning in real life (Deisenroth and Rasmussen, 2011):\n\n\n\n\n\nAtari games\nThese toy control problems were used for decades to test various RL algorithms, although serious applications existed. The big breakthrough in (deep) reinforcement learning occurred in 2013, as a small startup in London, DeepMind Technologies Limited (now Google Deepmind), led by Demis Hassabis, successfully coupled reinforcement learning with deep neural networks for the first time. Their proposed algorithm, the deep Q-network (DQN, Mnih et al., 2013, 2015), used a convolutional neural network (CNN) to learn to play many Atari games by trial and error, without any prior information about the game and only using raw images as inputs.\n\nSimilar pixel-based games were quickly mastered, such as the TORCS simulator with the A3C algporithm (Mnih et al., 2016):\n\nSimulated robotics were addressed by similar techniques:\n\n\n\nAlphaGo\nThe next major breakthrough for Deep RL happened in 2016, as AlphaGo, also created by Deepmind, was able to beat Lee Sedol, world champion of the game of Go. AlphaGo coupled the power of deep neural networks and RL with Monte Carlo Tree Search (MCTS), a popular tree-based search algorithm, to learn through self-play the optimal strategy for the game of Go. This performance was extremely commented, as such a dominance of an AI system was not expected before decades.\n\n\n\nOpenAI Five and AlphaStar\nDespite its complexity, the game of Go is actually quite simple for a computer: there is full observability (the entire state of the game is known to the players), the number of possible actions is quite limited, the consequence of an action is fully predictable, and planning is only necessary over a few dozens of plays. On the contrary, modern video games like DotA 2 or Starcraft have partial observability (the player only sees its immediate surroundings), hundreds of hierarchical actions can be made at each time step, the dynamics of the game are largely unknown, and a game can last hours. In some sense, it is much more difficult to become good at DotA than it is at chess or Go.\nDeepmind and OpenAI recognized this difference and started applying deep RL methods to Starcraft II and DotA 2, respectively, between 2017 and 2020. They managed to beat teams of professional players under limited conditions, but eventually stopped because of the huge training costs (and they had to work on LLMs…).\n\n\n\n\nProcess control\nIn 2016, Deepmind applied its RL algorithms to the control of the cooling systems in Google’s datacenters. The system learned passively from observations of the current cooling sytem what the optimal policy was. When they gave control to the RL agent, it instantly led to a 40% reduction of energy consumption, which, for Google’s data centers, represents a huge amount of money. See Luo et al. (2022) for a recent review of RL controlling cooling systems.\n\n\n\n\n\n\nFigure 5: Source: https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/\n\n\n\nThis showed that the progress in deep RL was not limited to toy problems or video games, but could be relevant for complex process control problems. An amazing illustration of that idea is the magnetic control of tokamak plasmas by Degrave et al. (2022).\n\n\n\n\n\n\nFigure 6: Magnetic control of tokamak plasmas through deep reinforcement learning. Source: Degrave et al. (2022)\n\n\n\nAnother recent illustration is how RL can be used in the design of silicon chips by Nvidia (Roy et al., 2022).\n\n\n\n\n\n\nFigure 7: Optimization of Parallel Prefix Circuits using Deep Reinforcement Learning. Roy et al. (2022)\n\n\n\n\n\nRobotics\nA natural application of RL is in the domain of (autonomous) robotics, where agents / robots can autonomously solve tasks by themselves. A very influential lab working on robotics and Deep RL is RAIL lab of Sergey Levine at Berkeley.\n\nOpenAI worked on dexterity tasks.\n\nAutonomous driving is another promising field of application for deep RL. The english startup Wayve made for example a very exciting demo of their system (see Kendall et al. (2018) and https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning).\n\nA recent contribution by the UZH Robotics and Perception Group leveraged deep RL to control drones flying at high speeds (Kaufmann et al., 2023).\n\n\n\nChatGPT\nRL can also be used to fine-tune generative AI models, such as diffusion models or large language models such as ChatGPT. Generative models typically learn from raw data using pretext tasks: predicting missing parts of the data, or predicting the next item in a sequence. While this allows to learn a world model, it does not lead to meaningful behaviors for a task at hand. Using RL allows to fine-tune the model so that its outputs align with the task. Such a model alignment is central to the acceptance of LLMs.\n\n\n\n\n\n\nFigure 8: Source: https://openai.com/blog/chatgpt/\n\n\n\n\n\nand many more…\nRL finds applications in virtually any field involving sequential decision making, including finance technology (Malibari et al., 2023), inventory management (Madeka et al., 2022), missile guidance (Li et al., 2022) or healthcare (Yu et al., 2020).\n\n\n\n\nDegrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese, F., et al. (2022). Magnetic control of tokamak plasmas through deep reinforcement learning. Nature 602, 414–419. doi:10.1038/s41586-021-04301-9.\n\n\nKaufmann, E., Bauersfeld, L., Loquercio, A., Müller, M., Koltun, V., and Scaramuzza, D. (2023). Champion-level drone racing using deep reinforcement learning. Nature 620, 982–987. doi:10.1038/s41586-023-06419-4.\n\n\nKendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen, J.-M., et al. (2018). Learning to Drive in a Day. Available at: http://arxiv.org/abs/1807.00412 [Accessed December 19, 2018].\n\n\nLi, W., Zhu, Y., and Zhao, D. (2022). Missile guidance with assisted deep reinforcement learning for head-on interception of maneuvering target. Complex Intell. Syst. 8, 1205–1216. doi:10.1007/s40747-021-00577-6.\n\n\nLuo, J., Paduraru, C., Voicu, O., Chervonyi, Y., Munns, S., Li, J., et al. (2022). Controlling Commercial Cooling Systems Using Reinforcement Learning. doi:10.48550/arXiv.2211.07357.\n\n\nMadeka, D., Torkkola, K., Eisenach, C., Luo, A., Foster, D. P., and Kakade, S. M. (2022). Deep Inventory Management. doi:10.48550/arXiv.2210.03137.\n\n\nMalibari, N., Katib, I., and Mehmood, R. (2023). Systematic Review on Reinforcement Learning in the Field of Fintech. doi:10.48550/arXiv.2305.07466.\n\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., et al. (2016). Asynchronous Methods for Deep Reinforcement Learning. in Proc. ICML Available at: http://arxiv.org/abs/1602.01783.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., et al. (2013). Playing Atari with Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1312.5602.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). Human-level control through deep reinforcement learning. Nature 518, 529–533. doi:10.1038/nature14236.\n\n\nRoy, R., Raiman, J., Kant, N., Elkin, I., Kirby, R., Siu, M., et al. (2022). PrefixRL: Optimization of Parallel Prefix Circuits using Deep Reinforcement Learning. doi:10.1109/DAC18074.2021.9586094.\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press.\n\n\nSutton, R. S., and Barto, A. G. (2017). Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press Available at: http://incompleteideas.net/book/the-book-2nd.html.\n\n\nYu, C., Liu, J., and Nemati, S. (2020). Reinforcement Learning in Healthcare: A Survey. doi:10.48550/arXiv.1908.08796.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "src/1.1-Bandits.html",
    "href": "src/1.1-Bandits.html",
    "title": "Sampling and Bandits",
    "section": "",
    "text": "n-armed bandits\nThe n-armed bandit (or multi-armed bandit) is the simplest form of learning by trial and error. Learning and action selection take place in the same single state, with n available actions having different reward distributions. The goal is to find out through trial and error which action provides the most reward on average.\nWe have the choice between N different actions (a_1, ..., a_N). Each action a taken at time t provides a reward r_t drawn from the action-specific probability distribution r(a).\nThe mathematical expectation of that distribution is the expected reward, called the true value of the action Q^*(a).\nQ^*(a) = \\mathbb{E} [r(a)]\nThe reward distribution also has a variance: we usually ignore it in RL, as all we care about is the optimal action a^* (but see distributional RL later).\na^* = \\text{argmax}_a \\, Q^*(a)\nIf we take the optimal action an infinity of times, we maximize the reward intake on average. The question is how to find out the optimal action through trial and error, i.e. without knowing the exact reward distribution r(a). We only have access to samples of r(a) by taking the action a at time t (a trial, play or step).\nr_t \\sim r(a)\nThe received rewards r_t vary around the true value over time. We need to build estimates Q_t(a) of the value of each action based on the samples. These estimates will be very wrong at the beginning, but should get better over time.",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling and Bandits</span>"
    ]
  },
  {
    "objectID": "src/1.1-Bandits.html#n-armed-bandits",
    "href": "src/1.1-Bandits.html#n-armed-bandits",
    "title": "Sampling and Bandits",
    "section": "",
    "text": "Figure 2.1: Example of a bandit with 10 actions. The mean and the variance of each reward distribution are depicted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Sampled reward over time for the same action.",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling and Bandits</span>"
    ]
  },
  {
    "objectID": "src/1.1-Bandits.html#random-sampling",
    "href": "src/1.1-Bandits.html#random-sampling",
    "title": "Sampling and Bandits",
    "section": "Random sampling",
    "text": "Random sampling\n\nExpectation\nAn important metric for a random variable is its mathematical expectation or expected value. For discrete distributions, it is the “mean” realization / outcome weighted by the corresponding probabilities:\n\n    \\mathbb{E}[X] = \\sum_{i=1}^n P(X = x_i) \\, x_i\n\nFor continuous distributions, one needs to integrate the probability density function (pdf) instead of the probabilities:\n\n    \\mathbb{E}[X] = \\int_{x \\in \\mathcal{D}_X} f(x) \\, x \\, dx\n\nOne can also compute the expectation of a function of a random variable:\n\n    \\mathbb{E}[g(X)] = \\int_{x \\in \\mathcal{D}_X} f(x) \\, g(x) \\, dx\n\n\n\nRandom sampling\nIn ML and RL, we deal with random variables whose exact probability distribution is unknown, but we are interested in their expectation or variance anyway.\n\n\n\n\n\n\nFigure 2.3: Samples from the normal distribution are centered around its expected value.\n\n\n\nRandom sampling or Monte Carlo sampling (MC) consists of taking N samples x_i out of the distribution X (discrete or continuous) and computing the sample average:\n\n    \\mathbb{E}[X] = \\mathbb{E}_{x \\sim X} [x] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N x_i\n\nMore samples will be obtained where f(x) is high (x is probable), so the average of the sampled data will be close to the expected value of the distribution.\n\n\n\n\n\n\nLaw of big numbers\n\n\n\nAs the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean.\n\n\nMC estimates are only correct when:\n\nthe samples are i.i.d (independent and identically distributed):\n\nindependent: the samples must be unrelated with each other.\nidentically distributed: the samples must come from the same distribution X.\n\nthe number of samples is large enough.\n\nOne can estimate any function of the random variable with random sampling:\n\n    \\mathbb{E}[f(X)] = \\mathbb{E}_{x \\sim X} [f(x)] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N f(x_i)\n\n\n\nCentral limit theorem\nSuppose we have an unknown distribution X with expected value \\mu = \\mathbb{E}[X] and variance \\sigma^2. We can take randomly N samples from X to compute the sample average:\n\n    S_N = \\frac{1}{N} \\, \\sum_{i=1}^N x_i\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nThe distribution of sample averages is normally distributed with mean \\mu and variance \\frac{\\sigma^2}{N}.\nS_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{N}})\n\n\nIf we perform the sampling multiple times, even with few samples, the average of the sampling averages will be very close to the expected value. The more samples we get, the smaller the variance of the estimates. Although the distribution X can be anything, the sampling averages are normally distributed.\n\n\n\n\n\n\nFigure 2.4: Illustration of the central limit theorem. Source:: https://en.wikipedia.org/wiki/Central_limit_theorem\n\n\n\nCLT shows that the sampling average is an unbiased estimator of the expected value of a distribution:\n\\mathbb{E}(S_N) = \\mathbb{E}(X)\nAn estimator is a random variable used to measure parameters of a distribution (e.g. its expectation). The problem is that estimators can generally be biased.\nTake the example of a thermometer M measuring the temperature T. T is a random variable (normally distributed with \\mu=20 and \\sigma=10) and the measurements M relate to the temperature with the relation:\n\n    M = 0.95 \\, T + 0.65\n\n\n\n\n\n\n\nFigure 2.5: Thermometer and temperature.\n\n\n\nThe thermometer is not perfect, but do random measurements allow us to estimate the expected value of the temperature? We could repeatedly take 100 random samples of the thermometer and see how the distribution of sample averages look like:\n\n\n\n\n\n\nFigure 2.6: Distribution of the sampling averages.\n\n\n\nBut, as the expectation is linear, we actually have:\n\n    \\mathbb{E}[M] = \\mathbb{E}[0.95 \\, T + 0.65] = 0.95 \\, \\mathbb{E}[T] + 0.65 = 19.65 \\neq \\mathbb{E}[T]\n\nThe thermometer is a biased estimator of the temperature.\nLet’s note \\theta a parameter of a probability distribution X that we want to estimate (it does not have to be its mean). An estimator \\hat{\\theta} is a random variable mapping the sample space of X to a set of sample estimates. The bias of an estimator is the mean error made by the estimator:\n\n    \\mathcal{B}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta} - \\theta] = \\mathbb{E}[\\hat{\\theta}] - \\theta\n\nThe variance of an estimator is the deviation of the samples around the expected value:\n\n    \\text{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}] )^2]\n\nIdeally, we would like estimators with a low bias, as the estimations would be correct on average (= equal to the true parameter) and a low variance, as we would not need many estimates to get a correct estimate (CLT: \\frac{\\sigma}{\\sqrt{N}})\n\n\n\n\n\n\nFigure 2.7: Bias-variance trade-off. Source: https://www.machinelearningplus.com/machine-learning/bias-variance-tradeoff/\n\n\n\nUnfortunately, the perfect estimator does not exist. Estimators will have a bias and a variance. For estimators with a high bias, the estimated values will be wrong, and the policy not optimal. For estimators with a high variance, we will need a lot of samples (trial and error) to have correct estimates. One usually talks of a bias/variance trade-off: if you have a small bias, you will have a high variance, or vice versa. There is a sweet spot balancing the two. In machine learning, bias corresponds to underfitting, variance to overfitting.",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling and Bandits</span>"
    ]
  },
  {
    "objectID": "src/1.1-Bandits.html#sampling-based-evaluation",
    "href": "src/1.1-Bandits.html#sampling-based-evaluation",
    "title": "Sampling and Bandits",
    "section": "Sampling-based evaluation",
    "text": "Sampling-based evaluation\n\n\n\n\n\n\nFigure 2.8: Samples and expected reward over time.\n\n\n\nThe expectation of the reward distribution can be approximated by the mean of its samples:\n\n    \\mathbb{E} [r(a)] \\approx  \\frac{1}{N} \\sum_{t=1}^N r_t |_{a_t = a}\n\nSuppose that the action a had been selected t times, producing rewards\n\n    (r_1, r_2, ..., r_t)\n\nThe estimated value of action a at play t is then:\n\n    Q_t (a) = \\frac{r_1 + r_2 + ... + r_t }{t}\n\nOver time, the estimated action-value converges to the true action-value:\n\n   \\lim_{t \\to \\infty} Q_t (a) = Q^* (a)\n\nThe drawback of maintaining the mean of the received rewards is that it consumes a lot of memory:\n\n    Q_t (a) = \\frac{r_1 + r_2 + ... + r_t }{t} = \\frac{1}{t} \\, \\sum_{i=1}^{t} r_i\n\nIt is possible to update an estimate of the mean in an online or incremental manner:\n\n\\begin{aligned}\n    Q_{t+1}(a) &= \\frac{1}{t+1} \\, \\sum_{i=1}^{t+1} r_i = \\frac{1}{t+1} \\, (r_{t+1} + \\sum_{i=1}^{t} r_i )\\\\\n            &= \\frac{1}{t+1} \\, (r_{t+1} + t \\,  Q_{t}(a) ) \\\\\n            &= \\frac{1}{t+1} \\, (r_{t+1} + (t + 1) \\,  Q_{t}(a) - Q_t(a))\n\\end{aligned}\n\nThe estimate at time t+1 depends on the previous estimate at time t and the last reward r_{t+1}:\n\n    Q_{t+1}(a) = Q_t(a) + \\frac{1}{t+1} \\, (r_{t+1} - Q_t(a))\n\nThe problem with the exact mean is that it is only exact when the reward distribution is stationary, i.e. when the probability distribution does not change over time. If the reward distribution is non-stationary, the \\frac{1}{t+1} term will become very small and prevent rapid updates of the mean.\n\n\n\n\n\n\nFigure 2.9\n\n\n\nThe solution is to replace \\frac{1}{t+1} with a fixed parameter called the learning rate (or step size) \\alpha:\n\n\\begin{aligned}\n    Q_{t+1}(a) & = Q_t(a) + \\alpha \\, (r_{t+1} - Q_t(a)) \\\\\n                & \\\\\n                & = (1 - \\alpha) \\, Q_t(a) + \\alpha \\, r_{t+1}\n\\end{aligned}\n\n\n\n\n\n\n\nFigure 2.10\n\n\n\nThe computed value is called an exponentially moving average (or sliding average), as if one used only a small window of the past history.\n\n    Q_{t+1}(a) = Q_t(a) + \\alpha \\, (r_{t+1} - Q_t(a))\n\nor:\n\n    \\Delta Q(a) = \\alpha \\, (r_{t+1} - Q_t(a))\n\nThe moving average adapts very fast to changes in the reward distribution and should be used in non-stationary problems. It is however not exact and sensible to noise. Choosing the right value for \\alpha can be difficult.\nThe form of this update rule is very important to remember:\n\n    \\text{new estimate} = \\text{current estimate} + \\alpha \\, (\\text{target} - \\text{current estimate})\n\nEstimates following this update rule track the mean of their sampled target values. \\text{target} - \\text{current estimate} is the prediction error between the target and the estimate.",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling and Bandits</span>"
    ]
  },
  {
    "objectID": "src/1.1-Bandits.html#action-selection",
    "href": "src/1.1-Bandits.html#action-selection",
    "title": "Sampling and Bandits",
    "section": "Action selection",
    "text": "Action selection\nLet’s suppose we have formed reasonable estimates of the Q-values Q_t(a) at time t. Which action should we do next? If we select the next action a_{t+1} randomly (random agent), we do not maximize the rewards we receive, but we can continue learning the Q-values. Choosing the action to perform next is called action selection and several schemes are possible.\n\nGreedy action selection\nThe greedy action is the action whose expected value is maximal at time t based on our current estimates:\n\n    a^*_t = \\text{argmax}_{a} Q_t(a)\n\nIf our estimates Q_t are correct (i.e. close from Q^*), the greedy action is the optimal action and we maximize the rewards on average. If our estimates are wrong, the agent will perform sub-optimally.\n\n\n\n\n\n\nFigure 2.11: Greedy action selection. The action with the highest expected value is selected all the time.\n\n\n\nThis defines the greedy policy, where the probability of taking the greedy action is 1 and the probability of selecting another action is 0:\n\n    \\pi(a) = \\begin{cases}\n                    1 \\; \\text{if} \\; a = a_t^* \\\\\n                    0 \\; \\text{otherwise.} \\\\\n            \\end{cases}\n\nThe greedy policy is deterministic: the action taken is always the same for a fixed Q_t.\nHowever, the greedy action selection scheme only works when the estimates are good enough. Imagine that estimates are initially bad (e.g. 0), and an action is sampled randomly. If the received reward is positive, the new Q-value of that action becomes positive, so it becomes the greedy action. At the next step, greedy action selection will always select that action, although the second one could have been better but it was never explored.\n\nThis exploration-exploitation dilemma is the hardest problem in RL:\n\nExploitation is using the current estimates to select an action: they might be wrong!\nExploration is selecting non-greedy actions in order to improve their estimates: they might not be optimal!\n\nOne has to balance exploration and exploitation over the course of learning:\n\nMore exploration at the beginning of learning, as the estimates are initially wrong.\nMore exploitation at the end of learning, as the estimates get better.\n\n\n\n\n\n\n\nFigure 2.12: Source: UC Berkeley AI course slides, lecture 11\n\n\n\n\n\n\\epsilon-greedy action selection\n\\epsilon-greedy action selection ensures a trade-off between exploitation and exploration. The greedy action is selected with probability 1 - \\epsilon (with 0 &lt; \\epsilon &lt;1), the others with probability \\epsilon:\n\n    \\pi(a) = \\begin{cases} 1 - \\epsilon \\; \\text{if} \\; a = a_t^* \\\\ \\frac{\\epsilon}{|\\mathcal{A}| - 1} \\; \\text{otherwise.} \\end{cases}\n\n\n\n\n\n\n\nFigure 2.13: \\epsilon-greedy action selection. The greedy action is selected most of the time, but the other actions might be selected from time to time.\n\n\n\nThe parameter \\epsilon controls the level of exploration: the higher \\epsilon, the more exploration. One can set \\epsilon high at the beginning of learning and progressively reduce it to exploit more. However, it chooses equally among all actions: the worst action is as likely to be selected as the next-to-best action.\n\n\n\nSoftmax action selection\nSoftmax action selection defines the probability of choosing an action using all estimated value. It represents the policy using a Gibbs (or Boltzmann) distribution:\n\n    \\pi(a) = \\dfrac{\\exp \\dfrac{Q_t(a)}{\\tau}}{ \\displaystyle\\sum_{a'} \\exp \\dfrac{Q_t(a')}{\\tau}}\n\nwhere \\tau is a positive parameter called the temperature.\n\n\n\n\n\n\nFigure 2.14: Softmax action selection.\n\n\n\nJust as \\epsilon, the temperature \\tau controls the level of exploration:\n\nHigh temperature causes the actions to be nearly equiprobable (random agent).\nLow temperature causes the greediest actions only to be selected (greedy agent).\n\n\n\n\n\n\n\nFigure 2.15: Influence of the temperature parameter.\n\n\n\n\n\n\n\n\n\n\nExploration schedule\n\n\n\nA useful technique to cope with the exploration-exploitation dilemma is to slowly decrease the value of \\epsilon or \\tau with the number of plays. This allows for more exploration at the beginning of learning and more exploitation towards the end. It is however hard to find the right decay rate for the exploration parameters.\n\n\n\n\n\nOptimistic initial values\nThe problem with online evaluation is that it depends a lot on the initial estimates Q_0. If the initial estimates are already quite good (e.g. using expert knowledge), the Q-values will converge very fast. If the initial estimates are very wrong, we will need a lot of updates to correctly estimate the true values. This problem is called bootstrapping: the better your initial estimates, the better (and faster) the results.\n\n\\begin{aligned}\n    &Q_{t+1}(a) = (1 - \\alpha) \\, Q_t(a) + \\alpha \\, r_{t+1}  \\\\\n    &\\\\\n    & \\rightarrow Q_1(a) = (1 - \\alpha) \\, Q_0(a) + \\alpha \\, r_1 \\\\\n    & \\rightarrow Q_2(a) = (1 - \\alpha) \\, Q_1(a) + \\alpha \\, r_2 = (1- \\alpha)^2 \\, Q_0(a) + (1-\\alpha)\\alpha \\, r_1 + \\alpha r_2 \\\\\n\\end{aligned}\n\nThe influence of Q_0 on Q_t fades quickly with (1-\\alpha)^t, but that can be lost time or lead to a suboptimal policy. However, we can use this at our advantage with optimistic initialization. By choosing very high initial values for the estimates (they can only decrease), one can ensure that all possible actions will be selected during learning by the greedy method, solving the exploration problem. This leads however to an overestimation of the value of other actions.\n\n\n\nReinforcement comparison\nActions followed by large rewards should be made more likely to reoccur, whereas actions followed by small rewards should be made less likely to reoccur. But what is a large/small reward? Is a reward of 5 large or small? Reinforcement comparison methods only maintain a preference p_t(a) for each action, which is not exactly its Q-value. The preference for an action is updated after each play, according to the update rule:\n\n    p_{t+1}(a_t) =    p_{t}(a_t) + \\beta \\, (r_t - \\tilde{r}_t)\n\nwhere \\tilde{r}_t is the moving average of the recently received rewards (regardless the action):\n\n    \\tilde{r}_{t+1} =  \\tilde{r}_t + \\alpha \\, (r_t - \\tilde{r}_t)\n\nIf an action brings more reward than usual (good surprise), we increase the preference for that action. If an action brings less reward than usual (bad surprise), we decrease the preference for that action. \\beta &gt; 0 and 0 &lt; \\alpha &lt; 1 are two constant parameters.\nPreferences are updated by replacing the action-dependent Q-values by a baseline \\tilde{r}_t:\n\n    p_{t+1}(a_t) =    p_{t}(a_t) + \\beta \\, (r_t - \\tilde{r}_t)\n\nThe preferences can be used to select the action using the softmax method just as the Q-values (without temperature):\n\n    \\pi_t (a) = \\dfrac{\\exp p_t(a)}{ \\displaystyle\\sum_{a'} \\exp p_t(a')}\n\n\nReinforcement comparison can be very effective, as it does not rely only on the rewards received, but also on their comparison with a baseline, the average reward. This idea is at the core of actor-critic architectures which we will see later. The initial average reward \\tilde{r}_{0} can be set optimistically to encourage exploration.\n\n\nGradient bandit algorithm\nInstead of only increasing the preference for the executed action if it brings more reward than usual, we could also decrease the preference for the other actions. The preferences are used to select an action a_t via softmax:\n\n    \\pi_t (a) = \\dfrac{\\exp p_t(a)}{ \\displaystyle\\sum_{a'} \\exp p_t(a')}\n\nUpdate rule for the action taken a_t:\n\n    p_{t+1}(a_t) =    p_{t}(a_t) + \\beta \\, (r_t - \\tilde{r}_t) \\, (1 - \\pi_t(a_t))\n\nUpdate rule for the other actions a \\neq a_t:\n\n    p_{t+1}(a) =    p_{t}(a) - \\beta \\, (r_t - \\tilde{r}_t) \\, \\pi_t(a)\n\nUpdate of the reward baseline:\n\n    \\tilde{r}_{t+1} =  \\tilde{r}_t + \\alpha \\, (r_t - \\tilde{r}_t)\n\nThe preference can increase become quite high, making the policy greedy towards the end. No need for a temperature parameter!\n\n\n\nUpper-Confidence-Bound action selection\nIn the previous methods, exploration is controlled by an external parameter (\\epsilon or \\tau) which is global to each action an must be scheduled. A much better approach would be to decide whether to explore an action based on the uncertainty about its Q-value: If we are certain about the value of an action, there is no need to explore it further, we only have to exploit it if it is good.\nThe central limit theorem tells us that the variance of a sampling estimator decreases with the number of samples:\n\nThe distribution of sample averages is normally distributed with mean \\mu and variance \\frac{\\sigma^2}{N}.\n\nS_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{N}})\nThe more you explore an action a, the smaller the variance of Q_t(a), the more certain you are about the estimation, the less you need to explore it.\nUpper-Confidence-Bound (UCB) action selection is a greedy action selection method that uses an exploration bonus:\n\n    a^*_t = \\text{argmax}_{a} \\left[ Q_t(a) + c \\, \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]\n\nQ_t(a) is the current estimated value of a and N_t(a) is the number of times the action a has already been selected.\nIt realizes a balance between trusting the estimates Q_t(a) and exploring uncertain actions which have not been explored much yet. The term \\sqrt{\\frac{\\ln t}{N_t(a)}} is an estimate of the variance of Q_t(a). The sum of both terms is an upper-bound of the true value \\mu + \\sigma. When an action has not been explored much yet, the uncertainty term will dominate and the action be explored, although its estimated value might be low. When an action has been sufficiently explored, the uncertainty term goes to 0 and we greedily follow Q_t(a).\nThe exploration-exploitation trade-off is automatically adjusted by counting visits to an action.\n\n    a^*_t = \\text{argmax}_{a} \\left[ Q_t(a) + c \\, \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sampling and Bandits</span>"
    ]
  },
  {
    "objectID": "src/1.2-MDP.html",
    "href": "src/1.2-MDP.html",
    "title": "Markov Decision Process",
    "section": "",
    "text": "Markov Decision Process",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Markov Decision Process</span>"
    ]
  },
  {
    "objectID": "src/1.2-MDP.html#markov-decision-process",
    "href": "src/1.2-MDP.html#markov-decision-process",
    "title": "Markov Decision Process",
    "section": "",
    "text": "Definition\nReinforcement Learning methods apply to problems where an agent interacts with an environment in discrete time steps (Figure 3.1). At time t, the agent is in state s_t and decides to perform an action a_t. At the next time step, it arrives in the state s_{t+1} and obtains the reward r_{t+1}. In the genral case, transitions can be stochastic (there is a probability of arriving in a given state after an action), as well as the rewards (as in the bandits previously seen). The goal of the agent is to maximize the reward obtained on the long term.\n\n\n\n\n\n\nFigure 3.1: Interaction between an agent and its environment. Source: Sutton and Barto (1998).\n\n\n\nThese problems are formalized as Markov Decision Processes (MDP) and defined by six quantities &lt;\\mathcal{S}, \\mathcal{A}, p_0, \\mathcal{P}, \\mathcal{R}, \\gamma&gt;. For a finite MDP, we have:\n\nThe state space \\mathcal{S} = \\{ s_i\\}_{i=1}^N, where each state respects the Markov property.\nThe action space \\mathcal{A} = \\{ a_i\\}_{i=1}^M.\nAn initial state distribution p_0(s_0) (from which states the agent is most likely to start).\nThe state transition probability function, defining the probability of arriving in the state s' at time t+1 after being in the state s and performing the action a at time t:\n\n\n\\begin{aligned}\n    \\mathcal{P}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow & P(\\mathcal{S}) \\\\\n    p(s' | s, a) & =  P (s_{t+1} = s' | s_t = s, a_t = a) \\\\\n\\end{aligned}\n\n\nThe expected reward function defining the (stochastic) reward obtained after performing a in state s and arriving in s':\n\n\n\\begin{aligned}\n    \\mathcal{R}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow & \\Re \\\\\n    r(s, a, s') &=  \\mathbb{E} (r_{t+1} | s_t = s, a_t = a, s_{t+1} = s') \\\\\n\\end{aligned}\n\n\nThe discount factor \\gamma \\in [0, 1].\n\nIn deep RL, the state and action spaces can be infinite, but let’s focus on finite MDPs for now.\nThe behavior of the agent over time is a trajectory (also called episode, history or roll-out) \\tau = (s_0, a_0, s_1, a_, \\ldots, s_T, a_T) defined by the dynamics of the MDP. Each transition occurs with a probability p(s'|s, a) and provides a certain amount of reward defined by r(s, a, s'). In episodic tasks, the horizon T is finite, while in continuing tasks T is infinite.\n\n\nMarkov property\nThe state of the agent represents all the information needed to take decisions and solve the task. For a robot navigating in an environment, this may include all its sensors, its positions as tracked by a GPS, but also the relative position of all objects / persons it may interact with. For a board game, the description of the board is usually enough.\nImportantly, the Markov property states that:\n\nThe future is independent of the past given the present.\n\nIn mathematical terms for a transition (s_t, a_t, s_{t+1}):\n\n    p(s_{t+1}|s_t, a_t) = p(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \\dots s_0, a_0)\n\ni.e. you do not need the full history of the agent to predict where it will arrive after an action. In simple problems, this is just a question of providing enough information to the description of a state: if a transition depends on what happened in the past, just put that information in the state description.\nA state representation with the Markov property should therefore not only contain all the important information available at time t, but also information from the past that is necessary to take a decision.\nIf the Markov property is not met, RL methods may not converge (or poorly). In many problems, one does not have access to the true states of the agent, but one can only indirectly observe them. For example, in a video game, the true state is defined by a couple of variables: coordinates (x, y) of the two players, position of the ball, speed, etc. However, in Atari games all you have access to are the raw pixels: sometimes the ball may be hidden behind a wall or a tree, but it still exists in the state space. Speed information is also not observable in a single frame.\nIn a Partially Observable Markov Decision Process (POMDP), observations o_t come from a space \\mathcal{O} and are linked to underlying states using the density function p(o_t| s_t). Observations are usually not Markov, so the full history of observations h_t = (o_0, a_0, \\dots o_t, a_t) is needed to solve the problem. We will see later how recurrent neural networks can help with POMDPs.\n\n\nRewards and returns\nAs with n-armed bandits, we only care about the expected reward received during a transition s \\rightarrow s' (on average), but the actual reward received r_{t+1} may vary around the expected value with some unknown variance. In hard RL, we only care about the expected reward and ignore its variance, as we suppose that we can take actions an infinity of times. However, distributional RL investigates the role of this variance (see Section Distributional learning).\nr(s, a, s') =  \\mathbb{E} (r_{t+1} | s_t = s, a_t = a, s_{t+1} = s')\n\n\n\n\n\n\nFigure 3.2: Reward distributions for several actions in a single state.\n\n\n\nAn important distinction in practice is between sparse vs. dense rewards. Sparse rewards take non-zero values only during certain transitions: game won/lost, goal achieved, timeout, etc. Dense rewards provide non-zero values during each transition: distance to goal, energy consumption, speed of the robot, etc. As we will see later, MDPs with sparse rewards are much harder to learn.\n\n\n\n\n\n\nFigure 3.3: Dense vs. sparse rewards. Source: https://forns.lmu.build/classes/spring-2020/cmsi-432/lecture-13-2.html\n\n\n\nOver time, the MDP will be in a sequence of states (possibly infinite):\ns_0 \\rightarrow s_1 \\rightarrow s_2  \\rightarrow \\ldots \\rightarrow s_T\nand collect a sequence of rewards:\nr_1 \\rightarrow r_2 \\rightarrow r_3  \\rightarrow \\ldots \\rightarrow r_{T}\n\n\n\n\n\n\nFigure 3.4: Sequence of transitions over time in a MDP.\n\n\n\nIn a MDP, we are interested in maximizing the return R_t, i.e. the discounted sum of future rewards after the step t:\n\n    R_t = r_{t+1} + \\gamma \\, r_{t+2} + \\gamma^2 \\, r_{t+3} + \\ldots = \\sum_{k=0}^\\infty \\gamma^k \\, r_{t+k+1}\n\nThe return is sometimes called the reward-to-go: how much reward will I collect from now on? Of course, you can never know the return at time t: transitions and rewards are probabilistic, so the received rewards in the future are not exactly predictable at t. R_t is therefore purely theoretical: RL is all about estimating the return.\nMore generally, for a trajectory (episode) \\tau = (s_0, a_0, r_1, s_1, a_1, \\ldots, s_T), one can define its return as:\n R(\\tau) = \\sum_{t=0}^{T} \\gamma^t \\, r_{t+1} \nThe discount factor (or discount rate, or discount) \\gamma \\in [0, 1] is a very important parameter in RL: It defines the present value of future rewards. Receiving 10 euros now has a higher value than receiving 10 euros in ten years, although the reward is the same: you do not have to wait. The value of receiving a reward r after k+1 time steps is \\gamma^k \\, r, meaning that immediate rewards are better than delayed rewards.\n\\gamma determines the relative importance of future rewards for the behavior:\n\nif \\gamma is close to 0, only the immediately available rewards will count: the agent is greedy or myopic.\nif \\gamma is close to 1, even far-distance rewards will be taken into account: the agent is farsighted.\n\nAnother important property is that, when \\gamma &lt; 1, \\gamma^k tends to 0 when k goes to infinity: this makes sure that the return is always finite. We can therefore try to maximize it.\n\n    R_t = r_{t+1} + \\gamma \\, r_{t+2} + \\gamma^2 \\, r_{t+3} + \\ldots = \\sum_{k=0}^\\infty \\gamma^k \\, r_{t+k+1}\n\n\n\n\n\n\n\nFigure 3.5: The value of \\gamma^k decays over time. The closer \\gamma is to 1, the slower the decay.\n\n\n\nFor episodic tasks (which break naturally into finite episodes of length T, e.g. plays of a game, trips through a maze), the return is always finite and easy to compute at the end of the episode. The discount factor can be set to 1.\n\n    R_t = \\sum_{k=0}^{T} r_{t+k+1}\n\nFor continuing tasks (which can not be split into episodes), the return could become infinite if \\gamma = 1. The discount factor has to be smaller than 1.\n\n    R_t = \\sum_{k=0}^{\\infty} \\gamma^k \\, r_{t+k+1}\n\n\n\n\n\n\n\nWhy the reward on the long term?\n\n\n\n\n\n\n\n\n\nFigure 3.6: Example of a MDP with two actions in state s_1, lading to two different returns. The states s_5 and s_6 are terminal states, where no reward is received anymore.\n\n\n\nIn the MDP above, selecting the action a_1 in s_1 does not bring reward immediately (r_1 = 0) but allows to reach s_5 in the future and get a reward of 10. Selecting a_2 in s_1 brings immediately a reward of 1, but that will be all.\nDepending on the value of \\gamma, the optimal action might be a_1 or a_2, depending on which one brings more reward on the long term.\nWhen selecting a_1 in s_1, the discounted return is:\n\n    R = 0 + \\gamma \\, 0 + \\gamma^2 \\, 0 + \\gamma^3 \\, 10 + \\ldots = 10 \\, \\gamma^3\n\nwhile it is R= 1 for the action a_2.\nFor high values of \\gamma, 10\\, \\gamma^3 is higher than one, so the action a_1 is the optimal action. For small values of \\gamma, 10\\, \\gamma^3 becomes smaller than one, and the action a_2 becomes the optimal action. The discount rate \\gamma can totally change the optimal behavior of the agent, that is why it is part of the MDP definition and not just a hyperparameter.\n\n\n\n\nPolicy\nThe probability that an agent selects a particular action a in a given state s is called the policy \\pi.\n\n\\begin{align}\n    \\pi &: \\mathcal{S} \\times \\mathcal{A} \\rightarrow P(\\mathcal{S})\\\\\n    (s, a) &\\rightarrow \\pi(s, a)  = P(a_t = a | s_t = s) \\\\\n\\end{align}\n\nThe policy can be deterministic (one action has a probability of 1, the others 0) or stochastic. In all cases, the sum of the probabilities in a given state must be one:\n\n    \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) = 1\n\nThe goal of an agent is to find a policy that maximizes the sum of received rewards on the long term, i.e. the return R_t at each each time step. This policy is called the optimal policy \\pi^*. It maximizes the following objective function:\n\n    \\pi^* = \\text{argmax} \\, \\mathcal{J}(\\pi) = \\text{argmax} \\,  \\mathbb{E}_{\\tau \\sim \\rho_\\pi} [R(\\tau)]\n\nwhere \\rho_\\pi is the density distribution of the trajectories generated by the policy \\pi.\nIn summary, RL is an adaptive optimal control method for Markov Decision Processes using (sparse) rewards as a partial feedback. At each time step t, the agent observes its Markov state s_t \\in \\mathcal{S}, produces an action a_t \\in \\mathcal{A}(s_t), receives a reward according to this action r_{t+1} \\in \\Re and updates its state: s_{t+1} \\in \\mathcal{S}. The agent generates trajectories \\tau = (s_0, a_0, r_1, s_1, a_1, \\ldots, s_T) depending on its policy \\pi(s ,a). The goal is to find the optimal policy \\pi^* (s, a) that maximizes in expectation the return of each possible trajectory under that policy.",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Markov Decision Process</span>"
    ]
  },
  {
    "objectID": "src/1.2-MDP.html#value-functions",
    "href": "src/1.2-MDP.html#value-functions",
    "title": "Markov Decision Process",
    "section": "Value functions",
    "text": "Value functions\nA central notion in RL is to estimate the value (or utility) of every state and action of the MDP. The state-value V^{\\pi} (s) of a state s is defined as the mathematical expectation of the return when starting from that state and thereafter following the agent’s current policy \\pi:\n  V^{\\pi} (s) = \\mathbb{E}_{\\rho_\\pi} ( R_t | s_t = s) = \\mathbb{E}_{\\rho_\\pi} ( \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} |s_t=s ) \nThe mathematical expectation operator \\mathbb{E}(\\cdot) is indexed by \\rho_\\pi, the probability distribution of states achievable with \\pi. Indeed, several trajectories are possible after the state s:\n\nThe state transition probability function p(s' | s, a) leads to different states s', even if the same actions are taken.\nThe expected reward function r(s, a, s') provides stochastic rewards, even if the transition (s, a, s') is the same.\nThe policy \\pi itself is stochastic.\n\nOnly rewards that are obtained using the policy \\pi should be taken into account, not the complete distribution of states and rewards.\nThe value of a state is not intrinsic to the state itself, it depends on the policy: One could be in a state which is very close to the goal (only one action left to win the game), but if the policy is very bad, the “good” action will not be chosen and the state will have a small value.\nThe value of taking an action a in a state s under policy \\pi is the expected return starting\nSimilarly, the action-value (or Q-value) for a state-action pair (s, a) under the policy \\pi is defined as:\n\n\\begin{align}\n    Q^{\\pi} (s, a)  & = \\mathbb{E}_{\\rho_\\pi} ( R_t | s_t = s, a_t =a) \\\\\n                    & = \\mathbb{E}_{\\rho_\\pi} ( \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} |s_t=s, a_t=a) \\\\\n\\end{align}\n\nThe Q-value of an action is sometimes called its utility: is it worth taking this action?",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Markov Decision Process</span>"
    ]
  },
  {
    "objectID": "src/1.2-MDP.html#bellman-equations",
    "href": "src/1.2-MDP.html#bellman-equations",
    "title": "Markov Decision Process",
    "section": "Bellman equations",
    "text": "Bellman equations\n\nRelationship between V and Q\nThe value of a state V^{\\pi}(s) depends on the value Q^{\\pi} (s, a) of the action that will be chosen by the policy \\pi in s:\n\n        V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi(s,a)} [Q^{\\pi} (s, a)] = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, Q^{\\pi} (s, a)\n\nIf the policy \\pi is deterministic (the same action is chosen every time), the value of the state is the same as the value of that action (same expected return). If the policy \\pi is stochastic (actions are chosen with different probabilities), the value of the state is the weighted average (i.e. expectation) of the value of the actions.\n➡️ If the Q-values are known, the V-values can be found easily.\nWe can note that the return at time t depends on the immediate reward r_{t+1} and the return at the next time step t+1:\n\n\\begin{aligned}\n    R_t &= r_{t+1} + \\gamma \\, r_{t+2} +  \\gamma^2  \\, r_{t+3} + \\dots + \\gamma^k \\, r_{t+k+1} + \\dots \\\\\n        &= r_{t+1} + \\gamma \\, ( r_{t+2} +  \\gamma \\, r_{t+3} + \\dots + \\gamma^{k-1} \\, r_{t+k+1} + \\dots) \\\\\n        &= r_{t+1} + \\gamma \\,  R_{t+1} \\\\\n\\end{aligned}\n\nWhen taking the mathematical expectation of that identity, we obtain:\n\n    \\mathbb{E}_{\\rho_\\pi}[R_t] = r(s_t, a_t, s_{t+1}) + \\gamma \\, \\mathbb{E}_{\\rho_\\pi}[R_{t+1}]\n\nIt becomes clear that the value of an action depends on the immediate reward received just after the action, as well as the value of the next state:\n\n        Q^{\\pi}(s_t, a_t) = r(s_t, a_t, s_{t+1}) + \\gamma \\,  V^{\\pi} (s_{t+1})\n\nHowever, this is only for a fixed (s_t, a_t, s_{t+1}) transition. Taking transition probabilities into account, one can obtain the Q-values through the equation:\n\n    Q^{\\pi}(s, a) = \\mathbb{E}_{s' \\sim p(s'|s, a)} [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ] = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ]\n\nThe value of an action depends on:\n\nthe states s' one can arrive after the action (with a probability p(s' | s, a)).\nthe value of that state V^{\\pi} (s'), weighted by \\gamma as it is one step in the future.\nthe reward received immediately after taking that action r(s, a, s') (as it is not included in the value of s').\n\n➡️ If the V-values are known, the Q-values can be found easily by a 1-step look-ahead, i.e. looking at the achievable states.\n\n\nBellman equations\nPutting together those two equations, a fundamental property of value functions used throughout reinforcement learning is that they satisfy a particular recursive relationship:\n\n\\begin{aligned}\n        V^{\\pi}(s)  &= \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, Q^{\\pi} (s, a)\\\\\n                    &= \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ]\n\\end{aligned}\n\nThis equation is called the Bellman equation for V^{\\pi}. It expresses the relationship between the value of a state V^\\pi(s) and the value of its successors V^\\pi(s'), depending on the dynamics of the MDP (p(s' | s, a) and r(s, a, s')) and the current policy \\pi. The interesting property of the Bellman equation for RL is that it admits one and only one solution V^{\\pi}(s).\nThe same recursive relationship stands for Q^{\\pi}(s, a):\n\n\\begin{aligned}\n        Q^{\\pi}(s, a)  &= \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ] \\\\\n                    &=  \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, \\sum_{a' \\in \\mathcal{A}(s')} \\pi(s', a') \\, Q^{\\pi} (s', a')]\n\\end{aligned}\n\nwhich is called the Bellman equation for Q^{\\pi}.\n\n\nOptimal Bellman equations\nThe optimal policy is the policy that gathers the maximum of reward on the long term. Value functions define a partial ordering over policies:\n\n\n\n\n\n\nPartial ordering\n\n\n\nA policy \\pi is better than another policy \\pi' if its expected return is greater or equal than that of \\pi' for all states s.\n\n        \\pi \\geq \\pi' \\Leftrightarrow V^{\\pi}(s) \\geq V^{\\pi'}(s) \\quad \\forall s \\in \\mathcal{S}\n\n\n\nFor a MDP, there exists at least one policy that is better than all the others: this is the optimal policy \\pi^*. We note V^*(s) and Q^*(s, a) the optimal value of the different states and actions under \\pi^*.\n\n   V^* (s) = \\max_{\\pi} V^{\\pi}(s) \\quad \\forall s \\in \\mathcal{S}\n\n\n    Q^* (s, a) = \\max_{\\pi} Q^{\\pi}(s, a) \\quad \\forall s \\in \\mathcal{S}, \\quad \\forall a \\in \\mathcal{A}\n\nWhen the policy is optimal \\pi^*, the link between the V and Q values is even easier. The V and Q values are maximal for the optimal policy: there is no better alternative.\nThe optimal action a^* to perform in the state s is the one with the highest optimal Q-value Q^*(s, a).\n\n    a^* = \\text{argmax}_a \\, Q^*(s, a)\n\nBy definition, this action will bring the maximal return when starting in s. The optimal policy is therefore greedy with respect to Q^*(s, a), i.e. deterministic.\n\n    \\pi^*(s, a) = \\begin{cases}\n                1 \\; \\text{if} \\; a = a^* \\\\\n                0 \\; \\text{otherwise.}\n                \\end{cases}\n\nAs the optimal policy is deterministic, the optimal value of a state is equal to the value of the optimal action:\n\n    V^* (s)  = \\max_{a \\in \\mathcal{A}(s)} Q^{\\pi^*} (s, a)\n\nThe expected return after being in s is the same as the expected return after being in s and choosing the optimal action a^*, as this is the only action that can be taken. This allows to define the Bellman optimality equation for V^*:\n\n    V^* (s)  = \\max_{a \\in \\mathcal{A}(s)} \\sum_{s' \\in \\mathcal{S}}  p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{*} (s') ]\n\nThe same Bellman optimality equation stands for Q^*:\n\n    Q^* (s, a)  = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s')  + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q^* (s', a') ]\n\nThe optimal value of (s, a) depends on the optimal action in the next state s'.",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Markov Decision Process</span>"
    ]
  },
  {
    "objectID": "src/1.2-MDP.html#dynamic-programming",
    "href": "src/1.2-MDP.html#dynamic-programming",
    "title": "Markov Decision Process",
    "section": "Dynamic programming",
    "text": "Dynamic programming\n\n\n\n\n\n\nFigure 3.7: Generalized Policy Iteration. Source: Sutton and Barto (1998).\n\n\n\nIn general, RL algorithms iterate over two steps:\n\nPolicy evaluation\n\nFor a given policy \\pi, the value of all states V^\\pi(s) or all state-action pairs Q^\\pi(s, a) is calculated or estimated.\n\nPolicy improvement\n\nFrom the current estimated values V^\\pi(s) or Q^\\pi(s, a), a new better policy \\pi is derived.\n\n\nAfter enough iterations, the policy converges to the optimal policy (if the states are Markov).\nThis alternation between policy evaluation and policy improvement is called generalized policy iteration (GPI). One particular form of GPI is dynamic programming, where the Bellman equations are used to evaluate a policy.\n\nExact solution\nLet’s note \\mathcal{P}_{ss'}^\\pi the transition probability between s and s' (dependent on the policy \\pi) and \\mathcal{R}_{s}^\\pi the expected reward in s (also dependent):\n\n  \\mathcal{P}_{ss'}^\\pi = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, p(s' | s, a)\n\n\n  \\mathcal{R}_{s}^\\pi = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} \\, p(s' | s, a) \\ r(s, a, s')\n\nThe Bellman equation becomes V^{\\pi} (s)  = \\mathcal{R}_{s}^\\pi + \\gamma \\, \\displaystyle\\sum_{s' \\in \\mathcal{S}} \\, \\mathcal{P}_{ss'}^\\pi \\, V^{\\pi} (s'). As we have a fixed policy during the evaluation, the Bellman equation is simplified.\nLet’s now put the Bellman equations in a matrix-vector form.\n\n      V^{\\pi} (s)  = \\mathcal{R}_{s}^\\pi + \\gamma \\, \\sum_{s' \\in \\mathcal{S}} \\, \\mathcal{P}_{ss'}^\\pi \\, V^{\\pi} (s')\n\nWe first define the vector of state values \\mathbf{V}^\\pi:\n\n  \\mathbf{V}^\\pi = \\begin{bmatrix}\n      V^\\pi(s_1) \\\\ V^\\pi(s_2) \\\\ \\vdots \\\\ V^\\pi(s_n) \\\\\n  \\end{bmatrix}\n\nand the vector of expected reward \\mathbf{R}^\\pi:\n\n  \\mathbf{R}^\\pi = \\begin{bmatrix}\n      \\mathcal{R}^\\pi(s_1) \\\\ \\mathcal{R}^\\pi(s_2) \\\\ \\vdots \\\\ \\mathcal{R}^\\pi(s_n) \\\\\n  \\end{bmatrix}\n\nThe state transition matrix \\mathcal{P}^\\pi is defined as:\n\n  \\mathcal{P}^\\pi = \\begin{bmatrix}\n      \\mathcal{P}_{s_1 s_1}^\\pi & \\mathcal{P}_{s_1 s_2}^\\pi & \\ldots & \\mathcal{P}_{s_1 s_n}^\\pi \\\\\n      \\mathcal{P}_{s_2 s_1}^\\pi & \\mathcal{P}_{s_2 s_2}^\\pi & \\ldots & \\mathcal{P}_{s_2 s_n}^\\pi \\\\\n      \\vdots & \\vdots & \\vdots & \\vdots \\\\\n      \\mathcal{P}_{s_n s_1}^\\pi & \\mathcal{P}_{s_n s_2}^\\pi & \\ldots & \\mathcal{P}_{s_n s_n}^\\pi \\\\\n  \\end{bmatrix}\n\nYou can simply check that:\n\n  \\begin{bmatrix}\n      V^\\pi(s_1) \\\\ V^\\pi(s_2) \\\\ \\vdots \\\\ V^\\pi(s_n) \\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n      \\mathcal{R}^\\pi(s_1) \\\\ \\mathcal{R}^\\pi(s_2) \\\\ \\vdots \\\\ \\mathcal{R}^\\pi(s_n) \\\\\n  \\end{bmatrix}\n  + \\gamma \\, \\begin{bmatrix}\n      \\mathcal{P}_{s_1 s_1}^\\pi & \\mathcal{P}_{s_1 s_2}^\\pi & \\ldots & \\mathcal{P}_{s_1 s_n}^\\pi \\\\\n      \\mathcal{P}_{s_2 s_1}^\\pi & \\mathcal{P}_{s_2 s_2}^\\pi & \\ldots & \\mathcal{P}_{s_2 s_n}^\\pi \\\\\n      \\vdots & \\vdots & \\vdots & \\vdots \\\\\n      \\mathcal{P}_{s_n s_1}^\\pi & \\mathcal{P}_{s_n s_2}^\\pi & \\ldots & \\mathcal{P}_{s_n s_n}^\\pi \\\\\n  \\end{bmatrix} \\times \\begin{bmatrix}\n      V^\\pi(s_1) \\\\ V^\\pi(s_2) \\\\ \\vdots \\\\ V^\\pi(s_n) \\\\\n  \\end{bmatrix}\n\nleads to the same equations as:\n\n      V^{\\pi} (s)  = \\mathcal{R}_{s}^\\pi + \\gamma \\, \\sum_{s' \\in \\mathcal{S}} \\, \\mathcal{P}_{ss'}^\\pi \\, V^{\\pi} (s')\n\nfor all states s. The Bellman equations for all states s can therefore be written with a matrix-vector notation as:\n\n  \\mathbf{V}^\\pi = \\mathbf{R}^\\pi + \\gamma \\, \\mathcal{P}^\\pi \\, \\mathbf{V}^\\pi\n\nIf we know \\mathcal{P}^\\pi and \\mathbf{R}^\\pi (dynamics of the MDP for the policy \\pi), we can simply obtain the state values:\n\n  (\\mathbb{I} - \\gamma \\, \\mathcal{P}^\\pi ) \\times \\mathbf{V}^\\pi = \\mathbf{R}^\\pi\n\nwhere \\mathbb{I} is the identity matrix, what gives:\n\n  \\mathbf{V}^\\pi = (\\mathbb{I} - \\gamma \\, \\mathcal{P}^\\pi )^{-1} \\times \\mathbf{R}^\\pi\n\nIf we have n states, the matrix \\mathcal{P}^\\pi has n^2 elements. Inverting \\mathbb{I} - \\gamma \\, \\mathcal{P}^\\pi requires at least \\mathcal{O}(n^{2.37}) operations. Forget it if you have more than a thousand states (1000^{2.37} \\approx 13 million operations). In dynamic programming, we will use iterative methods to estimate \\mathbf{V}^\\pi.\n\n\nPolicy iteration\nThe idea of iterative policy evaluation (IPE) is to consider a sequence of consecutive state-value functions which should converge from initially wrong estimates V_0(s) towards the real state-value function V^{\\pi}(s).\n\n      V_0 \\rightarrow V_1 \\rightarrow V_2 \\rightarrow \\ldots \\rightarrow V_k \\rightarrow V_{k+1} \\rightarrow \\ldots \\rightarrow V^\\pi\n\n\n\n\nIterative policy estimaiton. Source: David Silver. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n\n\nThe value function at step k+1 V_{k+1}(s) is computed using the previous estimates V_{k}(s) and the Bellman equation transformed into an update rule.\n\n  \\mathbf{V}_{k+1} = \\mathbf{R}^\\pi + \\gamma \\, \\mathcal{P}^\\pi \\, \\mathbf{V}_k\n\nV_\\infty = V^{\\pi} is a fixed point of this update rule because of the uniqueness of the solution to the Bellman equation.\n\n\n\n\n\n\nIterative policy evaluation\n\n\n\n\nFor a fixed policy \\pi, initialize V(s)=0 \\; \\forall s \\in \\mathcal{S}.\nwhile not converged:\n\nfor all states s:\n\nV_\\text{target}(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n\n\\delta =0\nfor all states s:\n\n\\delta = \\max(\\delta, |V(s) - V_\\text{target}(s)|)\nV(s) = V_\\text{target}(s)\n\nif \\delta &lt; \\delta_\\text{threshold}:\n\nconverged = True\n\n\n\n\n\nFor each state s, we would like to know if we should deterministically choose an action a \\neq \\pi(s) or not in order to improve the policy. The value of an action a in the state s for the policy \\pi is given by:\n\n     Q^{\\pi} (s, a) = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ]\n\nIf the Q-value of an action a is higher than the one currently selected by the deterministic policy:\nQ^{\\pi} (s, a) &gt; Q^{\\pi} (s, \\pi(s)) = V^{\\pi}(s)\nthen it is better to select a once in s and thereafter follow \\pi. If there is no better action, we keep the previous policy for this state. This corresponds to a greedy action selection over the Q-values, defining a deterministic policy \\pi(s):\n\\pi(s) \\leftarrow \\text{argmax}_a \\, Q^{\\pi} (s, a) = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ]\nAfter the policy improvement, the Q-value of each deterministic action \\pi(s) has increased or stayed the same.\n\\text{argmax}_a \\; Q^{\\pi} (s, a) = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ] \\geq Q^\\pi(s, \\pi(s))\nThis defines an improved policy \\pi', where all states and actions have a higher value than previously. Greedy action selection over the state value function implements policy improvement:\n\\pi' \\leftarrow \\text{Greedy}(V^\\pi)\n\n\n\n\n\n\nGreedy policy improvement:\n\n\n\n\nfor each state s \\in \\mathcal{S}:\n\n\\pi(s) \\leftarrow \\text{argmax}_a \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ]\n\n\n\n\nOnce a policy \\pi has been improved using V^{\\pi} to yield a better policy \\pi', we can then compute V^{\\pi'} and improve it again to yield an even better policy \\pi''. The algorithm policy iteration successively uses policy evaluation and policy improvement to find the optimal policy.\n\n  \\pi_0 \\xrightarrow[]{E} V^{\\pi_0} \\xrightarrow[]{I} \\pi_1 \\xrightarrow[]{E} V^{\\pi^1} \\xrightarrow[]{I}  ... \\xrightarrow[]{I} \\pi^* \\xrightarrow[]{E} V^{*}\n\nThe optimal policy being deterministic, policy improvement can be greedy over the state-action values. If the policy does not change after policy improvement, the optimal policy has been found.\n\n\n\n\n\n\nPolicy iteration\n\n\n\n\nInitialize a deterministic policy \\pi(s) and set V(s)=0 \\; \\forall s \\in \\mathcal{S}.\nwhile \\pi is not optimal:\n\nwhile not converged: # Policy evaluation\n\nfor all states s:\n\nV_\\text{target}(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n\nfor all states s:\n\nV(s) = V_\\text{target}(s)\n\n\nfor each state s \\in \\mathcal{S}: # Policy improvement\n\n\\pi(s) \\leftarrow \\text{argmax}_a \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ]\n\nif \\pi has not changed: break\n\n\n\n\n\n\nValue iteration\nOne drawback of policy iteration is that it uses a full policy evaluation, which can be computationally exhaustive as the convergence of V_k is only at the limit and the number of states can be huge. The idea of value iteration is to interleave policy evaluation and policy improvement, so that the policy is improved after EACH iteration of policy evaluation, not after complete convergence.\nAs policy improvement returns a deterministic greedy policy, updating of the value of a state is then simpler:\n\n  V_{k+1}(s) = \\max_a \\sum_{s'} p(s' | s,a) [r(s, a, s') + \\gamma \\, V_k(s') ]\n\nNote that this is equivalent to turning the Bellman optimality equation into an update rule. Value iteration converges to V^*, faster than policy iteration, and should be stopped when the values do not change much anymore.\n\n\n\n\n\n\nValue iteration\n\n\n\n\nInitialize a deterministic policy \\pi(s) and set V(s)=0 \\; \\forall s \\in \\mathcal{S}.\nwhile not converged:\n\nfor all states s:\n\nV_\\text{target}(s) = \\max_a \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n\n\\delta = 0\nfor all states s:\n\n\\delta = \\max(\\delta, |V(s) - V_\\text{target}(s)|)\nV(s) = V_\\text{target}(s)\n\nif \\delta &lt; \\delta_\\text{threshold}:\n\nconverged = True\n\n\n\n\n\n\n\n\n\nSummary\nPolicy-iteration and value-iteration consist of alternations between policy evaluation and policy improvement, and converge to the optimal policy. This principle is called Generalized Policy Iteration (GPI). Solving the Bellman equations requires the following:\n\naccurate knowledge of environment dynamics p(s' | s, a) and r(s, a, s') for all transitions;\nenough memory and time to do the computations;\nthe Markov property.\n\nFinding an optimal policy is polynomial in the number of states and actions: \\mathcal{O}(N^2 \\, M) (N is the number of states, M the number of actions). The number of states is often astronomical (e.g., Go has about 10^{170} states), often growing exponentially with the number of state variables (what Bellman called “the curse of dimensionality”). In practice, classical DP can only be applied to problems with a few millions of states.\n\n\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press.",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Markov Decision Process</span>"
    ]
  },
  {
    "objectID": "src/1.4-MC.html",
    "href": "src/1.4-MC.html",
    "title": "Monte Carlo methods",
    "section": "",
    "text": "Monte Carlo control\nWhen the environment is a priori unknown, it has to be explored in order to build estimates of the V or Q value functions. The key idea of Monte Carlo sampling (MC) is rather simple: the expected return in state s is approximated by sampling M trajectories \\tau_i starting from s and computing the sampling average of the obtained returns:\nV^{\\pi}(s) = \\mathbb{E}_{\\rho_\\pi} (R_t | s_t = s) \\approx \\frac{1}{M} \\sum_{i=1}^M R(\\tau_i)\nIf you have enough trajectories, the sampling average is an unbiased estimator of the value function. The advantage of Monte Carlo methods is that they require only experience, not the complete dynamics p(s' | s,a) and r(s, a, s'). The idea of MC policy evaluation is therefore to repeatedly sample episodes starting from each possible state s_0 and maintain a running average of the obtained returns for each state:\nQ-values can also be approximated using the same procedure:\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha (R_t - Q(s_t, a_t))\nThe two main drawbacks of MC methods are:\nThe second issue is linked to the exploration-exploitation dilemma already seen with bandits: the episode is generated using the current policy (or a policy derived from it, see later). If the policy always select the same actions from the beginning (exploitation), the agent will never discover better alternatives: the values will converge to a local minimum. If the policy always pick randomly actions (exploration), the policy which is evaluated is not the current policy \\pi, but the random policy. A trade-off between the two therefore has to be maintained: usually a lot of exploration at the beginning of learning to accumulate knowledge about the environment, less towards the end to actually use the knowledge and perform optimally.\nThere are two types of methods trying to cope with exploration:",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo methods</span>"
    ]
  },
  {
    "objectID": "src/1.4-MC.html#monte-carlo-control",
    "href": "src/1.4-MC.html#monte-carlo-control",
    "title": "Monte Carlo methods",
    "section": "",
    "text": "Monte Carlo policy evaluation\n\n\n\nwhile True:\n\nStart from an initial state s_0.\nGenerate a sequence of transitions according to the current policy \\pi until a terminal state s_T is reached.\n\n\n    \\tau = (s_o, a_o, r_ 1, s_1, a_1, \\ldots, s_T)\n\n\nCompute the return R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} for all encountered states s_0, s_1, \\ldots, s_T.\nUpdate the estimated state value V(s_t) of all encountered states using the obtained return:\n\n\n    V(s_t) \\leftarrow V(s_t) + \\alpha \\, (R_t - V(s_t))\n\n\n\n\n\n\n\nThe task must be episodic, i.e. stop after a finite amount of transitions. Updates are only applied at the end of an episode.\nA sufficient level of exploration has to be ensured to make sure the estimates converge to the optimal values.\n\n\n\n\nOn-policy methods generate the episodes using the learned policy \\pi, but it has to be \\epsilon-soft, i.e. stochastic: it has to let a probability of at least \\epsilon of selecting another action than the greedy action (the one with the highest estimated Q-value).\nOff-policy methods use a second policy called the behavior policy to generate the episodes, but learn a different policy for exploitation, which can even be deterministic.",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo methods</span>"
    ]
  },
  {
    "objectID": "src/1.4-MC.html#on-policy-monte-carlo-methods",
    "href": "src/1.4-MC.html#on-policy-monte-carlo-methods",
    "title": "Monte Carlo methods",
    "section": "On-policy Monte Carlo methods",
    "text": "On-policy Monte Carlo methods\n\\epsilon-soft policies are easy to create, and we have already seen them in Section Sampling and Bandits. The simplest one is the \\epsilon-greedy action selection method, which assigns a probability (1-\\epsilon) of selecting the greedy action (the one with the highest Q-value), and a probability \\epsilon of selecting any of the other available actions:\n\n    a_t = \\begin{cases} a_t^* \\quad \\text{with probability} \\quad (1 - \\epsilon) \\\\\n                       \\text{any other action with probability } \\epsilon \\end{cases}\n\nAnother solution is the Softmax (or Gibbs distribution) action selection method, which assigns to each action a probability of being selected depending on their relative Q-values:\n\n    P(s, a) = \\frac{\\exp Q^\\pi(s, a) / \\tau}{ \\sum_b \\exp Q^\\pi(s, b) / \\tau}\n\n\\tau is a positive parameter called the temperature: high temperatures cause the actions to be nearly equiprobable, while low temperatures cause \\tau is a positive parameter called the temperature.\nIn on-policy MC control, each sample episode is generated using the current policy, which ensures exploration, while the control method still converges towards the optimal \\epsilon-policy.\n\n\n\n\n\n\nOn-policy Monte Carlo control\n\n\n\nwhile True:\n\nGenerate an episode \\tau = (s_0, a_0, r_1, \\ldots, s_T) using the current stochastic policy \\pi.\nFor each state-action pair (s_t, a_t) in the episode, update the estimated Q-value:\n\n\n    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (R_t - Q(s_t, a_t))\n\n\nFor each state s_t in the episode, improve the policy (e.g. \\epsilon-greedy):\n\n\n    \\pi(s_t, a) = \\begin{cases}\n                    1 - \\epsilon \\; \\text{if} \\; a = \\text{argmax}\\, Q(s, a) \\\\\n                    \\frac{\\epsilon}{|\\mathcal{A(s_t)}-1|} \\; \\text{otherwise.} \\\\\n                    \\end{cases}",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo methods</span>"
    ]
  },
  {
    "objectID": "src/1.4-MC.html#off-policy-monte-carlo-methods",
    "href": "src/1.4-MC.html#off-policy-monte-carlo-methods",
    "title": "Monte Carlo methods",
    "section": "Off-policy Monte Carlo methods",
    "text": "Off-policy Monte Carlo methods\nAnother option to ensure exploration is to generate the sample episodes using a behavior policy b(s, a) different from the learned policy \\pi(s, a) of the agent. The behavior policy b(s, a) used to generate the episodes is only required to select at least occasionally the same actions as the learned policy \\pi(s, a) (coverage assumption).\n \\pi(s,a) &gt; 0 \\Rightarrow b(s,a) &gt; 0\nThere are mostly two choices regarding the behavior policy:\n\nAn \\epsilon-soft behavior policy over the Q-values as in on-policy MC is often enough, while a deterministic (greedy) policy can be learned implictly.\nThe behavior policy could also come from expert knowledge, i.e. known episodes from the MDP generated by somebody else (human demonstrator, classical algorithm).\n\nBut are we mathematically allowed to do this? We search for the optimal policy that maximizes in expectation the return of each trajectory (episode) possible under the learned policy \\pi:\n\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_\\pi} [R(\\tau)]\n\\rho_\\pi denotes the probability distribution of trajectories achievable using the policy \\pi. If we generate the trajectories from the behavior policy b(s, a), we end up maximizing something else:\n\\mathcal{J}'(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_b} [R(\\tau)]\nThe policy that maximizes \\mathcal{J}'(\\pi) is not the optimal policy of the MDP.\n\nIf you try to estimate a parameter of a random distribution \\pi using samples of another distribution b, the sample average will have a strong bias. We need to correct the samples from b in order to be able to estimate the parameters of \\pi correctly, through importance sampling (IS).\n\nImportance sampling\nWe want to estimate the expected return of the trajectories generated by the policy \\pi:\n\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_\\pi} [R(\\tau)]\nWe start by using the definition of the mathematical expectation:\n\\mathcal{J}(\\pi) = \\int_\\tau \\rho_\\pi(\\tau) \\, R(\\tau) \\, d\\tau\nThe expectation is the integral over all possible trajectories of their return R(\\tau), weighted by the likelihood \\rho_\\pi(\\tau) that a trajectory \\tau is generated by the policy \\pi.\n\n\n\nOnly certain trajectories are likely under a given policy.\n\n\nThe trick is to introduce the behavior policy b in what we want to estimate:\n\\mathcal{J}(\\pi) = \\int_\\tau \\frac{\\rho_b(\\tau)}{\\rho_b(\\tau)} \\, \\rho_\\pi(\\tau) \\, R(\\tau) \\, d\\tau\n\\rho_b(\\tau) is the likelihood that a trajectory \\tau is generated by the behavior policy b. We shuffle a bit the terms:\n\\mathcal{J}(\\pi) = \\int_\\tau \\rho_b(\\tau) \\, \\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} \\,  R(\\tau) \\, d\\tau\nand notice that it has the form of an expectation over trajectories generated by b:\n\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_b} [\\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} \\, R(\\tau)]\nThis means that we can sample trajectories from b, but we need to correct the observed return by the importance sampling weight \\dfrac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)}.\nThe importance sampling weight corrects the mismatch between \\pi and b.\n\nIf the two distributions are the same (on-policy), the IS weight is 1, no need to correct the return.\n\nIf a sample is likely under b but not under \\pi, we should not care about its return: \\dfrac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} &lt;&lt; 1\nIf a sample is likely under \\pi but not much under b, we increase its importance in estimating the return: \\dfrac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} &gt;&gt; 1\n\nThe sampling average of the corrected samples will be closer from the true estimate (unbiased).\nHow do we compute these probability distributions \\rho_\\pi(\\tau) and \\rho_b(\\tau) for a trajectory \\tau? A trajectory \\tau is a sequence of state-action transitions (s_0, a_0, s_1, a_1, \\ldots, s_T) whose probability depends on:\n\nthe probability of choosing an action a_t in state s_t: the policy \\pi(s, a).\nthe probability of arriving in the state s_{t+1} from the state s_t with the action a_t: the transition probability p(s_{t+1} | s_t, a_t).\n\n\nThe likelihood of a trajectory \\tau = (s_0, a_0, s_1, a_1, \\ldots, s_T) under a policy \\pi depends on the policy and the transition probabilities (Markov property):\n\n    \\rho_\\pi(\\tau) = p_\\pi(s_0, a_0, s_1, a_1, \\ldots, s_T) = p(s_0) \\, \\prod_{t=0}^{T-1} \\pi_\\theta(s_t, a_t) \\, p(s_{t+1} | s_t, a_t)\n\np(s_0) is the probability of starting an episode in s_0, we do not have control over it.\nWhat is interesting is that the transition probabilities disappear when calculating the importance sampling weight:\n\n    \\rho_{0:T-1} = \\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} = \\frac{p_0 (s_0) \\, \\prod_{t=0}^{T-1} \\pi(s_t, a_t) p(s_{t+1} | s_t, a_t)}{p_0 (s_0) \\, \\prod_{t=0}^T b(s_t, a_t) p(s_{t+1} | s_t, a_t)} = \\frac{\\prod_{t=0}^{T-1} \\pi(s_t, a_t)}{\\prod_{t=0}^T b(s_t, a_t)} = \\prod_{t=0}^{T-1} \\frac{\\pi(s_t, a_t)}{b(s_t, a_t)}\n\nThe importance sampling weight is simply the product over the length of the episode of the ratio between \\pi(s_t, a_t) and b(s_t, a_t).\n\n\nOff-policy Monte Carlo control\nIn off-policy MC control, we generate episodes using the behavior policy b and update greedily the learned policy \\pi. For the state s_t, the obtained returns just need to be weighted by the relative probability of occurrence of the rest of the episode following the policies \\pi and b:\n\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)}\nV^\\pi(s_t) = \\mathbb{E}_{\\tau \\sim \\rho_b} [\\rho_{t:T-1} \\, R_t]\nThis gives us the updates:\n\n    V(s_t) = V(s_t) + \\alpha  \\, \\rho_{t:T-1} \\, (R_t - V(s_t))\n\nand:\n\n    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha  \\, \\rho_{t:T-1} \\, (R_t - Q(s_t, a_t))\n\nUnlikely episodes under \\pi are barely used for learning, likely ones are used a lot.\n\n\n\n\n\n\nOff-policy Monte Carlo control\n\n\n\nwhile True:\n\nGenerate an episode \\tau = (s_0, a_0, r_1, \\ldots, s_T) using the behavior policy b.\nFor each state-action pair (s_t, a_t) in the episode, update the estimated Q-value:\n\n\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)}\n\n    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha  \\, \\rho_{t:T-1} \\, (R_t - Q(s_t, a_t))\n\n\nFor each state s_t in the episode, update the learned deterministic policy (greedy):\n\n\n    \\pi(s_t, a) = \\begin{cases}\n                    1\\; \\text{if} \\; a = \\text{argmax} \\, Q(s_t, a) \\\\\n                    0 \\; \\text{otherwise.} \\\\\n                    \\end{cases}\n\n\n\nProblem 1: if the learned policy is greedy, the IS weight becomes quickly 0 for a non-greedy action a_t:\n\\pi(s_t, a_t) = 0 \\rightarrow \\rho_{0:T-1} = \\prod_{k=0}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)} = 0\nOff-policy MC control only learns from the last greedy actions, what is slow at the beginning.\nSolution: \\pi and b should not be very different. Usually \\pi is greedy and b is a softmax (or \\epsilon-greedy) over it.\nProblem 2: if the learned policy is stochastic, the IS weights can quickly vanish to 0 or explode to infinity:\n\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)}\nIf \\dfrac{\\pi(s_k, a_k)}{b(s_k, a_k)} is smaller than 1, the products go to 0. If it is bigger than 1, it grows to infinity.\nSolution: one can normalize the IS weight between different episodes (see Sutton and Barto) or clip it (e.g. restrict it to [0.9, 1.1], see PPO later in this course).\n\n\nAdvantages of off-policy methods\nThe main advantage of off-policy strategies is that you can learn from other’s actions, you don’t have to rely on your initially wrong policies to discover the solution by chance. Example: learning to play chess by studying thousands/millions of plays by chess masters. In a given state, only a subset of the possible actions are actually executed by experts: the others may be too obviously wrong. The exploration is then guided by this expert knowledge, not randomly among all possible actions.\nOff-policy methods greatly reduce the number of transitions needed to learn a policy: very stupid actions are not even considered, but the estimation policy learns an optimal strategy from the “classical” moves. Drawback: if a good move is not explored by the behavior policy, the learned policy will never try it.\n\n\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press.",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo methods</span>"
    ]
  },
  {
    "objectID": "src/1.5-TD.html",
    "href": "src/1.5-TD.html",
    "title": "Temporal Difference learning",
    "section": "",
    "text": "Temporal difference\nThe main drawback of Monte Carlo methods is that the task must be composed of finite episodes. Not only is it not always possible, but value updates have to wait for the end of the episode, what slows learning down. Temporal difference methods simply replace the actual return obtained after a state or an action, by an estimation composed of the reward immediately received plus the value of the next state or action:\nR_t \\approx r(s, a, s') + \\gamma \\, V^\\pi(s') \\approx r + \\gamma \\, Q^\\pi(s', a')\nThis gives us the following learning rules:\nV^\\pi(s) \\leftarrow V^\\pi(s) + \\alpha (r(s, a, s') + \\gamma \\, V^\\pi(s') - V^\\pi(s))\nQ^\\pi(s, a) \\leftarrow Q^\\pi(s, a) + \\alpha (r(s, a, s') + \\gamma \\, Q^\\pi(s', a') - Q^\\pi(s, a))\nThe quantity:\n\\delta = r(s, a, s') + \\gamma \\, V^\\pi(s') - V^\\pi(s)\nor:\n\\delta = r(s, a, s') + \\gamma \\, Q^\\pi(s', a') - Q^\\pi(s, a)\nis called the reward-prediction error (RPE) or TD error: it defines the surprise between the current reward prediction (V^\\pi(s) or Q^\\pi(s, a)) and the sum of the immediate reward plus the reward prediction in the next state / after the next action.\nThe main advantage of this learning method is that the update of the V- or Q-value can be applied immediately after a transition: no need to wait until the end of an episode, or even to have episodes at all: this is called online learning and allows very fast learning from single transitions. The main drawback is that the updates depend on other estimates, which are initially wrong: it will take a while before all estimates are correct.\nWhen learning Q-values directly, the question is which next action a' should be used in the update rule: the action that will actually be taken for the next transition (defined by \\pi(s', a')), or the greedy action (a^* = \\text{argmax}_a Q^\\pi(s', a)). This relates to the on-policy / off-policy distinction already seen for MC methods:\n\\delta = r(s, a, s') + \\gamma \\, Q^\\pi(s', \\pi(s')) - Q^\\pi(s, a)\n\\delta = r(s, a, s') + \\gamma \\, \\max_{a'} Q^\\pi(s', a') - Q^\\pi(s, a)\nIn Q-learning, the behavior policy has to ensure exploration, while this is achieved implicitly by the learned policy in SARSA, as it must be \\epsilon-soft. An easy way of building a behavior policy based on a deterministic learned policy is \\epsilon-greedy: the deterministic action \\mu(s_t) is chosen with probability 1 - \\epsilon, the other actions with probability \\epsilon. In continuous action spaces, additive noise (e.g. Ohrstein-Uhlenbeck) can be added to the action.\nAlternatively, domain knowledge can be used to create the behavior policy and restrict the search to meaningful actions: compilation of expert moves in games, approximate solutions, etc. Again, the risk is that the behavior policy never explores the actually optimal actions. See off-policy actor-critic for more details on the difference between on-policy and off-policy methods.",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Difference learning</span>"
    ]
  },
  {
    "objectID": "src/1.5-TD.html#temporal-difference",
    "href": "src/1.5-TD.html#temporal-difference",
    "title": "Temporal Difference learning",
    "section": "",
    "text": "If \\delta &gt; 0, the transition was positively surprising: one obtains more reward or lands in a better state than expected. The initial state or action was actually underrated, so its estimated value must be increased.\nIf \\delta &lt; 0, the transition was negatively surprising. The initial state or action was overrated, its value must be decreased.\nIf \\delta = 0, the transition was fully predicted: one obtains as much reward as expected, so the values should stay as they are.\n\n\n\n\n\n\n\n\nFigure 5.1: Temporal difference algorithms update values after a single transition. Source: Sutton and Barto (1998).\n\n\n\n\n\nOn-policy TD learning is called SARSA (state-action-reward-state-action). It uses the next action sampled from the policy \\pi(s', a') to update the current transition. This selected action could be noted \\pi(s') for simplicity. It is required that this next action will actually be performed for the next transition. The policy must be \\epsilon-soft, for example \\epsilon-greedy or softmax:\n\n\n\nOff-policy TD learning is called Q-learning (Watkins, 1989). The greedy action in the next state (the one with the highest Q-value) is used to update the current transition. It does not mean that the greedy action will actually have to be selected for the next transition. The learned policy can therefore also be deterministic:\n\n\n\n\n\nEligibility traces\nThe main drawback of TD learning is that learning can be slow and necessitate many transitions to converge (sample complexity). This is particularly true when the problem provides sparse rewards (as opposed to dense rewards). For example in a game like chess, a reward is given only at the end of a game (+1 for winning, -1 for losing). All other actions receive a reward of 0, although they are as important as the last one in order to win.\nImagine you initialize all Q-values to 0 and apply Q-learning. During the first episode, all actions but the last one will receive a reward r(s, a, s') of 0 and arrive in a state where the greedy action has a value Q^\\pi(s', a') of 0, so the TD error \\delta is 0 and their Q-value will not change. Only the very last action will receive a non-zero reward and update its value slightly (because of the learning rate \\alpha). When this episode is performed again, the last action will again be updated, but also the one just before: Q^\\pi(s', a') is now different from 0 for this action, so the TD error is now different from 0. It is straightforward to see that if the episode has a length of 100 moves, the agent will need at least 100 episodes to “backpropagate” the final sparse reward to the first action of the episode. In practice, this is even worse: the learning rate \\alpha and the discount rate \\gamma will slow learning down even more. MC methods suffer less from this problem, as the first action of the episode would be updated using the actual return, which contains the final reward (although it is discounted by \\gamma).\nEligibility traces can be seen a trick to mix the advantages of MC (faster updates) with the ones of TD (online learning, smaller variance). The idea is that the TD error at time t (\\delta_t) will be used not only to update the action taken at time t (\\Delta Q(s_t, a_t) = \\alpha \\, \\delta_t), but also all the preceding actions, which are also responsible for the success or failure of the action taken at time t. A parameter \\lambda between 0 and 1 (decaying factor) controls how far back in time a single TD error influences past actions. This is important when the policy is mostly exploratory: initial actions may be mostly random and finally find the the reward by chance. They should learn less from the reward than the last one, otherwise they would be systematically reproduced. Figure 5.2 shows the principle of eligibility traces in a simple Gridworld environment.\n\n\n\n\n\n\nFigure 5.2: Principle of eligibility traces applied to the Gridworld problem using SARSA(\\lambda). Source: Sutton and Barto (1998).\n\n\n\nThere are many possible implementations of eligibility traces (Watkin’s, Peng, Tree Backup, etc. See the Chapter 12 of Sutton and Barto (2017)). Generally, one distinguished a forward and a backward view of eligibility traces.\n\nThe forward view considers that one transition (s_t, a_t) gathers the TD errors made at future time steps t' and discounts them with the parameter \\lambda:\n\n\n    Q^\\pi(s_t, a_t) \\leftarrow  Q^\\pi(s_t, a_t) + \\alpha \\, \\sum_{t'=t}^T (\\gamma \\lambda)^{t'-t} \\delta_{t'}\n\nFrom this equation, \\gamma and \\lambda seem to play a relatively similar role, but remember that \\gamma is also used in the TD error, so they control different aspects of learning. The drawback of this approach is that the future transitions at t'&gt;t and their respective TD errors must be known when updating the transition, so this prevents online learning (the episode must be terminated to apply the updates).\n\nThe backward view considers that the TD error made at time t is sent backwards in time to all transitions previously executed. The easiest way to implement this is to update an eligibility trace e(s,a) for each possible transition, which is incremented every time a transition is visited and otherwise decays exponentially with a speed controlled by \\lambda:\n\n\n    e(s, a) = \\begin{cases} e(s, a) + 1 \\quad \\text{if} \\quad s=s_t \\quad \\text{and} \\quad a=a_t \\\\\n                            \\lambda \\, e(s, a) \\quad \\text{otherwise.}\n              \\end{cases}\n\nThe Q-value of all transitions (s, a) (not only the one just executed) is then updated proportionally to the corresponding trace and the current TD error:\n\n    Q^\\pi(s, a) \\leftarrow  Q^\\pi(s, a) + \\alpha \\, e(s, a) \\, \\delta_{t} \\quad \\forall s, a\n\nThe forward and backward implementations are equivalent: the first requires to know the future, the second requires to update many transitions at each time step. The best solution will depend on the complexity of the problem.\nTD learning, SARSA and Q-learning can all be efficiently extended using eligibility traces. This gives the algorithms TD(\\lambda), SARSA(\\lambda) and Q(\\lambda), which can learn much faster than their 1-step equivalent, at the cost of more computations.\n\n\nActor-critic architectures\nLet’s consider the TD error based on state values:\n\n\\delta = r(s, a, s') + \\gamma \\, V^\\pi(s') - V^\\pi(s)\n\nAs noted in the previous sections, the TD error represents how surprisingly good (or bad) a transition between two states has been (ergo the corresponding action). It can be used to update the value of the state s_t:\n\n    V^\\pi(s) \\leftarrow V^\\pi(s) + \\alpha \\, \\delta\n\nThis allows to estimate the values of all states for the current policy. However, this does not help to 1) directy select the best action or 2) improve the policy. When only the V-values are given, one can only want to reach the next state V^\\pi(s') with the highest value: one needs to know which action leads to this better state, i.e. have a model of the environment. Actually, one selects the action with the highest Q-value:\n\n    Q^{\\pi}(s, a) = \\sum_{s' \\in \\mathcal{S}} p(s'|s, a) [r(s, a, s') + \\gamma \\, V^\\pi(s')]\n\nAn action may lead to a high-valued state, but with such a small probability that it is actually not worth it. p(s'|s, a) and r(s, a, s') therefore have to be known (or at least approximated), what defeats the purpose of sample-based methods.\n\n\n\n\n\n\nFigure 5.3: Actor-critic architecture (Sutton and Barto, 1998).\n\n\n\nActor-critic architectures have been proposed to solve this problem:\n\nThe critic learns to estimate the value of a state V^\\pi(s) and compute the RPE \\delta = r(s, a, s') + \\gamma \\, V^\\pi(s') - V^\\pi(s).\nThe actor uses the RPE to update a preference for the executed action: action with positive RPEs (positively surprising) should be reinforced (i.e. taken again in the future), while actions with negative RPEs should be avoided in the future.\n\nThe main interest of this architecture is that the actor can take any form (neural network, decision tree), as long as it able to use the RPE for learning. The simplest actor would be a softmax action selection mechanism, which maintains a preference p(s, a) for each action and updates it using the TD error:\n\n    p(s, a) \\leftarrow p(s, a) + \\alpha \\, \\delta_t\n\nThe policy uses the softmax rule on these preferences:\n\n    \\pi(s, a) = \\frac{p(s, a)}{\\sum_a p(s, a)}\n\nActor-critic algorithms learn at the same time two aspects of the problem:\n\nA value function (e.g. V^\\pi(s)) to compute the TD error in the critic,\nA policy \\pi in the actor.\n\nClassical TD learning only learn a value function (V^\\pi(s) or Q^\\pi(s, a)): these methods are called value-based methods. Actor-critic architectures are particularly important in policy search methods.\n\n\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press.\n\n\nSutton, R. S., and Barto, A. G. (2017). Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press Available at: http://incompleteideas.net/book/the-book-2nd.html.\n\n\nWatkins, C. J. (1989). Learning from delayed rewards.",
    "crumbs": [
      "**Basic RL**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Temporal Difference learning</span>"
    ]
  },
  {
    "objectID": "src/2.1-FunctionApproximation.html",
    "href": "src/2.1-FunctionApproximation.html",
    "title": "Function approximation",
    "section": "",
    "text": "Value-based function approximation\nIn value-based methods, we want to approximate the Q-values Q^\\pi(s,a) of all possible state-action pairs for a given policy. The function approximator depends on a set of parameters \\theta. \\theta can for example represent all the weights and biases of a neural network. The approximated Q-value can now be noted Q(s, a ;\\theta) or Q_\\theta(s, a). As the parameters will change over time during learning, we can omit the time t from the notation. Similarly, action selection is usually \\epsilon-greedy or softmax, so the policy \\pi depends directly on the estimated Q-values and can therefore on the parameters: it is noted \\pi_\\theta.\nThere are basically two options regarding the structure of the function approximator (Figure 8.1):\nThe second option is of course only possible when the action space is discrete, but has the advantage to generalize better over similar states.\nThe goal of a function approximator is to minimize a loss function (or cost function) \\mathcal{L}(\\theta), so that the estimated Q-values converge for all state-pairs towards their target value, depending on the chosen algorithm:\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\pi[(R_t - Q_\\theta(s, a))^2]\nIf we learn over N episodes of length T, the loss function can be approximated as:\n\\mathcal{L}(\\theta) \\approx \\frac{1}{N} \\sum_{e=1}^N \\sum_{t = 1}^T [R^e_t - Q_\\theta(s_t, a_t)]^2\nAny function approximator able to minimize these loss functions can be used.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Function approximation</span>"
    ]
  },
  {
    "objectID": "src/2.1-FunctionApproximation.html#value-based-function-approximation",
    "href": "src/2.1-FunctionApproximation.html#value-based-function-approximation",
    "title": "Function approximation",
    "section": "",
    "text": "Figure 6.1: Function approximators can either take a state-action pair as input and output the Q-value, or simply take a state as input and output the Q-values of all possible actions.\n\n\n\n\n\nThe approximator takes a state-action pair (s, a) as input and returns a single Q-value Q(s, a).\nIt takes a state s as input and returns the Q-value of all possible actions in that state.\n\n\n\n\nMonte Carlo methods: the Q-value of each (s, a) pair should converge towards the expected return:\n\n\n\n\n\nTemporal difference methods: the Q-values should converge towards an estimation of the expected return.\n\nFor SARSA:\n\n\n  \\mathcal{L}(\\theta) = \\mathbb{E}_\\pi[(r(s, a, s') + \\gamma \\, Q_\\theta(s', \\pi(s')) - Q_\\theta(s, a))^2]\n  \n\nFor Q-learning:\n\n\n  \\mathcal{L}(\\theta) = \\mathbb{E}_\\pi[(r(s, a, s') + \\gamma \\, \\max_{a'} Q_\\theta(s', a') - Q_\\theta(s, a))^2]",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Function approximation</span>"
    ]
  },
  {
    "objectID": "src/2.1-FunctionApproximation.html#policy-based-function-approximation",
    "href": "src/2.1-FunctionApproximation.html#policy-based-function-approximation",
    "title": "Function approximation",
    "section": "Policy-based function approximation",
    "text": "Policy-based function approximation\nIn policy-based function approximation, we want to directly learn a policy \\pi_\\theta(s, a) that maximizes the expected return of each possible transition, i.e. the ones which are selected by the policy. The objective function to be maximized is defined over all trajectories \\tau = (s_0, a_0, s_1, a_1, \\ldots, s_T, a_T) conditioned by the policy:\n\n    J(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} [R_t]\n\nIn short, the learned policy \\pi_\\theta should only produce trajectories \\tau where each state is associated to a high return R_t and avoid trajectories with low returns. Although this objective function leads to the desired behavior, it is not computationally tractable as we would need to integrate over all possible trajectories. The methods presented in Section Policy Gradient methods will provide estimates of the gradient of this objective function.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Function approximation</span>"
    ]
  },
  {
    "objectID": "src/2.2-DeepNetworks.html",
    "href": "src/2.2-DeepNetworks.html",
    "title": "Deep learning",
    "section": "",
    "text": "Deep RL uses deep neural networks as function approximators, allowing complex representations of the value of state-action pairs to be learned. This section provides a very quick overview of deep learning. For additional details, refer to the excellent book of Goodfellow et al. (2016).\n\nDeep neural networks\nA deep neural network (DNN) consists of one input layer \\mathbf{x}, one or several hidden layers \\mathbf{h_1}, \\mathbf{h_2}, \\ldots, \\mathbf{h_n} and one output layer \\mathbf{y}.\n\n\n\n\n\n\nFigure 7.1: Architecture of a deep neural network. Figure Source: Nielsen (2015), CC-BY-NC.\n\n\n\nEach layer k (called fully-connected) transforms the activity of the previous layer (the vector \\mathbf{h_{k-1}}) into another vector \\mathbf{h_{k}} by multiplying it with a weight matrix W_k, adding a bias vector \\mathbf{b_k} and applying a non-linear activation function f.\n\n    \\mathbf{h_{k}} = f(W_k \\times \\mathbf{h_{k-1}} + \\mathbf{b_k})\n\\tag{7.1}\nThe activation function can theoretically be of any type as long as it is non-linear (sigmoid, tanh…), but modern neural networks use preferentially the Rectified Linear Unit (ReLU) function f(x) = \\max(0, x) or its parameterized variants.\nThe goal of learning is to find the weights and biases \\theta minimizing a given loss function on a training set \\mathcal{D}.\n\nIn regression problems, the mean square error (mse) is minimized:\n\n\n    \\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}} [||\\mathbf{t} - \\mathbf{y}||^2]\n\nwhere \\mathbf{x} is the input, \\mathbf{t} the true output (defined in the training set) and \\mathbf{y} the prediction of the NN for the input \\mathbf{x}. The closer the prediction from the true value, the smaller the mse.\n\nIn classification problems, the cross entropy (or negative log-likelihood) is minimized:\n\n\n    \\mathcal{L}(\\theta) = - \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}} [\\sum_i t_i \\log y_i]\n\nwhere the log-likelihood of the prediction \\mathbf{y} to match the data \\mathbf{t} is maximized over the training set. The mse could be used for classification problems too, but the output layer usually has a softmax activation function for classification problems, which works nicely with the cross entropy loss function. See https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss for the link between cross entropy and log-likelihood and https://deepnotes.io/softmax-crossentropy for the interplay between softmax and cross entropy.\nOnce the loss function is defined, it has to be minimized by searching optimal values for the free parameters \\theta. This optimization procedure is based on gradient descent, which is an iterative procedure modifying estimates of the free parameters in the opposite direction of the gradient of the loss function:\n\n\\Delta \\theta = -\\eta \\, \\nabla_\\theta \\mathcal{L}(\\theta) = -\\eta \\, \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta}\n\nThe learning rate \\eta is chosen very small to ensure a smooth convergence. Intuitively, the gradient (or partial derivative) represents how the loss function changes when each parameter is slightly increased. If the gradient w.r.t a single parameter (e.g. a weight w) is positive, increasing the weight increases the loss function (i.e. the error), so the weight should be slightly decreased instead. If the gradient is negative, one should increase the weight.\nThe question is now to compute the gradient of the loss function w.r.t all the parameters of the DNN, i.e. each single weight and bias. The solution is given by the backpropagation algorithm, which is simply an application of the chain rule to feedforward neural networks:\n\n    \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial W_k} = \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\mathbf{y}} \\times \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{h_n}} \\times \\frac{\\partial \\mathbf{h_n}}{\\partial \\mathbf{h_{n-1}}} \\times \\ldots \\times \\frac{\\partial \\mathbf{h_k}}{\\partial W_k}\n\nEach layer of the network adds a contribution to the gradient when going backwards from the loss function to the parameters. Importantly, all functions used in a NN are differentiable, i.e. those partial derivatives exist (and are easy to compute). For the fully connected layer represented by Equation 7.1, the partial derivative is given by:\n\n    \\frac{\\partial \\mathbf{h_{k}}}{\\partial \\mathbf{h_{k-1}}} = f'(W_k \\times \\mathbf{h_{k-1}} + \\mathbf{b_k}) \\, W_k\n\nand its dependency on the parameters is:\n\n    \\frac{\\partial \\mathbf{h_{k}}}{\\partial W_k} = f'(W_k \\times \\mathbf{h_{k-1}} + \\mathbf{b_k}) \\, \\mathbf{h_{k-1}}\n \n    \\frac{\\partial \\mathbf{h_{k}}}{\\partial \\mathbf{b_k}} = f'(W_k \\times \\mathbf{h_{k-1}} + \\mathbf{b_k})\n\nActivation functions are chosen to have an easy-to-compute derivative, such as the ReLU function:\n\n    f'(x) = \\begin{cases} 1 \\quad \\text{if} \\quad x &gt; 0 \\\\ 0 \\quad \\text{otherwise.} \\end{cases}\n\nPartial derivatives are automatically computed by the underlying libraries, such as tensorflow, theano, pytorch, etc. The next step is choose an optimizer, i.e. a gradient-based optimization method allow to modify the free parameters using the gradients. Optimizers do not work on the whole training set, but use minibatches (a random sample of training examples: their number is called the batch size) to compute iteratively the loss function. The most popular optimizers are:\n\nSGD (stochastic gradient descent): vanilla gradient descent on random minibatches.\nSGD with momentum (Nesterov or not): additional momentum to avoid local minima of the loss function.\nAdagrad\nAdadelta\nRMSprop\nAdam\nMany others. Check the doc of keras to see what is available: https://keras.io/optimizers\n\nSee this useful post for a comparison of the different optimizers: http://ruder.io/optimizing-gradient-descent (Ruder, 2016). The common wisdom is that SGD with Nesterov momentum works best (i.e. it finds a better minimum) but its meta-parameters (learning rate, momentum) are hard to find, while Adam works out-of-the-box, at the cost of a slightly worse minimum. For deep RL, Adam is usually preferred, as the goal is to quickly find a working solution, not to optimize it to the last decimal.\n\nAdditional regularization mechanisms are now typically part of DNNs in order to avoid overfitting (learning by heart the training set but failing to generalize): L1/L2 regularization, dropout, batch normalization, etc. Refer to Goodfellow et al. (2016) for further details.\n\n\nConvolutional networks\nConvolutional Neural Networks (CNN) are an adaptation of DNNs to deal with highly dimensional input spaces such as images. The idea is that neurons in the hidden layer reuse (“share”) weights over the input image, as the features learned by early layers are probably local in visual classification tasks: in computer vision, an edge can be detected by the same filter all over the input image.\nA convolutional layer learns to extract a given number of features (typically 16, 32, 64, etc) represented by 3x3 or 5x5 matrices. These matrices are then convoluted over the whole input image (or the previous convolutional layer) to produce feature maps. If the input image has a size NxMx1 (grayscale) or NxMx3 (colored), the convolutional layer will be a tensor of size NxMxF, where F is the number of extracted features. Padding issues may reduce marginally the spatial dimensions. One important aspect is that the convolutional layer is fully differentiable, so backpropagation and the usual optimizers can be used to learn the filters.\n\n\n\n\n\n\nFigure 7.2: Convolutional layer. Source: https://github.com/vdumoulin/conv_arithmetic.\n\n\n\nAfter a convolutional layer, the spatial dimensions are preserved. In classification tasks, it does not matter where the object is in the image, the only thing that matters is what it is: classification requires spatial invariance in the learned representations. The max-pooling layer was introduced to downsample each feature map individually and increase their spatial invariance. Each feature map is divided into 2x2 blocks (generally): only the maximal feature activation in that block is preserved in the max-pooling layer. This reduces the spatial dimensions by a factor two in each direction, but keeps the number of features equal.\n\n\n\n\n\n\nFigure 7.3: Max-pooling layer. Source: Stanford’s CS231n course http://cs231n.github.io/convolutional-networks\n\n\n\nA convolutional neural network is simply a sequence of convolutional layers and max-pooling layers (sometime two convolutional layers are applied in a row before max-pooling, as in VGG (Simonyan and Zisserman, 2015)), followed by a couple of fully-connected layers and a softmax output layer. Figure 7.4 shows the architecture of AlexNet, the winning architecture of the ImageNet challenge in 2012 (Krizhevsky et al., 2012).\n\n\n\n\n\n\nFigure 7.4: Architecture of the AlexNet CNN. Source: Krizhevsky et al. (2012).\n\n\n\nMany improvements have been proposed since 2012 (e.g. ResNets (He et al., 2015)) but the idea stays similar. Generally, convolutional and max-pooling layers are alternated until the spatial dimensions are so reduced (around 10x10) that they can be put into a single vector and fed into a fully-connected layer. This is NOT the case in deep RL! Contrary to object classification, spatial information is crucial in deep RL: position of the ball, position of the body, etc. It matters whether the ball is to the right or to the left of your paddle when you decide how to move it. Max-pooling layers are therefore omitted and the CNNs only consist of convolutional and fully-connected layers. This greatly increases the number of weights in the networks, hence the number of training examples needed to train the network. This is still the main limitation of using CNNs in deep RL.\n\n\nRecurrent neural networks\nFeedforward neural networks learn to efficiently map static inputs \\mathbf{x} to outputs \\mathbf{y} but have no memory or context: the output at time t does not depend on the inputs at time t-1 or t-2, only the one at time t. This is problematic when dealing with video sequences for example: if the task is to classify videos into happy/sad, a frame by frame analysis is going to be inefficient (most frames a neutral). Concatenating all frames in a giant input vector would increase dramatically the complexity of the classifier and no generalization can be expected.\nRecurrent Neural Networks (RNN) are designed to deal with time-varying inputs, where the relevant information to take a decision at time t may have happened at different times in the past. The general structure of a RNN is depicted on Figure 7.5:\n\n\n\n\n\n\nFigure 7.5: Architecture of a RNN. Left: recurrent architecture. Right: unrolled network, showing that a RNN is equivalent to a deep network. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\n\nThe output \\mathbf{h}_t of the RNN at time t depends on its current input \\mathbf{x}_t, but also on its previous output \\mathbf{h}_{t-1}, which, by recursion, depends on the whole history of inputs (x_0, x_1, \\ldots, x_t).\n\n    \\mathbf{h}_t = f(W_x \\, \\mathbf{x}_{t} + W_h \\, \\mathbf{h}_{t-1} + \\mathbf{b})\n\nOnce unrolled, a RNN is equivalent to a deep network, with t layers of weights between the first input \\mathbf{x}_0 and the current output \\mathbf{h}_t. The only difference with a feedforward network is that weights are reused between two time steps / layers. Backpropagation though time (BPTT) can be used to propagate the gradient of the loss function backwards in time and learn the weights W_x and W_h using the usual optimizer (SGD, Adam…).\nHowever, this kind of RNN can only learn short-term dependencies because of the vanishing gradient problem (Hochreiter, 1991). When the gradient of the loss function travels backwards from \\mathbf{h}_t to \\mathbf{x}_0, it will be multiplied t times by the recurrent weights W_h. If |W_h| &gt; 1, the gradient will explode with increasing t, while if |W_h| &lt; 1, the gradient will vanish to 0.\nThe solution to this problem is provided by long short-term memory networks [LSTM;Hochreiter and Schmidhuber (1997)]. LSTM layers maintain additionally a state \\mathbf{C}_t (also called context or memory) which is manipulated by three learnable gates (input, forget and output gates). As in regular RNNs, a candidate state \\tilde{\\mathbf{C}_t} is computed based on the current input and the previous output:\n\n    \\tilde{\\mathbf{C}_t} = f(W_x \\, \\mathbf{x}_{t} + W_h \\, \\mathbf{h}_{t-1} + \\mathbf{b})\n\n\n\n\n\n\n\nFigure 7.6: Architecture of a LSTM layer. Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs.\n\n\n\nThe activation function f is usually a tanh function. The input and forget learn to decide how the candidate state should be used to update the current state:\n\nThe input gate decides which part of the candidate state \\tilde{\\mathbf{C}_t} will be used to update the current state \\mathbf{C}_t:\n\n\n    \\mathbf{i}_t = \\sigma(W^i_x \\, \\mathbf{x}_{t} + W^i_h \\, \\mathbf{h}_{t-1} + \\mathbf{b}^i)\n\nThe sigmoid activation function \\sigma is used to output a number between 0 and 1 for each neuron: 0 means the candidate state will not be used at all, 1 means completely.\n\nThe forget gate decides which part of the current state should be kept or forgotten:\n\n\n    \\mathbf{f}_t = \\sigma(W^f_x \\, \\mathbf{x}_{t} + W^f_h \\, \\mathbf{h}_{t-1} + \\mathbf{b}^f)\n\nSimilarly, 0 means that the corresponding element of the current state will be erased, 1 that it will be kept.\nOnce the input and forget gates are computed, the current state can be updated based on its previous value and the candidate state:\n\n   \\mathbf{C}_t =  \\mathbf{i}_t \\odot \\tilde{\\mathbf{C}_t} + \\mathbf{f}_t \\odot \\mathbf{C}_{t-1}\n\nwhere \\odot is the element-wise multiplication.\n\nThe output gate finally learns to select which part of the current state \\mathbf{C}_t should be used to produce the current output \\mathbf{h}_t:\n\n\n    \\mathbf{o}_t = \\sigma(W^o_x \\, \\mathbf{x}_{t} + W^o_h \\, \\mathbf{h}_{t-1} + \\mathbf{b}^o)\n\n\n    \\mathbf{h}_t = \\mathbf{o}_t \\odot \\tanh \\mathbf{C}_t\n\nThe architecture may seem complex, but everything is differentiable: backpropagation though time can be used to learn not only the input and recurrent weights for the candidate state, but also the weights and and biases of the gates. The main advantage of LSTMs is that they solve the vanishing gradient problem: if the input at time t=0 is important to produce a response at time t, the input gate will learn to put it into the memory and the forget gate will learn to maintain in the current state until it is not needed anymore. During this “working memory” phase, the gradient is multiplied by exactly one as nothing changes: the dependency can be learned with arbitrary time delays!\nThere are alternatives to the classical LSTM layer such as the gated recurrent unit [GRU; Cho et al. (2014)] or peephole connections (Gers, 2001). See http://colah.github.io/posts/2015-08-Understanding-LSTMs, https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714 or http://blog.echen.me/2017/05/30/exploring-lstms/ for more visual explanations of LSTMs and their variants.\nRNNs are particularly useful for deep RL when considering POMDPs, i.e. partially observable problems. If an observation does not contain enough information about the underlying state (e.g. a single image does not contain speed information), LSTM can integrate these observations over time and learn to implicitly represent speed in its context vector, allowing efficient policies to be learned.\n\n\n\n\nCho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., et al. (2014). Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Available at: http://arxiv.org/abs/1406.1078.\n\n\nGers, F. (2001). Long Short-Term Memory in Recurrent Neural Networks. Available at: http://www.felixgers.de/papers/phd.pdf.\n\n\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press Available at: http://www.deeplearningbook.org.\n\n\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. Available at: http://arxiv.org/abs/1512.03385.\n\n\nHochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Available at: http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf.\n\n\nHochreiter, S., and Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation 9, 1735–1780. doi:10.1162/neco.1997.9.8.1735.\n\n\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. in Advances in Neural Information Processing Systems (NIPS) Available at: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.\n\n\nNielsen, M. A. (2015). Neural Networks and Deep Learning. Determination Press Available at: http://neuralnetworksanddeeplearning.com/.\n\n\nRuder, S. (2016). An overview of gradient descent optimization algorithms. Available at: http://arxiv.org/abs/1609.04747.\n\n\nSimonyan, K., and Zisserman, A. (2015). Very Deep Convolutional Networks for Large-Scale Image Recognition. International Conference on Learning Representations (ICRL), 1–14. doi:10.1016/j.infsof.2008.09.005.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Deep learning</span>"
    ]
  },
  {
    "objectID": "src/2.3-DQN.html",
    "href": "src/2.3-DQN.html",
    "title": "Deep Q-network (DQN)",
    "section": "",
    "text": "Limitations of deep neural networks for function approximation\nThe goal of value-based deep RL is to approximate the Q-value of each possible state-action pair using a deep (convolutional) neural network. As shown on Figure 8.1, the network can either take a state-action pair as input and return a single output value, or take only the state as input and return the Q-value of all possible actions (only possible if the action space is discrete), In both cases, the goal is to learn estimates Q_\\theta(s, a) with a NN with parameters \\theta.\nWhen using Q-learning, we have already seen that the problem is a regression problem, where the following mse loss function has to be minimized:\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\pi[(r_t + \\gamma \\, \\max_{a'} Q_\\theta(s', a') - Q_\\theta(s, a))^2]\nIn short, we want to reduce the prediction error, i.e. the mismatch between the estimate of the value of an action Q_\\theta(s, a) and the real return, here approximated with r(s, a, s') + \\gamma \\, \\text{max}_{a'} Q_\\theta(s', a').\nWe can compute this loss by gathering enough samples (s, a, r, s') (i.e. single transitions), concatenating them randomly in minibatches, and let the DNN learn to minimize the prediction error using backpropagation and SGD, indirectly improving the policy. The following pseudocode would describe the training procedure when gathering transitions online, i.e. when directly interacting with the environment:\nHowever, the definition of the loss function uses the mathematical expectation operator E over all transitions, which can only be approximated by randomly sampling the distribution (the MDP). This implies that the samples concatenated in a minibatch should be independent from each other (i.i.d). When gathering transitions online, the samples are correlated: (s_t, a_t, r_{t+1}, s_{t+1}) will be followed by (s_{t+1}, a_{t+1}, r_{t+2}, s_{t+2}), etc. When playing video games, two successive frames will be very similar (a few pixels will change, or even none if the sampling rate is too high) and the optimal action will likely not change either (to catch the ball in pong, you will need to perform the same action - going left - many times in a row).\nCorrelated inputs/outputs are very bad for deep neural networks: the DNN will overfit and fall into a very bad local minimum. That is why stochastic gradient descent works so well: it randomly samples values from the training set to form minibatches and minimize the loss function on these uncorrelated samples (hopefully). If all samples of a minibatch were of the same class (e.g. zeros in MNIST), the network would converge poorly. This is the first problem preventing an easy use of deep neural networks as function approximators in RL.\nThe second major problem is the non-stationarity of the targets in the loss function. In classification or regression, the desired values \\mathbf{t} are fixed throughout learning: the class of an object does not change in the middle of the training phase.\n\\mathcal{L}(\\theta) = - \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}}[ ||\\mathbf{t} - \\mathbf{y}||^2]\nIn Q-learning, the target r(s, a, s') + \\gamma \\, \\max_{a'} Q_\\theta(s', a') will change during learning, as Q_\\theta(s', a') depends on the weights \\theta and will hopefully increase as the performance improves. This is the second problem of deep RL: deep NN are particularly bad on non-stationary problems, especially feedforward networks. They iteratively converge towards the desired value, but have troubles when the target also moves (like a dog chasing its tail).",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deep Q-network (DQN)</span>"
    ]
  },
  {
    "objectID": "src/2.3-DQN.html#limitations-of-deep-neural-networks-for-function-approximation",
    "href": "src/2.3-DQN.html#limitations-of-deep-neural-networks-for-function-approximation",
    "title": "Deep Q-network (DQN)",
    "section": "",
    "text": "Figure 8.1: Function approximators can either associate a state-action pair (s, a) to its Q-value (left), or associate a state s to the Q-values of all actions possible in that state (right).\n\n\n\n\n\n\n\n\n\nInitialize value network Q_{\\theta} with random weights.\nInitialize empty minibatch \\mathcal{D} of maximal size n.\nObserve the initial state s_0.\nfor t \\in [0, T_\\text{total}]:\n\nSelect the action a_t based on the behavior policy derived from Q_\\theta(s_t, a) (e.g. softmax).\nPerform the action a_t and observe the next state s_{t+1} and the reward r_{t+1}.\nPredict the Q-value of the greedy action in the next state \\max_{a'} Q_\\theta(s_{t+1}, a')\nStore (s_t, a_t, r_{t+1} + \\gamma \\, \\max_{a'} Q_\\theta(s_{t+1}, a')) in the minibatch.\nIf minibatch \\mathcal{D} is full:\n\nTrain the value network Q_{\\theta} on \\mathcal{D} to minimize \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D}[(r(s, a, s') + \\gamma \\, \\text{max}_{a'} Q_\\theta(s', a') - Q_\\theta(s, a))^2]\nEmpty the minibatch \\mathcal{D}.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deep Q-network (DQN)</span>"
    ]
  },
  {
    "objectID": "src/2.3-DQN.html#deep-q-network-dqn",
    "href": "src/2.3-DQN.html#deep-q-network-dqn",
    "title": "Deep Q-network (DQN)",
    "section": "Deep Q-Network (DQN)",
    "text": "Deep Q-Network (DQN)\nMnih et al. (2015) (originally arXived in Mnih et al. (2013)) proposed an elegant solution to the problems of correlated inputs/outputs and non-stationarity inherent to RL. This article is a milestone of deep RL and it is fair to say that it started or at least strongly renewed the interest for deep RL.\nThe first idea proposed by Mnih et al. (2015) solves the problem of correlated input/outputs and is actually quite simple: instead of feeding successive transitions into a minibatch and immediately training the NN on it, transitions are stored in a huge buffer called experience replay memory (ERM) or replay buffer able to store 100000 transitions. When the buffer is full, new transitions replace the old ones. SGD can now randomly sample the ERM to form minibatches and train the NN.\n\n\n\n\n\n\nFigure 8.2: Experience replay memory. Interactions with the environment are stored in the ERM. Random minibatches are sampled from it to train the DQN value network.\n\n\n\nThe second idea solves the non-stationarity of the targets r(s, a, s') + \\gamma \\, \\max_{a'} Q_\\theta(s', a'). Instead of computing it with the current parameters \\theta of the NN, they are computed with an old version of the NN called the target network with parameters \\theta'. The target network is updated only infrequently (every thousands of iterations or so) with the learned weights \\theta. As this target network does not change very often, the targets stay constant for a long period of time, and the problem becomes more stationary.\nThe resulting algorithm is called Deep Q-Network (DQN). It is summarized by the following pseudocode:\n\n\nInitialize value network Q_{\\theta} with random weights.\nCopy Q_{\\theta} to create the target network Q_{\\theta'}.\nInitialize experience replay memory \\mathcal{D} of maximal size N.\nObserve the initial state s_0.\nfor t \\in [0, T_\\text{total}]:\n\nSelect the action a_t based on the behavior policy derived from Q_\\theta(s_t, a) (e.g. softmax).\nPerform the action a_t and observe the next state s_{t+1} and the reward r_{t+1}.\nStore (s_t, a_t, r_{t+1}, s_{t+1}) in the experience replay memory.\nEvery T_\\text{train} steps:\n\nSample a minibatch \\mathcal{D}_s randomly from \\mathcal{D}.\nFor each transition (s, a, r, s') in the minibatch:\n\nPredict the Q-value of the greedy action in the next state \\max_{a'} Q_{\\theta'}(s', a') using the target network.\nCompute the target value y = r + \\gamma \\, \\max_{a'} Q_{\\theta'}(s', a').\n\nTrain the value network Q_{\\theta} on \\mathcal{D}_s to minimize \\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{D}_s}[(y - Q_\\theta(s, a))^2]\n\nEvery T_\\text{target} steps:\n\nUpdate the target network with the trained value network: \\theta' \\leftarrow \\theta\n\n\n\n\nIn this document, pseudocode will omit many details to simplify the explanations (for example here, the case where a state is terminal - the game ends - and the next state has to be chosen from the distribution of possible starting states). Refer to the original publication for more exact algorithms.\nThe first thing to notice is that experienced transitions are not immediately used for learning, but simply stored in the ERM to be sampled later. Due to the huge size of the ERM, it is even likely that the recently experienced transition will only be used for learning hundreds or thousands of steps later. Meanwhile, very old transitions, generated using an initially bad policy, can be used to train the network for a very long time.\nThe second thing is that the target network is not updated very often (T_\\text{target}=10000), so the target values are going to be wrong a long time. More recent algorithms such as DDPG use a smoothed version of the current weights, as proposed in Lillicrap et al. (2015):\n\n    \\theta' = \\tau \\, \\theta + (1-\\tau) \\, \\theta'\n\nIf this rule is applied after each step with a very small rate \\tau, the target network will slowly track the learned network, but never be the same.\nThese two facts make DQN extremely slow to learn: millions of transitions are needed to obtain a satisfying policy. This is called the sample complexity, i.e. the number of transitions needed to obtain a satisfying performance. DQN finds very good policies, but at the cost of a very long training time.\nDQN was initially applied to solve various Atari 2600 games. Video frames were used as observations and the set of possible discrete actions was limited (left/right/up/down, shoot, etc). The CNN used is depicted on Figure 8.3. It has two convolutional layers, no max-pooling, 2 fully-connected layer and one output layer representing the Q-value of all possible actions in the games.\n\n\n\n\n\n\nFigure 8.3: Architecture of the CNN used in the original DQN paper. Source: Mnih et al. (2015).\n\n\n\nThe problem of partial observability is solved by concatenating the four last video frames into a single tensor used as input to the CNN. The convolutional layers become able through learning to extract the speed information from it. Some of the Atari games (Pinball, Breakout) were solved with a performance well above human level, especially when they are mostly reactive. Games necessitating more long-term planning (Montezuma’ Revenge) were still poorly learned, though.\nBeside being able to learn using delayed and sparse rewards in highly dimensional input spaces, the true tour de force of DQN is that it was able to learn the 49 Atari games using the same architecture and hyperparameters, showing the generality of the approach.\n\n\n\n\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., et al. (2015). Continuous control with deep reinforcement learning. CoRR. Available at: http://arxiv.org/abs/1509.02971.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., et al. (2013). Playing Atari with Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1312.5602.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). Human-level control through deep reinforcement learning. Nature 518, 529–533. doi:10.1038/nature14236.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deep Q-network (DQN)</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html",
    "href": "src/2.4-DQNvariants.html",
    "title": "DQN variants",
    "section": "",
    "text": "Double DQN\nIn DQN, the experience replay memory and the target network were decisive in allowing the CNN to learn the tasks through RL. Their drawback is that they drastically slow down learning and increase the sample complexity. Additionally, DQN has stability issues: the same network may not converge the same way in different runs. One first improvement on DQN was proposed by van Hasselt et al. (2015) and called double DQN.\nThe idea is that the target value y = r(s, a, s') + \\gamma \\, \\max_{a'} Q_{\\theta'}(s', a') is frequently over-estimating the true return because of the max operator. Especially at the beginning of learning when Q-values are far from being correct, if an action is over-estimated (Q_{\\theta'}(s', a) is higher that its true value) and selected by the target network as the next greedy action, the learned Q-value Q_{\\theta}(s, a) will also become over-estimated, what will propagate to all previous actions on the long-term. van Hasselt (2010) showed that this over-estimation is inevitable in regular Q-learning and proposed double learning.\nThe idea is to train independently two value networks: one will be used to find the greedy action (the action with the maximal Q-value), the other to estimate the Q-value itself. Even if the first network choose an over-estimated action as the greedy action, the other might provide a less over-estimated value for it, solving the problem.\nApplying double learning to DQN is particularly straightforward: there are already two value networks, the trained network and the target network. Instead of using the target network to both select the greedy action in the next state and estimate its Q-value, here the trained network \\theta is used to select the greedy action a^* = \\text{argmax}_{a'} Q_\\theta (s', a') while the target network only estimates its Q-value. The target value becomes:\ny = r(s, a, s') + \\gamma \\, Q_{\\theta'}(s', \\text{argmax}_{a'} Q_\\theta (s', a'))\nThis induces only a small modification of the DQN algorithm and significantly improves its performance and stability:",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#double-dqn",
    "href": "src/2.4-DQNvariants.html#double-dqn",
    "title": "DQN variants",
    "section": "",
    "text": "Every T_\\text{train} steps:\n\nSample a minibatch \\mathcal{D}_s randomly from \\mathcal{D}.\nFor each transition (s, a, r, s') in the minibatch:\n\nSelect the greedy action in the next state a^* = \\text{argmax}_{a'} Q_\\theta (s', a') using the trained network.\nPredict its Q-value Q_{\\theta'}(s', a^*) using the target network.\nCompute the target value y = r + \\gamma \\, Q_{\\theta'}(s', a*).",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#prioritized-experience-replay",
    "href": "src/2.4-DQNvariants.html#prioritized-experience-replay",
    "title": "DQN variants",
    "section": "Prioritized experience replay",
    "text": "Prioritized experience replay\nAnother drawback of the original DQN is that the experience replay memory is sampled uniformly. Novel and interesting transitions are selected with the same probability as old well-predicted transitions, what slows down learning. The main idea of prioritized experience replay (Schaul et al., 2015) is to order the transitions in the experience replay memory in decreasing order of their TD error:\n\n    \\delta = r(s, a, s') + \\gamma \\, Q_{\\theta'}(s', \\text{argmax}_{a'} Q_\\theta (s', a')) - Q_\\theta(s, a)\n\nand sample with a higher probability those surprising transitions to form a minibatch. However, non-surprising transitions might become relevant again after enough training, as the Q_\\theta(s, a) change, so prioritized replay has a softmax function over the TD error to ensure “exploration” of memorized transitions. This data structure has of course a non-negligible computational cost, but accelerates learning so much that it is worth it. See https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/ for a presentation of double DQN with prioritized replay.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#duelling-network",
    "href": "src/2.4-DQNvariants.html#duelling-network",
    "title": "DQN variants",
    "section": "Duelling network",
    "text": "Duelling network\nThe classical DQN architecture uses a single NN to predict directly the value of all possible actions Q_\\theta(s, a). The value of an action depends on two factors:\n\nthe value of the underlying state s: in some states, all actions are bad, you lose whatever you do.\nthe interest of that action: some actions are better than others for a given state.\n\nThis leads to the definition of the advantage A^\\pi(s,a) of an action:\n\n    A^\\pi(s, a) = Q^\\pi(s, a) - V^\\pi(s)\n\\tag{9.1}\nThe advantage of the optimal action in s is equal to zero: the expected return in s is the same as the expected return when being in s and taking a, as the optimal policy will choose a in s anyway. The advantage of all other actions is negative: they bring less reward than the optimal action (by definition), so they are less advantageous. Note that this is only true if your estimate of V^\\pi(s) is correct.\nBaird (1993) has shown that it is advantageous to decompose the Q-value of an action into the value of the state and the advantage of the action (advantage updating):\n\n    Q^\\pi(s, a) = V^\\pi(s) + A^\\pi(s, a)\n\nIf you already know that the value of a state is very low, you do not need to bother exploring and learning the value of all actions in that state, they will not bring much. Moreover, the advantage function has less variance than the Q-values, which is a very good property when using neural networks for function approximation. The variance of the Q-values comes from the fact that they are estimated based on other estimates, which themselves evolve during learning (non-stationarity of the targets) and can drastically change during exploration (stochastic policies). The advantages only track the relative change of the value of an action compared to its state, what is going to be much more stable over time.\nThe range of values taken by the advantages is also much smaller than the Q-values. Let’s suppose we have two states with values -10 and 10, and two actions with advantages 0 and -1 (it does not matter which one). The Q-values will vary between -11 (the worst action in the worst state) and 10 (the best action in the best state), while the advantage only varies between -1 and 0. It is therefore going to be much easier for a neural network to learn the advantages than the Q-values, as they are theoretically not bounded.\n\n\n\n\n\n\nFigure 9.1: Duelling network architecture. Top: classical feedforward architecture to predict Q-values. Bottom: Duelling networks predicting state values and advantage functions to form the Q-values. Source: Wang et al. (2016).\n\n\n\nWang et al. (2016) incorporated the idea of advantage updating in a double DQN architecture with prioritized replay (Figure 9.1). As in DQN, the last layer represents the Q-values of the possible actions and has to minimize the mse loss:\n\n    \\mathcal{L}(\\theta) = \\mathbb{E}_\\pi([r(s, a, s') + \\gamma \\, Q_{\\theta', \\alpha', \\beta'}(s', \\text{argmax}_{a'} Q_{\\theta, \\alpha, \\beta} (s', a')) - Q_{\\theta, \\alpha, \\beta}(s, a)]^2)\n\nThe difference is that the previous fully-connected layer is forced to represent the value of the input state V_{\\theta, \\beta}(s) and the advantage of each action A_{\\theta, \\alpha}(s, a) separately. There are two separate sets of weights in the network, \\alpha and \\beta, to predict these two values, sharing representations from the early convolutional layers through weights \\theta. The output layer performs simply a parameter-less summation of both sub-networks:\n\n    Q_{\\theta, \\alpha, \\beta}(s, a) = V_{\\theta, \\beta}(s) + A_{\\theta, \\alpha}(s, a)\n\nThe issue with this formulation is that one could add a constant to V_{\\theta, \\beta}(s) and substract it from A_{\\theta, \\alpha}(s, a) while obtaining the same result. An easy way to constrain the summation is to normalize the advantages, so that the greedy action has an advantage of zero as expected:\n\n    Q_{\\theta, \\alpha, \\beta}(s, a) = V_{\\theta, \\beta}(s) + (A_{\\theta, \\alpha}(s, a) - \\max_a A_{\\theta, \\alpha}(s, a))\n\nBy doing this, the advantages are still free, but the state value will have to take the correct value. Wang et al. (2016) found that it is actually better to replace the \\max operator by the mean of the advantages. In this case, the advantages only need to change as fast as their mean, instead of having to compensate quickly for any change in the greedy action as the policy improves:\n\n    Q_{\\theta, \\alpha, \\beta}(s, a) = V_{\\theta, \\beta}(s) + (A_{\\theta, \\alpha}(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_a A_{\\theta, \\alpha}(s, a))\n\nApart from this specific output layer, everything works as usual, especially the gradient of the mse loss function can travel backwards using backpropagation to update the weights \\theta, \\alpha and \\beta. The resulting architecture outperforms double DQN with prioritized replay on most Atari games, particularly games with repetitive actions.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#rainbow-dqn",
    "href": "src/2.4-DQNvariants.html#rainbow-dqn",
    "title": "DQN variants",
    "section": "Rainbow DQN",
    "text": "Rainbow DQN\nAs we have seen. the original formulation of DQN (Mnih et al., 2015) has seen many improvements over the years.\n\nDouble DQN (van Hasselt et al., 2015) separates the selection of the greedy action in the next state from its evaluation in order to prevent over-estimation of Q-values:\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(s´, \\text{argmax}_{a'} Q_{\\theta}(s', a')) - Q_\\theta(s, a))^2]\n\nPrioritized Experience Replay (Schaul et al., 2015) selects transitions from the ERM proportionally to their current TD error:\n\nP(k) = \\frac{(|\\delta_k| + \\epsilon)^\\alpha}{\\sum_k (|\\delta_k| + \\epsilon)^\\alpha}\n\nDueling DQN (Wang et al., 2016) splits learning of Q-values into learning of advantages and state values:\n\nQ_\\theta(s, a) = V_\\alpha(s) + A_\\beta(s, a)\n\nCategorical DQN (Bellemare et al., 2017) learns the distribution of returns instead of their expectation:\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{D}_s}[ - \\mathbf{t}_k \\, \\log Z_\\theta(s_k, a_k)]\n\nn-step returns (Sutton and Barto, 2017) reduce the bias of the estimation by taking the next n rewards into account, at the cost of a slightly higher variance.\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(\\sum_{k=1}^n r_{t+k} + \\gamma \\max_a Q_\\theta(s_{t+n+1}, a) - Q_\\theta(s_t, a_t))^2\n\nNoisy DQN (Fortunato et al., 2017) ensures exploration by adding noise to the parameters of the network instead of a softmax / \\epsilon-greedy action selection over the Q-values.\n\nAll these improvements exceed the performance of vanilla DQN on most if not all Atari game. But which ones are the most important?\nHessel et al. (2017) designed a Rainbow DQN integrating all these improvements. Not only does the combined network outperform all the DQN variants, but each of its components is important for its performance as shown by ablation studies (apart from double learning and duelling networks), see Figure 9.2.\n\n\n\n\n\n\nFigure 9.2: Performance of the Rainbow DQN compared to other DQN variants (left) and ablation studies. Figures Source: Hessel et al. (2017).",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#distributed-dqn-gorila",
    "href": "src/2.4-DQNvariants.html#distributed-dqn-gorila",
    "title": "DQN variants",
    "section": "Distributed DQN (GORILA)",
    "text": "Distributed DQN (GORILA)\nThe main limitation of deep RL is the slowness of learning, which is mainly influenced by two factors:\n\nthe sample complexity, i.e. the number of transitions needed to learn a satisfying policy.\nthe online interaction with the environment (states are visited one after the other).\n\nThe second factor is particularly critical in real-world applications like robotics: physical robots evolve in real time, so the acquisition speed of transitions will be limited. Even in simulation (video games, robot emulators), the simulator might turn out to be much slower than training the underlying neural network. Google Deepmind proposed the GORILA (General Reinforcement Learning Architecture) framework to speed up the training of DQN networks using distributed actors and learners (Nair et al., 2015). The framework is quite general and the distribution granularity can change depending on the task.\n\n\n\n\n\n\nFigure 9.3: GORILA architecture. Multiple actors interact with multiple copies of the environment and store their experiences in a (distributed) experience replay memory. Multiple DQN learners sample from the ERM and compute the gradient of the loss function w.r.t the parameters \\theta. A master network (parameter server, possibly distributed) gathers the gradients, apply weight updates and synchronizes regularly both the actors and the learners with new parameters. Source: Nair et al. (2015).\n\n\n\nIn GORILA, multiple actors interact with the environment to gather transitions. Each actor has an independent copy of the environment, so they can gather N times more samples per second if there are N actors. This is possible in simulation (starting N instances of the same game in parallel) but much more complicated for real-world systems (but see Gu et al. (2017) for an example where multiple identical robots are used to gather experiences in parallel).\nThe experienced transitions are sent as in DQN to an experience replay memory, which may be distributed or centralized. Multiple DQN learners will then sample a minibatch from the ERM and compute the DQN loss on this minibatch (also using a target network). All learners start with the same parameters \\theta and simply compute the gradient of the loss function \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta} on the minibatch. The gradients are sent to a parameter server (a master network) which uses the gradients to apply the optimizer (e.g. SGD) and find new values for the parameters \\theta. Weight updates can also be applied in a distributed manner. This distributed method to train a network using multiple learners is now quite standard in deep learning: on multiple GPU systems, each GPU has a copy of the network and computes gradients on a different minibatch, while a master network integrates these gradients and updates the slaves.\nThe parameter server regularly updates the actors (to gather samples with the new policy) and the learners (to compute gradients w.r.t the new parameter values). Such a distributed system can greatly accelerate learning, but it can be quite tricky to find the optimum number of actors and learners (too many learners might degrade the stability) or their update rate (if the learners are not updated frequently enough, the gradients might not be correct).\nFurther variants of distributed DQN learning include Ape-X (Horgan et al., 2018) and IMPALA (Espeholt et al., 2018). A similar idea is at the core of the A3C algorithm, which is a policy gradient method.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#deep-recurrent-q-learning-drqn",
    "href": "src/2.4-DQNvariants.html#deep-recurrent-q-learning-drqn",
    "title": "DQN variants",
    "section": "Deep Recurrent Q-learning (DRQN)",
    "text": "Deep Recurrent Q-learning (DRQN)\nThe Atari games used as a benchmark for value-based methods are partially observable MDPs (POMDP), i.e. a single frame does not contain enough information to predict what is going to happen next (e.g. the speed and direction of the ball on the screen is not known). In DQN, partial observability is solved by stacking four consecutive frames and using the resulting tensor as an input to the CNN. if this approach worked well for most Atari games, it has several limitations (as explained in https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc):\n\nIt increases the size of the experience replay memory, as four video frames have to be stored for each transition.\nIt solves only short-term dependencies (instantaneous speeds). If the partial observability has long-term dependencies (an object has been hidden a long time ago but now becomes useful), the input to the neural network will not have that information. This is the main explanation why the original DQN performed so poorly on games necessitating long-term planning like Montezuma’s revenge.\n\nBuilding on previous ideas from the Schmidhuber’s group (Bakker, 2001; Wierstra et al., 2007), Hausknecht and Stone (2015) replaced one of the fully-connected layers of the DQN network by a LSTM layer while using single frames as inputs. The resulting deep recurrent q-learning (DRQN) network became able to solve POMDPs thanks to the astonishing learning abilities of LSTMs: the LSTM layer learn to remember which part of the sensory information will be useful to take decisions later.\nHowever, LSTMs are not a magical solution either. They are trained using truncated BPTT, i.e. on a limited history of states. Long-term dependencies exceeding the truncation horizon cannot be learned. Additionally, all states in that horizon (i.e. all frames) have to be stored in the ERM to train the network, increasing drastically its size. Despite these limitations, DRQN is a much more elegant solution to the partial observability problem, letting the network decide which horizon it needs to solve long-term dependencies.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#recurrent-replay-distributed-dqn-r2d2",
    "href": "src/2.4-DQNvariants.html#recurrent-replay-distributed-dqn-r2d2",
    "title": "DQN variants",
    "section": "Recurrent Replay Distributed DQN (R2D2)",
    "text": "Recurrent Replay Distributed DQN (R2D2)\nKapturowski et al. (2019)",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants</span>"
    ]
  },
  {
    "objectID": "src/2.4-DQNvariants.html#other-variants-of-dqn",
    "href": "src/2.4-DQNvariants.html#other-variants-of-dqn",
    "title": "DQN variants",
    "section": "Other variants of DQN",
    "text": "Other variants of DQN\nAverage-DQN (Anschel et al., 2016) proposes to increase the stability and performance of DQN by replacing the single target network (a copy of the trained network) by an average of the last parameter values, in other words an average of many past target networks.\nHe et al. (2016) proposed fast reward propagation through optimality tightening to speedup learning: when rewards are sparse, they require a lot of episodes to propagate these rare rewards to all actions leading to it. Their method combines immediate rewards (single steps) with actual returns (as in Monte Carlo) via a constrained optimization approach.\n\n\n\n\nAnschel, O., Baram, N., and Shimkin, N. (2016). Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1611.01929.\n\n\nBaird, L. C. (1993). Advantage updating. Wright-Patterson Air Force Base Available at: http://leemon.com/papers/1993b.pdf.\n\n\nBakker, B. (2001). Reinforcement Learning with Long Short-Term Memory. in Advances in Neural Information Processing Systems 14 (NIPS 2001), 1475–1482. Available at: https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory.\n\n\nBellemare, M. G., Dabney, W., and Munos, R. (2017). A Distributional Perspective on Reinforcement Learning. Available at: http://arxiv.org/abs/1707.06887.\n\n\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., et al. (2018). IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. doi:10.48550/arXiv.1802.01561.\n\n\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., et al. (2017). Noisy Networks for Exploration. Available at: http://arxiv.org/abs/1706.10295 [Accessed March 2, 2020].\n\n\nGu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates. in Proc. ICRA Available at: http://arxiv.org/abs/1610.00633.\n\n\nHausknecht, M., and Stone, P. (2015). Deep Recurrent Q-Learning for Partially Observable MDPs. Available at: http://arxiv.org/abs/1507.06527.\n\n\nHe, F. S., Liu, Y., Schwing, A. G., and Peng, J. (2016). Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening. Available at: http://arxiv.org/abs/1611.01606.\n\n\nHessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., et al. (2017). Rainbow: Combining Improvements in Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1710.02298.\n\n\nHorgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van Hasselt, H., et al. (2018). Distributed Prioritized Experience Replay. Available at: http://arxiv.org/abs/1803.00933 [Accessed December 14, 2019].\n\n\nKapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W. (2019). Recurrent experience replay in distributed reinforcement learning. in, 19. Available at: https://openreview.net/pdf?id=r1lyTjAqYX.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). Human-level control through deep reinforcement learning. Nature 518, 529–533. doi:10.1038/nature14236.\n\n\nNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., et al. (2015). Massively Parallel Methods for Deep Reinforcement Learning. Available at: https://arxiv.org/pdf/1507.04296.pdf.\n\n\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized Experience Replay. Available at: http://arxiv.org/abs/1511.05952.\n\n\nSutton, R. S., and Barto, A. G. (2017). Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press Available at: http://incompleteideas.net/book/the-book-2nd.html.\n\n\nvan Hasselt, H. (2010). Double Q-learning. in Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2 (Curran Associates Inc.), 2613–2621. Available at: https://dl.acm.org/citation.cfm?id=2997187.\n\n\nvan Hasselt, H., Guez, A., and Silver, D. (2015). Deep Reinforcement Learning with Double Q-learning. Available at: http://arxiv.org/abs/1509.06461.\n\n\nWang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de Freitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1511.06581 [Accessed November 21, 2019].\n\n\nWierstra, D., Foerster, A., Peters, J., and Schmidhuber, J. (2007). “Solving Deep Memory POMDPs with Recurrent Policy Gradients,” in (Springer, Berlin, Heidelberg), 697–706. doi:10.1007/978-3-540-74690-4_71.",
    "crumbs": [
      "**Value-based deep RL**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DQN variants</span>"
    ]
  },
  {
    "objectID": "src/3.1-PolicyGradient.html",
    "href": "src/3.1-PolicyGradient.html",
    "title": "Policy Gradient methods",
    "section": "",
    "text": "REINFORCE",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Policy Gradient methods</span>"
    ]
  },
  {
    "objectID": "src/3.1-PolicyGradient.html#reinforce",
    "href": "src/3.1-PolicyGradient.html#reinforce",
    "title": "Policy Gradient methods",
    "section": "",
    "text": "Estimating the policy gradient\nWilliams (1992) proposed a useful estimate of the policy gradient. Considering that the return R(\\tau) of a trajectory does not depend on the parameters \\theta, one can simplify the policy gradient in the following way:\n\n    \\nabla_\\theta J(\\theta) = \\nabla_\\theta \\int_\\tau \\rho_\\theta (\\tau) \\, R(\\tau) \\, d\\tau =  \\int_\\tau (\\nabla_\\theta \\rho_\\theta (\\tau)) \\, R(\\tau) \\, d\\tau\n\nWe now use the log-trick, a simple identity based on the fact that:\n\n    \\frac{d \\log f(x)}{dx} = \\frac{f'(x)}{f(x)}\n\nto rewrite the policy gradient of a single trajectory:\n\n    \\nabla_\\theta \\rho_\\theta (\\tau) = \\rho_\\theta (\\tau) \\, \\nabla_\\theta \\log \\rho_\\theta (\\tau)\n\nThe policy gradient becomes:\n\n    \\nabla_\\theta J(\\theta) =  \\int_\\tau \\rho_\\theta (\\tau) \\, \\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, R(\\tau) \\, d\\tau\n\nwhich now has the form of a mathematical expectation:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[ \\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, R(\\tau) ]\n\nThis means that we can obtain an estimate of the policy gradient by simply sampling different trajectories \\{\\tau_i\\} and averaging \\nabla_\\theta \\log \\rho_\\theta (\\tau_i) \\, R(\\tau_i) (Monte Carlo sampling).\nLet’s now look further at how the gradient of the log-likelihood of a trajectory \\log \\pi_\\theta (\\tau) look like. Through its definition (Equation 10.1), the log-likelihood of a trajectory is:\n\n    \\log \\rho_\\theta(\\tau) = \\log p_0 (s_0) + \\sum_{t=0}^T \\log \\pi_\\theta(s_t, a_t) + \\sum_{t=0}^T \\log p(s_{t+1} | s_t, a_t)\n\\tag{10.2}\n\\log p_0 (s_0) and \\log p(s_{t+1} | s_t, a_t) do not depend on the parameters \\theta (they are defined by the MDP), so the gradient of the log-likelihood is simply:\n\n    \\nabla_\\theta \\log \\rho_\\theta(\\tau) = \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)\n\\tag{10.3}\n\\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) is called the score function.\nThis is the main reason why policy gradient algorithms are used: the gradient is independent from the MDP dynamics, allowing model-free learning. The policy gradient is then given by:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, R(\\tau) ] =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, (\\sum_{t=0}^T \\gamma^t r_{t+1})]\n\nEstimating the policy gradient now becomes straightforward using Monte Carlo sampling. The resulting algorithm is called the REINFORCE algorithm (Williams, 1992):\n\n\nwhile not converged:\n\nSample N trajectories \\{\\tau_i\\} using the current policy \\pi_\\theta and observe the returns \\{R(\\tau_i)\\}.\nEstimate the policy gradient as an average over the trajectories:\n\n\n     \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, R(\\tau_i)\n  \n\nUpdate the policy using gradient ascent:\n\n\n      \\theta \\leftarrow \\theta + \\eta \\, \\nabla_\\theta J(\\theta)\n  \n\n\nWhile very simple, the REINFORCE algorithm does not work very well in practice:\n\nThe returns \\{R(\\tau_i)\\} have a very high variance (as the Q-values in value-based methods), which is problematic for NNs.\nIt requires a lot of episodes to converge (sample inefficient).\nIt only works with online learning: trajectories must be frequently sampled and immediately used to update the policy.\nThe problem must be episodic (T finite).\n\nHowever, it has two main advantages:\n\nIt is a model-free method, i.e. one does not need to know anything about the MDP.\nIt also works on partially observable problems (POMDP): as the return is computed over complete trajectories, it does not matter if the states are not Markovian.\n\nThe methods presented in this section basically try to solve the limitations of REINFORCE (high variance, sample efficiency, online learning) to produce efficient policy gradient algorithms.\n\n\nReducing the variance\nThe main problem with the REINFORCE algorithm is the high variance of the policy gradient. This variance comes from the fact that we learn stochastic policies (it is often unlikely to generate twice the exact same trajectory) in stochastic environments (rewards are stochastic, the same action in the same state may receive). Two trajectories which are identical at the beginning will be associated with different returns depending on the stochasticity of the policy, the transition probabilities and the probabilistic rewards.\nConsider playing a game like chess with always the same opening, and then following a random policy. You may end up winning (R=1) or losing (R=-1) with some probability. The initial actions of the opening will receive a policy gradient which is sometimes positive, sometimes negative: were these actions good or bad? Should they be reinforced? In supervised learning, this would mean that the same image of a cat will be randomly associated to the labels “cat” or “dog” during training: the NN will not like it.\nIn supervised learning, there is no problem of variance in the outputs, as training sets are fixed. This is in contrary very hard to ensure in deep RL and constitutes one of its main limitations. The only direct solution is to sample enough trajectories and hope that the average will be able to smooth the variance. The problem is even worse in the following conditions:\n\nHigh-dimensional action spaces: it becomes difficult to sample the environment densely enough if many actions are possible.\nLong horizons: the longer the trajectory, the more likely it will be unique.\nFinite samples: if we cannot sample enough trajectories, the high variance can introduce a bias in the gradient, leading to poor convergence.\n\nSee https://medium.com/mlreview/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565 for a nice explanation of the bias/variance trade-off in deep RL.\nAnother related problem is that the REINFORCE gradient is sensitive to reward scaling. Let’s consider a simple MDP where only two trajectories \\tau_1 and \\tau_2 are possible. Depending on the choice of the reward function, the returns may be different:\n\nR(\\tau_1) = 1 and R(\\tau_2) = -1\nR(\\tau_1) = 3 and R(\\tau_2) = 1\n\nIn both cases, the policy should select the trajectory \\tau_1. However, the policy gradient for \\tau_2 will change its sign between the two cases, although the problem is the same! What we want to do is to maximize the returns, regardless the absolute value of the rewards, but the returns are unbounded. Because of the non-stationarity of the problem (the agent becomes better with training, so the returns of the sampled trajectories will increase), the policy gradients will increase over time, what is linked to the variance problem. Value-based methods addressed this problem by using target networks, but it is not a perfect solution (the gradients become biased).\nA first simple but effective idea to solve both problems would be to subtract the mean of the sampled returns from the returns:\n\n\nwhile not converged:\n\nSample N trajectories \\{\\tau_i\\} using the current policy \\pi_\\theta and observe the returns \\{R(\\tau_i)\\}.\nCompute the mean return: \n  \\hat{R} = \\frac{1}{N} \\sum_{i=1}^N R(\\tau_i)\n  \nEstimate the policy gradient as an average over the trajectories: \n\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, ( R(\\tau_i) - \\hat{R})\n  \nUpdate the policy using gradient ascent: \n  \\theta \\leftarrow \\theta + \\eta \\, \\nabla_\\theta J(\\theta)\n  \n\n\n\nThis obviously solves the reward scaling problem, and reduces the variance of the gradients. But are we allowed to do this (i.e. does it introduce a bias to the gradient)? Williams (1992) showed that subtracting a constant b from the returns still leads to an unbiased estimate of the gradient:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, (R(\\tau) -b) ]\n\nThe proof is actually quite simple:\n\n    \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, b ] = \\int_\\tau \\rho_\\theta (\\tau) \\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, b \\, d\\tau = \\int_\\tau \\nabla_\\theta  \\rho_\\theta (\\tau) \\, b \\, d\\tau = b \\, \\nabla_\\theta \\int_\\tau \\rho_\\theta (\\tau) \\, d\\tau =  b \\, \\nabla_\\theta 1 = 0\n\nAs long as the constant b does not depend on \\theta, the estimator is unbiased. The resulting algorithm is called REINFORCE with baseline. Williams (1992) has actually showed that the best baseline (the one which also reduces the variance) is the mean return weighted by the square of the gradient of the log-likelihood:\n\n    b = \\frac{\\mathbb{E}_{\\tau \\sim \\rho_\\theta}[(\\nabla_\\theta \\log \\rho_\\theta (\\tau))^2 \\, R(\\tau)]}{\\mathbb{E}_{\\tau \\sim \\rho_\\theta}[(\\nabla_\\theta \\log \\rho_\\theta (\\tau))^2]}\n\nbut the mean reward actually work quite well. Advantage actor-critic methods replace the constant b with an estimate of the value of each state \\hat{V}(s_t).\n\n\nPolicy Gradient theorem\nLet’s have another look at the REINFORCE estimate of the policy gradient after sampling:\n\n   \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, R(\\tau_i) = \\frac{1}{N} \\sum_{i=1}^N (\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) ) \\, (\\sum_{t'=0}^T \\gamma^{t'} \\, r(s_{t'}, a_{t'}, s_{t'+1}) )\n\nFor each transition (s_t, a_t), the gradient of its log-likelihood (score function) \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) ) is multiplied by the return of the whole episode R(\\tau) = \\sum_{t'=0}^T \\gamma^{t'} \\, r(s_{t'}, a_{t'}, s_{t'+1}). However, the causality principle dictates that the reward received at t=0 does not depend on actions taken in the future, so we can simplify the return for each transition:\n\n\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N (\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)  \\, \\sum_{t'=t}^T \\gamma^{t'-t} \\, r(s_{t'}, a_{t'}, s_{t'+1}) )\n\nThe quantity \\hat{Q}(s_t, a_t) = \\sum_{t'=t}^T \\gamma^{t'-t} \\, r(s_{t'}, a_{t'}, s_{t'+1}) is called the reward to-go from the transition (s_t, a_t), i.e. the discounted sum of future rewards after that transition. Quite obviously, the Q-value of that action is the mathematical expectation of this reward to-go.\n\n\n\n\n\n\nFigure 10.1: The reward to-go is the sum of rewards gathered during a single trajectory after a transition (s, a). The Q-value of the action (s, a) is the expectation of the reward to-go. Source: S. Levine’s lecture http://rll.berkeley.edu/deeprlcourse/.\n\n\n\n\n\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, \\hat{Q}(s_t, a_t)\n\nSutton et al. (1999) showed that the policy gradient can be estimated by replacing the return of the sampled trajectory with the Q-value of each action, what leads to the policy gradient theorem (Equation 10.4):\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)]\n\\tag{10.4}\nwhere \\rho_\\theta is the distribution of states reachable under the policy \\pi_\\theta. Because the actual return R(\\tau) is replaced by its expectation Q^{\\pi_\\theta}(s, a), the policy gradient is now a mathematical expectation over single transitions instead of complete trajectories, allowing bootstrapping as in temporal difference methods.\nOne clearly sees that REINFORCE is actually a special case of the policy gradient theorem, where the Q-value of an action replaces the return obtained during the corresponding trajectory.\nThe problem is of course that the true Q-value of the actions is as unknown as the policy. However, Sutton et al. (1999) showed that it is possible to estimate the Q-values with a function approximator Q_\\varphi(s, a) with parameters \\varphi and obtain an unbiased estimation:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q_\\varphi(s, a))]\n\\tag{10.5}\nFormally, the Q-value approximator must respect the Compatible Function Approximation Theorem, which states that the value approximator must be compatible with the policy (\\nabla_\\varphi Q_\\varphi(s, a) = \\nabla_\\theta \\log \\pi_\\theta(s, a)) and minimize the mean-square error with the true Q-values \\mathbb{E}_{s \\sim \\rho^\\pi, a \\sim \\pi_\\theta} [(Q^{\\pi_\\theta}(s, a) - Q_\\varphi(s, a))^2]. In the algorithms presented in this section, these conditions are either met or neglected.\nThe resulting algorithm belongs to the actor-critic class, in the sense that:\n\nThe actor \\pi_\\theta(s, a) learns to approximate the policy by maximizing Equation 10.5.\nThe critic Q_\\varphi(s, a) learns to estimate the policy by minimizing the mse with the true Q-values.\n\nFigure 10.2 shows the architecture of the algorithm. The only problem left is to provide the critic with the true Q-values.\n\n\n\n\n\n\nFigure 10.2: Architecture of the policy gradient (PG) method.\n\n\n\nMost policy-gradient algorithms (A3C, DPPG, TRPO) are actor-critic architectures. Some remarks already:\n\nTrajectories now appear only implicitly in the policy gradient, one can even sample single transitions. It should therefore be possible (with modifications) to do off-policy learning, for example with using importance sampling or a replay buffer of stored transitions as in DQN (see ACER). REINFORCE works strictly on-policy.\nThe policy gradient theorem suffers from the same high variance problem as REINFORCE. The different algorithms presented later are principally attempts to solve this problem and reduce the sample complexity: advantages, deterministic policies, natural gradients…\nThe actor and the critic can be completely separated, or share some parameters.\n\n\n\n\n\nPeters, J., and Schaal, S. (2008). Reinforcement learning of motor skills with policy gradients. Neural Networks 21, 682–697. doi:10.1016/j.neunet.2008.02.003.\n\n\nSutton, R. S., McAllester, D., Singh, S., and Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. in Proceedings of the 12th International Conference on Neural Information Processing Systems (MIT Press), 1057–1063. Available at: https://dl.acm.org/citation.cfm?id=3009806.\n\n\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8, 229–256.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Policy Gradient methods</span>"
    ]
  },
  {
    "objectID": "src/3.2-ActorCritic.html",
    "href": "src/3.2-ActorCritic.html",
    "title": "Asynchronous Advantage Actor-Critic (A3C)",
    "section": "",
    "text": "Actor-critic algorithms\nThe policy gradient theorem provides an actor-critic architecture able to learn parameterized policies. In comparison to REINFORCE, the policy gradient depends on the Q-values of the actions taken during the trajectory rather than on the obtained returns R(\\tau). Quite obviously, it will also suffer from the high variance of the gradient, requiring the use of baselines. In this section, the baseline is state-dependent and chosen equal to the value of the state V^\\pi(s), so the factor multiplying the log-likelihood of the policy is:\nA^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^\\pi(s)\nwhich is the advantage of the action a in s, as already seen in duelling networks.\nNow the problem is that the critic would have to approximate two functions: Q^{\\pi}(s, a) and V^{\\pi}(s). Advantage actor-critic methods presented in this section (A2C, A3C, GAE) approximate the advantage of an action:\n\\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho^\\pi, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, A_\\varphi(s, a)]\nA_\\varphi(s, a) is called the advantage estimate and should be equal to the real advantage in expectation.\nDifferent methods could be used to compute the advantage estimate:\nThe most popular approach is the n-step advantage, which is at the core of the methods A2C and A3C, and can be understood as a trade-off between MC and TD. MC and TD advantages could be used as well, but come with the respective disadvantages of MC (need for finite episodes, slow updates) and TD (unstable). Generalized Advantage Estimation (GAE) takes another interesting approach to estimate the advantage.\nNote: A2C is actually derived from the A3C algorithm presented later, but it is simpler to explain it first. See https://openai.com/index/openai-baselines-acktr-a2c/ for an explanation of the reasons. A good explanation of A2C and A3C with Python code is available at https://cgnicholls.github.io/reinforcement-learning/2017/03/27/a3c.html.",
    "crumbs": [
      "**Policy-gradient methods**",
      "Asynchronous Advantage Actor-Critic (A3C)"
    ]
  },
  {
    "objectID": "src/3.2-ActorCritic.html#actor-critic-algorithms",
    "href": "src/3.2-ActorCritic.html#actor-critic-algorithms",
    "title": "Asynchronous Advantage Actor-Critic (A3C)",
    "section": "",
    "text": "A_\\varphi(s, a) =  R(s, a) - V_\\varphi(s) is the MC advantage estimate, the Q-value of the action being replaced by the actual return.\nA_\\varphi(s, a) =   r(s, a, s') + \\gamma \\, V_\\varphi(s') - V_\\varphi(s) is the TD advantage estimate or TD error.\nA_\\varphi(s, a) =   \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n+1}) - V_\\varphi(s_t) is the n-step advantage estimate.\n\n\n\n\nAdvantage Actor-Critic (A2C)\nThe first aspect of A2C is that it relies on n-step updating, which is a trade-off between MC and TD:\n\nMC waits until the end of an episode to update the value of an action using the reward to-go (sum of obtained rewards) R(s, a).\nTD updates immediately the action using the immediate reward r(s, a, s') and approximates the rest with the value of the next state V^\\pi(s).\nn-step uses the n next immediate rewards and approximates the rest with the value of the state visited n steps later.\n\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho^\\pi, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, ( \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n+1}) - V_\\varphi(s_t))]\n\\tag{1}\nTD can be therefore be seen as a 1-step algorithm. For sparse rewards (mostly zero, +1 or -1 at the end of a game for example), this allows to update the n last actions which lead to a win/loss, instead of only the last one in TD, speeding up learning. However, there is no need for finite episodes as in MC. In other words, n-step estimation ensures a trade-off between bias (wrong updates based on estimated values as in TD) and variance (variability of the obtained returns as in MC). An alternative to n-step updating is the use of eligibility traces (see Sutton and Barto (1998)).\nA2C has an actor-critic architecture:\n\nThe actor outputs the policy \\pi_\\theta for a state s, i.e. a vector of probabilities for each action.\nThe critic outputs the value V_\\varphi(s) of a state s.\n\n\n\n\n\n\n\nFigure 1: Advantage actor-critic architecture.\n\n\n\nHaving a computable formula for the policy gradient, the algorithm is rather simple:\n\nAcquire a batch of transitions (s, a, r, s') using the current policy \\pi_\\theta (either a finite episode or a truncated one).\nFor each state encountered, compute the discounted sum of the next n rewards \\sum_{k=0}^{n} \\gamma^{k} \\, r_{t+k+1} and use the critic to estimate the value of the state encountered n steps later V_\\varphi(s_{t+n+1}).\n\n\n    R_t = \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n+1})\n\n\nUpdate the actor using Equation 1.\n\n\n    \\nabla_\\theta J(\\theta) =  \\sum_t \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, (R_t - V_\\varphi(s_t))\n\n\nUpdate the critic to minimize the TD error between the estimated value of a state and its true value.\n\n\n    \\mathcal{L}(\\varphi) = \\sum_t (R_t - V_\\varphi(s_t))^2\n\n\nRepeat.\n\nThis is not very different in essence from REINFORCE (sample transitions, compute the return, update the policy), apart from the facts that episodes do not need to be finite and that a critic has to be learned in parallel. A more detailed pseudo-algorithm for a single A2C learner is the following:\n\n\nInitialize the actor \\pi_\\theta and the critic V_\\varphi with random weights.\nObserve the initial state s_0.\nfor t \\in [0, T_\\text{total}]:\n\nInitialize empty episode minibatch.\nfor k \\in [0, n]: # Sample episode\n\nSelect a action a_k using the actor \\pi_\\theta.\nPerform the action a_k and observe the next state s_{k+1} and the reward r_{k+1}.\nStore (s_k, a_k, r_{k+1}) in the episode minibatch.\n\nif s_n is not terminal: set R = V_\\varphi(s_n) with the critic, else R=0.\nReset gradient d\\theta and d\\varphi to 0.\nfor k \\in [n-1, 0]: # Backwards iteration over the episode\n\nUpdate the discounted sum of rewards R = r_k + \\gamma \\, R\nAccumulate the policy gradient using the critic: \n  d\\theta \\leftarrow d\\theta + \\nabla_\\theta \\log \\pi_\\theta(s_k, a_k) \\, (R - V_\\varphi(s_k))\n  \nAccumulate the critic gradient: \n  d\\varphi \\leftarrow d\\varphi + \\nabla_\\varphi (R - V_\\varphi(s_k))^2\n  \n\nUpdate the actor and the critic with the accumulated gradients using gradient descent or similar: \n  \\theta \\leftarrow \\theta + \\eta \\, d\\theta \\qquad \\varphi \\leftarrow \\varphi + \\eta \\, d\\varphi\n  \n\n\n\nNote that not all states are updated with the same horizon n: the last action encountered in the sampled episode will only use the last reward and the value of the final state (TD learning), while the very first action will use the n accumulated rewards. In practice it does not really matter, but the choice of the discount rate \\gamma will have a significant influence on the results.\nAs many actor-critic methods, A2C performs online learning: a couple of transitions are explored using the current policy, which is immediately updated. As for value-based networks (e.g. DQN), the underlying NN will be affected by the correlated inputs and outputs: a single batch contains similar states and action (e.g. consecutive frames of a video game). The solution retained in A2C and A3C does not depend on an experience replay memory as DQN, but rather on the use of multiple parallel actors and learners.\nThe idea is depicted on Figure 2 (actually for A3C, but works with A2C). The actor and critic are stored in a global network. Multiple instances of the environment are created in different parallel threads (the workers or actor-learners). At the beginning of an episode, each worker receives a copy of the actor and critic weights from the global network. Each worker samples an episode (starting from different initial states, so the episodes are uncorrelated), computes the accumulated gradients and sends them back to the global network. The global networks merges the gradients and uses them to update the parameters of the policy and critic networks. The new parameters are send to each worker again, until it converges.\n\n\nInitialize the actor \\pi_\\theta and the critic V_\\varphi in the global network.\nrepeat:\n\nfor each worker i in parallel:\n\nGet a copy of the global actor \\pi_\\theta and critic V_\\varphi.\nSample an episode of n steps.\nReturn the accumulated gradients d\\theta_i and d\\varphi_i.\n\nWait for all workers to terminate.\nMerge all accumulated gradients into d\\theta and d\\varphi.\nUpdate the global actor and critic networks.\n\n\n\nThis solves the problem of correlated inputs and outputs, as each worker explores different regions of the environment (one can set different initial states in each worker, vary the exploration rate, etc), so the final batch of transitions used for training the global networks is much less correlated. The only drawback of this approach is that it has to be possible to explore multiple environments in parallel. This is easy to achieve in simulated environments (e.g. video games) but much harder in real-world systems like robots. A brute-force solution for robotics is simply to buy enough robots and let them learn in parallel (Gu et al., 2017).\n\n\nAsynchronous Advantage Actor-Critic (A3C)\n\n\n\n\n\n\nFigure 2: Architecture of A3C. A master network interacts asynchronously with several workers, each having a copy of the network and interacting with a separate environment. At the end of an episode, the accumulated gradients are sent back to the master network, and a new value of the parameters is sent to the workers. Source: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2.\n\n\n\nAsynchronous Advantage Actor-Critic (A3C, Mnih et al., 2016) extends the approach of A2C by removing the need of synchronization between the workers at the end of each episode before applying the gradients. The rationale behind this is that each worker may need different times to complete its task, so they need to be synchronized. Some workers might then be idle most of the time, what is a waste of resources. Gradient merging and parameter updates are sequential operations, so no significant speedup is to be expected even if one increases the number of workers.\nThe solution retained in A3C is to simply skip the synchronization step: each worker reads and writes the network parameters whenever it wants. Without synchronization barriers, there is of course a risk that one worker tries to read the network parameters while another writes them: the obtained parameters would be a mix of two different networks. Surprisingly, it does not matter: if the learning rate is small enough, there is anyway not a big difference between two successive versions of the network parameters. This kind of “dirty” parameter sharing is called HogWild! updating (Niu et al., 2011) and has been proven to work under certain conditions which are met here.\nThe resulting A3C pseudocode is summarized here:\n\n\nInitialize the actor \\pi_\\theta and the critic V_\\varphi in the global network.\nfor each worker i in parallel:\n\nrepeat:\n\nGet a copy of the global actor \\pi_\\theta and critic V_\\varphi.\nSample an episode of n steps.\nCompute the accumulated gradients d\\theta_i and d\\varphi_i.\nUpdate the global actor and critic networks asynchronously (HogWild!).\n\n\n\n\nThe workers are fully independent: their only communication is through the asynchronous updating of the global networks. This can lead to very efficient parallel implementations: in the original A3C paper (Mnih et al., 2016), they solved the same Atari games than DQN using 16 CPU cores instead of a powerful GPU, while achieving a better performance in less training time (1 day instead of 8). The speedup is almost linear: the more workers, the faster the computations, the better the performance (as the policy updates are less correlated).\n\nEntropy regularization\nAn interesting addition in A3C is the way they enforce exploration during learning. In actor-critic methods, exploration classically relies on the fact that the learned policies are stochastic (on-policy): \\pi(s, a) describes the probability of taking the action a in the state s. In discrete action spaces, the output of the actor is usually a softmax layer, ensuring that all actions get a non-zero probability of being selected during training. In continuous action spaces, the executed action is sampled from the output probability distribution. However, this is often not sufficient and hard to control.\nIn A3C, the authors added an entropy regularization term (Williams and Peng, 1991) to the policy gradient update:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho^\\pi, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, ( R_t - V_\\varphi(s_t)) + \\beta \\, \\nabla_\\theta H(\\pi_\\theta(s_t))]\n\\tag{2}\nFor discrete actions, the entropy of the policy for a state s_t is simple to compute: H(\\pi_\\theta(s_t)) = - \\sum_a \\pi_\\theta(s_t, a) \\, \\log \\pi_\\theta(s_t, a). For continuous actions, replace the sum with an integral. It measures the “randomness” of the policy: if the policy is fully deterministic (the same action is systematically selected), the entropy is zero as it carries no information. If the policy is completely random, the entropy is maximal. Maximizing the entropy at the same time as the returns improves exploration by forcing the policy to be as non-deterministic as possible.\nThe parameter \\beta controls the level of regularization: we do not want the entropy to dominate either, as a purely random policy does not bring much reward. If \\beta is chosen too low, the entropy won’t play a significant role in the optimization and we may obtain a suboptimal deterministic policy early during training as there was not enough exploration. If \\beta is too high, the policy will be random. Entropy regularization adds yet another hyperparameter to the problem, but can be really useful for convergence when adequately chosen.\n\n\nComparison between A3C and DQN\n\nDQN uses an experience replay memory to solve the correlation of inputs/outputs problem, while A3C uses parallel actor-learners. If multiple copies of the environment are available, A3C should be preferred because the ERM slows down learning (very old transitions are still used for learning) and requires a lot of memory.\nA3C is on-policy: the learned policy must be used to explore the environment. DQN is off-policy: a behavior policy can be used for exploration, allowing to guide externally which regions of the state-action space should be explored. Off-policy are often preferred when expert knowledge is available.\nDQN has to use target networks to fight the non-stationarity of the Q-values. A3C uses state-values and advantages, which are much more stable over time than Q-values, so there is no need for target networks.\nA3C can deal with continuous action spaces easily, as it uses a parameterized policy. DQN has to be strongly modified to deal with this problem.\nBoth can deal with POMDP by using LSTMs in the actor network: A3C (Mirowski et al., 2016; Mnih et al., 2016), DQN (Hausknecht and Stone, 2015).\n\n\n\n\nGeneralized Advantage Estimation (GAE)\nThe different versions of the policy gradient seen so far take the form:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho^\\pi, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, \\psi_t ]\n\nwhere:\n\n\\psi_t = R_t is the REINFORCE algorithm (MC sampling).\n\\psi_t = R_t - b is the REINFORCE with baseline algorithm.\n\\psi_t = Q^\\pi(s_t, a_t) is the policy gradient theorem.\n\\psi_t = A^\\pi(s_t, a_t) is the advantage actor critic.\n\\psi_t = r_{t+1} + \\gamma \\, V^\\pi(s_{t+1}) - V^\\pi(s_t) is the TD actor critic.\n\\psi_t = \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V^\\pi(s_{t+n+1}) - V^\\pi(s_t) is the n-step algorithm (A2C).\n\nGenerally speaking:\n\nthe more \\psi_t relies on real rewards (e.g. R_t), the more the gradient will be correct on average (small bias), but the more it will vary (high variance). This increases the sample complexity: we need to average more samples to correctly estimate the gradient.\nthe more \\psi_t relies on estimations (e.g. the TD error), the more stable the gradient (small variance), but the more incorrect it is (high bias). This can lead to suboptimal policies, i.e. local optima of the objective function.\n\nThis is the classical bias/variance trade-off in machine learning. The n-step algorithm used in A2C is an attempt to mitigate between these extrema. Schulman et al. (2015) proposed the Generalized Advantage Estimate (GAE) to further control the bias/variance trade-off.\nLet’s define the n-step advantage:\n\n    A^{n}_t = \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V^\\pi(s_{t+n+1}) - V^\\pi(s_t)\n\nIt is easy to show recursively that it depends on the TD error \\delta_t = r_{t+1} + \\gamma \\, V^\\pi(s_{t+1}) - V^\\pi(s_t) of the n next steps:\n\n    A^{n}_t = \\sum_{l=0}^{n-1} \\gamma^l \\, \\delta_{t+l}\n\nIn other words, the prediction error over n steps is the (discounted) sum of the prediction errors between two successive steps. Now, what is the optimal value of n? GAE decides not to choose and to simply average all n-step advantages and to weight them with a discount parameter \\lambda. This defines the Generalized Advantage Estimator A^{\\text{GAE}(\\gamma, \\lambda)}_t:\n\n    A^{\\text{GAE}(\\gamma, \\lambda)}_t = (1-\\lambda) \\, \\sum_{l=0}^\\infty \\lambda^l A^l_t = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}\n\nThe GAE is the discounted sum of all n-step advantages. When \\lambda=0, we have A^{\\text{GAE}(\\gamma, 0)}_t = A^{0}_t = \\delta_t, i.e. the TD advantage (high bias, low variance). When \\lambda=1, we have (at the limit) A^{\\text{GAE}(\\gamma, 1)}_t = R_t, i.e. the MC advantage (low bias, high variance). Choosing the right value of \\lambda between 0 and 1 allows to control the bias/variance trade-off.\n\\gamma and \\lambda play different roles in GAE: \\gamma determines the scale or horizon of the value functions: how much future rewards rewards are to be taken into account. The higher \\gamma &lt;1, the smaller the bias, but the higher the variance. Empirically, Schulman et al. (2015) found that small \\lambda values introduce less bias than \\gamma, so \\lambda can be chosen smaller than \\gamma (which is typically 0.99).\nThe policy gradient for Generalized Advantage Estimation is therefore:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho^\\pi, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l} ]\n\nNote that Schulman et al. (2015) additionally use trust region optimization to stabilize learning and further reduce the bias, for now just consider it is a better optimization method than gradient descent. The GAE algorithm is summarized here:\n\n\nInitialize the actor \\pi_\\theta and the critic V_\\varphi with random weights.\nfor t \\in [0, T_\\text{total}]:\n\nInitialize empty minibatch.\nfor k \\in [0, n]:\n\nSelect a action a_k using the actor \\pi_\\theta.\nPerform the action a_k and observe the next state s_{k+1} and the reward r_{k+1}.\nStore (s_k, a_k, r_{k+1}) in the minibatch.\n\nfor k \\in [0, n]:\n\nCompute the TD error \\delta_k = r_{k+1} + \\gamma \\, V_\\varphi(s_{k+1}) - V_\\varphi(s_k)\n\nfor k \\in [0, n]:\n\nCompute the GAE advantage A^{\\text{GAE}(\\gamma, \\lambda)}_k = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{k+l}\n\nUpdate the actor using the GAE advantage and natural gradients (TRPO).\nUpdate the critic using natural gradients (TRPO)\n\n\n\n\n\nStochastic actor-critic for continuous action spaces\nThe actor-critic method presented above use stochastic policies \\pi_\\theta(s, a) assigning parameterized probabilities of being selecting to each (s, a) pair.\n\nWhen the action space is discrete, the output layer of the actor is simply a softmax layer with as many neurons as possible actions in each state, making sure the probabilities sum to one. It is then straightforward to sample an action from this layer.\nWhen the action space is continuous (e.g. the different joints of a robotic arm), one has to make an assumption on the underlying distribution. The actor learns the parameters of the distribution (for example the mean and variance of a Gaussian distribution) and the executed action is simply sampled from the parameterized distribution.\n\nThe most used distribution is the Gaussian distribution, leading to Gaussian policies. In this case, the output of the actor is a mean vector \\mu_\\theta(s) and possibly a variance vector \\sigma_\\theta(s). The policy is then simply defined as:\n\n    \\pi_\\theta(s, a) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_\\theta(s)}} \\, \\exp -\\frac{(a - \\mu_\\theta(s))^2}{2\\sigma_\\theta(s)^2}\n\nIn order to use backpropagation on the policy gradient (i.e. getting an analytical form of the score function \\nabla_\\theta \\log \\pi_\\theta (s, a)), one can use the re-parameterization trick (Heess et al., 2015) by rewriting the policy as:\n\n    a = \\mu_\\theta(s) + \\sigma_\\theta(s) \\times \\xi \\qquad \\text{where} \\qquad \\xi \\sim \\mathcal{N}(0,1)\n\nTo select an action, we only need to sample \\xi from the unit normal distribution, multiply it by the standard deviation and add the mean. To compute the score function, we use the following partial derivatives:\n\n    \\nabla_\\mu \\log \\pi_\\theta (s, a) = \\frac{a - \\mu_\\theta(s)}{\\sigma_\\theta(s)^2} \\qquad \\nabla_\\sigma \\log \\pi_\\theta (s, a) = \\frac{(a - \\mu_\\theta(s))^2}{\\sigma_\\theta(s)^3} - \\frac{1}{\\sigma_\\theta(s)}\n\nand use the chain rule to obtain the score function. The reparameterization trick is a cool trick to apply backpropagation on stochastic problems: it is for example used in the variational auto-encoders [VAE; Kingma and Welling (2013)].\nDepending on the problem, one could use: 1) a fixed \\sigma for the whole action space, 2) a fixed \\sigma per DoF, 3) a learnable \\sigma per DoF (assuming all action dimensions to be mutually independent) or even 4) a covariance matrix \\Sigma when the action dimensions are dependent.\nOne limitation of Gaussian policies is that their support is infinite: even with a small variance, samples actions can deviate a lot (albeit rarely) from the mean. This is particularly a problem when action must have a limited range: the torque of an effector, the linear or angular speed of a car, etc. Clipping the sampled action to minimal and maximal values introduces a bias which can impair learning. Chou et al. (2017) proposed to use beta-distributions instead of Gaussian ones in the actor. Sampled values have a [0,1] support, which can rescaled to [v_\\text{min},v_\\text{max}] easily. They show that beta policies have less bias than Gaussian policies in most continuous problems.\n\n\n\n\nChou, P.-W., Maturana, D., and Scherer, S. (2017). Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution. in International Conference on Machine Learning Available at: http://proceedings.mlr.press/v70/chou17a/chou17a.pdf.\n\n\nGu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates. in Proc. ICRA Available at: http://arxiv.org/abs/1610.00633.\n\n\nHausknecht, M., and Stone, P. (2015). Deep Recurrent Q-Learning for Partially Observable MDPs. Available at: http://arxiv.org/abs/1507.06527.\n\n\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T. (2015). Learning continuous control policies by stochastic value gradients. Proc. International Conference on Neural Information Processing Systems, 2944–2952. Available at: http://dl.acm.org/citation.cfm?id=2969569.\n\n\nKingma, D. P., and Welling, M. (2013). Auto-Encoding Variational Bayes. Available at: http://arxiv.org/abs/1312.6114.\n\n\nMirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A. J., Banino, A., et al. (2016). Learning to Navigate in Complex Environments. Available at: http://arxiv.org/abs/1611.03673.\n\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., et al. (2016). Asynchronous Methods for Deep Reinforcement Learning. in Proc. ICML Available at: http://arxiv.org/abs/1602.01783.\n\n\nNiu, F., Recht, B., Re, C., and Wright, S. J. (2011). HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. in Proc. Advances in Neural Information Processing Systems, 21–21. Available at: http://arxiv.org/abs/1106.5730.\n\n\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust Region Policy Optimization. in Proceedings of the 31 st International Conference on Machine Learning, 1889–1897. Available at: http://proceedings.mlr.press/v37/schulman15.html.\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press.\n\n\nWilliams, R. J., and Peng, J. (1991). Function optimization using connectionist reinforcement learning algorithms. Connection Science 3, 241–268.",
    "crumbs": [
      "**Policy-gradient methods**",
      "Asynchronous Advantage Actor-Critic (A3C)"
    ]
  },
  {
    "objectID": "src/3.3-ImportanceSampling.html",
    "href": "src/3.3-ImportanceSampling.html",
    "title": "Off-policy Actor-Critic",
    "section": "",
    "text": "Actor-critic architectures are generally on-policy algorithms: the actions used to explore the environment must have been generated by the actor, otherwise the feedback provided by the critic (the advantage) will introduce a huge bias (i.e. an error) in the policy gradient. This comes from the definition of the policy gradient theorem:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho^\\pi, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)]\n\nThe state distribution \\rho^\\pi defines the ensemble of states that can be visited using the actor policy \\pi_\\theta. If, during Monte Carlo sampling of the policy gradient, the states s do not come from this state distribution, the approximated policy gradient will be wrong (high bias) and the resulting policy will be suboptimal.\nThe major drawback of on-policy methods is their sample complexity: it is difficult to ensure that the “interesting” regions of the policy are actually discovered by the actor (see Figure 11.1). If the actor is initialized in a flat region of the reward space (where there is not a lot of rewards), policy gradient updates will only change slightly the policy and it may take a lot of iterations until interesting policies are discovered and fine-tuned.\n\n\n\n\n\n\nFigure 11.1: Illustration of the sample complexity inherent to on-policy methods, where the actor has only two parameters \\theta_1 and \\theta_2. If only very small regions of the actor parameters are associated with high rewards, the policy might wander randomly for a very long time before “hitting”the interesting regions.\n\n\n\nThe problem becomes even worse when the state or action spaces are highly dimensional, or when rewards are sparse. Imagine the scenario where you are searching for your lost keys at home (a sparse reward is delivered only once you find them): you could spend hours trying randomly each action at your disposal (looking in your jackets, on your counter, but also jumping around, cooking something, watching TV…) until finally you explore the action “look behind the curtains” and find them. (Note: with deep RL, you would even have to do that one million times in order to allow gradient descent to train your brain…). If you had somebody telling you “if I were you, I would first search in your jackets, then on your counter and finally behind the curtains, but forget about watching TV, you will never find anything by doing that”, this would certainly reduce your exploration time.\nThis is somehow the idea behind off-policy algorithms: they use a behavior policy b(s, a) to explore the environment and train the target policy \\pi(s, a) to reproduce the best ones by estimating how good they are. This does not come without caveats: if the behavior policy does not explore the optimal actions, the target policy will likely not be able to find it by itself, except by chance. But if the behavior policy is good enough, this can drastically reduce the amount of exploration needed to obtain a satisfying policy. Sutton and Barto (2017) noted that:\n“On-policy methods are generally simpler and are considered first. Off-policy methods require additional concepts and notation, and because the data is due to a different policy, off-policy methods are often of greater variance and are slower to converge.”\nThe most famous off-policy method is Q-learning. The reason why it is off-policy is that it does not use the next executed action (a_{t+1}) to update the value of an action, but the greedy action in the next state, which is independent from exploration:\n\n    \\delta = r(s, a, s') + \\gamma \\, \\max_{a'} Q^\\pi(s', a') - Q^\\pi(s, a)\n\nThe only condition for Q-learning to work (in the tabular case) is that the behavior policy b(s,a) must be able to explore actions which are selected by the target policy:\n\n    \\pi(s, a) &gt; 0 \\rightarrow b(s, a) &gt; 0\n\\tag{11.1}\nActions which would be selected by the target policy should be selected at least from time to time by the behavior policy in order to allow their update: if the target policy thinks this action should be executed, the behavior policy should try it to confirm or infirm this assumption. In mathematical terms, there is an assumption of coverage of \\pi by b (the support of b includes the one of \\pi).\nThere are mostly two ways to create the behavior policy:\n\nUse expert knowledge / human demonstrations. Not all available actions should be explored: the programmer already knows they do not belong to the optimal policy. When an agent learns to play chess, for example, the behavior policy could consist of the moves typically played by human experts: if chess masters play this move, it is likely to be a good action, so it should be tried out, valued and possibly incorporated into the target policy (if it is indeed a good action, experts might be wrong). A similar idea was used to bootstrap early versions of AlphaGo (Silver et al., 2016). In robotics, one could for example use “classical” engineering methods to control the exploration of the robot, while learning (hopefully) a better policy. It is also possible to perform imitation learning, where the agent learns from human demonstrations (e.g. Levine and Koltun (2013)).\nDerive it from the target policy. In Q-learning, the target policy can be deterministic, i.e. always select the greedy action (with the maximum Q-value). The behavior policy can be derived from the target policy by making it \\epsilon-soft, for example using a \\epsilon-greedy or softmax action selection scheme on the Q-values learned by the target policy.\n\nThe second option allows to control the level of exploration during learning (by controlling \\epsilon or the softmax temperature) while making sure that the target policy (the one used in production) is deterministic and optimal. It furthermore makes sure that Equation 11.1 is respected: the greedy action of the target policy always has a non-zero probability of being selected by an \\epsilon-greedy or softmax action selection. This is harder to ensure using expert knowledge.\nQ-learning methods such as DQN use this second option. The target policy in DQN is actually a greedy policy with respect to the Q-values (i.e. the action with the maximum Q-value will be deterministically chosen), but an \\epsilon-soft behavior policy is derived from it to ensure exploration. This explains now the following comment in the description of the DQN algorithm:\n\nSelect the action a_t based on the behavior policy derived from Q_\\theta(s_t, a) (e.g. softmax).\n\nOff-policy learning furthermore allows the use of an experience replay memory: in this case, the transitions used for training the target policy were generated by an older version of it (sometimes much older). Only off-policy methods can work with replay buffers. A3C is for example on-policy: it relies on multiple parallel learners to fight against the correlation of inputs and outputs.\n\nImportance sampling\nOff-policy methods learn a target policy \\pi(s,a) while exploring with a behavior policy b(s,a). The environment is sampled using the behavior policy to form estimates of the state or action values (for value-based methods) or of the policy gradient (for policy gradient methods). But is it mathematically correct?\nIn policy gradient methods, we want to maximize the expected return of trajectories:\n\n    J(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[R(\\tau)] = \\int_\\tau \\rho_\\theta(\\tau) \\, R(\\tau) \\, d\\tau \\approx \\frac{1}{N} \\sum_{i=1}^N R(\\tau_i)\n\nwhere \\rho_\\theta is the distribution of trajectories \\tau generated by the target policy \\pi_\\theta. Mathematical expectations can be approximating by an average of enough samples of the estimator (Monte Carlo). In policy gradient, we estimate the gradient, but let’s consider we sample the objective function for now. If we use a behavior policy to generate the trajectories, what we are actually estimating is:\n\n    \\hat{J}(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_b}[R(\\tau)] = \\int_\\tau \\rho_b(\\tau) \\, R(\\tau) \\, d\\tau\n\nwhere \\rho_b is the distribution of trajectories generated by the behavior policy. In the general case, there is no reason why \\hat{J}(\\theta) should be close from J(\\theta), even when taking their gradient.\nImportance sampling is a classical statistical method used to estimate properties of a distribution (here the expected return of the trajectories of the target policy) while only having samples generated from a different distribution (here the trajectories of the behavior policy). See for example https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf and http://timvieira.github.io/blog/post/2014/12/21/importance-sampling for more generic explanations.\nThe trick is simply to rewrite the objective function as:\n\n\\begin{aligned}\n    J(\\theta) & = \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[R(\\tau)]  \\\\\n              & = \\int_\\tau \\rho_\\theta(\\tau) \\, R(\\tau) \\, d\\tau \\\\\n              & = \\int_\\tau \\frac{\\rho_b(\\tau)}{\\rho_b(\\tau)} \\, \\rho_\\theta(\\tau) \\, R(\\tau) \\, d\\tau \\\\\n              & = \\int_\\tau \\rho_b(\\tau) \\frac{\\rho_\\theta(\\tau)}{\\rho_b(\\tau)} \\, R(\\tau) \\, d\\tau \\\\\n              & = \\mathbb{E}_{\\tau \\sim \\rho_b}[\\frac{\\rho_\\theta(\\tau)}{\\rho_b(\\tau)} \\, R(\\tau)]  \\\\\n\\end{aligned}\n\nThe ratio \\frac{\\rho_\\theta(\\tau)}{\\rho_b(\\tau)} is called the importance sampling weight for the trajectory. If a trajectory generated by b is associated with a lot of rewards R(\\tau) (with \\rho_b(\\tau) significantly high), the actor should learn to reproduce that trajectory with a high probability \\rho_\\theta(\\tau), as its goal is to maximize J(\\theta). Conversely, if the associated reward is low (R(\\tau)\\approx 0), the target policy can forget about it (by setting \\rho_\\theta(\\tau) = 0), even though the behavior policy still generates it!\nThe problem is now to estimate the importance sampling weight. Using the definition of the likelihood of a trajectory, the importance sampling weight only depends on the policies, not the dynamics of the environment (they cancel out):\n\n    \\frac{\\rho_\\theta(\\tau)}{\\rho_b(\\tau)} = \\frac{p_0 (s_0) \\, \\prod_{t=0}^T \\pi_\\theta(s_t, a_t) p(s_{t+1} | s_t, a_t)}{p_0 (s_0) \\, \\prod_{t=0}^T b(s_t, a_t) p(s_{t+1} | s_t, a_t)} = \\frac{\\prod_{t=0}^T \\pi_\\theta(s_t, a_t)}{\\prod_{t=0}^T b(s_t, a_t)} = \\prod_{t=0}^T \\frac{\\pi_\\theta(s_t, a_t)}{b(s_t, a_t)}\n\nThis allows to estimate the objective function J(\\theta) using Monte Carlo sampling (Meuleau et al., 2000; Peshkin and Shelton, 2002):\n\n  J(\\theta) \\approx \\frac{1}{m} \\, \\sum_{i=1}^m \\frac{\\rho_\\theta(\\tau_i)}{\\rho_b(\\tau_i)} \\, R(\\tau_i)\n\nAll one needs to do is to repeatedly apply the following algorithm:\n\n\nGenerate m trajectories \\tau_i using the behavior policy:\n\nFor each transition (s_t, a_t, s_{t+1}) of each trajectory, store:\n\nThe received reward r_{t+1}.\nThe probability b(s_t, a_t) that the behavior policy generates this transition.\nThe probability \\pi_\\theta(s_t, a_t) that the target policy generates this transition.\n\n\nEstimate the objective function with:\n\n\n  \\hat{J}(\\theta) = \\frac{1}{m} \\, \\sum_{i=1}^m \\left(\\prod_{t=0}^T \\frac{\\pi_\\theta(s_t, a_t)}{b(s_t, a_t)} \\right) \\, \\left(\\sum_{t=0}^T \\gamma^t \\, r_{t+1} \\right)\n\n\nUpdate the target policy to maximize \\hat{J}(\\theta).\n\n\nTang and Abbeel (2010) showed that the same idea can be applied to the policy gradient, under assumptions often met in practice:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_b}[ \\nabla_\\theta \\log \\rho_\\theta(\\tau) \\, \\frac{\\rho_\\theta(\\tau)}{\\rho_b(\\tau)} \\, R(\\tau)]\n\nWhen decomposing the policy gradient for each state encountered, one can also use the causality principle to simplify the terms:\n\nThe return after being in a state s_t only depends on future states.\nThe importance sampling weight (relative probability of arriving in s_t using the behavior and target policies) only depends on the past weights.\n\nThis gives the following approximation of the policy gradient, used for example in Guided policy search (Levine and Koltun, 2013):\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_b}[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, \\left(\\prod_{t'=0}^t \\frac{\\pi_\\theta(s_{t'}, a_{t'})}{b(s_{t'}, a_{t'})} \\right) \\, \\left(\\sum_{t'=t}^T \\gamma^{t'-t} \\, r(s_{t'}, a_{t'}) \\right)]\n\n\n\nLinear Off-Policy Actor-Critic (Off-PAC)\nThe first off-policy actor-critic method was proposed by Degris et al. (2012) for linear approximators. Another way to express the objective function in policy search is by using the Bellman equation (here in the off-policy setting):\n\n    J(\\theta) = \\mathbb{E}_{s \\sim \\rho_b} [V^{\\pi_\\theta}(s)] = \\mathbb{E}_{s \\sim \\rho_b} [\\sum_{a\\in\\mathcal{A}} \\pi(s, a) \\, Q^{\\pi_\\theta}(s, a)]\n\nMaximizing the value of all states reachable by the policy is the same as finding the optimal policy: the encoutered states bring the maximum return. The policy gradient becomes:\n\n    \\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho_b} [\\sum_{a\\in\\mathcal{A}} \\nabla_\\theta  (\\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a))]\n\nBecause both \\pi(s, a) and Q^\\pi(s, a) depend on the target policy \\pi_\\theta (hence its parameters \\theta), one should normally write:\n\n    \\nabla_\\theta  (\\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)) = Q^{\\pi_\\theta}(s, a) \\, \\nabla_\\theta  \\pi_\\theta(s, a) + \\pi_\\theta(s, a) \\, \\nabla_\\theta Q^{\\pi_\\theta}(s, a)\n\nThe second term depends on \\nabla_\\theta Q^{\\pi_\\theta}(s, a), which is very difficult to estimate. Degris et al. (2012) showed that when the Q-values are estimated by an unbiased critic Q_\\varphi(s, a), this second term can be omitted. Using the log-trick and importance sampling, the policy gradient can be expressed as:\n\n\\begin{aligned}\n    \\nabla_\\theta J(\\theta) & = \\mathbb{E}_{s \\sim \\rho_b} [\\sum_{a\\in\\mathcal{A}} Q_\\varphi(s, a) \\, \\nabla_\\theta \\pi_\\theta(s, a)] \\\\\n                            & = \\mathbb{E}_{s \\sim \\rho_b} [\\sum_{a\\in\\mathcal{A}} b(s, a) \\, \\frac{\\pi_\\theta(s, a)}{b(s, a)} \\, Q_\\varphi(s, a) \\, \\frac{\\nabla_\\theta \\pi_\\theta(s, a)}{\\pi_\\theta(s, a)}] \\\\\n                            & = \\mathbb{E}_{s,a \\sim \\rho_b} [\\frac{\\pi_\\theta(s, a)}{b(s, a)} \\, Q_\\varphi(s, a) \\, \\nabla_\\theta \\log \\pi_\\theta(s, a)] \\\\\n\\end{aligned}\n\nWe now have an actor-critic architecture (actor \\pi_\\theta(s, a), critic Q_\\varphi(s, a)) able to learn from single transitions (s,a) (online update instead of complete trajectories) generated off-policy (behavior policy b(s,a) and importance sampling weight \\frac{\\pi_\\theta(s, a)}{b(s, a)}). The off-policy actor-critic (Off-PAC) algorithm of Degris et al. (2012) furthermore uses eligibility traces to stabilize learning. However, it was limited to linear function approximators because its variance is too high to train deep neural networks.\n\n\nRetrace\nFor a good deep RL algorithm, we need the two following properties:\n\nOff-policy learning: it allows to learn from transitions stored in a replay buffer (i.e. generated with an older policy). As NN need many iterations to converge, it is important to be able to re-use old transitions for its training, instead of constantly sampling new ones (sample complexity). Multiple parallel actors as in A3C allow to mitigate this problem, but it is still too complex.\nMulti-step returns: the two extremes of RL are TD (using a single “real” reward for the update, the rest is estimated) and Monte Carlo (use only “real” rewards, no estimation). TD has a smaller variance, but a high bias (errors in estimates propagate to all other values), while MC has a small bias but a high variance (learns from many real rewards, but the returns may vary a lot between two almost identical episodes). Eligibility traces and n-step returns (used in A3C) are the most common trade-off between TD and MC.\n\nThe Retrace algorithm (Munos et al., 2016) is designed to exhibit both properties when learning Q-values. It can therefore be used to train the critic (instead of classical Q-learning) and provide the actor with safe, efficient and low-variance values.\nIn the generic form, Q-learning updates the Q-value of a transition (s_t, a_t) using the TD error:\n\n    \\Delta Q^\\pi(s_t, a_t) = \\alpha \\, \\delta_t = \\alpha \\, (r_{t+1} + \\gamma \\, \\max_a Q^\\pi(s_{t+1}, a_{t+1}) - Q^\\pi(s_t, a_t))\n\nWhen using eligibility traces in the forward view, the change in Q-value depends also on the TD error of future transitions at times t' &gt; t. A parameter \\lambda ensures the stability of the update:\n\n    \\Delta Q^\\pi(s_t, a_t) = \\alpha \\, \\sum_{t'=t}^T (\\gamma \\lambda)^{t'-t} \\delta_{t'}\n\nThe Retrace algorithm proposes to generalize this formula using a parameter c_s for each time step between t and t':\n\n    \\Delta Q^\\pi(s_t, a_t) = \\alpha \\, \\sum_{t'=t}^T (\\gamma)^{t'-t} \\left(\\prod_{s=t+1}^{t'} c_s \\right) \\, \\delta_{t'}\n\nDepending on the choice of c_s, the formula covers different existing methods:\n\nc_s = \\lambda is the classical eligibility trace mechanism (Q(\\lambda)) in its forward view, which is not safe: the behavior policy b must be very close from the target policy \\tau:\n\n\n    || \\pi - b ||_1 \\leq \\frac{1 - \\gamma}{\\lambda \\gamma}\n\nAs \\gamma is typically chosen very close from 1 (e.g. 0.99), this does not leave much room for the target policy to differ from the behavior policy (see Harutyunyan et al., 2016 for the proof).\n\nc_s = \\frac{\\pi(s_s, a_s)}{b(s_s, a_s)} is the importance sampling weight. Importance sampling is unbiased in off-policy settings, but can have a very large variance: the product of ratios \\prod_{s=t+1}^{t'} \\frac{\\pi(s_s, a_s)}{b(s_s, a_s)} can quickly vary between two episodes.\nc_s = \\pi(s_s, a_s) corresponds to the tree-backup algorithm TB(\\lambda) (Precup et al., 2000). It has the advantage to work for arbitrary policies \\pi and b, but the product of such probabilities decays very fast to zero when the time difference t' - t increases: TD errors will be efficiently shared over a couple of steps only.\n\nFor Retrace, Munos et al. (2016) showed that a much better value for c_s is:\n\n    c_s = \\lambda \\min (1, \\frac{\\pi(s_s, a_s)}{b(s_s, a_s)})\n\nThe importance sampling weight is clipped to 1, and decays exponentially with the parameter \\lambda. It can be seen as a trade-off between importance sampling and eligibility traces. The authors showed that Retrace(\\lambda) has a low variance (as it uses multiple returns), is safe (works for all \\pi and b) and efficient (it can propagate rewards over many time steps). They used retrace to learn Atari games and compared it positively with DQN, both in terms of optimality and speed of learning. These properties make Retrace particularly suited for deep RL and actor-critic architectures: it is for example used in ACER and the Reactor.\nRémi Munos uploaded some slides explaining Retrace in a simpler manner than in the original paper: https://ewrl.files.wordpress.com/2016/12/munos.pdf.\n\n\nSelf-Imitation Learning (SIL)\nWe have discussed or now only strictly on-policy or off-policy methods. Off-policy methods are much more stable and efficient, but they learn generally a deterministic policy, what can be problematic in stochastic environments (e.g. two players games: being predictable is clearly an issue). Hybrid methods combining on- and off-policy mechanisms have clearly a great potential.\nOh et al. (2018) proposed a Self-Imitation Learning (SIL) method that can extend on-policy actor-critic algorithms (e.g. A2C) with a replay buffer able to feed past good experiences to the NN to speed up learning.\nThe main idea is to use prioritized experience replay (Schaul et al. (2015)) to select only transitions whose actual return is higher than their current expected value. This defines two additional losses for the actor and the critic:\n\n    \\mathcal{L}^\\text{SIL}_\\text{actor}(\\theta) = \\mathbb{E}_{s, a \\in \\mathcal{D}}[\\log \\pi_\\theta(s, a) \\, (R(s, a) - V_\\varphi(s))^+]\n \n    \\mathcal{L}^\\text{SIL}_\\text{critic}(\\varphi) = \\mathbb{E}_{s, a \\in \\mathcal{D}}[((R(s, a) - V_\\varphi(s))^+)^2]\n\nwhere (x)^+ = \\max(0, x) is the positive function. Transitions sampled from the replay buffer will participate to the off-policy learning only if their return is higher that the current value of the state, i.e. if they are good experiences compared to what is currently known (V_\\varphi(s)). The pseudo-algorithm is actually quite simple and simply extends the A2C procedure:\n\n\nInitialize the actor \\pi_\\theta and the critic V_\\varphi with random weights.\nInitialize the prioritized experience replay buffer \\mathcal{D}.\nObserve the initial state s_0.\nfor t \\in [0, T_\\text{total}]:\n\nInitialize empty episode minibatch.\nfor k \\in [0, n]: # Sample episode\n\nSelect a action a_k using the actor \\pi_\\theta.\nPerform the action a_k and observe the next state s_{k+1} and the reward r_{k+1}.\nStore (s_k, a_k, r_{k+1}) in the episode minibatch.\n\nif s_n is not terminal: set R_n = V_\\varphi(s_n) with the critic, else R_n=0.\nfor k \\in [n-1, 0]: # Backwards iteration over the episode\n\nUpdate the discounted sum of rewards R_k = r_k + \\gamma \\, R_{k+1} and store it in the replay buffer \\mathcal{D}.\n\nUpdate the actor and the critic on-policy with the episode:\n\n\n      \\theta \\leftarrow \\theta + \\eta \\, \\sum_k \\nabla_\\theta \\log \\pi_\\theta(s_k, a_k) \\, (R_k - V_\\varphi(s_k))\n  \n\n      \\varphi \\leftarrow \\varphi + \\eta \\, \\sum_k \\nabla_\\varphi (R - V_\\varphi(s_k))^2\n  \n\nfor m \\in [0, M]:\n\nSample a minibatch of K transitions (s_k, a_k, R_k) from the replay buffer \\mathcal{D} prioritized with high (R_k - V_\\varphi(s_k)).\nUpdate the actor and the critic off-policy with self-imitation.\n\n\n      \\theta \\leftarrow \\theta + \\eta \\, \\sum_k \\nabla_\\theta \\log \\pi_\\theta(s_k, a_k) \\, (R_k - V_\\varphi(s_k))^+\n  \n\n      \\varphi \\leftarrow \\varphi + \\eta \\, \\sum_k \\nabla_\\varphi ((R_k - V_\\varphi(s_k))^+)^2\n  \n\n\n\nIn the paper, they furthermore used entropy regularization as in A3C. They showed that A2C+SIL has a better performance both on Atari games and continuous control problems (Mujoco) than state-of-the art methods (A3C, TRPO, Reactor, PPO). It shows that self-imitation learning can be very useful in problems where exploration is hard: a proper level of exploitation of past experiences actually fosters a deeper exploration of environment.\n\n\n\n\nDegris, T., White, M., and Sutton, R. S. (2012). Linear Off-Policy Actor-Critic. in Proceedings of the 2012 International Conference on Machine Learning Available at: http://arxiv.org/abs/1205.4839.\n\n\nHarutyunyan, A., Bellemare, M. G., Stepleton, T., and Munos, R. (2016). Q(λ) with off-policy corrections. Available at: http://arxiv.org/abs/1602.04951.\n\n\nLevine, S., and Koltun, V. (2013). Guided Policy Search. in Proceedings of Machine Learning Research, 1–9. Available at: http://proceedings.mlr.press/v28/levine13.html.\n\n\nMeuleau, N., Peshkin, L., Kaelbling, L. P., and Kim, K. (2000). Off-Policy Policy Search. Available at: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894.\n\n\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016). Safe and Efficient Off-Policy Reinforcement Learning. Available at: http://arxiv.org/abs/1606.02647.\n\n\nOh, J., Guo, Y., Singh, S., and Lee, H. (2018). Self-Imitation Learning. Available at: http://arxiv.org/abs/1806.05635.\n\n\nPeshkin, L., and Shelton, C. R. (2002). Learning from Scarce Experience. Available at: http://arxiv.org/abs/cs/0204043.\n\n\nPrecup, D., Sutton, R. S., and Singh, S. (2000). Eligibility traces for off-policy policy evaluation. in Proceedings of the Seventeenth International Conference on Machine Learning.\n\n\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized Experience Replay. Available at: http://arxiv.org/abs/1511.05952.\n\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature 529, 484–489. doi:10.1038/nature16961.\n\n\nSutton, R. S., and Barto, A. G. (2017). Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press Available at: http://incompleteideas.net/book/the-book-2nd.html.\n\n\nTang, J., and Abbeel, P. (2010). On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient. in Adv. Neural inf. Process. Syst. Available at: http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Off-policy Actor-Critic</span>"
    ]
  },
  {
    "objectID": "src/3.4-DPG.html",
    "href": "src/3.4-DPG.html",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "",
    "text": "Deterministic policy gradient theorem\nWe now assume that we want to learn a parameterized deterministic policy \\mu_\\theta(s). As for the stochastic policy gradient theorem, the goal is to maximize the expectation over all states reachable by the policy of the reward to-go (return) after each action:\nJ(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\mu}[R(s, \\mu_\\theta(s))]\nAs in the stochastic case, the distribution of states reachable by the policy \\rho_\\mu is impossible to estimate, so we will have to perform approximations. Building on Hafner and Riedmiller (2011), Silver et al. (2014) showed how to obtain a usable gradient for the objective function when the policy is deterministic.\nConsidering that the Q-value of an action is the expectation of the reward to-go after that action Q^\\pi(s, a) = \\mathbb{E}_\\pi[R(s, a)], maximizing the returns or maximizing the true Q-value of all actions leads to the same optimal policy. This is the basic idea behind dynamic programming, where policy evaluation first finds the true Q-value of all state-action pairs and policy improvement changes the policy by selecting the action with the maximal Q-value a^*_t = \\text{argmax}_a Q_\\theta(s_t, a).\nIn the continuous case, we will simply state that the gradient of the objective function is the same as the gradient of the Q-value. Supposing we have an unbiased estimate Q^\\mu(s, a) of the value of any action in s, changing the policy \\mu_\\theta(s) in the direction of \\nabla_\\theta Q^\\mu(s, a) leads to an action with a higher Q-value, therefore with a higher associated return:\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\mu}[\\nabla_\\theta Q^\\mu(s, a) |_{a = \\mu_\\theta(s)}]\nThis notation means that the gradient w.r.t a of the Q-value is taken at a = \\mu_\\theta(s). We now use the chain rule to expand the gradient of the Q-value:\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\mu}[\\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q^\\mu(s, a) |_{a = \\mu_\\theta(s)}]\nIt is perhaps clearer using partial derivatives and simplifying the notations:\n\\frac{\\partial Q(s,a)}{\\partial \\theta} = \\frac{\\partial Q(s,a)}{\\partial a} \\times \\frac{\\partial a}{\\partial \\theta}\nThe first term defines of the Q-value of an action changes when one varies slightly the action (if I move my joint a bit more to the right, do I get a higher Q-value, hence more reward?), the second term defines how the action changes when the parameters \\theta of the actor change (which weights should be changed in order to produce that action with a slightly higher Q-value?).\nWe already see an actor-critic architecture emerging from this equation: \\nabla_\\theta \\mu_\\theta(s) only depends on the parameterized actor, while \\nabla_a Q^\\mu(s, a) is a sort of critic, telling the actor in which direction to change its policy: towards actions associated with more reward.\nAs in the stochastic policy gradient theorem, the question is now how to obtain an unbiased estimate of the Q-value of any action and compute its gradient. Silver et al. (2014) showed that it is possible to use a function approximator Q_\\varphi(s, a) as long as it is compatible and minimize the quadratic error with the true Q-values:\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\mu}[\\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_\\varphi(s, a) |_{a = \\mu_\\theta(s)}]\n \n    J(\\varphi) = \\mathbb{E}_{s \\sim \\rho_\\mu}[(Q^\\mu(s, \\mu_\\theta(s)) - Q_\\varphi(s, \\mu_\\theta(s)))^2]\nFigure 12.1 outlines the actor-critic architecture of the DPG (deterministic policy gradient) method, to compare with the actor-critic architecture of the stochastic policy gradient (Figure 10.2).\nSilver et al. (2014) investigated the performance of DPG using linear function approximators and showed that it compared positively to stochastic algorithms in high-dimensional or continuous action spaces. However, non-linear function approximators such as deep NN would not work yet.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Deep Deterministic Policy Gradient (DDPG)</span>"
    ]
  },
  {
    "objectID": "src/3.4-DPG.html#deterministic-policy-gradient-theorem",
    "href": "src/3.4-DPG.html#deterministic-policy-gradient-theorem",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "",
    "text": "Figure 12.1: Architecture of the DPG (deterministic policy gradient) method.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Deep Deterministic Policy Gradient (DDPG)</span>"
    ]
  },
  {
    "objectID": "src/3.4-DPG.html#deep-deterministic-policy-gradient-ddpg",
    "href": "src/3.4-DPG.html#deep-deterministic-policy-gradient-ddpg",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "Deep Deterministic Policy Gradient (DDPG)",
    "text": "Deep Deterministic Policy Gradient (DDPG)\nLillicrap et al. (2015) extended the DPG approach to work with non-linear function approximators. In fact, they combined ideas from DQN and DPG to create a very successful algorithm able to solve continuous problems off-policy, the deep deterministic policy gradient (DDPG) algorithm..\nThe key ideas borrowed from DQN are:\n\nUsing an experience replay memory to store past transitions and learn off-policy.\nUsing target networks to stabilize learning.\n\nThey modified the update frequency of the target networks originally used in DQN. In DQN, the target networks are updated with the parameters of the trained networks every couple of thousands of steps. The target networks therefore change a lot between two updates, but not very often. Lillicrap et al. (2015) found that it is actually better to make the target networks slowly track the trained networks, by updating their parameters after each update of the trained network using a sliding average for both the actor and the critic:\n\n    \\theta' = \\tau \\, \\theta + (1-\\tau) \\, \\theta'\n\nwith \\tau &lt;&lt;1. Using this update rule, the target networks are always “late” with respect to the trained networks, providing more stability to the learning of Q-values.\nThe key idea borrowed from DPG is the policy gradient for the actor. The critic is learned using regular Q-learning and target networks:\n\n    J(\\varphi) = \\mathbb{E}_{s \\sim \\rho_\\mu}[(r(s, a, s') + \\gamma \\, Q_{\\varphi'}(s', \\mu_{\\theta'}(s')) - Q_\\varphi(s, a))^2]\n\nOne remaining issue is exploration: as the policy is deterministic, it can very quickly produce always the same actions, missing perhaps more rewarding options. Some environments are naturally noisy, enforcing exploration by itself, but this cannot be assumed in the general case. The solution retained in DDPG is an additive noise added to the deterministic action to explore the environment:\n\n    a_t = \\mu_\\theta(s_t) + \\xi\n\nThis additive noise could be anything, but the most practical choice is to use an Ornstein-Uhlenbeck process (Uhlenbeck and Ornstein, 1930) to generate temporally correlated noise with zero mean. Ornstein-Uhlenbeck processes are used in physics to model the velocity of Brownian particles with friction. It updates a variable x_t using a stochastic differential equation (SDE):\n dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t \\qquad \\text{with} \\qquad dW_t = \\mathcal{N}(0, dt)\n\\mu is the mean of the process (usually 0), \\theta is the friction (how fast it varies with noise) and \\sigma controls the amount of noise. Figure 12.2 shows three independent runs of a Ornstein-Uhlenbeck process: successive values of the variable x_t vary randomly but coherently over time.\n\n\n\n\n\n\nFigure 12.2: Three independent runs of an Ornstein-Uhlenbeck process with \\mu=0, \\sigma=0.3, \\theta=0.15 and dt=0.1. The code is adapted from https://gist.github.com/jimfleming/9a62b2f7ed047ff78e95b5398e955b9e\n\n\n\nThe architecture of the DDPG algorithm is depicted on Figure 12.3.\n\n\n\n\n\n\nFigure 12.3: Architecture of the DDPG (deep deterministic policy gradient) algorithm.\n\n\n\nThe pseudo-algorithm is as follows:\n\n\nInitialize actor network \\mu_{\\theta} and critic Q_\\varphi with random weights.\nCreate the target networks \\mu_{\\theta'} and Q_{\\varphi'}.\nInitialize experience replay memory \\mathcal{D} of maximal size N.\nfor episode \\in [1, M]:\n\nInitialize random process \\xi.\nObserve the initial state s_0.\nfor t \\in [0, T_\\text{max}]:\n\nSelect the action a_t = \\mu_\\theta(s_t) + \\xi according to the current policy and the noise.\nPerform the action a_t and observe the next state s_{t+1} and the reward r_{t+1}.\nStore (s_t, a_t, r_{t+1}, s_{t+1}) in the experience replay memory.\nSample a minibatch of N transitions randomly from \\mathcal{D}.\nFor each transition (s_k, a_k, r_k, s'_k) in the minibatch:\n\nCompute the target value using target networks y_k = r_k + \\gamma \\, Q_{\\varphi'}(s'_k, \\mu_{\\theta'}(s'_k)).\n\nUpdate the critic by minimizing: \n  \\mathcal{L}(\\varphi) = \\frac{1}{N} \\sum_k (y_k - Q_\\varphi(s_k, a_k))^2\n  \nUpdate the actor using the sampled policy gradient: \n  \\nabla_\\theta J(\\theta) = \\frac{1}{N} \\sum_k \\nabla_\\theta \\mu_\\theta(s_k) \\times \\nabla_a Q_\\varphi(s_k, a) |_{a = \\mu_\\theta(s_k)}\n  \nUpdate the target networks: \\theta' \\leftarrow \\tau \\theta + (1-\\tau) \\, \\theta' \\varphi' \\leftarrow \\tau \\varphi + (1-\\tau) \\, \\varphi'\n\n\n\n\nThe question that arises is how to obtain the gradient of the Q-value w.r.t the action \\nabla_a Q_\\varphi(s, a), when the critic only outputs the Q-value Q_\\varphi(s, a). Fortunately, deep neural networks are simulated using automatic differentiation libraries such as tensorflow, theano, pytorch and co, which can automatically output this gradient. If not available, one could simply use the finite difference method (Euler) to approximate this gradient. One has to evaluate the Q-value in a +da, where da is a very small change of the executed action, and estimate the gradient using:\n\n    \\nabla_a Q_\\varphi(s, a) \\approx \\frac{Q_\\varphi(s, a + da) - Q_\\varphi(s, a)}{da}\n\nNote that the DDPG algorithm is off-policy: the samples used to train the actor come from the replay buffer, i.e. were generated by an older version of the target policy. DDPG does not rely on importance sampling: as the policy is deterministic (we maximize \\mathbb{E}_{s}[Q(s, \\mu_\\theta(s))]), there is no need to balance the probabilities of the behavior and target policies (with stochastic policies, one should maximize \\mathbb{E}_{s}[\\sum_{a\\in\\mathcal{A}} \\pi(s, a) Q(s, a)]). In other words, the importance sampling weight can safely be set to 1 for deterministic policies.\nDDPG has rapidly become the state-of-the-art model-free method for continuous action spaces (although now PPO is preferred). It is able to learn efficent policies on most contiuous problems, either pixel-based or using individual state variables. In the original DDPG paper, they showed that batch normalization (Ioffe and Szegedy, 2015) is crucial in stabilizing the training of deep networks on such problems. Its main limitation is its high sample complexity. Distributed versions of DDPG have been proposed to speed up learning, similarly to the parallel actor learners of A3C (Barth-Maron et al., 2018; Lötzsch et al., 2017; Popov et al., 2017).\nAdditional references: see http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html for additional explanations and step-by-step tensorflow code and https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html for contextual explanations.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Deep Deterministic Policy Gradient (DDPG)</span>"
    ]
  },
  {
    "objectID": "src/3.4-DPG.html#distributed-distributional-ddpg-d4pg",
    "href": "src/3.4-DPG.html#distributed-distributional-ddpg-d4pg",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "Distributed Distributional DDPG (D4PG)",
    "text": "Distributed Distributional DDPG (D4PG)\nSimilarly to the Rainbow DQN for DQN. Distributed Distributional DDPG (D4PG, Barth-Maron et al., 2018) proposed several improvements on DDPG to make it more efficient:\n\nThe critic is trained using distributional learning (Bellemare et al. (2017)) instead of classical Q-learning to improve the stability of learning in the actor (less variance).\nThe critic uses n-step returns instead of simple one-step TD returns as in A3C (Mnih et al. (2016)).\nMultiple Distributed Parallel Actors gather (s, a, r, s') transitions in parallel and write them to the same replay buffer (as in distributed DQN).\nThe replay buffer uses Prioritized Experience Replay (Schaul et al., 2015) to sample transitions based the information gain.\n\n\n\n\n\nBarth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., TB, D., et al. (2018). Distributed Distributional Deterministic Policy Gradients. Available at: http://arxiv.org/abs/1804.08617.\n\n\nBellemare, M. G., Dabney, W., and Munos, R. (2017). A Distributional Perspective on Reinforcement Learning. Available at: http://arxiv.org/abs/1707.06887.\n\n\nHafner, R., and Riedmiller, M. (2011). Reinforcement learning in feedback control. Machine Learning 84, 137–169. doi:10.1007/s10994-011-5235-x.\n\n\nIoffe, S., and Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Available at: http://arxiv.org/abs/1502.03167.\n\n\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., et al. (2015). Continuous control with deep reinforcement learning. CoRR. Available at: http://arxiv.org/abs/1509.02971.\n\n\nLötzsch, W., Vitay, J., and Hamker, F. H. (2017). Training a deep policy gradient-based neural network with asynchronous learners on a simulated robotic problem. in INFORMATIK 2017. Gesellschaft für Informatik, eds. M. Eibl and M. Gaedke (Gesellschaft für Informatik, Bonn), 2143–2154. Available at: https://dl.gi.de/handle/20.500.12116/3986.\n\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., et al. (2016). Asynchronous Methods for Deep Reinforcement Learning. in Proc. ICML Available at: http://arxiv.org/abs/1602.01783.\n\n\nPopov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G., Vecerik, M., et al. (2017). Data-efficient Deep Reinforcement Learning for Dexterous Manipulation. Available at: http://arxiv.org/abs/1704.03073.\n\n\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized Experience Replay. Available at: http://arxiv.org/abs/1511.05952.\n\n\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic Policy Gradient Algorithms. in Proc. ICML Proceedings of Machine Learning Research., eds. E. P. Xing and T. Jebara (PMLR), 387–395. Available at: http://proceedings.mlr.press/v32/silver14.html.\n\n\nUhlenbeck, G. E., and Ornstein, L. S. (1930). On the Theory of the Brownian Motion. Physical Review 36. doi:10.1103/PhysRev.36.823.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Deep Deterministic Policy Gradient (DDPG)</span>"
    ]
  },
  {
    "objectID": "src/3.5-NaturalGradient.html",
    "href": "src/3.5-NaturalGradient.html",
    "title": "Policy Optimization (TRPO, PPO)",
    "section": "",
    "text": "The deep networks used as function approximators in the methods presented until now were all optimized (trained) using stochastic gradient descent (SGD) or any of its variants (RMSProp, Adam, etc). The basic idea is to change the parameters \\theta in the opposite direction of the gradient of the loss function (or the same direction as the policy gradient, in which case it is called gradient ascent), proportionally to a small learning rate \\eta:\n\n    \\Delta \\theta = - \\eta \\, \\nabla_\\theta \\mathcal{L}(\\theta)\n\nSGD is also called a steepest descent method: one searches for the smallest parameter change \\Delta \\theta inducing the biggest negative change of the loss function. In classical supervised learning, this is what we want: we want to minimize the loss function as fast as possible, while keeping weight changes as small as possible, otherwise learning might become unstable (weight changes computed for a single minibatch might erase the changes made on previous minibatches). The main difficulty of supervised learning is to choose the right value for the learning rate: too high and learning is unstable; too low and learning takes forever.\nIn deep RL, we have an additional problem: the problem is not stationary. In Q-learning, the target r(s, a, s') + \\gamma \\, \\max_{a'} Q_\\theta(S', a') is changing with \\theta. If the Q-values change a lot between two minibatches, the network will not get any stable target signal to learn from, and the policy will end up suboptimal. The trick is to use target networks to compute the target, which can be either an old copy of the current network (vanilla DQN), or a smoothed version of it (DDPG). Obviously, this introduces a bias (the targets are always wrong during training), but this bias converges to zero (after sufficient training, the targets will be almost correct), at the cost of a huge sample complexity.\nTarget networks cannot be used in on-policy methods, especially actor-critic architectures.The critic must learn from transitions recently generated by the actor (although importance sampling and the Retrace algorithm might help). The problem with on-policy methods is that they waste a lot of data: they always need fresh samples to learn from and never reuse past experiences. The policy gradient theorem shows why:\n\n\\begin{aligned}\n    \\nabla_\\theta J(\\theta) & =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)] \\\\\n    & \\approx  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q_\\varphi(s, a)]\n\\end{aligned}\n\nIf the policy \\pi_\\theta changes a lot between two updates, the estimated Q-value Q_\\varphi(s, a) will represent the value of the action for a totally different policy, not the true Q-value Q^{\\pi_\\theta}(s, a). The estimated policy gradient will then be strongly biased and learning will be suboptimal. In other words, the actor should not change much faster than the critic, and vice versa. A naive solution would be to use a very small learning rate for the actor, but this just slows down learning (adding to the sample complexity) without solving the problem.\nTo solve the problem, we should actually do the opposite of the steepest descent: search for the biggest parameter change \\Delta \\theta inducing the smallest change in the policy, but in the right direction. If the parameter change is high, the actor will learn a lot internally from each experience. But if the policy change is small between two updates (although the parameters have changed a lot), we might be able to reuse past experiences, as the targets will not be that wrong.\nThis is where natural gradients come into play, which are originally a statistical method to optimize over spaces of probability distributions, for example for variational inference. The idea to use natural gradients to train neural networks comes from Amari (1998). Kakade (2001) applied natural gradients to policy gradient methods, while Peters and Schaal (2008) proposed a natural actor-critic algorithm for linear function approximators. The idea was adapted to deep RL by Schulman and colleagues, with Trust Region Policy Optimization (TRPO, Schulman et al., 2015) and Proximal Policy Optimization (PPO, Schulman et al., 2017), the latter gaining momentum over DDPG as the go-to method for continuous RL problems, particularly because of its smaller sample complexity and its robustness to hyperparameters.\n\nPrinciple of natural gradients\n\n\n\n\n\n\nFigure 13.1: Euclidian distances in the parameter space do not represent well the statistical distance between probability distributions. The two Gaussians on the left (\\mathcal{N}(0, 0.2) and \\mathcal{N}(1, 0.2)) have the same Euclidian distance in the parameter space (d = \\sqrt{(\\mu_0 - \\mu_1)^2+(\\sigma_0 - \\sigma_1)^2}) than the two Gaussians on the right (\\mathcal{N}(0, 10) and \\mathcal{N}(1, 10)). However, the Gaussians on the right are much more similar than the two on the left: if you have a single sample, you could not say from which distribution it comes for the Gaussians on the right, while it is obvious for the Gaussians on the left.\n\n\n\nConsider the two Gaussian distributions in the left part of Figure 13.1 (\\mathcal{N}(0, 0.2) and \\mathcal{N}(1, 0.2)) and the two on the right (\\mathcal{N}(0, 10) and \\mathcal{N}(1, 10)). In both cases, the distance in the Euclidian space of parameters d = \\sqrt{(\\mu_0 - \\mu_1)^2+(\\sigma_0 - \\sigma_1)^2} is the same between the two Gaussians. Obviously, the two distributions on the left are however further away from each other than the two on the the right. This indicates that the Euclidian distance in the parameter space (which is what regular gradients act on) is not a correct measurement of the statistical distance between two distributions (which what we want to minimize between two iterations of PG).\nIn statistics, a common measurement of the statistical distance between two distributions p and q is the Kullback-Leibler (KL) divergence D_{KL}(p||q), also called relative entropy or information gain. It is defined as:\n\n    D_{KL}(p || q) = \\mathbb{E}_{x \\sim p} [\\log \\frac{p(x)}{q(x)}]  = \\int p(x) \\, \\log \\frac{p(x)}{q(x)} \\, dx\n\nIts minimum is 0 when p=q (as \\log \\frac{p(x)}{q(x)} is then 0) and is positive otherwise. Minimizing the KL divergence is equivalent to “matching” two distributions. Note that supervised methods in machine learning can all be interpreted as a minimization of the KL divergence: if p(x) represents the distribution of the data (the label of a sample x) and q(x) the one of the model (the prediction of a neural network for the same sample x), supervised methods want the output distribution of the model to match the distribution of the data, i.e. make predictions that are the same as the labels. For generative models, this is for example at the core of generative adversarial networks (Arjovsky et al., 2017; Goodfellow et al., 2014) or variational autoencoders (Kingma and Welling, 2013).\nThe KL divergence is however not symmetrical (D_{KL}(p || q) \\neq D_{KL}(q || p)), so a more useful divergence is the symmetric KL divergence, also known as Jensen-Shannon (JS) divergence:\n\n    D_{JS}(p || q) = \\frac{D_{KL}(p || q) + D_{KL}(q || p)}{2}\n\nOther forms of divergence measurements exist, such as the Wasserstein distance which improves generative adversarial networks (Arjovsky et al., 2017), but they are not relevant here. See https://www.alexirpan.com/2017/02/22/wasserstein-gan.html for more explanations.\nWe now have a global measurement of the similarity between two distributions on the whole input space, but which is hard to compute. How can we use it anyway in our optimization problem? As mentioned above, we search for the biggest parameter change \\Delta \\theta inducing the smallest change in the policy. We need a metric linking changes in the parameters of the distribution (the weights of the network) to changes in the distribution itself. In other terms, we will apply gradient descent on the statistical manifold defined by the parameters rather than on the parameters themselves.\n\n\n\n\n\n\nFigure 13.2: Naive illustration of the Riemannian metric. The Euclidian distance between p(x; \\theta) and p(x; \\theta + \\Delta \\theta) depends on the Euclidian distance between \\theta and \\theta + \\Delta\\theta, i.e. \\Delta \\theta. Riemannian metrics follow the geometry of the manifold to compute that distance, depending on its curvature. This figure is only for illustration: Riemannian metrics are purely local, \\Delta \\theta should be much smaller.\n\n\n\nLet’s consider a parameterized distribution p(x; \\theta) and its new value p(x; \\theta + \\Delta \\theta) after applying a small parameter change \\Delta \\theta. As depicted on Figure 13.2, the Euclidian metric in the parameter space (||\\theta + \\Delta \\theta - \\theta||^2) does not take the structure of the statistical manifold into account. We need to define a Riemannian metric which accounts locally for the curvature of the manifold between \\theta and \\theta + \\Delta \\theta. The Riemannian distance is defined by the dot product:\n\n    ||\\Delta \\theta||^2 = &lt; \\Delta \\theta , F(\\theta) \\, \\Delta \\theta &gt;\n\nwhere F(\\theta) is called the Riemannian metric tensor and is an inner product on the tangent space of the manifold at the point \\theta.\nWhen using the symmetric KL divergence to measure the distance between two distributions, the corresponding Riemannian metric is the Fisher Information Matrix (FIM), defined as the Hessian matrix of the KL divergence around \\theta, i.e. the matrix of second order derivatives w.r.t the elements of \\theta. See https://stats.stackexchange.com/questions/51185/connection-between-fisher-metric-and-the-relative-entropy and https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/ for an explanation of the link between the Fisher matrix and KL divergence.\nThe Fisher information matrix is defined as the Hessian of the KL divergence around \\theta, i.e. how the manifold locally changes around \\theta:\n\n    F(\\theta) = \\nabla^2 D_{JS}(p(x; \\theta) || p(x; \\theta + \\Delta \\theta))|_{\\Delta \\theta = 0}\n\nwhich necessitates to compute second order derivatives which are very complex and slow to obtain, especially when there are many parameters \\theta (the weights of the NN). Fortunately, it also has a simpler form which only depends on the outer product between the gradients of the log-likelihoods:\n\n    F(\\theta) = \\mathbb{E}_{x \\sim p(x, \\theta)}[ \\nabla \\log p(x; \\theta)  (\\nabla \\log p(x; \\theta))^T]\n\nwhich is something we can easily sample and compute.\nWhy is it useful? The Fisher Information matrix allows to locally approximate (for small \\Delta \\theta) the KL divergence between the two close distributions (using a second-order Taylor series expansion):\n\n    D_{JS}(p(x; \\theta) || p(x; \\theta + \\Delta \\theta)) \\approx \\Delta \\theta^T \\, F(\\theta) \\, \\Delta \\theta\n\nThe KL divergence is then locally quadratic, which means that the update rules obtained when minimizing the KL divergence with gradient descent will be linear. Suppose we want to minimize a loss function L parameterized by \\theta and depending on the distribution p. Natural gradient descent (Amari, 1998) attempts to move along the statistical manifold defined by p by correcting the gradient of L(\\theta) using the local curvature of the KL-divergence surface, i.e. moving some given distance in the direction \\tilde{\\nabla_\\theta} L(\\theta):\n\n    \\tilde{\\nabla_\\theta} L(\\theta) = F(\\theta)^{-1} \\, \\nabla_\\theta L(\\theta)\n\n\\tilde{\\nabla_\\theta} L(\\theta) is the natural gradient of L(\\theta). Natural gradient descent simply takes steps in this direction:\n\n    \\Delta \\theta = - \\eta \\, \\tilde{\\nabla_\\theta} L(\\theta)\n\nWhen the manifold is not curved (F(\\theta) is the identity matrix), natural gradient descent is the regular gradient descent.\nBut what is the advantage of natural gradients? The problem with regular gradient descent is that it relies on a fixed learning rate. In regions where the loss function is flat (a plateau), the gradient will be almost zero, leading to very slow improvements. Because the natural gradient depends on the inverse of the curvature (Fisher), the magnitude of the gradient will be higher in flat regions, leading to bigger steps, and smaller in very steep regions (around minima). Natural GD therefore converges faster and better than regular GD.\nNatural gradient descent is a generic optimization method, it can for example be used to train more efficiently deep networks in supervised learning. Its main drawback is the necessity to inverse the Fisher information matrix, whose size depends on the number of free parameters (if you have N weights in the NN, you need to inverse a NxN matrix). Several approximations allows to remediate to this problem, for example Conjugate Gradients or Kronecker-Factored Approximate Curvature (K-FAC).\nAdditional resources to understand natural gradients:\n\nhttp://andymiller.github.io/2016/10/02/natural_gradient_bbvi.html\nhttps://hips.seas.harvard.edu/blog/2013/01/25/the-natural-gradient/\nhttp://kvfrans.com/what-is-the-natural-gradient-and-where-does-it-appear-in-trust-region-policy-optimization\nhttps://wiseodd.github.io/techblog/2018/03/14/natural-gradient/\nA tutorial by John Schulman (OpenAI) https://www.youtube.com/watch?v=xvRrgxcpaHY\nA blog post on the related Hessian-free optimization and conjuguate gradients http://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/\nK-FAC: https://syncedreview.com/2017/03/25/optimizing-neural-networks-using-structured-probabilistic-models/\nConjugate gradients: https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\n\nNote: Natural gradients can also be used to train DQN architectures, resulting in more efficient and stable learning behaviors (Knight and Lerner, 2018).\n\n\nNatural policy gradient and Natural Actor Critic (NAC)\nKakade (2001) applied the principle of natural gradients proposed by Amari (1998) to the policy gradient theorem:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)]\n\nThis regular gradient does not take into account the underlying structure of the policy distribution \\pi(s, a). The Fisher information matrix for the policy is defined by:\n\n    F(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[ \\nabla \\log \\pi_\\theta(s, a)  (\\nabla \\log \\pi_\\theta(s, a))^T]\n\nThe natural policy gradient is simply:\n\n    \\tilde{\\nabla}_\\theta J(\\theta) = F(\\theta)^{-1} \\, \\nabla_\\theta J(\\theta)  = \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[ F(\\theta)^{-1} \\,  \\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)]\n\nKakade (2001) also shows that you can replace the true Q-value Q^{\\pi_\\theta}(s, a) with a compatible approximation Q_\\varphi(s, a) (as long as it minimizes the quadratic error) and still obtained an unbiased natural gradient. An important theoretical result is that policy improvement is guaranteed with natural gradients: the new policy after an update is always better (more expected returns) than before. He experimented this new rule on various simple MDPs and observed drastic improvements over vanilla PG.\nPeters and Schaal (2008) extended on the work of Kakade (2001) to propose the natural actor-critic (NAC). The exact derivations would be too complex to summarize here, but the article is an interesting read. He particularly reviews the progress at that time on policy gradient for its use in robotics. He showed that the F(\\theta)) is a true Fisher information matrix even when using sampled episodes, and derived a baseline b to reduce the variance of the natural policy gradient. He demonstrated the power of this algorithm by letting a robot learning motor primitives for baseball.\n\n\nTrust Region Policy Optimization (TRPO)\n\nPrinciple\nSchulman et al. (2015) extended the idea of natural gradients to allow their use for non-linear function approximators (e.g. deep networks), as the previous algorithms only worked efficiently for linear approximators. The proposed algorithm, Trust Region Policy Optimization (TRPO), has now been replaced in practice by Proximal Policy Optimization (PPO, see next section) but its novel ideas are important to understand already.\nLet’s note the expected return of a policy \\pi as:\n\n    \\eta(\\pi) = \\mathbb{E}_{s \\sim \\rho_\\pi, a \\sim \\pi}[\\sum_{t=0}^\\infty \\gamma^t \\, r(s_t, a_t, s_{t+1})]\n\nwhere \\rho_\\pi is the discounted visitation frequency distribution (the probability that a state s will be visited at some point in time by the policy \\pi):\n\n    \\rho_\\pi(s) = P(s_0=s) + \\gamma \\, P(s_1=s) + \\gamma^2 \\, P(s_2=s) + \\ldots\n\nKakade and Langford (2002) had shown that it is possible to relate the expected return of two policies \\pi_\\theta and \\pi_{\\theta_\\text{old}} using advantages (omitting \\pi in the notations):\n\n    \\eta(\\theta) = \\eta(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\pi_\\theta}, a \\sim \\pi_\\theta} [A_{\\pi_{\\theta_\\text{old}}}(s, a)]\n\nThe advantage A_{\\pi_{\\theta_\\text{old}}}(s, a) denotes the change in the expected return obtained after (s, a) when using the new policy \\pi_\\theta, in comparison to the one obtained with the old policy \\pi_{\\theta_\\text{old}}. While this formula seems interesting (it measures how good the new policy is with regard to the average performance of the old policy, so we could optimize directly), it is difficult to estimate as the mathematical expectation depends on state-action pairs generated by the new policy : s \\sim \\rho_{\\pi_\\theta}, a \\sim \\pi_\\theta.\nSchulman et al. (2015) propose an approximation to this formula, by considering that if the two policies \\pi_\\theta and \\pi_{\\theta_\\text{old}} are not very different from another, one can sample the states from the old distribution:\n\n    \\eta(\\theta) \\approx \\eta(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_\\text{old}}}, a \\sim \\pi_\\theta} [A_{\\pi_{\\theta_\\text{old}}}(s, a)]\n\nOne can already recognize the main motivation behind natural gradients: finding a weight update that moves the policy in the right direction (getting more rewards) while keeping the change in the policy distribution as small as possible (to keep the assumption correct).\nLet’s now define the following objective function:\n\n    J_{\\theta_\\text{old}}(\\theta) = \\eta(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_\\text{old}}}, a \\sim \\pi_\\theta} [A_{\\pi_{\\theta_\\text{old}}}(s, a)]\n\nIt is easy to see that J_{\\theta_\\text{old}}(\\theta_\\text{old}) = \\eta(\\theta_\\text{old}) by definition of the advantage, and that its gradient w.r.t to \\theta taken in \\theta_\\text{old} is the same as the one of \\eta(\\theta_\\text{old}):\n\n    \\nabla_\\theta J_{\\theta_\\text{old}}(\\theta)|_{\\theta = \\theta_\\text{old}} = \\nabla_\\theta \\eta(\\theta)|_{\\theta = \\theta_\\text{old}}\n\nThis means that, at least locally, one maximization step of J_{\\theta_\\text{old}}(\\theta) goes in the same direction as maximizing \\eta(\\theta) if we do not go too far. J is called a surrogate objective function: it is not what we want to optimize, but it leads to the same result. TRPO belongs to the class of minorization-majorization algorithms (MM, we first find a local lower bound and then maximize it, iteratively).\nLet’s now suppose that we can find its maximum, i.e. a policy \\pi' that maximizes the advantage of each state-action pair over \\pi_{\\theta_\\text{old}}. There would be no guarantee that \\pi' and \\pi_{\\theta_\\text{old}} are close enough so that the assumption stands. We could therefore make only a small step in its direction and hope for the best:\n\n    \\pi_\\theta(s, a) = (1-\\alpha) \\, \\pi_{\\theta_\\text{old}}(s, a) + \\alpha \\, \\pi'(s,a)\n\\tag{13.1}\nThis is the conservative policy iteration method of Kakade and Langford (2002), where a bound on the difference between \\eta(\\pi_{\\theta_\\text{old}}) and J_{\\theta_\\text{old}}(\\theta) can be derived.\nSchulman et al. (2015) propose to penalize instead the objective function by the KL divergence between the new and old policies. There are basically two ways to penalize an optimization problem:\n\nAdding a hard constraint on the KL divergence, leading to a constrained optimization problem (where Lagrange methods can be applied):\n\n\n    \\text{maximize}_\\theta \\qquad J_{\\theta_\\text{old}}(\\theta) \\\\\n    \\qquad \\text{subject to} \\qquad D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) \\leq \\delta\n\n\nRegularizing the objective function with the KL divergence:\n\n\n    \\text{maximize}_\\theta \\qquad L(\\theta) = J_{\\theta_\\text{old}}(\\pi_\\theta) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta)\n\nIn the first case, we force the KL divergence to stay below a certain threshold. In the second case, we penalize solutions that would maximize J_{\\theta_\\text{old}}(\\theta) but would be too different from the previous policy. In both cases, we want to find a policy \\pi_\\theta maximizing the expected return (the objective), but which is still close (in terms of KL divergence) from the current one. Both methods are however sensible to the choice of the parameters \\delta and C.\nFormally, the KL divergence D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) should be the maximum KL divergence over the state space:\n\n    D^\\text{max}_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) = \\max_s D_{KL}(\\pi_{\\theta_\\text{old}}(s, .) || \\pi_\\theta(s, .))\n\nThis maximum KL divergence over the state space would be very hard to compute. Empirical evaluations showed however that it is safe to use the mean KL divergence, or even to sample it:\n\n    \\bar{D}_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) = \\mathbb{E}_s [D_{KL}(\\pi_{\\theta_\\text{old}}(s, .) || \\pi_\\theta(s, .))] \\approx \\frac{1}{N} \\sum_{i=1}^N D_{KL}(\\pi_{\\theta_\\text{old}}(s_i, .) || \\pi_\\theta(s_i, .))\n\n\n\nTrust regions\n\n\n\n\n\n\nFigure 13.3: Graphical illustration of trust regions. From the current parameters \\theta_\\text{old}, we search for the maximum \\theta^* of the real objective \\eta(\\theta). The unconstrained objective J_{\\theta_\\text{old}}(\\theta) is locally similar to \\eta(\\theta) but quickly diverge as \\pi_\\theta and \\pi_{\\theta_\\text{old}} become very different. The surrogate objective L(\\theta) = J_{\\theta_\\text{old}} (\\theta) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) is always smaller than \\eta(\\theta) and has a maximum close to \\theta^* which keeps \\pi_\\theta and \\pi_{\\theta_\\text{old}} close from each other in terms of KL divergence. The region around \\theta_\\text{old} where big optimization steps can be taken without changing the policy too much is called the trust region.\n\n\n\nBefore diving further into how these optimization problems can be solved, let’s wonder why the algorithm is called trust region policy optimization using the regularized objective. Figure 13.3 illustrates the idea. The “real” objective function \\eta(\\theta) should be maximized (with gradient descent or similar) starting from the parameters \\theta_\\text{old}. We cannot estimate the objective function directly, so we build a surrogate objective function L(\\theta) = J_{\\theta_\\text{old}} (\\theta) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta). We know that:\n\nThe two objectives have the same value in \\theta_\\text{old}: L(\\theta_\\text{old}) = J_{\\theta_\\text{old}}(\\theta_\\text{old}) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_{\\theta_\\text{old}}) = \\eta(\\theta_\\text{old})\nTheir gradient w.r.t \\theta are the same in \\theta_\\text{old}: \\nabla_\\theta L(\\theta)|_{\\theta = \\theta_\\text{old}} = \\nabla_\\theta \\eta(\\theta)|_{\\theta = \\theta_\\text{old}}\nThe surrogate objective is always smaller than the real objective, as the KL divergence is positive: \\eta(\\theta) \\geq J_{\\theta_\\text{old}}(\\theta) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta).\n\nUnder these conditions, the surrogate objective is also called a lower bound of the primary objective. The interesting fact is that the value of \\theta that maximizes L(\\theta) is at the same time:\n\nA big step in the parameter space towards the maximum of \\eta(\\theta), as \\theta and \\theta_\\text{old} can be very different.\nA small step in the policy distribution space, as the KL divergence between the previous and the new policies is kept small.\n\nExactly what we needed! The parameter region around \\theta_\\text{old} where the KL divergence is kept small is called the trust region. This means that we can safely take big optimization steps (e.g. with a high learning rate or even analytically) without risking to violate the initial assumptions.\n\n\nSample-based formulation\nAlthough the theoretical proofs in Schulman et al. (2015) used the regularized optimization method, the practical implementation uses the constrained optimization problem:\n\n    \\text{maximize}_\\theta \\qquad J_{\\theta_\\text{old}}(\\theta) = \\eta(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_\\text{old}}}, a \\sim \\pi_\\theta} [A_{\\pi_{\\theta_\\text{old}}}(s, a)] \\\\\n    \\qquad \\text{subject to} \\qquad D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) \\leq \\delta\n\nThe first thing to notice is that \\eta(\\theta_\\text{old}) does not depend on \\theta, so it is constant in the optimization problem. We only need to maximize the advantage of the actions taken by \\pi_\\theta in each state visited by \\pi_{\\theta_\\text{old}}. The problem is that \\pi_\\theta is what we search, so we can not sample actions from it. The solution is to use importance sampling to allow sampling actions from \\pi_{\\theta_\\text{old}}. This is possible as long as we correct the objective with the importance sampling weight:\n\n    \\text{maximize}_\\theta \\qquad \\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_\\text{old}}}, a \\sim \\pi_{\\theta_\\text{old}}} [\\frac{\\pi_\\theta(s, a)}{\\pi_{\\theta_\\text{old}}(s, a)} \\, A_{\\pi_{\\theta_\\text{old}}}(s, a)] \\\\\n    \\qquad \\text{subject to} \\qquad D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) \\leq \\delta\n\nNow that states and actions are generated by the old policy, we can safely sample many trajectories using \\pi_{\\theta_\\text{old}} (Schulman et al. (2015) proposes two methods called single path and Vine, but we ignore it here), compute the advantages of all state-action pairs (using real rewards along the trajectories), form the surrogate objective function and optimize it using second-order optimization methods.\nOne last thing to notice is that the advantages A_{\\pi_{\\theta_\\text{old}}}(s, a) = Q_{\\pi_{\\theta_\\text{old}}}(s, a) - V_{\\pi_{\\theta_\\text{old}}}(s) depend on the value of the states encountered by \\pi_{\\theta_\\text{old}}. The state values do not depend on the policies, they are constant for each optimization step, so they can also be safely removed:\n\n    \\text{maximize}_\\theta \\qquad \\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_\\text{old}}}, a \\sim \\pi_{\\theta_\\text{old}}} [\\frac{\\pi_\\theta(s, a)}{\\pi_{\\theta_\\text{old}}(s, a)} \\, Q_{\\pi_{\\theta_\\text{old}}}(s, a)] \\\\\n    \\qquad \\text{subject to} \\qquad D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) \\leq \\delta\n\nHere we go, that’s TRPO. It could seem a bit disappointing to come up with such a simple formulation (find a policy which maximizes the Q-value of sampled actions while being not too different from the previous one) after so many mathematical steps, but that is also the beauty of it: not only it works, but it is guaranteed to work. With TRPO, each optimization step brings the policy closer from an optimum, what is called monotonic improvement.\n\n\nPractical implementation\nNow, how do we solve the constrained optimization problem? And what is the link with natural gradients?\nTo solve constrained optimization problems, we can form a Lagrange function with an additional parameter \\lambda and search for its maximum:\n\n    \\mathcal{L}(\\theta, \\lambda) = J_{\\theta_\\text{old}}(\\theta)  - \\lambda \\, (D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) - \\delta)\n\nNotice how close the Lagrange function is from the regularized problem used in the theory. We can form a first-order approximation of J_{\\theta_\\text{old}}(\\theta):\n\n    J_{\\theta_\\text{old}}(\\theta) = \\nabla_\\theta J_{\\theta_\\text{old}}(\\theta) \\, (\\theta- \\theta_\\text{old})\n\nas J_{\\theta_\\text{old}}(\\theta_\\text{old}) = 0. g = \\nabla_\\theta J_{\\theta_\\text{old}}(\\theta) is the now familiar policy gradient with importance sampling. Higher-order terms do not matter, as they are going to be dominated by the KL divergence term.\nWe will use a second-order approximation of the KL divergence term using the Fisher Information Matrix:\n\n    D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) = (\\theta- \\theta_\\text{old})^T \\, F(\\theta_\\text{old}) \\,  (\\theta- \\theta_\\text{old})\n\nWe get the following Lagrangian function:\n\n    \\mathcal{L}(\\theta, \\lambda) = \\nabla_\\theta J_{\\theta_\\text{old}}(\\theta) \\, (\\theta- \\theta_\\text{old})  - \\lambda \\, (\\theta- \\theta_\\text{old})^T \\, F(\\theta_\\text{old}) \\,  (\\theta- \\theta_\\text{old})\n\nwhich is quadratic in \\Delta \\theta = \\theta- \\theta_\\text{old}. It has therefore a unique maximum, characterized by a first-order derivative equal to 0:\n\n    \\nabla_\\theta J_{\\theta_\\text{old}}(\\theta) = \\lambda \\, F(\\theta_\\text{old}) \\,  \\Delta \\theta\n\nor:\n\n    \\Delta \\theta  = \\frac{1}{\\lambda} \\, F(\\theta_\\text{old})^{-1} \\,  \\nabla_\\theta J_{\\theta_\\text{old}}(\\theta)\n\nwhich is the natural gradient descent! The size of the step \\frac{1}{\\lambda} still has to be determined, but it can also be replaced by a fixed hyperparameter.\nThe main problem is now to compute and inverse the Fisher information matrix, which is quadratic with the number of parameters \\theta, i.e. with the number of weights in the NN. Schulman et al. (2015) proposes to used conjugate gradients to iteratively approximate the Fisher, a second-order method which will not be presented here (see https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf for a detailed introduction). After the conjugate gradient optimization step, the constraint D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta) \\leq \\delta is however not ensured anymore, so a line search is made as in Equation 13.1 until that criteria is met.\n\n\nSummary\nTRPO is a policy gradient method using natural gradients to monotonically improve the expected return associated to the policy. As a minorization-maximization (MM) method, it uses a surrogate objective function (a lower bound on the expected return) to iteratively change the parameters of the policy using large steps, but without changing the policy too much (as measured by the KL divergence). Its main advantage over DDPG is that it is much less sensible to the choice of the learning rate.\nHowever, it has several limitations:\n\nIt is hard to use with neural networks having multiple outputs (e.g. the policy and the value function, as in actor-critic methods) as natural gradients are dependent on the policy distribution and its relationship with the parameters.\nIt works well when the NN has only fully-connected layers, but empirically performs poorly on tasks requiring convolutional layers or recurrent layers.\nThe use of conjugate gradients makes the implementation much more complicated and less flexible than regular SGD.\n\nAdditional resources\n\nhttp://178.79.149.207/posts/trpo.html\nhttps://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9\nhttps://medium.com/@sanketgujar95/trust-region-policy-optimization-trpo-and-proximal-policy-optimization-ppo-e6e7075f39ed\nhttps://www.depthfirstlearning.com/2018/TRPO\nhttp://rll.berkeley.edu/deeprlcoursesp17/docs/lec5.pdf\n\n\n\n\nProximal Policy Optimization (PPO)\nProximal Policy Optimization (PPO) was proposed by Schulman et al. (2017) to overcome the problems of TRPO (complexity, inability to share parameters or to use complex NN architectures) while increasing the range of tasks learnable by the system (compared to DQN) and improving the sample complexity (compared to online PG methods, which perform only one update per step).\nFor that, they investigated various surrogate objectives (lower bounds) that could be solved using first-order optimization techniques (gradient descent). Let’s rewrite the surrogate loss of TRPO in the following manner:\n\n    L^\\text{CPI}(\\theta) = \\mathbb{E}_{t} [\\frac{\\pi_\\theta(s_t, a_t)}{\\pi_{\\theta_\\text{old}}(s_t, a_t)} \\, A_{\\pi_{\\theta_\\text{old}}}(s_t, a_t)] = \\mathbb{E}_{t} [\\rho_t(\\theta) \\, A_{\\pi_{\\theta_\\text{old}}}(s_t, a_t)]\n\nby making the dependency over time explicit and noting the importance sampling weight \\rho_t(\\theta). The superscript CPI refers to conservative policy iteration (Kakade and Langford, 2002). Without a constraint, the maximization of L^\\text{CPI} would lead to an excessively large policy updates. The authors searched how to modify the objective, in order to penalize changes to the policy that make \\rho_t(\\theta) very different from 1, i.e. where the KL divergence between the new and old policies would become high. They ended up with the following surrogate loss:\n\n    L^\\text{CLIP}(\\theta) = \\mathbb{E}_{t} [ \\min (\\rho_t(\\theta) \\, A_{\\pi_{\\theta_\\text{old}}}(s_t, a_t), \\text{clip}(\\rho_t(\\theta) , 1- \\epsilon, 1+\\epsilon) \\,  A_{\\pi_{\\theta_\\text{old}}}(s_t, a_t)]\n\nThe left part of the min operator is the surrogate objective of TRPO L^\\text{CPI}(\\theta). The right part restricts the importance sampling weight between 1-\\epsilon and 1 +\\epsilon. Let’s consider two cases (depicted on Figure 13.4):\n\n\n\n\n\n\nFigure 13.4: Illustration of the effect of clipping the importance sampling weight. Source: Schulman et al. (2017).\n\n\n\n\nthe transition s_t, a_t has a positive advantage, i.e. it is a better action than expected. The probability of selecting that action again should be increased, i.e. \\pi_\\theta(s_t, a_t) &gt; \\pi_{\\theta_\\text{old}}(s_t, a_t). However, the importance sampling weight could become very high (a change from 0.01 to 0.05 is a ration of \\rho_t(\\theta) = 5). In that case, \\rho_t(\\theta) will be clipped to 1+\\epsilon, for example 1.2. As a consequence, the parameters \\theta will move in the right direction, but the distance between the new and the old policies will stay small.\nthe transition s_t, a_t has a negative advantage, i.e. it is a worse action than expected. Its probability will be decreased and the importance sampling weight might become much smaller than 1. Clipping it to 1-\\epsilon avoids drastic changes to the policy, while still going in the right direction.\n\nFinally, they take the minimum of the clipped and unclipped objective, so that the final objective is a lower bound of the unclipped objective. In the original paper, they use generalized advantage estimation (GAE) to estimate A_{\\pi_{\\theta_\\text{old}}}(s_t, a_t), but anything could be used (n-steps, etc). Transitions are sampled by multiple actors in parallel, as in A2C.\nThe pseudo-algorithm of PPO is as follows:\n\n\nInitialize an actor \\pi_\\theta and a critic V_\\varphi with random weights.\nwhile not converged :\n\nfor N actors in parallel:\n\nCollect T transitions using \\pi_\\text{old}.\nCompute the generalized advantage of each transition using the critic.\n\nfor K epochs:\n\nSample M transitions from the ones previously collected.\nTrain the actor to maximize the clipped surrogate objective.\nTrain the critic to minimize the mse using TD learning.\n\n\\theta_\\text{old} \\leftarrow \\theta\n\n\n\nThe main advantage of PPO with respect to TRPO is its simplicity: the clipped objective can be directly maximized using first-order methods like stochastic gradient descent or Adam. It does not depend on assumptions about the parameter space: CNNs and RNNs can be used for the policy. It is sample-efficient, as several epochs of parameter updates are performed between two transition samplings: the policy network therefore needs less fresh samples that strictly on-policy algorithms to converge.\nThe only drawbacks of PPO is that there no convergence guarantee (although in practice it converges more often than other state-of-the-art methods) and that the right value for \\epsilon has to be determined. PPO has improved the state-of-the-art on Atari games and Mujoco robotic tasks. It has become the go-to method for continuous control problems.\nAdditional resources\n\nMore explanations and demos from OpenAI: https://blog.openai.com/openai-baselines-ppo\n\n\n\nActor-Critic with Experience Replay (ACER)\nThe natural gradient methods presented above are stochastic actor-critic methods, therefore strictly on-policy. Off-policy methods such as DQN or DDPG allow to reuse past transitions through the usage of an experience replay memory, potentially reducing the sample complexity at the cost of a higher variance and worse stability. Wang et al. (2017) proposed an off-policy actor-critic architecture using variance reduction techniques, the off-policy Retrace algorithm (Munos et al., 2016), parallel training of multiple actor-learners (Mnih et al., 2016), truncated importance sampling with bias correction, stochastic duelling network architectures (Wang et al., 2016), and efficient trust region policy optimization. It can be seen as the off-policy counterpart to A3C.\nThe first aspect of ACER is that it interleaves on-policy learning with off-policy: the agent samples a trajectory \\tau, learns from it on-policy, stores it in the replay buffer, samples n trajectories from the replay buffer and learns off-policy from each of them:\n\n\nSample a trajectory \\tau using the current policy.\nApply ACER on-policy on \\tau.\nStore \\tau in the replay buffer.\nSample n trajectories from the replay buffer.\nfor each sampled trajectory \\tau_k:\n\nApply ACER off-policy on \\tau_k.\n\n\n\nMixing on-policy learning with off-policy is quite similar to the Self-Imitation Learning approach of (Oh et al., 2018).\n\nRetrace evaluation\nACER comes in two flavors: one for discrete action spaces, one for continuous spaces. The discrete version is simpler, so let’s focus on this one. As any policy-gradient method, ACER tries to estimate the policy gradient for each transition of a trajectory, but using importance sampling (Degris et al., 2012):\n\n    \\nabla_\\theta J(\\theta)  = \\mathbb{E}_{s_t, a_t \\sim \\rho_b} [\\frac{\\pi_\\theta(s_t, a_t)}{b(s_t, a_t)} \\, Q_\\varphi(s_t, a_t) \\, \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)]\n\nThe problem is now to train the critic Q_\\varphi(s_t, a_t) by computing the correct target. ACER learning builds on the Retrace algorithm (Munos et al., 2016):\n\n    \\Delta Q_\\varphi(s_t, a_t) = \\alpha \\, \\sum_{t'=t}^T (\\gamma)^{t'-t} \\left(\\prod_{s=t+1}^{t'} c_s \\right) \\, \\delta_{t'}\n\nwith c_s = \\lambda \\min (1, \\frac{\\pi_\\theta(s_s, a_s)}{b(s_s, a_s)}) being the clipped importance sampling weight and \\delta_{t'} is the TD error at time t'&gt;t:\n\n    \\delta_{t'} = r_{t'+1} + \\gamma \\, V(s_{t'+1}) - V(s_{t'})\n\nBy noting Q^\\text{ret} the target value for the update of the critic (neglecting the learning rate \\alpha):\n\n    Q^\\text{ret}(s_t, a_t) = Q_\\varphi(s_t, a_t) +  \\sum_{t'=t}^T (\\gamma)^{t'-t} \\left(\\prod_{s=t+1}^{t'} c_s \\right) \\, \\delta_{t'}\n\nwe can transform the Retrace formula into a recurrent one:\n\n\\begin{aligned}\n    Q^\\text{ret}(s_t, a_t) & = Q_\\varphi(s_t, a_t) + \\delta_t + \\sum_{t'=t+1}^T (\\gamma)^{t'-t} \\left(\\prod_{s=t+1}^{t'} c_s \\right) \\, \\delta_{t'} \\\\\n    & = Q_\\varphi(s_t, a_t) + \\delta_t + \\gamma \\, c_{t+1} \\, (Q^\\text{ret}(s_{t+1}, a_{t+1}) - Q_\\varphi(s_{t+1}, a_{t+1})) \\\\\n\\end{aligned}\n\nQ_\\varphi(s_t, a_t) + \\delta_t = Q_\\varphi(s_t, a_t) + r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t) can furthermore be reduced to r_{t+1} + \\gamma \\, V(s_{t+1}) by considering that Q_\\varphi(s_t, a_t) \\approx V(s_t) (the paper does not justify this assumption, but it should be true in expectation).\nThis gives us the following target value for the Q-values:\n\n    Q^\\text{ret}(s_t, a_t) = r_{t+1} + \\gamma \\, c_{t+1} \\, (Q^\\text{ret}(s_{t+1}, a_{t+1}) - Q_\\varphi(s_{t+1}, a_{t+1})) + \\gamma \\, V(s_{t+1})\n\nOne remaining issue is that the critic would also need to output the value of each state V(s_{t+1}), in addition to the Q-values Q_\\varphi(s_t, a_t). In the discrete case, this is not necessary, as the value of a state is the expectation of the value of the available actions under the current policy:\n\n    V(s_{t+1}) = \\mathbb{E}_{a_{t+1} \\sim \\pi_\\theta} [Q_\\varphi(s_{t+1}, a_{t+1})] = \\sum_a \\pi_\\theta(s_{t+1}, a) \\, Q_\\varphi(s_{t+1}, a))\n\nThe value of the next state can be easily computed when we have the policy \\pi_\\theta(s, a) (actor) and the Q-value Q_\\varphi(s, a) (critic) of each action a in a state s.\nThe actor-critic architecture needed for ACER is therefore the following:\n\nThe actor \\pi_\\theta takes a state s as input and outputs a vector of probabilities \\pi_\\theta for each available action.\nThe critic Q_\\varphi takes a state s as input and outputs a vector of Q-values.\n\nThis is different from the architecture of A3C, where the critic “only” had to output the value of a state V_\\varphi(s): it is now a vector of Q-values. Note that the actor and the critic can share most of their parameters: the network only needs to output two different vectors \\pi_\\theta(s) and Q_\\varphi(s) for each input state s (Figure 13.5). This makes a “two heads” NN, similar to the duelling architecture of (Wang et al., 2016).\n\n\n\n\n\n\nFigure 13.5: Architecture of the ACER actor-critic.\n\n\n\nThe target Q-value Q^\\text{ret}(s, a) can be found recursively by iterating backwards over the episode:\n\n\nInitialize Q^\\text{ret}(s_T, a_T), Q_\\varphi(s_T, a_T) and V(s_T) to 0, as the terminal state has no value.\nfor t \\in [T-1, \\ldots, 0]:\n\nUpdate the target Q-value using the received reward, the critic and the previous target value:\n\n\n      Q^\\text{ret}(s_t, a_t) = r_{t+1} + \\gamma \\, c_{t+1} \\, (Q^\\text{ret}(s_{t+1}, a_{t+1}) - Q_\\varphi(s_{t+1}, a_{t+1})) + \\gamma \\, V(s_{t+1})\n  \n\nApply the critic on the current action:\n\n\n      Q_\\varphi(s_t, a_t)\n  \n\nEstimate the value of the state using the critic:\n\n\n      V(s_t) = \\sum_a \\pi_\\theta(s_t) \\, Q_\\varphi(s_t, a)\n  \n\n\nAs the target value Q^\\text{ret}(s, a) use multiple “real” rewards r_{t+1}, it is actually less biased than the critic Q_\\varphi(s, a). It is then better to use it to update the actor:\n\n    \\nabla_\\theta J(\\theta)  = \\mathbb{E}_{s_t, a_t \\sim \\rho_b} [\\frac{\\pi_\\theta(s_t, a_t)}{b(s_t, a_t)} \\, Q^\\text{ret}(s_t, a_t) \\, \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)]\n\nThe critic just has to minimize the mse with the target value:\n\n    \\mathcal{L}(\\varphi) = \\mathbb{E}_{s_t, a_t \\sim \\rho_b} [(Q^\\text{ret}(s, a) - Q_\\varphi(s, a))^2]\n\n\n\nImportance weight truncation with bias correction\nWhen updating the actor, we rely on the importance sampling weight \\rho_t = \\frac{\\pi_\\theta(s_t, a_t)}{b(s_t, a_t)} which can vary a lot and destabilize learning.\n\n    \\nabla_\\theta J(\\theta)  = \\mathbb{E}_{s_t, a_t \\sim \\rho_b} [\\rho_t \\, Q^\\text{ret}(s_t, a_t) \\, \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)]\n\nPPO solved this problem by clipping the importance sampling weight between 1- \\epsilon and 1+\\epsilon. ACER uses a similar strategy, but only using an upper bound c = 10 on the weight:\n\n    \\bar{\\rho}_t = \\min(c, \\rho_t)\n\nUsing \\bar{\\rho}_t in the policy gradient directly would introduce a bias: actions whose importance sampling weight \\rho_t is higher than c would contribute to the policy gradient with a smaller value than they should, introducing a bias.\nThe solution in ACER is to add a bias correcting term, that corrects the policy gradient when an action has a weight higher than c:\n\n\\begin{aligned}\n    \\nabla_\\theta J(\\theta)  = & \\mathbb{E}_{s_t \\sim \\rho_b} [\\mathbb{E}_{a_t \\sim b} [\\bar{\\rho}_t \\, Q^\\text{ret}(s_t, a_t) \\, \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)] \\\\\n    & + \\mathbb{E}_{a \\sim \\pi_\\theta}[(\\frac{\\rho_t(a) - c}{\\rho_t(a)})^+ \\, Q_\\varphi(s_t, a) \\, \\nabla_\\theta \\log \\pi_\\theta(s_t, a)]] \\\\\n\\end{aligned}\n\nThe left part of that equation is the same policy gradient as before, but using a clipped importance sampling weight.\nThe right part requires to integrate over all possible actions in the state s_t according to the learned policy \\pi_\\theta, although only the action a_t was selected by the behavior policy b. The term (\\frac{\\rho_t(a) - c}{\\rho_t(a)})^+ is zero for all actions having an importance sampling weight smaller than c, and has a maximum of 1. In practice, this correction term can be computed using the vectors \\pi_\\theta(s, a) and Q_\\varphi(s, a), which are the outputs of the actor and the critic, respectively.\nFinally, the Q-values Q^\\text{ret}(s_t, a_t) and Q_\\varphi(s_t, a) are transformed into advantages Q^\\text{ret}(s_t, a_t)  - V_\\varphi(s_t) and Q_\\varphi(s_t, a) - V_\\varphi(s_t) by substracting the value of the state in order to reduce the variance of the policy gradient.\nIn short, we now have an estimator of the policy gradient which is unbiased and of smaller variance.\n\n\nEfficient trust region policy optimization\nHowever, the variance is still too high. The last important step of ACER is an efficient TRPO update for the parameters of the actor.\nA first component of their TRPO update is they use a target actor network \\theta' (called averaged policy network in the paper) slowly tracking the actor \\theta after each update:\n\n    \\theta' \\leftarrow \\alpha \\, \\theta' + (1 - \\alpha) \\, \\theta\n\nA second component is that the actor is decomposed into two components:\n\na distribution f.\nthe statistics \\Phi_\\theta(x) of this distribution.\n\nThis is what you do when you apply the softmax action selection on Q-values: the distribution is the Gibbs (or Boltzmann) distribution and the Q-values are its statistics. In the discrete case, they take a categorical (or multinouilli) distribution: \\Phi_\\theta(s) is the probability for each action to be taken and the distribution selects one of them. Think of a dice with one side per action and probabilities governed by the policy. In the continuous case, it could be anything, for example a normal distribution.\nLet’s rewrite the policy gradient with that formulation (we omit here the bias correction, but ACER uses it), but only w.r.t the output of the actor \\Phi_\\theta(s_t) for a state s_t:\n\n    \\hat{g_t}^\\text{ACER} = \\nabla_{\\Phi_\\theta(s_t)} J(\\theta)  = \\bar{\\rho}_t \\, (Q^\\text{ret}(s_t, a_t) - V_\\phi(s_t) ) \\, \\nabla_{\\Phi_\\theta(s_t)} \\log f(a_t | \\Phi_\\theta(s_t))\n\nTo compute the policy gradient, we would only need to apply the chain rule:\n\n    \\nabla_\\theta J(\\theta) = \\mathbb{E}_{s_t, a_t \\sim \\rho_b} [ \\hat{g_t}^\\text{ACER} \\, \\nabla_\\theta \\Phi_\\theta(s_t) ]\n\nThe variance of \\hat{g_t}^\\text{ACER} is too high. ACER defines the following TRPO problem: we search for a gradient z solution of:\n\n    \\min_z ||\\hat{g_t}^\\text{ACER} - z ||^2 \\\\\n    \\text{s.t.} \\quad \\nabla_{\\Phi_\\theta(s_t)} D_{KL}( f(\\cdot | \\Phi_{\\theta'}(s_t) ) || f(\\cdot | \\Phi_{\\theta'}(s_t)) )^T \\times z &lt; \\delta\n\nThe exact meaning of the constraint is hard to grasp, but here some intuition: the change of policy z (remember that \\hat{g_t}^\\text{ACER} is defined w.r.t the output of the actor) should be as orthogonal as possible (within a margin \\delta) to the change of the Kullback-Leibler divergence between the policy defined by the actor (\\theta) and the one defined by the target actor (\\theta'). In other words, we want to update the actor, but without making the new policy too different from its past values (the target actor).\nThe advantage of this formulation is that the objective function is quadratic in z and the constraint is linear. We can therefore very easily find its solution using KKT optimization:\n\n    z^* = \\hat{g_t}^\\text{ACER} - \\max(0, \\frac{k^T \\, \\hat{g_t}^\\text{ACER} - \\delta}{||k||^2}) \\, k\n\nwhere k = \\nabla_{\\Phi_\\theta(s_t)} D_{KL}( f(\\cdot | \\Phi_{\\theta'}(s_t) ) || f(\\cdot | \\Phi_{\\theta'}(s_t)) ).\nHaving obtained z^*, we can safely update the parameters of the actor in the direction of:\n\n    \\nabla_\\theta J(\\theta) = \\mathbb{E}_{s_t, a_t \\sim \\rho_b} [ z^* \\, \\nabla_\\theta \\Phi_\\theta(s_t) ]\n\nAs noted in the paper: “The trust region step is carried out in the space of the statistics of the distribution f , and not in the space of the policy parameters. This is done deliberately so as to avoid an additional back-propagation step through the policy network”. We indeed need only one network update per transition. If the KL divergence was computed with respect to \\pi_\\theta directly, one would need to apply backpropagation on the target network too.\nThe target network \\theta' is furthermore used as the behavior policy b(s, a). here is also a target critic network \\varphi', which is primarily used to compute the value of the states V_{\\varphi'}(s) for variance reduction.\nFor a complete description of the algorithm, refer to the paper… To summarize, ACER is an actor-critic architecture using Retrace estimated values, importance weight truncation with bias correction and efficient TRPO. Its variant for continuous action spaces furthermore uses a Stochastic Dueling Network (SDN) in order estimate both Q_\\varphi(s, a) and V_\\varphi(s). It is straightforward in the discrete case (multiply the policy with the Q-values and take the average) but hard in the continuous case.\nACER improved the performance and/or the sample efficiency of the state-of-the-art (A3C, DDPG, etc) on a variety of tasks (Atari, Mujoco). Apart from truncation with bias correction, all aspects of the algorithm are essential to obtain this improvement, as shown by ablation studies.\n\n\n\n\nAmari, S.-I. (1998). Natural gradient works efficiently in learning. Neural Computation 10, 251–276.\n\n\nArjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein GAN. Available at: http://arxiv.org/abs/1701.07875.\n\n\nDegris, T., White, M., and Sutton, R. S. (2012). Linear Off-Policy Actor-Critic. in Proceedings of the 2012 International Conference on Machine Learning Available at: http://arxiv.org/abs/1205.4839.\n\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al. (2014). Generative Adversarial Networks. Available at: http://arxiv.org/abs/1406.2661.\n\n\nKakade, S. (2001). A Natural Policy Gradient. in Advances in Neural Information Processing Systems 14 Available at: https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf.\n\n\nKakade, S., and Langford, J. (2002). Approximately Optimal Approximate Reinforcement Learning. Proc. 19th International Conference on Machine Learning, 267–274. Available at: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601.\n\n\nKingma, D. P., and Welling, M. (2013). Auto-Encoding Variational Bayes. Available at: http://arxiv.org/abs/1312.6114.\n\n\nKnight, E., and Lerner, O. (2018). Natural Gradient Deep Q-learning. Available at: http://arxiv.org/abs/1803.07482.\n\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., et al. (2016). Asynchronous Methods for Deep Reinforcement Learning. in Proc. ICML Available at: http://arxiv.org/abs/1602.01783.\n\n\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016). Safe and Efficient Off-Policy Reinforcement Learning. Available at: http://arxiv.org/abs/1606.02647.\n\n\nOh, J., Guo, Y., Singh, S., and Lee, H. (2018). Self-Imitation Learning. Available at: http://arxiv.org/abs/1806.05635.\n\n\nPeters, J., and Schaal, S. (2008). Reinforcement learning of motor skills with policy gradients. Neural Networks 21, 682–697. doi:10.1016/j.neunet.2008.02.003.\n\n\nSchulman, J., Chen, X., and Abbeel, P. (2017). Equivalence Between Policy Gradients and Soft Q-Learning. Available at: http://arxiv.org/abs/1704.06440 [Accessed June 12, 2019].\n\n\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2015). High-Dimensional Continuous Control Using Generalized Advantage Estimation. Available at: http://arxiv.org/abs/1506.02438.\n\n\nWang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., et al. (2017). Learning to reinforcement learn. Available at: http://arxiv.org/abs/1611.05763 [Accessed February 5, 2021].\n\n\nWang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de Freitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1511.06581 [Accessed November 21, 2019].",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Policy Optimization (TRPO, PPO)</span>"
    ]
  },
  {
    "objectID": "src/3.6-EntropyRL.html",
    "href": "src/3.6-EntropyRL.html",
    "title": "Maximum Entropy RL (SAC)",
    "section": "",
    "text": "Entropy regularization\nEntropy regularization (Williams and Peng, 1991) adds a regularization term to the objective function:\nJ(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho^\\pi, a_t \\sim \\pi_\\theta}[ R(s_t, a_t) + \\beta \\,  H(\\pi_\\theta(s_t))]\n\\tag{14.1}\nWe will neglect here how the objective function is sampled (policy gradient, etc.) and focus on the second part.\nThe entropy of a discrete policy \\pi_\\theta in a state s_t is given by:\nH(\\pi_\\theta(s_t)) = - \\sum_a \\pi_\\theta(s_t, a) \\, \\log \\pi_\\theta(s_t, a)\nFor continuous actions, one can replace the sum with an integral. The entropy of the policy measures its “randomness”:\nBy adding the entropy as a regularization term directly to the objective function, we force the policy to be as non-deterministic as possible, i.e. to explore as much as possible, while still getting as many rewards as possible. The parameter \\beta controls the level of regularization: we do not want the entropy to dominate either, as a purely random policy does not bring much reward. If \\beta is chosen too low, the entropy won’t play a significant role in the optimization and we may obtain a suboptimal deterministic policy early during training as there was not enough exploration. If \\beta is too high, the policy will be random and suboptimal.\nBesides exploration, why would we want to learn a stochastic policy, while the solution to the Bellman equations is deterministic by definition? A first answer is that we rarely have a MDP: most interesting problems are POMDP, where the states are indirectly inferred through observations, which can be probabilistic. Todorov (2008) showed that a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference (see also Toussaint, 2009).\nConsider a two-opponents game like chess: if you have a deterministic policy, you will always play the same moves in the same configuration. In particular, you will always play the same opening moves. Your game strategy becomes predictable for your opponent, who can adapt accordingly. Having a variety of opening moves (as long as they are not too stupid) is obviously a better strategy on the long term. This is due to the fact that chess is actually a POMDP: the opponent’s strategy and beliefs are not accessible.\nAnother way to view the interest of entropy regularization is to realize that learning a deterministic policy only leads to a single optimal solution to the problem. Learning a stochastic policy forces the agent to learn many optimal solutions to the same problem: the agent is somehow forced to learn as much information as possible for the experienced transitions, potentially reducing the sample complexity.\nEntropy regularization is nowadays used very commonly used in deep RL networks (e.g. O’Donoghue et al., 2016), as it is “only” an additional term to set in the objective function passed to the NN, adding a single hyperparameter \\beta.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Maximum Entropy RL (SAC)</span>"
    ]
  },
  {
    "objectID": "src/3.6-EntropyRL.html#entropy-regularization",
    "href": "src/3.6-EntropyRL.html#entropy-regularization",
    "title": "Maximum Entropy RL (SAC)",
    "section": "",
    "text": "if the policy is fully deterministic (the same action is systematically selected), the entropy is zero as it carries no information.\nif the policy is completely random (all actions are equally surprising), the entropy is maximal.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Maximum Entropy RL (SAC)</span>"
    ]
  },
  {
    "objectID": "src/3.6-EntropyRL.html#soft-q-learning",
    "href": "src/3.6-EntropyRL.html#soft-q-learning",
    "title": "Maximum Entropy RL (SAC)",
    "section": "Soft Q-learning",
    "text": "Soft Q-learning\nEntropy regularization greedily maximizes the entropy of the policy in each state (the objective is the return plus the entropy in the current state). Building on the maximum entropy RL framework (Nachum et al., 2017; Schulman et al., 2017; Ziebart et al., 2008), Haarnoja et al. (2017) proposed a version of soft-Q-learning by extending the definition of the objective:\n\n    J(\\theta) =  \\sum_t \\mathbb{E}_{s_t \\sim \\rho^\\pi, a_t \\sim \\pi_\\theta}[ r(s_t, a_t) + \\beta \\,  H(\\pi_\\theta(s_t))]\n\\tag{14.2}\nIn this formulation based on trajectories, the agent seeks a policy that maximizes the entropy of the complete trajectories rather than the entropy of the policy in each state. This is a very important distinction: the agent does not only search a policy with a high entropy, but a policy that brings into states with a high entropy, i.e. where the agent is the most uncertain. This allows for very efficient exploration strategies, where the agent will try to reduce its uncertainty about the world and gather a lot more information than when simply searching for a good policy.\nNote that it is always possible to fall back to classical Q-learning by setting \\beta=0 and that it is possible to omit this hyperparameter by scaling the rewards with \\frac{1}{\\beta}. The discount rate \\gamma is omitted here for simplicity, but it should be added back when the task has an infinite horizon.\nIn soft Q-learning, the policy can be defined by a softmax over the soft Q-values Q_\\text{soft}(s, a), where \\beta plays the role of the temperature parameter:\n\n    \\pi(s, a) \\propto \\exp(Q_\\text{soft}(s_t, a_t) / \\beta)\n\nNote that -Q_\\text{soft}(s_t, a_t) / \\beta plays the role of the energy of the policy (as in Boltzmann machines), hence the name of the paper (Reinforcement Learning with Deep Energy-Based Policies). We will ignore this analogy here. The normalization term of the softmax (the log-partition function in energy-based models) is also omitted as it later disappears from the equations anyway.\nThe soft Q-values are defined by the following Bellman equation:\n\n    Q_\\text{soft}(s_t, a_t) = r(s_t, a_t) + \\gamma \\, \\mathbb{E}_{s_{t+1} \\in \\rho} [V_\\text{soft}(s_{t+1})]\n\\tag{14.3}\nThis is the regular Bellman equation that can be turned into an update rule for the soft Q-values (minimizing the mse between the l.h.s and the r.h.s). The soft value of a state is given by:\n\n    V_\\text{soft}(s_t) = \\mathbb{E}_{a_{t} \\in \\pi} [Q_\\text{soft}(s_{t}, a_{t}) - \\log \\, \\pi(s_t, a_t)]\n\\tag{14.4}\nThe notation in Haarnoja et al. (2017) is much more complex than that (the paper includes the theoretical proofs), but it boils down to this in Haarnoja et al. (2018). When Equation 14.3 is applied repeatedly with the definition of Equation 14.4, it converges to the optimal solution of Equation 14.2, at least in the tabular case.\nThe soft V-value of a state is the expectation of the Q-values in that state (as in regular RL) minus the log probability of each action. This last term measures the entropy of the policy in each state (when expanding the expectation over the policy, we obtain - \\pi \\log \\pi, which is the entropy).\nIn a nutshell, the soft Q-learning algorithm is:\n\nSample transitions (s, a, r, s') and store them in a replay memory.\nFor each transition (s, a, r, s') in a minibatch of the replay memory:\n\nEstimate V_\\text{soft}(s') with Equation 14.4 by sampling several actions.\nUpdate the soft Q-value of (s,a) with Equation 14.3.\nUpdate the policy (if not using the softmax over soft Q-values directly).\n\n\nThe main drawback of this approach is that several actions have to be sampled in the next state in order to estimate its current soft V-value, what makes it hard to implement in practice. The policy also has to be sampled from the Q-values, what is not practical for continuous action spaces.\nBut the real interesting thing is the policies that are learned in multi-goal settings, as in Figure 14.1. The agent starts in the middle of the environment and can obtain one of the four rewards (north, south, west, east). A regular RL agent would very quickly select only one of the rewards and stick to it. With soft Q-learning, the policy stays stochastic and the four rewards can be obtained even after convergence. This indicates that the soft agent has learned much more about its environment than its hard equivalent, thanks to its maximum entropy formulation.\n\n\n\n\n\n\nFigure 14.1: Policy learned by Soft Q-learning in a multi-goal setting. Source: Haarnoja et al. (2017).",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Maximum Entropy RL (SAC)</span>"
    ]
  },
  {
    "objectID": "src/3.6-EntropyRL.html#soft-actor-critic-sac",
    "href": "src/3.6-EntropyRL.html#soft-actor-critic-sac",
    "title": "Maximum Entropy RL (SAC)",
    "section": "Soft Actor-Critic (SAC)",
    "text": "Soft Actor-Critic (SAC)\nHaarnoja et al. (2018) proposed the Soft Actor-Critic (SAC), an off-policy actor-critic which allows to have a stochastic actor (contrary to DDPG) while being more optimal and sample efficient than on-policy methods such as A3C or PPO. It is also less sensible to hyperparameters than all these methods.\nSAC builds on soft Q-learning to achieve these improvements. It relies on three different function approximators:\n\na soft state value function V_\\varphi(s).\na soft Q-value function Q_\\psi(s,a).\na stochastic policy \\pi_\\theta(s, a).\n\nThe paper uses a different notation for the parameters \\theta, \\varphi, \\psi, but I choose to be consistent with the rest of this document.\nThe soft state-value function V_\\varphi(s) is learned using Equation 14.4 which is turned into a loss function:\n\n    \\mathcal{L}(\\varphi) = \\mathbb{E}_{s_t \\in \\mathcal{D}} [\\mathbb{E}_{a_{t} \\in \\pi} [(Q_\\psi(s_{t}, a_{t}) - \\log \\, \\pi_\\theta(s_t, a_t)] - V_\\varphi(s_t) )^2]\n\nIn practice, we only need the gradient of this loss function to train the corresponding neural network. The expectation over the policy inside the loss function can be replaced by a single sample action a using the current policy \\pi_\\theta (but not a_{t+1} in the replay memory \\mathcal{D}, which is only used for the states s_t).\n\n    \\nabla_\\varphi \\mathcal{L}(\\varphi) = \\nabla_\\varphi V_\\varphi(s_t) \\, (V_\\varphi(s_t) - Q_\\psi(s_{t}, a) + \\log \\, \\pi_\\theta(s_t, a) )\n\nThe soft Q-values Q_\\psi(s_{t}, a_{t}) can be trained from the replay memory \\mathcal{D} on (s_t, a_t, r_{t+1} , s_{t+1}) transitions by minimizing the mse:\n\n    \\mathcal{L}(\\psi) = \\mathbb{E}_{s_t, a_t \\in \\mathcal{D}} [(r_{t+1} + \\gamma \\, V_\\varphi(s_{t+1}) - Q_\\psi(s_t, a_t))^2]\n\nFinally, the policy \\pi_\\theta can be trained to maximize the obtained returns. There are many ways to do that, but Haarnoja et al. (2018) proposes to minimize the Kullback-Leibler (KL) divergence between the current policy \\pi_\\theta and a softmax function over the soft Q-values:\n\n    \\mathcal{L}(\\theta) = \\mathbb{E}_{s_t \\in \\mathcal{D}} [D_\\text{KL}(\\pi_\\theta(s, \\cdot) | \\frac{\\exp Q_\\psi(s_t, \\cdot)}{Z(s_t)})]\n\nwhere Z is the partition function to normalize the softmax. Fortunately, it disappears when using the reparameterization trick and taking the gradient of this loss (see the paper for details).\nThere are additional tricks to make it more efficient and robust, such as target networks or the use of two independent function approximators for the soft Q-values in order to reduce the bias, but the gist of the algorithm is the following:\n\n\nSample a transition (s_t, a_t, r_{t+1}, a_{t+1}) using the current policy \\pi_\\theta and store it in the replay memory \\mathcal{D}.\nFor each transition (s_t, a_t, r_{t+1}, a_{t+1}) of a minibatch of \\mathcal{D}:\n\nSample an action a \\in \\pi_\\theta(s_t, \\cdot) from the current policy.\nUpdate the soft state-value function V_\\varphi(s_t): \n  \\nabla_\\varphi \\mathcal{L}(\\varphi) = \\nabla_\\varphi V_\\varphi(s_t) \\, (V_\\varphi(s_t) - Q_\\psi(s_{t}, a) + \\log \\, \\pi_\\theta(s_t, a) )\n  \nUpdate the soft Q-value function Q_\\psi(s_t, a_t): \n  \\nabla_\\psi \\mathcal{L}(\\psi) = - \\nabla_\\psi Q_\\psi(s_t, a_t) \\, (r_{t+1} + \\gamma \\, V_\\varphi(s_{t+1}) - Q_\\psi(s_t, a_t))\n  \nUpdate the policy \\pi_\\theta(s_t, \\cdot): \n  \\nabla_\\theta \\mathcal{L}(\\theta) = \\nabla_\\theta D_\\text{KL}(\\pi_\\theta(s, \\cdot) | \\frac{\\exp Q_\\psi(s_t, \\cdot)}{Z(s_t)})\n  \n\n\n\nSAC was compared to DDPG, PPO, soft Q-learning and others on a set of gym and humanoid robotics tasks (with 21 joints!). It outperforms all these methods in both the final performance and the sample complexity, the difference being even more obvious for the complex tasks. The exploration bonus given by the maximum entropy allows the agent to discover better policies than its counterparts. SAC is an actor-critic architecture (the critic computing both V and Q) working off-policy (using an experience replay memory, so re-using past experiences) allowing to learn stochastic policies, even in high dimensional spaces.\n\n\n\n\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement Learning with Deep Energy-Based Policies. Available at: http://arxiv.org/abs/1702.08165 [Accessed February 13, 2019].\n\n\nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., et al. (2018). Soft Actor-Critic Algorithms and Applications. Available at: http://arxiv.org/abs/1812.05905 [Accessed February 5, 2019].\n\n\nMachado, M. C., Bellemare, M. G., and Bowling, M. (2018). Count-Based Exploration with the Successor Representation. Available at: http://arxiv.org/abs/1807.11622 [Accessed February 23, 2019].\n\n\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017). Bridging the Gap Between Value and Policy Based Reinforcement Learning. Available at: http://arxiv.org/abs/1702.08892 [Accessed June 12, 2019].\n\n\nO’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2016). Combining policy gradient and Q-learning. Available at: http://arxiv.org/abs/1611.01626 [Accessed February 13, 2019].\n\n\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal Policy Optimization Algorithms. Available at: http://arxiv.org/abs/1707.06347.\n\n\nTodorov, E. (2008). General duality between optimal control and estimation. in 2008 47th IEEE Conference on Decision and Control, 4286–4292. doi:10.1109/CDC.2008.4739438.\n\n\nToussaint, M. (2009). Robot Trajectory Optimization Using Approximate Inference. in Proceedings of the 26th Annual International Conference on Machine Learning ICML ’09. (New York, NY, USA: ACM), 1049–1056. doi:10.1145/1553374.1553508.\n\n\nWilliams, R. J., and Peng, J. (1991). Function optimization using connectionist reinforcement learning algorithms. Connection Science 3, 241–268.\n\n\nZiebart, B. D., Maas, A., Bagnell, J. A., and Dey, A. K. (2008). Maximum Entropy Inverse Reinforcement Learning. in, 6.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Maximum Entropy RL (SAC)</span>"
    ]
  },
  {
    "objectID": "src/3.7-DistributionalRL.html",
    "href": "src/3.7-DistributionalRL.html",
    "title": "Distributional learning",
    "section": "",
    "text": "Categorical DQN\nBellemare et al. (2017) proposed to learn the value distribution (the probability distribution of the returns) through a modification of the Bellman equation. They show that learning the complete distribution of rewards instead of their mean leads to performance improvements on Atari games over modern variants of DQN.\nTheir proposed categorical DQN (also called C51) has an architecture based on DQN, but where the output layer predicts the distribution of the returns for each action a in state s, instead of its mean Q^\\pi(s, a). In practice, each action a is represented by N output neurons, who encode the support of the distribution of returns. If the returns take values between V_\\text{min} and V_\\text{max}, one can represent their distribution \\mathcal{Z} by taking N discrete “bins” (called atoms in the paper) in that range. Figure 15.1 shows how the distribution of returns between -10 and 10 can be represented using 21 atoms.\nOf course, the main problem is to know in advance the range of returns [V_\\text{min}, V_\\text{max}] (it depends largely on the choice of the discount rate \\gamma), but you can infer it from training another algorithm such as DQN beforehand. Dabney et al. (2017) got rid of this problem with quantile regression. In the paper, the authors found out experimentally that 51 is the most efficient number of atoms (hence the name C51).\nLet’s note z_i these atoms with 1 \\leq i &lt; N. The atom probability that the return associated to a state-action pair (s, a) lies within the bin associated to the atom z_i is noted p_i(s, a). These probabilities can be predicted by a neural network, typically by using a softmax function over outputs f_i(s, a; \\theta):\np_i(s, a; \\theta) = \\frac{\\exp f_i(s, a; \\theta)}{\\sum_{j=1}^{N} \\exp f_j(s, a; \\theta)}\nThe distribution of the returns \\mathcal{Z} is simply a sum over the atoms (represented by the Dirac distribution \\delta_{z_i}):\n\\mathcal{Z}_\\theta(s, a) = \\sum_{i=1}^{N} p_i(s, a; \\theta) \\, \\delta_{z_i}\nIf these probabilities are correctly estimated, the Q-value is easy to compute as the mean of the distribution:\nQ_\\theta(s, a) = \\mathbb{E} [\\mathcal{Z}_\\theta(s, a)] = \\sum_{i=1}^{N} p_i(s, a; \\theta) \\, z_i\nThese Q-values can then be used for action selection as in the regular DQN. The problem is now to learn the value distribution \\mathcal{Z}_\\theta, i.e. to find a learning rule / loss function for the p_i(s, a; \\theta). Let’s consider a single transition (s, a, r, s') and select the greedy action a' in s' using the current policy \\pi_\\theta. The value distribution \\mathcal{Z}_\\theta can be evaluated by applying recursively the Bellman operator \\mathcal{T}:\n\\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) = \\mathcal{R}(s, a) + \\gamma \\, \\mathcal{Z}_\\theta(s', a')\nwhere \\mathcal{R}(s, a) is the distribution of immediate rewards after (s, a). This use of the Bellman operator is the same as in Q-learning:\n\\mathcal{T} \\, \\mathcal{Q}_\\theta(s, a) = \\mathbb{E}[r(s, a)] + \\gamma \\, \\mathcal{Q}_\\theta(s', a')\nIn Q-learning, one minimizes the difference (mse) between \\mathcal{T} \\, \\mathcal{Q}_\\theta(s, a) and \\mathcal{Q}_\\theta(s, a), which are expectations (so we only manipulate scalars). Here, we will minimize the statistical distance between the distributions \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) and \\mathcal{Z}_\\theta(s, a) themselves, using for example the KL divergence, Wasserstein metric, total variation or whatnot.\nThe problem is mostly that the distributions \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) and \\mathcal{Z}_\\theta(s, a) do not have the same support: for a particular atom z_i, \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) can have a non-zero probability p_i(s, a), while \\mathcal{Z}_\\theta(s, a) has a zero probability. Besides, the probabilities must sum to 1, so one cannot update the z_i independently from one another.\nThe proposed method consists of three steps:\n\\mathcal{T} \\, z_i = r + \\gamma \\, z_i\nand clip the obtained value to [V_\\text{min}, V_\\text{max}]. The reward r translates the distribution of atoms, while the discount rate \\gamma scales it. Figure 15.2 shows the distribution of \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) compared to \\mathcal{Z}_\\theta(s, a). Note that the atoms of the two distributions are not aligned.\n\\Delta p_{l}(s', a'; \\theta) = p_{b}(s', a'; \\theta) \\, (b - u)\n \n    \\Delta p_{u}(s', a'; \\theta) = p_{b}(s', a'; \\theta) \\, (l - b)\nFigure 15.3 shows how the projected update distribution \\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) now matches the support of \\mathcal{Z}_\\theta(s, a)\nThe projection of the Bellman update onto an atom z_i can be summarized by the following equation:\n(\\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a))_i = \\sum_{j=1}^N \\big [1 - \\frac{| [\\mathcal{T}\\, z_j]_{V_\\text{min}}^{V_\\text{max}} - z_i|}{\\Delta z} \\big ]_0^1 \\, p_j (s', a'; \\theta)\nwhere [\\cdot]_a^b bounds its argument in [a, b] and \\Delta z is the step size between two atoms.\n\\mathcal{L}(\\theta) = D_\\text{KL} (\\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_{\\theta'}(s, a) | \\mathcal{Z}_\\theta(s, a))\nusing a target network \\theta' for the target. It is to be noted that minimizing the KL divergence is the same as minimizing the cross-entropy between the two, as in classification tasks:\n\\mathcal{L}(\\theta) =  - \\sum_i (\\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_{\\theta'}(s, a))_i \\log p_i (s, a; \\theta)\nThe projected Bellman update plays the role of the one-hot encoded target vector in classification (except that it is not one-hot encoded). DQN performs a regression on the Q-values (mse loss), while categorical DQN performs a classification (cross-entropy loss). Apart from the way the target is computed, categorical DQN is very similar to DQN: architecture, experience replay memory, target networks, etc.\nFigure 15.4 illustrates how the predicted value distribution changes when playing Space invaders (also have a look at the Youtube video at https://www.youtube.com/watch?v=yFBwyPuO2Vg). C51 outperforms DQN on most Atari games, both in terms of the achieved performance and the sample complexity.\nAdditional resources:",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Distributional learning</span>"
    ]
  },
  {
    "objectID": "src/3.7-DistributionalRL.html#categorical-dqn",
    "href": "src/3.7-DistributionalRL.html#categorical-dqn",
    "title": "Distributional learning",
    "section": "",
    "text": "Figure 15.1: Example of a value distribution using 21 atoms between -10 and 10. The average return is 3, but its variance is explicitly represented.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputation of the Bellman update \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a). They simply compute translated values for each z_i according to:\n\n\n\n\n\n\n\n\n\nFigure 15.2: Computation of the Bellman update \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a). The atoms of the two distributions are not aligned.\n\n\n\n\nDistribution of the probabilities of \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) on the support of \\mathcal{Z}_\\theta(s, a). The projected atom \\mathcal{T} \\, z_i lie between two “real” atoms z_l and z_u, with a non-integer index b (for example b = 3.4, l = 3 and u=4). The corresponding probability p_{b}(s', a'; \\theta) of the next greedy action (s', a') is “spread” to its neighbors through a local interpolation depending on the distances between b, l and u:\n\n\n\n\n\n\n\n\n\nFigure 15.3: Projected update \\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) on the support of \\mathcal{Z}_\\theta(s, a). The atoms are now aligned, the statistical distance between the two distributions can be minimized.\n\n\n\n\n\n\n\nMinimizing the statistical distance between \\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) and \\mathcal{Z}_\\theta(s, a). Now that the Bellman update has the same support as the value distribution, we can minimize the KL divergence between the two for a single transition:\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.4: Evolution of the value distribution for the categorical DQN playing Space Invaders. Animation Source: https://deepmind.com/blog/going-beyond-average-reinforcement-learning/\n\n\n\n\n\n\n\nhttps://deepmind.com/blog/going-beyond-average-reinforcement-learning\nhttps://physai.sciencesconf.org/data/pages/distributional_RL_Remi_Munos.pdf\nhttps://flyyufelix.github.io/2017/10/24/distributional-bellman.html, with keras code for C51.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Distributional learning</span>"
    ]
  },
  {
    "objectID": "src/3.7-DistributionalRL.html#the-reactor",
    "href": "src/3.7-DistributionalRL.html#the-reactor",
    "title": "Distributional learning",
    "section": "The Reactor",
    "text": "The Reactor\nThe Reactor (Retrace Actor) of Gruslys et al. (2017) combines many architectural and algorithmic contributions seen until now in order to provide an algorithm that is both sample efficient and with a good run-time performance. A3C has for example a better run-time performance (smaller wall-clock time for the training) than DQN or categorical DQN thanks to the use of multiple actor-learners in parallel, but its sample complexity is actually higher (as it is on-policy).\nThe Reactor combines and improves on:\n\nAn actor-critic architecture using policy gradient with importance sampling,\nOff-policy corrected returns computed by the Retrace algorithm,\nDistributional learning of the Q-values in the critic,\nPrioritized experience replay for sequences.\n\nOne could consider REACTOR as the distributional version of ACER. We will not go into all the details here, but simply outline the main novelties.\nThe Reactor is composed of an actor \\pi_\\theta(s, a) and a critic Q_\\varphi(s, a). The actor is trained using policy gradient with importance sampling, as in Off-PAC. For a single state s and an action \\hat{a} sampled by the behavior policy b, the gradient of the objective is defined as:\n\n\\begin{aligned}\n    \\nabla_\\theta J(\\theta) = \\frac{\\pi_\\theta(s, \\hat{a})}{b(s, \\hat{a})} & \\, (R(s, \\hat{a}) - Q_\\varphi(s, \\hat{a})) \\, \\nabla_\\theta \\log \\pi_\\theta(s, \\hat{a}) \\\\\n    & + \\sum_a Q_\\varphi(s, a) \\, \\nabla_\\theta \\pi_\\theta(s, a) \\\\\n\\end{aligned}\n\nThe first term comes from Off-PAC and only concerns the chosen action \\hat{a} from the behavior policy. The actual return R(s, a) is compared to its estimate Q_\\varphi(s, \\hat{a}) in order to reduce its variance. The second term \\sum_a Q_\\varphi(s, a) \\, \\nabla_\\theta \\pi_\\theta(s, a) depends on all available actions in s. Its role is to reduce the bias of the first term, without adding any variance as it is only based on estimates. As the value of the state is defined by V^\\pi(s) = \\sum_a \\pi(s, a) \\, Q^\\pi(s, a), maximizing this term maximizes the value of the state, i.e. the associated returns. This rule is called leave-one-out (LOO), as one action is left out from the sum and estimated from actual returns instead of other estimates.\nFor a better control on the variance, the behavior probability b(s, a) is replaced by a parameter \\beta:\n\n    \\nabla_\\theta J(\\theta) = \\beta \\, (R(s, \\hat{a}) - Q_\\varphi(s, \\hat{a})) \\, \\nabla_\\theta \\pi_\\theta(s, \\hat{a}) + \\sum_a Q_\\varphi(s, a) \\, \\nabla_\\theta \\pi_\\theta(s, a)\n\n\\beta is defined as \\min (c, \\frac{1}{b(s, \\hat{a})}), where c&gt;1 is a constant. This truncated term is similar to what was used in ACER. The rule is now called \\beta-LOO and is a novel proposition of the Reactor.\nThe second importance contribution of the Reactor is how to combine the Retrace algorithm (Munos et al. (2016)) for estimating the return R(s, \\hat{a}) on multiple steps, with the distributional learning method of Categorical DQN. As Retrace uses n-steps returns iteratively, the n-step distributional Bellman target can updated using the n future rewards:\n\n    z_i^n = \\mathcal{T}^n \\, z_i = \\sum_{k=t}^{t+n} \\gamma^{k-t} r_k + \\gamma^n \\, z_i\n\nWe leave out the details on how Retrace is combined with these distributional Bellman updates: the notation is complicated but the idea is simple. The last importance contribution of the paper is the use of prioritized sequence replay. Prioritized experience replay allows to select in priority transitions from the replay buffer which are the most surprising, i.e. where the TD error is the highest. These transitions are the ones carrying the most information. A similar principle can be applied to sequences of transitions, which are needed by the n-step updates. They devised a specific sampling algorithm in order to achieve this and reduce the variance of the samples.\nThe last particularities of the Reactor is that it uses a LSTM layer to make the problem Markovian (instead of stacking four frames as in DQN) and train multiple actor-learners as in A3C. The algorithm is trained on CPU, with 10 or 20 actor-learners. The Reactor outperforms DQN and its variants, A3C and ACER on Atari games. Importantly, Reactor only needs one day of training on CPU, compared to the 8 days of GPU training needed by DQN.\n\n\n\n\nBellemare, M. G., Dabney, W., and Munos, R. (2017). A Distributional Perspective on Reinforcement Learning. Available at: http://arxiv.org/abs/1707.06887.\n\n\nDabney, W., Rowland, M., Bellemare, M. G., and Munos, R. (2017). Distributional Reinforcement Learning with Quantile Regression. Available at: http://arxiv.org/abs/1710.10044 [Accessed June 28, 2019].\n\n\nGruslys, A., Dabney, W., Azar, M. G., Piot, B., Bellemare, M., and Munos, R. (2017). The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning. Available at: http://arxiv.org/abs/1704.04651.\n\n\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016). Safe and Efficient Off-Policy Reinforcement Learning. Available at: http://arxiv.org/abs/1606.02647.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Distributional learning</span>"
    ]
  },
  {
    "objectID": "src/3.8-OtherPolicyGradient.html",
    "href": "src/3.8-OtherPolicyGradient.html",
    "title": "Miscellaneous model-free algorithms",
    "section": "",
    "text": "Comparison between value-based and policy gradient methods\nHaving now reviewed both value-based methods (DQN and its variants) and policy gradient methods (A3C, DDPG, PPO), the question is which method to choose? While not much happens right now for value-based methods, policy gradient methods are attracting a lot of attention, as they are able to learn policies in continuous action spaces, what is very important in robotics. https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html summarizes the advantages and inconvenients of policy gradient methods.\nAdvantages of PG:\nDisadvantages of PG:",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Miscellaneous model-free algorithms</span>"
    ]
  },
  {
    "objectID": "src/3.8-OtherPolicyGradient.html#comparison-between-value-based-and-policy-gradient-methods",
    "href": "src/3.8-OtherPolicyGradient.html#comparison-between-value-based-and-policy-gradient-methods",
    "title": "Miscellaneous model-free algorithms",
    "section": "",
    "text": "Better convergence properties, more stable (Duan et al., 2016).\nEffective in high-dimensional or continuous action spaces.\nCan learn stochastic policies.\n\n\n\nTypically converge to a local rather than global optimum.\nEvaluating a policy is often inefficient and having a high variance.\nWorse sample efficiency (but it is getting better).",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Miscellaneous model-free algorithms</span>"
    ]
  },
  {
    "objectID": "src/3.8-OtherPolicyGradient.html#gradient-free-policy-search",
    "href": "src/3.8-OtherPolicyGradient.html#gradient-free-policy-search",
    "title": "Miscellaneous model-free algorithms",
    "section": "Gradient-free policy search",
    "text": "Gradient-free policy search\nThe policy gradient methods presented above rely on backpropagation and gradient descent/ascent to update the parameters of the policy and maximize the objective function. Gradient descent is generally slow, sample inefficient and subject to local minima, but is nevertheless the go-to method in neural networks. However, it is not the only optimization that can be used in deep RL. Here are some of the alternatives.\n\nCross-entropy Method (CEM) (Szita and Lörincz, 2006)\nEvolutionary Search (ES) (Salimans et al., 2017)\n\nExplanations from OpenAI: https://blog.openai.com/evolution-strategies/\nDeep neuroevolution at Uber: https://eng.uber.com/deep-neuroevolution/\n\n\n\n\nDuan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016). Benchmarking Deep Reinforcement Learning for Continuous Control. Available at: http://arxiv.org/abs/1604.06778.\n\n\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and Levine, S. (2016a). Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic. Available at: http://arxiv.org/abs/1611.02247.\n\n\nGu, S., Lillicrap, T., Sutskever, I., and Levine, S. (2016b). Continuous Deep Q-Learning with Model-based Acceleration. Available at: http://arxiv.org/abs/1603.00748.\n\n\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T. (2015). Learning continuous control policies by stochastic value gradients. Proc. International Conference on Neural Information Processing Systems, 2944–2952. Available at: http://dl.acm.org/citation.cfm?id=2969569.\n\n\nHeinrich, J., Lanctot, M., and Silver, D. (2015). Fictitious Self-Play in Extensive-Form Games. 805–813. Available at: http://proceedings.mlr.press/v37/heinrich15.html.\n\n\nHeinrich, J., and Silver, D. (2016). Deep Reinforcement Learning from Self-Play in Imperfect-Information Games. Available at: http://arxiv.org/abs/1603.01121.\n\n\nSalimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017). Evolution Strategies as a Scalable Alternative to Reinforcement Learning. Available at: http://arxiv.org/abs/1703.03864.\n\n\nSzita, I., and Lörincz, A. (2006). Learning Tetris Using the Noisy Cross-Entropy Method. Neural Computation 18, 2936–2941. doi:10.1162/neco.2006.18.12.2936.",
    "crumbs": [
      "**Policy-gradient methods**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Miscellaneous model-free algorithms</span>"
    ]
  },
  {
    "objectID": "src/4.1-ModelBased.html",
    "href": "src/4.1-ModelBased.html",
    "title": "Model-based RL",
    "section": "",
    "text": "Model-free vs. model-based RL\nIn model-free RL (MF) methods seen sofar, we do not need to know anything about the dynamics of the environment to start learning a policy:\np(s_{t+1} | s_t, a_t) \\; \\; r(s_t, a_t, s_{t+1})\nWe just sample transitions (s, a, r, s') and update Q-values or a policy network. The main advantage is that the agent does not need to “think” when acting: just select the action with highest Q-value (reflexive behavior). The other advantage is that you can use MF methods on any MDP: you do not need to know anything about them.\nBut MF methods are very slow (sample complexity): as they make no assumption, they have to learn everything by trial-and-error from scratch. If you had a model of the environment, you could plan ahead (what would happen if I did that?) and speed up learning (do not explore stupid ideas): model-based RL (MB).\nIn chess, for example, players plan ahead the possible moves up to a certain horizon and evaluate moves based on their emulated consequences. In real-time strategy games, learning the environment (world model) is part of the strategy: you do not attack right away.\nThis chapter presents several MB algorithms, including MPC planning algorithms, World models and the different variants of AlphaGo. We first present here the main distinction in MB RL, planning algorithms (MPC) versus MB-augmented MF (Dyna) methods. Another dichotomy that we will see later is about learned models vs. given models.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model-based RL</span>"
    ]
  },
  {
    "objectID": "src/4.1-ModelBased.html#model-free-vs.-model-based-rl",
    "href": "src/4.1-ModelBased.html#model-free-vs.-model-based-rl",
    "title": "Model-based RL",
    "section": "",
    "text": "Source: Dayan and Niv (2008) Reinforcement learning: The Good, The Bad and The Ugly. Current Opinion in Neurobiology, Cognitive neuroscience 18:185–196. doi:10.1016/j.conb.2008.08.003\n\n\n\n\n\n\n\n\n\n\n\nSource: https://github.com/avillemin/RL-Personnal-Notebook",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model-based RL</span>"
    ]
  },
  {
    "objectID": "src/4.1-ModelBased.html#learning-the-model",
    "href": "src/4.1-ModelBased.html#learning-the-model",
    "title": "Model-based RL",
    "section": "Learning the model",
    "text": "Learning the model\nLearning the world model is not complicated in theory. We just need to collect enough transitions (s, a, r , s') using a random agent (or an expert) and train a supervised model to predict the next state and the correspondingreward.\n\n\n     M(s, a) = (s', r )\n\nSuch a model is called the dynamics model, the transition model or the forward model, and basically answers the question:\n\nWhat would happen if I did that?\n\nThe model can be deterministic (in which can we should use neural networks) or stochastic (in which case we should use Gaussian Processes). Any kind of supervised learning method can be used in principle.\nOnce you have trained a good transition model, you can generate rollouts, i.e. imaginary trajectories / episodes \\tau using the model. Given an initial state s_0 and a policy \\pi, you can unroll the future using the model s', r = M(s, a).\n\n    s_0  \\xrightarrow[\\pi]{} a_0 \\xrightarrow[M]{} s_1  \\xrightarrow[\\pi]{} a_1 \\xrightarrow[\\pi]{} s_2 \\xrightarrow[]{} \\ldots \\xrightarrow[M]{} s_T\n\nGiven the model, you can also compute the return R(\\tau) of the emulated trajectory. Everything is as if you were interacting with the environment, but you actually do not need it anymore: the model becomes the environment. You can now search for an optimal policy on these emulated trajectories:\n\n\n\n\n\n\nTraining in imagination\n\n\n\n\nCollect transitions (s, a, r, s') using a (random/expert) policy b and create a dataset \\mathcal{D} = \\{(s_k, a_k, r_, s'_k\\}_{k}.\nTrain the model M(s, a) = (s', r) on \\mathcal{D} using supervised learning.\nOptimize the policy \\pi on rollouts \\tau generated by the model.\n\n\n\nAny method can be used to optimize the policy. We can obviously use a model-free algorithm to maximize the expected return of the trajectories:\n\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_\\pi}[R(\\tau)]\nThe only sample complexity is the one needed to train the model: the rest is emulated. For problems where a physical step (t \\rightarrow t+1) is very expensive compared to an inference step of the model (neural networks can predict very fast), this can even allow to use inefficient but optimal methods to find the policy, such as:\n\nThe Cross-entropy Method (CEM) (Szita and Lörincz, 2006), where the policy is randomly sampled and improved over successive rollouts.\nGenetic algorithms, such as Evolutionary Search (ES) (Salimans et al., 2017).\n\nBrute-force optimization becomes possible if using the model is much faster that the real environment. However, this approach has two major drawbacks:\n\nThe model can only be as good as the data, and errors are going to accumulate, especially for long trajectories or probabilistic MDPs.\nIf the dataset does not contain the important transitions (for example when there are sparse rewards), the policy will likely be sub-optimal. In extreme cases, training the model up to a sufficient precision might necessitate more samples than learning the policy directly with MF methods.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model-based RL</span>"
    ]
  },
  {
    "objectID": "src/4.1-ModelBased.html#dyna-q",
    "href": "src/4.1-ModelBased.html#dyna-q",
    "title": "Model-based RL",
    "section": "Dyna-Q",
    "text": "Dyna-Q\nOne variant of the previous approach is to augment MF algorithms with MB rollouts. The MF algorithm (e.g. Q-learning) learns from transitions (s, a, r, s') sampled either with:\n\nreal experience: interaction with the environment.\nsimulated experience: simulation by the model.\n\n\n\n\nDyna-Q. Source: https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8\n\n\nIf the simulated transitions are realistic enough, the MF algorithm can converge using much less real transitions, thereby reducing its sample complexity. This Dyna-Q algorithm (Sutton, 1990) is an extension of Q-learning to integrate a model M(s, a) = (s', r').\n\n\n\n\n\n\nDyna-Q (Sutton, 1990)\n\n\n\n\nInitialize values Q(s, a) and model M(s, a).\nfor t \\in [0, T_\\text{total}]:\n\nSelect a_t using Q, take it on the real environment and observe s_{t+1} and r_{t+1}.\nUpdate the Q-value of the real action:\n\n\\Delta Q(s_t, a_t) = \\alpha \\, (r_{t+1} + \\gamma \\, \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\n\nUpdate the model:\n\nM(s_t, a_t) \\leftarrow (s_{t+1}, r_{t+1})\n\nfor K steps:\n\nSample a state s_k from a list of visited states.\nSelect a_k using Q, predict s_{k+1} and r_{k+1} using the model M(s_k, a_k).\nUpdate the Q-value of the imagined action:\n\n\\Delta Q(s_k, a_k) = \\alpha \\, (r_{k+1} + \\gamma \\, \\max_a Q(s_{k+1}, a) - Q(s_k, a_k))\n\n\n\n\nIt is interesting to notice that Dyna-Q is the inspiration for DQN and its experience replay memory. In DQN, the ERM stores real transitions generated in the past, while in Dyna-Q, the model generates imagined transitions based on past real transitions.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model-based RL</span>"
    ]
  },
  {
    "objectID": "src/4.1-ModelBased.html#model-predictive-control",
    "href": "src/4.1-ModelBased.html#model-predictive-control",
    "title": "Model-based RL",
    "section": "Model Predictive Control",
    "text": "Model Predictive Control\nFor long horizons, the slightest imperfection in the model can accumulate over time (drift) and lead to completely wrong trajectories.\n\n\n\nSource: https://medium.com/@jonathan_hui/rl-model-based-reinforcement-learning-3c2b6f0aa323\n\n\nThe emulated trajectory will have a biased return, and the algorithm will not converge to the optimal policy. If you have a perfect model, you should not be using RL anyway, as classical control methods would be much faster (but see AlphaGo).\nThe solution is to replan at each time step and execute only the first planned action in the real environment.\n\n\n\nSource: https://medium.com/@jonathan_hui/rl-model-based-reinforcement-learning-3c2b6f0aa323\n\n\nModel Predictive Control iteratively plans complete trajectories, but only selects the first action.\n\n\n\nSource: http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_9_model_based_rl.pdf\n\n\nThe planner (or MPC controller) can actually be anything, it does not have to be a RL algorithm. It could for example be iLQR (Iterative Linear Quadratic Regulator), a non-linear optimization method classically used in control (see https://jonathan-hui.medium.com/rl-lqr-ilqr-linear-quadratic-regulator-a5de5104c750 for explanations).\n\n\n\nModel-predictive control using a learned model. Source: Nagabandi et al. (2017).\n\n\nIn Nagabandi et al. (2017), the dynamics model is a neural network predicted the next state and the associated reward. The controller uses random-sampling shooting:\n\nIn the current state, a set of possible actions is selected.\nRollouts are generated from these actions using the model and their return is computed.\nThe initial action of the rollout with the highest return is selected.\nRepeat.\n\n\n\n\nRandom-sampling shooting. Using different initial actions, several imaginary rollouts are performed. The one leading to the maximal return is chosen and executed. Source: https://bair.berkeley.edu/blog/2017/11/30/model-based-rl/\n\n\nThe main advantage of MPC is that you can change the reward function (the goal) on the fly: what you learn is the model, but planning is just an optimization procedure. You can set intermediary goals to the agent very flexibly: no need for a well-defined reward function. Model imperfection is not a problem as you replan all the time. As seen below, the model can adapt to changes in the environment (slippery terrain, simulation to real-world).\n\n\n\nSource: https://bair.berkeley.edu/blog/2017/11/30/model-based-rl/\n\n\n\n\n\nSource: https://bair.berkeley.edu/blog/2017/11/30/model-based-rl/\n\n\n\n\n\n\nDayan, P., and Niv, Y. (2008). Reinforcement learning: The Good, The Bad and The Ugly. Current Opinion in Neurobiology 18, 185–196. doi:10.1016/j.conb.2008.08.003.\n\n\nNagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. (2017). Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. Available at: http://arxiv.org/abs/1708.02596 [Accessed March 3, 2019].\n\n\nSalimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017). Evolution Strategies as a Scalable Alternative to Reinforcement Learning. Available at: http://arxiv.org/abs/1703.03864.\n\n\nSutton, R. S. (1990). Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming. Machine Learning Proceedings 1990, 216–224. doi:10.1016/B978-1-55860-141-3.50030-4.\n\n\nSzita, I., and Lörincz, A. (2006). Learning Tetris Using the Noisy Cross-Entropy Method. Neural Computation 18, 2936–2941. doi:10.1162/neco.2006.18.12.2936.",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model-based RL</span>"
    ]
  },
  {
    "objectID": "src/4.2-WorldModels.html",
    "href": "src/4.2-WorldModels.html",
    "title": "World models",
    "section": "",
    "text": "work in progress",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>World models</span>"
    ]
  },
  {
    "objectID": "src/4.3-AlphaGo.html",
    "href": "src/4.3-AlphaGo.html",
    "title": "AlphaGo",
    "section": "",
    "text": "work in progress",
    "crumbs": [
      "**Model-based deep RL**",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>AlphaGo</span>"
    ]
  },
  {
    "objectID": "src/5.1-Hierarchical.html",
    "href": "src/5.1-Hierarchical.html",
    "title": "Hierarchical Reinforcement Learning",
    "section": "",
    "text": "work in progress\nVery good link: https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/\nNachum et al. (2018)\nHaarnoja et al. (2018)\nCo-Reyes et al. (2018)\n\n\n\n\nCo-Reyes, J. D., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., and Levine, S. (2018). Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings.\n\n\nHaarnoja, T., Hartikainen, K., Abbeel, P., and Levine, S. (2018). Latent Space Policies for Hierarchical Reinforcement Learning. Available at: http://arxiv.org/abs/1804.02808.\n\n\nNachum, O., Gu, S., Lee, H., and Levine, S. (2018). Data-Efficient Hierarchical Reinforcement Learning. Available at: http://arxiv.org/abs/1805.08296.",
    "crumbs": [
      "**Advanced topics**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hierarchical Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "src/5.2-Inverse.html",
    "href": "src/5.2-Inverse.html",
    "title": "Inverse Reinforcement Learning",
    "section": "",
    "text": "work in progress\nHindsight experience replay: Andrychowicz et al. (2017)\nGoal-conditioned inverse RL: Ding et al. (2019)\n\n\n\n\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., et al. (2017). Hindsight Experience Replay. Available at: http://arxiv.org/abs/1707.01495.\n\n\nDing, Y., Florensa, C., Phielipp, M., and Abbeel, P. (2019). Goal-conditioned Imitation Learning. in (Long Beach, California: PMLR), 8. Available at: https://openreview.net/pdf?id=HkglHcSj2N.",
    "crumbs": [
      "**Advanced topics**",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Inverse Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "src/5.3-OfflineRL.html",
    "href": "src/5.3-OfflineRL.html",
    "title": "Offline RL",
    "section": "",
    "text": "work in progress",
    "crumbs": [
      "**Advanced topics**",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Offline RL</span>"
    ]
  },
  {
    "objectID": "src/5.4-Meta.html",
    "href": "src/5.4-Meta.html",
    "title": "Meta learning",
    "section": "",
    "text": "work in progress",
    "crumbs": [
      "**Advanced topics**",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Meta learning</span>"
    ]
  },
  {
    "objectID": "src/references.html",
    "href": "src/references.html",
    "title": "References",
    "section": "",
    "text": "Amari, S.-I. (1998). Natural gradient works efficiently in learning.\nNeural Computation 10, 251–276.\n\n\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R.,\nWelinder, P., et al. (2017). Hindsight Experience Replay.\nAvailable at: http://arxiv.org/abs/1707.01495.\n\n\nAnschel, O., Baram, N., and Shimkin, N. (2016).\nAveraged-DQN: Variance Reduction and\nStabilization for Deep Reinforcement Learning.\nAvailable at: http://arxiv.org/abs/1611.01929.\n\n\nArjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein\nGAN. Available at: http://arxiv.org/abs/1701.07875.\n\n\nBaird, L. C. (1993). Advantage updating. Wright-Patterson Air Force Base\nAvailable at: http://leemon.com/papers/1993b.pdf.\n\n\nBakker, B. (2001). Reinforcement Learning with Long\nShort-Term Memory. in Advances in Neural Information\nProcessing Systems 14 (NIPS 2001), 1475–1482.\nAvailable at: https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory.\n\n\nBarth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., TB,\nD., et al. (2018). Distributed Distributional Deterministic Policy\nGradients. Available at: http://arxiv.org/abs/1804.08617.\n\n\nBellemare, M. G., Dabney, W., and Munos, R. (2017). A\nDistributional Perspective on Reinforcement\nLearning. Available at: http://arxiv.org/abs/1707.06887.\n\n\nCho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,\nSchwenk, H., et al. (2014). Learning Phrase Representations\nusing RNN Encoder-Decoder for Statistical Machine\nTranslation. Available at: http://arxiv.org/abs/1406.1078.\n\n\nChou, P.-W., Maturana, D., and Scherer, S. (2017). Improving\nStochastic Policy Gradients in Continuous\nControl with Deep Reinforcement Learning using the\nBeta Distribution. in International\nConference on Machine Learning Available\nat: http://proceedings.mlr.press/v70/chou17a/chou17a.pdf.\n\n\nCo-Reyes, J. D., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., and\nLevine, S. (2018). Self-Consistent Trajectory Autoencoder:\nHierarchical Reinforcement Learning with Trajectory\nEmbeddings.\n\n\nDabney, W., Rowland, M., Bellemare, M. G., and Munos, R. (2017).\nDistributional Reinforcement Learning with Quantile\nRegression. Available at: http://arxiv.org/abs/1710.10044\n[Accessed June 28, 2019].\n\n\nDayan, P., and Niv, Y. (2008). Reinforcement learning: The\nGood, The Bad and The Ugly. Current\nOpinion in Neurobiology 18, 185–196. doi:10.1016/j.conb.2008.08.003.\n\n\nDegrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese,\nF., et al. (2022). Magnetic control of tokamak plasmas through deep\nreinforcement learning. Nature 602, 414–419. doi:10.1038/s41586-021-04301-9.\n\n\nDegris, T., White, M., and Sutton, R. S. (2012). Linear Off-Policy\nActor-Critic. in Proceedings of the 2012 International\nConference on Machine Learning Available at: http://arxiv.org/abs/1205.4839.\n\n\nDing, Y., Florensa, C., Phielipp, M., and Abbeel, P. (2019).\nGoal-conditioned Imitation Learning. in (Long Beach,\nCalifornia: PMLR), 8. Available at: https://openreview.net/pdf?id=HkglHcSj2N.\n\n\nDuan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016).\nBenchmarking Deep Reinforcement Learning for\nContinuous Control. Available at: http://arxiv.org/abs/1604.06778.\n\n\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., et\nal. (2018). IMPALA: Scalable Distributed\nDeep-RL with Importance Weighted Actor-Learner\nArchitectures. doi:10.48550/arXiv.1802.01561.\n\n\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves,\nA., et al. (2017). Noisy Networks for\nExploration. Available at: http://arxiv.org/abs/1706.10295\n[Accessed March 2, 2020].\n\n\nGers, F. (2001). Long Short-Term Memory in Recurrent\nNeural Networks. Available at: http://www.felixgers.de/papers/phd.pdf.\n\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,\nD., Ozair, S., et al. (2014). Generative Adversarial\nNetworks. Available at: http://arxiv.org/abs/1406.2661.\n\n\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep\nLearning. MIT Press Available at: http://www.deeplearningbook.org.\n\n\nGruslys, A., Dabney, W., Azar, M. G., Piot, B., Bellemare, M., and\nMunos, R. (2017). The Reactor: A fast and\nsample-efficient Actor-Critic agent for Reinforcement\nLearning. Available at: http://arxiv.org/abs/1704.04651.\n\n\nGu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep\nReinforcement Learning for Robotic\nManipulation with Asynchronous Off-Policy Updates.\nin Proc. ICRA Available at: http://arxiv.org/abs/1610.00633.\n\n\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and Levine, S.\n(2016a). Q-Prop: Sample-Efficient Policy\nGradient with An Off-Policy Critic. Available at: http://arxiv.org/abs/1611.02247.\n\n\nGu, S., Lillicrap, T., Sutskever, I., and Levine, S. (2016b). Continuous\nDeep Q-Learning with Model-based\nAcceleration. Available at: http://arxiv.org/abs/1603.00748.\n\n\nHaarnoja, T., Hartikainen, K., Abbeel, P., and Levine, S. (2018a).\nLatent Space Policies for Hierarchical Reinforcement\nLearning. Available at: http://arxiv.org/abs/1804.02808.\n\n\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement\nLearning with Deep Energy-Based Policies.\nAvailable at: http://arxiv.org/abs/1702.08165\n[Accessed February 13, 2019].\n\n\nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., et\nal. (2018b). Soft Actor-Critic Algorithms and\nApplications. Available at: http://arxiv.org/abs/1812.05905\n[Accessed February 5, 2019].\n\n\nHafner, R., and Riedmiller, M. (2011). Reinforcement learning in\nfeedback control. Machine Learning 84, 137–169. doi:10.1007/s10994-011-5235-x.\n\n\nHarutyunyan, A., Bellemare, M. G., Stepleton, T., and Munos, R. (2016).\nQ(λ) with off-policy corrections. Available at: http://arxiv.org/abs/1602.04951.\n\n\nHausknecht, M., and Stone, P. (2015). Deep Recurrent\nQ-Learning for Partially Observable MDPs. Available\nat: http://arxiv.org/abs/1507.06527.\n\n\nHe, F. S., Liu, Y., Schwing, A. G., and Peng, J. (2016). Learning to\nPlay in a Day: Faster Deep Reinforcement\nLearning by Optimality Tightening. Available at: http://arxiv.org/abs/1611.01606.\n\n\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual\nLearning for Image Recognition. Available at: http://arxiv.org/abs/1512.03385.\n\n\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T.\n(2015). Learning continuous control policies by stochastic value\ngradients. Proc. International Conference on Neural Information\nProcessing Systems, 2944–2952. Available at: http://dl.acm.org/citation.cfm?id=2969569.\n\n\nHeinrich, J., Lanctot, M., and Silver, D. (2015). Fictitious\nSelf-Play in Extensive-Form Games. 805–813.\nAvailable at: http://proceedings.mlr.press/v37/heinrich15.html.\n\n\nHeinrich, J., and Silver, D. (2016). Deep Reinforcement\nLearning from Self-Play in\nImperfect-Information Games. Available at: http://arxiv.org/abs/1603.01121.\n\n\nHessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G.,\nDabney, W., et al. (2017). Rainbow: Combining Improvements\nin Deep Reinforcement Learning. Available at: http://arxiv.org/abs/1710.02298.\n\n\nHochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen\nNetzen. Available at: http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf.\n\n\nHochreiter, S., and Schmidhuber, J. (1997). Long Short-Term\nMemory. Neural Computation 9, 1735–1780. doi:10.1162/neco.1997.9.8.1735.\n\n\nHorgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van\nHasselt, H., et al. (2018). Distributed Prioritized Experience\nReplay. Available at: http://arxiv.org/abs/1803.00933\n[Accessed December 14, 2019].\n\n\nIoffe, S., and Szegedy, C. (2015). Batch Normalization:\nAccelerating Deep Network Training by Reducing\nInternal Covariate Shift. Available at: http://arxiv.org/abs/1502.03167.\n\n\nKakade, S. (2001). A Natural Policy Gradient. in\nAdvances in Neural Information Processing Systems\n14 Available at: https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf.\n\n\nKakade, S., and Langford, J. (2002). Approximately Optimal\nApproximate Reinforcement Learning. Proc. 19th International\nConference on Machine Learning, 267–274. Available at: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601.\n\n\nKapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W.\n(2019). Recurrent experience replay in distributed reinforcement\nlearning. in, 19. Available at: https://openreview.net/pdf?id=r1lyTjAqYX.\n\n\nKaufmann, E., Bauersfeld, L., Loquercio, A., Müller, M., Koltun, V., and\nScaramuzza, D. (2023). Champion-level drone racing using deep\nreinforcement learning. Nature 620, 982–987. doi:10.1038/s41586-023-06419-4.\n\n\nKendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen, J.-M., et\nal. (2018). Learning to Drive in a Day.\nAvailable at: http://arxiv.org/abs/1807.00412\n[Accessed December 19, 2018].\n\n\nKingma, D. P., and Welling, M. (2013). Auto-Encoding Variational\nBayes. Available at: http://arxiv.org/abs/1312.6114.\n\n\nKnight, E., and Lerner, O. (2018). Natural Gradient\nDeep Q-learning. Available at: http://arxiv.org/abs/1803.07482.\n\n\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet\nClassification with Deep Convolutional Neural\nNetworks. in Advances in Neural Information Processing\nSystems (NIPS) Available at: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.\n\n\nLevine, S., and Koltun, V. (2013). Guided Policy Search. in\nProceedings of Machine Learning Research, 1–9.\nAvailable at: http://proceedings.mlr.press/v28/levine13.html.\n\n\nLi, W., Zhu, Y., and Zhao, D. (2022). Missile guidance with assisted\ndeep reinforcement learning for head-on interception of maneuvering\ntarget. Complex Intell. Syst. 8, 1205–1216. doi:10.1007/s40747-021-00577-6.\n\n\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa,\nY., et al. (2015). Continuous control with deep reinforcement learning.\nCoRR. Available at: http://arxiv.org/abs/1509.02971.\n\n\nLötzsch, W., Vitay, J., and Hamker, F. H. (2017). Training a deep policy\ngradient-based neural network with asynchronous learners on a simulated\nrobotic problem. in INFORMATIK 2017.\nGesellschaft für Informatik, eds. M. Eibl\nand M. Gaedke (Gesellschaft für Informatik, Bonn), 2143–2154. Available\nat: https://dl.gi.de/handle/20.500.12116/3986.\n\n\nLuo, J., Paduraru, C., Voicu, O., Chervonyi, Y., Munns, S., Li, J., et\nal. (2022). Controlling Commercial Cooling Systems Using\nReinforcement Learning. doi:10.48550/arXiv.2211.07357.\n\n\nMachado, M. C., Bellemare, M. G., and Bowling, M. (2018).\nCount-Based Exploration with the Successor\nRepresentation. Available at: http://arxiv.org/abs/1807.11622\n[Accessed February 23, 2019].\n\n\nMadeka, D., Torkkola, K., Eisenach, C., Luo, A., Foster, D. P., and\nKakade, S. M. (2022). Deep Inventory Management. doi:10.48550/arXiv.2210.03137.\n\n\nMalibari, N., Katib, I., and Mehmood, R. (2023). Systematic\nReview on Reinforcement Learning in the\nField of Fintech. doi:10.48550/arXiv.2305.07466.\n\n\nMeuleau, N., Peshkin, L., Kaelbling, L. P., and Kim, K. (2000).\nOff-Policy Policy Search. Available at: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894.\n\n\nMirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A. J., Banino,\nA., et al. (2016). Learning to Navigate in Complex\nEnvironments. Available at: http://arxiv.org/abs/1611.03673.\n\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley,\nT., et al. (2016). Asynchronous Methods for Deep\nReinforcement Learning. in Proc. ICML\nAvailable at: http://arxiv.org/abs/1602.01783.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I.,\nWierstra, D., et al. (2013). Playing Atari with Deep\nReinforcement Learning. Available at: http://arxiv.org/abs/1312.5602.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J.,\nBellemare, M. G., et al. (2015). Human-level control through deep\nreinforcement learning. Nature 518, 529–533. doi:10.1038/nature14236.\n\n\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016).\nSafe and Efficient Off-Policy Reinforcement Learning.\nAvailable at: http://arxiv.org/abs/1606.02647.\n\n\nNachum, O., Gu, S., Lee, H., and Levine, S. (2018). Data-Efficient\nHierarchical Reinforcement Learning. Available at: http://arxiv.org/abs/1805.08296.\n\n\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017). Bridging the\nGap Between Value and Policy Based Reinforcement\nLearning. Available at: http://arxiv.org/abs/1702.08892\n[Accessed June 12, 2019].\n\n\nNagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. (2017). Neural\nNetwork Dynamics for Model-Based Deep Reinforcement\nLearning with Model-Free Fine-Tuning. Available at:\nhttp://arxiv.org/abs/1708.02596\n[Accessed March 3, 2019].\n\n\nNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De\nMaria, A., et al. (2015). Massively Parallel Methods for\nDeep Reinforcement Learning. Available at: https://arxiv.org/pdf/1507.04296.pdf.\n\n\nNielsen, M. A. (2015). Neural Networks and Deep\nLearning. Determination Press Available at: http://neuralnetworksanddeeplearning.com/.\n\n\nNiu, F., Recht, B., Re, C., and Wright, S. J. (2011).\nHOGWILD!: A Lock-Free Approach to\nParallelizing Stochastic Gradient Descent. in Proc.\nAdvances in Neural Information Processing\nSystems, 21–21. Available at: http://arxiv.org/abs/1106.5730.\n\n\nO’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2016).\nCombining policy gradient and Q-learning.\nAvailable at: http://arxiv.org/abs/1611.01626\n[Accessed February 13, 2019].\n\n\nOh, J., Guo, Y., Singh, S., and Lee, H. (2018). Self-Imitation\nLearning. Available at: http://arxiv.org/abs/1806.05635.\n\n\nPeshkin, L., and Shelton, C. R. (2002). Learning from Scarce\nExperience. Available at: http://arxiv.org/abs/cs/0204043.\n\n\nPeters, J., and Schaal, S. (2008). Reinforcement learning of motor\nskills with policy gradients. Neural Networks 21, 682–697.\ndoi:10.1016/j.neunet.2008.02.003.\n\n\nPopov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G.,\nVecerik, M., et al. (2017). Data-efficient Deep Reinforcement\nLearning for Dexterous Manipulation. Available at:\nhttp://arxiv.org/abs/1704.03073.\n\n\nPrecup, D., Sutton, R. S., and Singh, S. (2000). Eligibility traces for\noff-policy policy evaluation. in Proceedings of the\nSeventeenth International Conference on Machine\nLearning.\n\n\nRoy, R., Raiman, J., Kant, N., Elkin, I., Kirby, R., Siu, M., et al.\n(2022). PrefixRL: Optimization of\nParallel Prefix Circuits using Deep Reinforcement\nLearning. doi:10.1109/DAC18074.2021.9586094.\n\n\nRuder, S. (2016). An overview of gradient descent optimization\nalgorithms. Available at: http://arxiv.org/abs/1609.04747.\n\n\nSalimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017).\nEvolution Strategies as a Scalable Alternative\nto Reinforcement Learning. Available at: http://arxiv.org/abs/1703.03864.\n\n\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized\nExperience Replay. Available at: http://arxiv.org/abs/1511.05952.\n\n\nSchulman, J., Chen, X., and Abbeel, P. (2017a). Equivalence\nBetween Policy Gradients and Soft Q-Learning.\nAvailable at: http://arxiv.org/abs/1704.06440\n[Accessed June 12, 2019].\n\n\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.\n(2015a). Trust Region Policy Optimization. in\nProceedings of the 31 st International Conference on\nMachine Learning, 1889–1897. Available at: http://proceedings.mlr.press/v37/schulman15.html.\n\n\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.\n(2015b). High-Dimensional Continuous Control Using Generalized\nAdvantage Estimation. Available at: http://arxiv.org/abs/1506.02438.\n\n\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.\n(2017b). Proximal Policy Optimization Algorithms. Available\nat: http://arxiv.org/abs/1707.06347.\n\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den\nDriessche, G., et al. (2016). Mastering the game of Go with\ndeep neural networks and tree search. Nature 529, 484–489.\ndoi:10.1038/nature16961.\n\n\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and\nRiedmiller, M. (2014). Deterministic Policy Gradient\nAlgorithms. in Proc. ICML Proceedings of\nMachine Learning Research., eds. E. P. Xing and T. Jebara\n(PMLR), 387–395. Available at: http://proceedings.mlr.press/v32/silver14.html.\n\n\nSimonyan, K., and Zisserman, A. (2015). Very Deep Convolutional\nNetworks for Large-Scale Image Recognition.\nInternational Conference on Learning Representations (ICRL),\n1–14. doi:10.1016/j.infsof.2008.09.005.\n\n\nSutton, R. S. (1990). Integrated Architectures for\nLearning, Planning, and Reacting\nBased on Approximating Dynamic Programming.\nMachine Learning Proceedings 1990, 216–224. doi:10.1016/B978-1-55860-141-3.50030-4.\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement\nLearning: An introduction. Cambridge, MA:\nMIT press.\n\n\nSutton, R. S., and Barto, A. G. (2017). Reinforcement\nLearning: An Introduction. 2nd ed.\nCambridge, MA: MIT Press Available at: http://incompleteideas.net/book/the-book-2nd.html.\n\n\nSutton, R. S., McAllester, D., Singh, S., and Mansour, Y. (1999). Policy\ngradient methods for reinforcement learning with function approximation.\nin Proceedings of the 12th International Conference on\nNeural Information Processing Systems (MIT Press),\n1057–1063. Available at: https://dl.acm.org/citation.cfm?id=3009806.\n\n\nSzita, I., and Lörincz, A. (2006). Learning Tetris Using\nthe Noisy Cross-Entropy Method. Neural Computation\n18, 2936–2941. doi:10.1162/neco.2006.18.12.2936.\n\n\nTang, J., and Abbeel, P. (2010). On a Connection between\nImportance Sampling and the Likelihood Ratio Policy\nGradient. in Adv. Neural inf.\nProcess. Syst. Available at: http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf.\n\n\nTodorov, E. (2008). General duality between optimal control and\nestimation. in 2008 47th IEEE Conference on\nDecision and Control, 4286–4292. doi:10.1109/CDC.2008.4739438.\n\n\nToussaint, M. (2009). Robot Trajectory Optimization Using\nApproximate Inference. in Proceedings of the 26th\nAnnual International Conference on Machine\nLearning ICML ’09. (New York, NY, USA: ACM),\n1049–1056. doi:10.1145/1553374.1553508.\n\n\nUhlenbeck, G. E., and Ornstein, L. S. (1930). On the Theory\nof the Brownian Motion. Physical Review 36. doi:10.1103/PhysRev.36.823.\n\n\nvan Hasselt, H. (2010). Double Q-learning.\nin Proceedings of the 23rd International Conference on\nNeural Information Processing Systems - Volume\n2 (Curran Associates Inc.), 2613–2621. Available at: https://dl.acm.org/citation.cfm?id=2997187.\n\n\nvan Hasselt, H., Guez, A., and Silver, D. (2015). Deep\nReinforcement Learning with Double\nQ-learning. Available at: http://arxiv.org/abs/1509.06461.\n\n\nWang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z.,\nMunos, R., et al. (2017). Learning to reinforcement learn. Available at:\nhttp://arxiv.org/abs/1611.05763\n[Accessed February 5, 2021].\n\n\nWang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de\nFreitas, N. (2016). Dueling Network Architectures for\nDeep Reinforcement Learning. Available at: http://arxiv.org/abs/1511.06581\n[Accessed November 21, 2019].\n\n\nWatkins, C. J. (1989). Learning from delayed rewards.\n\n\nWierstra, D., Foerster, A., Peters, J., and Schmidhuber, J. (2007).\n“Solving Deep Memory POMDPs with Recurrent\nPolicy Gradients,” in (Springer, Berlin, Heidelberg),\n697–706. doi:10.1007/978-3-540-74690-4_71.\n\n\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms\nfor connectionist reinforcement learning. Machine Learning 8,\n229–256.\n\n\nWilliams, R. J., and Peng, J. (1991). Function optimization using\nconnectionist reinforcement learning algorithms. Connection\nScience 3, 241–268.\n\n\nYu, C., Liu, J., and Nemati, S. (2020). Reinforcement\nLearning in Healthcare: A Survey.\ndoi:10.48550/arXiv.1908.08796.\n\n\nZiebart, B. D., Maas, A., Bagnell, J. A., and Dey, A. K. (2008). Maximum\nEntropy Inverse Reinforcement Learning. in, 6.",
    "crumbs": [
      "References"
    ]
  }
]