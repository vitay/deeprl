<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Sampling and Bandits – Deep Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/1.2-MDP.html" rel="next">
<link href="../src/0-Introduction.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title"><span id="sec-bandits" class="quarto-section-identifier"><span class="chapter-title">Sampling and Bandits</span></span></h1>
        </a>     
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Reinforcement Learning</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/vitay/deeprl" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/0-Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>Basic RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/1.1-Bandits.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Sampling and Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/1.2-MDP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Markov Decision Process</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/1.4-MC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte Carlo methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/1.5-TD.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Temporal Difference learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>Value-based deep RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.1-FunctionApproximation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function approximation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.2-DeepNetworks.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.3-DQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Q-network (DQN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.4-DQNvariants.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">DQN variants (Rainbow)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.5-DistributedLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Distributed learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>Policy-gradient methods</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.1-PolicyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy Gradient methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.2-ActorCritic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advantage Actor-Critic (A3C)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.3-ImportanceSampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Off-policy Actor-Critic</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.4-DPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Deterministic Policy Gradient (DDPG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.5-NaturalGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural gradients</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.6-PPO.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy optimization (TRPO, PPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.7-ACER.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Actor-Critic with Experience Replay (ACER)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.8-EntropyRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Entropy RL (SAC)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.9-OtherPolicyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Miscellaneous model-free algorithms</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>Model-based deep RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/4.1-ModelBased.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/4.2-WorldModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">World models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/4.3-AlphaGo.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AlphaGo</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>Advanced topics</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/5.1-Hierarchical.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Hierarchical Reinforcement Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/5.2-Inverse.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Inverse Reinforcement Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/5.3-OfflineRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Offline RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/5.4-Meta.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Meta learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#n-armed-bandits" id="toc-n-armed-bandits" class="nav-link active" data-scroll-target="#n-armed-bandits">n-armed bandits</a></li>
  <li><a href="#random-sampling" id="toc-random-sampling" class="nav-link" data-scroll-target="#random-sampling">Random sampling</a>
  <ul class="collapse">
  <li><a href="#expectation" id="toc-expectation" class="nav-link" data-scroll-target="#expectation">Expectation</a></li>
  <li><a href="#random-sampling-1" id="toc-random-sampling-1" class="nav-link" data-scroll-target="#random-sampling-1">Random sampling</a></li>
  <li><a href="#central-limit-theorem" id="toc-central-limit-theorem" class="nav-link" data-scroll-target="#central-limit-theorem">Central limit theorem</a></li>
  </ul></li>
  <li><a href="#sampling-based-evaluation" id="toc-sampling-based-evaluation" class="nav-link" data-scroll-target="#sampling-based-evaluation">Sampling-based evaluation</a></li>
  <li><a href="#action-selection" id="toc-action-selection" class="nav-link" data-scroll-target="#action-selection">Action selection</a>
  <ul class="collapse">
  <li><a href="#greedy-action-selection" id="toc-greedy-action-selection" class="nav-link" data-scroll-target="#greedy-action-selection">Greedy action selection</a></li>
  <li><a href="#epsilon-greedy-action-selection" id="toc-epsilon-greedy-action-selection" class="nav-link" data-scroll-target="#epsilon-greedy-action-selection"><span class="math inline">\epsilon</span>-greedy action selection</a></li>
  <li><a href="#softmax-action-selection" id="toc-softmax-action-selection" class="nav-link" data-scroll-target="#softmax-action-selection">Softmax action selection</a></li>
  <li><a href="#optimistic-initial-values" id="toc-optimistic-initial-values" class="nav-link" data-scroll-target="#optimistic-initial-values">Optimistic initial values</a></li>
  <li><a href="#reinforcement-comparison" id="toc-reinforcement-comparison" class="nav-link" data-scroll-target="#reinforcement-comparison">Reinforcement comparison</a></li>
  <li><a href="#gradient-bandit-algorithm" id="toc-gradient-bandit-algorithm" class="nav-link" data-scroll-target="#gradient-bandit-algorithm">Gradient bandit algorithm</a></li>
  <li><a href="#upper-confidence-bound-action-selection" id="toc-upper-confidence-bound-action-selection" class="nav-link" data-scroll-target="#upper-confidence-bound-action-selection">Upper-Confidence-Bound action selection</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span id="sec-bandits" class="quarto-section-identifier"><span class="chapter-title">Sampling and Bandits</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="n-armed-bandits" class="level2">
<h2 class="anchored" data-anchor-id="n-armed-bandits">n-armed bandits</h2>
<p>The <strong>n-armed bandit</strong> (or multi-armed bandit) is the simplest form of learning by trial and error. Learning and action selection take place in the same single state, with <span class="math inline">n</span> available actions having different reward distributions. The goal is to find out through trial and error which action provides the most reward on average.</p>
<div id="fig-bandit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bandit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/bandit-example.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;2.1: Example of a bandit with 10 actions. The mean and the variance of each reward distribution are depicted."><img src="img/bandit-example.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bandit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: Example of a bandit with 10 actions. The mean and the variance of each reward distribution are depicted.
</figcaption>
</figure>
</div>
<p>We have the choice between <span class="math inline">N</span> different actions <span class="math inline">(a_1, ..., a_N)</span>. Each action <span class="math inline">a</span> taken at time <span class="math inline">t</span> provides a <strong>reward</strong> <span class="math inline">r_t</span> drawn from the action-specific probability distribution <span class="math inline">r(a)</span>.</p>
<p>The mathematical expectation of that distribution is the <strong>expected reward</strong>, called the <strong>true value</strong> of the action <span class="math inline">Q^*(a)</span>.</p>
<p><span class="math display">
    Q^*(a) = \mathbb{E} [r(a)]
</span></p>
<p>The reward distribution also has a <strong>variance</strong>: we usually ignore it in RL, as all we care about is the <strong>optimal action</strong> <span class="math inline">a^*</span> (but see distributional RL later).</p>
<p><span class="math display">a^* = \text{argmax}_a \, Q^*(a)</span></p>
<p>If we take the optimal action an infinity of times, we maximize the reward intake <strong>on average</strong>. The question is how to find out the optimal action through <strong>trial and error</strong>, i.e.&nbsp;without knowing the exact reward distribution <span class="math inline">r(a)</span>. We only have access to <strong>samples</strong> of <span class="math inline">r(a)</span> by taking the action <span class="math inline">a</span> at time <span class="math inline">t</span> (a <strong>trial</strong>, <strong>play</strong> or <strong>step</strong>).</p>
<p><span class="math display">r_t \sim r(a)</span></p>
<div id="fig-bandit-sample" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bandit-sample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/bandit-samples.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2.2: Sampled reward over time for the same action."><img src="img/bandit-samples.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bandit-sample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: Sampled reward over time for the same action.
</figcaption>
</figure>
</div>
<p>The received rewards <span class="math inline">r_t</span> vary around the true value over time. We need to build <strong>estimates</strong> <span class="math inline">Q_t(a)</span> of the value of each action based on the samples. These estimates will be very wrong at the beginning, but should get better over time.</p>
</section>
<section id="random-sampling" class="level2">
<h2 class="anchored" data-anchor-id="random-sampling">Random sampling</h2>
<section id="expectation" class="level3">
<h3 class="anchored" data-anchor-id="expectation">Expectation</h3>
<p>An important metric for a random variable is its <strong>mathematical expectation</strong> or expected value. For discrete distributions, it is the “mean” realization / outcome weighted by the corresponding probabilities:</p>
<p><span class="math display">
    \mathbb{E}[X] = \sum_{i=1}^n P(X = x_i) \, x_i
</span></p>
<p>For continuous distributions, one needs to integrate the <strong>probability density function</strong> (pdf) instead of the probabilities:</p>
<p><span class="math display">
    \mathbb{E}[X] = \int_{x \in \mathcal{D}_X} f(x) \, x \, dx
</span></p>
<p>One can also compute the expectation of a function of a random variable:</p>
<p><span class="math display">
    \mathbb{E}[g(X)] = \int_{x \in \mathcal{D}_X} f(x) \, g(x) \, dx
</span></p>
</section>
<section id="random-sampling-1" class="level3">
<h3 class="anchored" data-anchor-id="random-sampling-1">Random sampling</h3>
<p>In ML and RL, we deal with random variables whose exact probability distribution is unknown, but we are interested in their expectation or variance anyway.</p>
<div id="fig-samplingnormal" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-samplingnormal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/normaldistribution.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;2.3: Samples from the normal distribution are centered around its expected value."><img src="img/normaldistribution.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-samplingnormal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Samples from the normal distribution are centered around its expected value.
</figcaption>
</figure>
</div>
<p><strong>Random sampling</strong> or <strong>Monte Carlo sampling</strong> (MC) consists of taking <span class="math inline">N</span> samples <span class="math inline">x_i</span> out of the distribution <span class="math inline">X</span> (discrete or continuous) and computing the <strong>sample average</strong>:</p>
<p><span class="math display">
    \mathbb{E}[X] = \mathbb{E}_{x \sim X} [x] \approx \frac{1}{N} \, \sum_{i=1}^N x_i
</span></p>
<p>More samples will be obtained where <span class="math inline">f(x)</span> is high (<span class="math inline">x</span> is probable), so the average of the sampled data will be close to the expected value of the distribution.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Law of big numbers">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Law of big numbers
</div>
</div>
<div class="callout-body-container callout-body">
<p>As the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean.</p>
</div>
</div>
<p>MC estimates are only correct when:</p>
<ol type="1">
<li><p>the samples are <strong>i.i.d</strong> (independent and identically distributed):</p>
<ul>
<li>independent: the samples must be unrelated with each other.</li>
<li>identically distributed: the samples must come from the same distribution <span class="math inline">X</span>.</li>
</ul></li>
<li><p>the number of samples is large enough.</p></li>
</ol>
<p>One can estimate any function of the random variable with random sampling:</p>
<p><span class="math display">
    \mathbb{E}[f(X)] = \mathbb{E}_{x \sim X} [f(x)] \approx \frac{1}{N} \, \sum_{i=1}^N f(x_i)
</span></p>
</section>
<section id="central-limit-theorem" class="level3">
<h3 class="anchored" data-anchor-id="central-limit-theorem">Central limit theorem</h3>
<p>Suppose we have an unknown distribution <span class="math inline">X</span> with expected value <span class="math inline">\mu = \mathbb{E}[X]</span> and variance <span class="math inline">\sigma^2</span>. We can take randomly <span class="math inline">N</span> samples from <span class="math inline">X</span> to compute the sample average:</p>
<p><span class="math display">
    S_N = \frac{1}{N} \, \sum_{i=1}^N x_i
</span></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Central Limit Theorem">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Central Limit Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>The distribution of sample averages is normally distributed with mean <span class="math inline">\mu</span> and variance <span class="math inline">\frac{\sigma^2}{N}</span>.</p>
<p><span class="math display">S_N \sim \mathcal{N}(\mu, \frac{\sigma}{\sqrt{N}})</span></p>
</div>
</div>
<p>If we perform the sampling multiple times, even with few samples, the average of the sampling averages will be very close to the expected value. The more samples we get, the smaller the variance of the estimates. Although the distribution <span class="math inline">X</span> can be anything, the sampling averages are normally distributed.</p>
<div id="fig-clt" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/IllustrationCentralTheorem.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;2.4: Illustration of the central limit theorem. Source:: https://en.wikipedia.org/wiki/Central_limit_theorem"><img src="img/IllustrationCentralTheorem.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: Illustration of the central limit theorem. Source:: <a href="https://en.wikipedia.org/wiki/Central_limit_theorem" class="uri">https://en.wikipedia.org/wiki/Central_limit_theorem</a>
</figcaption>
</figure>
</div>
<p>CLT shows that the sampling average is an <strong>unbiased estimator</strong> of the expected value of a distribution:</p>
<p><span class="math display">\mathbb{E}(S_N) = \mathbb{E}(X)</span></p>
<p>An estimator is a random variable used to measure parameters of a distribution (e.g.&nbsp;its expectation). The problem is that estimators can generally be <strong>biased</strong>.</p>
<p>Take the example of a thermometer <span class="math inline">M</span> measuring the temperature <span class="math inline">T</span>. <span class="math inline">T</span> is a random variable (normally distributed with <span class="math inline">\mu=20</span> and <span class="math inline">\sigma=10</span>) and the measurements <span class="math inline">M</span> relate to the temperature with the relation:</p>
<p><span class="math display">
    M = 0.95 \, T + 0.65
</span></p>
<div id="fig-thermometer" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-thermometer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/estimators-temperature.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;2.5: Thermometer and temperature."><img src="img/estimators-temperature.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-thermometer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.5: Thermometer and temperature.
</figcaption>
</figure>
</div>
<p>The thermometer is not perfect, but do random measurements allow us to estimate the expected value of the temperature? We could repeatedly take 100 random samples of the thermometer and see how the distribution of sample averages look like:</p>
<div id="fig-thermometer2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-thermometer2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/estimators-temperature2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;2.6: Distribution of the sampling averages."><img src="img/estimators-temperature2.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-thermometer2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.6: Distribution of the sampling averages.
</figcaption>
</figure>
</div>
<p>But, as the expectation is linear, we actually have:</p>
<p><span class="math display">
    \mathbb{E}[M] = \mathbb{E}[0.95 \, T + 0.65] = 0.95 \, \mathbb{E}[T] + 0.65 = 19.65 \neq \mathbb{E}[T]
</span></p>
<p>The thermometer is a <strong>biased estimator</strong> of the temperature.</p>
<p>Let’s note <span class="math inline">\theta</span> a parameter of a probability distribution <span class="math inline">X</span> that we want to estimate (it does not have to be its mean). An <strong>estimator</strong> <span class="math inline">\hat{\theta}</span> is a random variable mapping the sample space of <span class="math inline">X</span> to a set of sample estimates. The <strong>bias</strong> of an estimator is the mean error made by the estimator:</p>
<p><span class="math display">
    \mathcal{B}(\hat{\theta}) = \mathbb{E}[\hat{\theta} - \theta] = \mathbb{E}[\hat{\theta}] - \theta
</span></p>
<p>The <strong>variance</strong> of an estimator is the deviation of the samples around the expected value:</p>
<p><span class="math display">
    \text{Var}(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}] )^2]
</span></p>
<p>Ideally, we would like estimators with a <strong>low bias</strong>, as the estimations would be correct on average (= equal to the true parameter) and a <strong>low variance</strong>, as we would not need many estimates to get a correct estimate (CLT: <span class="math inline">\frac{\sigma}{\sqrt{N}}</span>)</p>
<div id="fig-biasvariance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-biasvariance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/biasvariance3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;2.7: Bias-variance trade-off. Source: https://www.machinelearningplus.com/machine-learning/bias-variance-tradeoff/"><img src="img/biasvariance3.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-biasvariance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.7: Bias-variance trade-off. Source: <a href="https://www.machinelearningplus.com/machine-learning/bias-variance-tradeoff/" class="uri">https://www.machinelearningplus.com/machine-learning/bias-variance-tradeoff/</a>
</figcaption>
</figure>
</div>
<p>Unfortunately, the perfect estimator does not exist. Estimators will have a bias and a variance. For estimators with a high bias, the estimated values will be wrong, and the policy not optimal. For estimators with a high variance, we will need a lot of samples (trial and error) to have correct estimates. One usually talks of a <strong>bias/variance</strong> trade-off: if you have a small bias, you will have a high variance, or vice versa. There is a sweet spot balancing the two. In machine learning, bias corresponds to underfitting, variance to overfitting.</p>
</section>
</section>
<section id="sampling-based-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="sampling-based-evaluation">Sampling-based evaluation</h2>
<div id="fig-bandit-samples2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bandit-samples2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/bandit-samples2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;2.8: Samples and expected reward over time."><img src="img/bandit-samples2.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bandit-samples2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.8: Samples and expected reward over time.
</figcaption>
</figure>
</div>
<p>The expectation of the reward distribution can be approximated by the <strong>mean</strong> of its samples:</p>
<p><span class="math display">
    \mathbb{E} [r(a)] \approx  \frac{1}{N} \sum_{t=1}^N r_t |_{a_t = a}
</span></p>
<p>Suppose that the action <span class="math inline">a</span> had been selected <span class="math inline">t</span> times, producing rewards</p>
<p><span class="math display">
    (r_1, r_2, ..., r_t)
</span></p>
<p>The estimated value of action <span class="math inline">a</span> at play <span class="math inline">t</span> is then:</p>
<p><span class="math display">
    Q_t (a) = \frac{r_1 + r_2 + ... + r_t }{t}
</span></p>
<p>Over time, the estimated action-value converges to the true action-value:</p>
<p><span class="math display">
   \lim_{t \to \infty} Q_t (a) = Q^* (a)
</span></p>
<p>The drawback of maintaining the mean of the received rewards is that it consumes a lot of memory:</p>
<p><span class="math display">
    Q_t (a) = \frac{r_1 + r_2 + ... + r_t }{t} = \frac{1}{t} \, \sum_{i=1}^{t} r_i
</span></p>
<p>It is possible to update an estimate of the mean in an <strong>online</strong> or incremental manner:</p>
<p><span class="math display">
\begin{aligned}
    Q_{t+1}(a) &amp;= \frac{1}{t+1} \, \sum_{i=1}^{t+1} r_i = \frac{1}{t+1} \, (r_{t+1} + \sum_{i=1}^{t} r_i )\\
            &amp;= \frac{1}{t+1} \, (r_{t+1} + t \,  Q_{t}(a) ) \\
            &amp;= \frac{1}{t+1} \, (r_{t+1} + (t + 1) \,  Q_{t}(a) - Q_t(a))
\end{aligned}
</span></p>
<p>The estimate at time <span class="math inline">t+1</span> depends on the previous estimate at time <span class="math inline">t</span> and the last reward <span class="math inline">r_{t+1}</span>:</p>
<p><span class="math display">
    Q_{t+1}(a) = Q_t(a) + \frac{1}{t+1} \, (r_{t+1} - Q_t(a))
</span></p>
<p>The problem with the exact mean is that it is only exact when the reward distribution is <strong>stationary</strong>, i.e.&nbsp;when the probability distribution does not change over time. If the reward distribution is <strong>non-stationary</strong>, the <span class="math inline">\frac{1}{t+1}</span> term will become very small and prevent rapid updates of the mean.</p>
<div id="fig-nonstationary" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonstationary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/bandit-nonstationary1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;2.9: "><img src="img/bandit-nonstationary1.png" id="fig-nonstationary" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-nonstationary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.9
</figcaption>
</figure>
</div>
<p>The solution is to replace <span class="math inline">\frac{1}{t+1}</span> with a fixed parameter called the <strong>learning rate</strong> (or <strong>step size</strong>) <span class="math inline">\alpha</span>:</p>
<p><span class="math display">
\begin{aligned}
    Q_{t+1}(a) &amp; = Q_t(a) + \alpha \, (r_{t+1} - Q_t(a)) \\
                &amp; \\
                &amp; = (1 - \alpha) \, Q_t(a) + \alpha \, r_{t+1}
\end{aligned}
</span></p>
<div id="fig-nonstationary2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonstationary2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/bandit-nonstationary2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;2.10: "><img src="img/bandit-nonstationary2.png" id="fig-nonstationary2" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-nonstationary2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.10
</figcaption>
</figure>
</div>
<p>The computed value is called an <strong>exponentially moving average</strong> (or sliding average), as if one used only a small window of the past history.</p>
<p><span class="math display">
    Q_{t+1}(a) = Q_t(a) + \alpha \, (r_{t+1} - Q_t(a))
</span></p>
<p>or:</p>
<p><span class="math display">
    \Delta Q(a) = \alpha \, (r_{t+1} - Q_t(a))
</span></p>
<p>The moving average adapts very fast to changes in the reward distribution and should be used in <strong>non-stationary problems</strong>. It is however not exact and sensible to noise. Choosing the right value for <span class="math inline">\alpha</span> can be difficult.</p>
<p>The form of this <strong>update rule</strong> is very important to remember:</p>
<p><span class="math display">
    \text{new estimate} = \text{current estimate} + \alpha \, (\text{target} - \text{current estimate})
</span></p>
<p>Estimates following this update rule track the mean of their sampled target values. <span class="math inline">\text{target} - \text{current estimate}</span> is the <strong>prediction error</strong> between the target and the estimate.</p>
</section>
<section id="action-selection" class="level2">
<h2 class="anchored" data-anchor-id="action-selection">Action selection</h2>
<p>Let’s suppose we have formed reasonable estimates of the Q-values <span class="math inline">Q_t(a)</span> at time <span class="math inline">t</span>. Which action should we do next? If we select the next action <span class="math inline">a_{t+1}</span> randomly (<strong>random agent</strong>), we do not maximize the rewards we receive, but we can continue learning the Q-values. Choosing the action to perform next is called <strong>action selection</strong> and several schemes are possible.</p>
<section id="greedy-action-selection" class="level3">
<h3 class="anchored" data-anchor-id="greedy-action-selection">Greedy action selection</h3>
<p>The <strong>greedy action</strong> is the action whose expected value is <strong>maximal</strong> at time <span class="math inline">t</span> based on our current estimates:</p>
<p><span class="math display">
    a^*_t = \text{argmax}_{a} Q_t(a)
</span></p>
<p>If our estimates <span class="math inline">Q_t</span> are correct (i.e.&nbsp;close from <span class="math inline">Q^*</span>), the greedy action is the <strong>optimal action</strong> and we maximize the rewards on average. If our estimates are wrong, the agent will perform <strong>sub-optimally</strong>.</p>
<div id="fig-greedy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-greedy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/bandit-estimates-greedy.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;2.11: Greedy action selection. The action with the highest expected value is selected all the time."><img src="img/bandit-estimates-greedy.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-greedy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.11: Greedy action selection. The action with the highest expected value is selected all the time.
</figcaption>
</figure>
</div>
<p>This defines the <strong>greedy policy</strong>, where the probability of taking the greedy action is 1 and the probability of selecting another action is 0:</p>
<p><span class="math display">
    \pi(a) = \begin{cases}
                    1 \; \text{if} \; a = a_t^* \\
                    0 \; \text{otherwise.} \\
            \end{cases}
</span></p>
<p>The greedy policy is <strong>deterministic</strong>: the action taken is always the same for a fixed <span class="math inline">Q_t</span>.</p>
<p>However, the greedy action selection scheme only works when the estimates are good enough. Imagine that estimates are initially bad (e.g.&nbsp;0), and an action is sampled randomly. If the received reward is positive, the new Q-value of that action becomes positive, so it becomes the greedy action. At the next step, greedy action selection will always select that action, although the second one could have been better but it was never explored.</p>
<p><a href="img/bandit-greedy.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="img/bandit-greedy.gif" class="img-fluid"></a></p>
<p>This <strong>exploration-exploitation</strong> dilemma is the hardest problem in RL:</p>
<ul>
<li><strong>Exploitation</strong> is using the current estimates to select an action: they might be wrong!</li>
<li><strong>Exploration</strong> is selecting non-greedy actions in order to improve their estimates: they might not be optimal!</li>
</ul>
<p>One has to balance exploration and exploitation over the course of learning:</p>
<ul>
<li>More exploration at the beginning of learning, as the estimates are initially wrong.</li>
<li>More exploitation at the end of learning, as the estimates get better.</li>
</ul>
<div id="fig-exploitationexploration" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-exploitationexploration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/exploration_vs_exploitation.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;2.12: Source: UC Berkeley AI course slides, lecture 11"><img src="img/exploration_vs_exploitation.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-exploitationexploration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.12: Source: UC Berkeley AI course <a href="http://ai.berkeley.edu/lecture_slides.html">slides</a>, <a href="http://ai.berkeley.edu/slides/Lecture%2011%20--%20Reinforcement%20Learning%20II/SP14%20CS188%20Lecture%2011%20--%20Reinforcement%20Learning%20II.pptx">lecture 11</a>
</figcaption>
</figure>
</div>
</section>
<section id="epsilon-greedy-action-selection" class="level3">
<h3 class="anchored" data-anchor-id="epsilon-greedy-action-selection"><span class="math inline">\epsilon</span>-greedy action selection</h3>
<p><strong><span class="math inline">\epsilon</span>-greedy action selection</strong> ensures a trade-off between exploitation and exploration. The greedy action is selected with probability <span class="math inline">1 - \epsilon</span> (with <span class="math inline">0 &lt; \epsilon &lt;1</span>), the others with probability <span class="math inline">\epsilon</span>:</p>
<p><span class="math display">
    \pi(a) = \begin{cases} 1 - \epsilon \; \text{if} \; a = a_t^* \\ \frac{\epsilon}{|\mathcal{A}| - 1} \; \text{otherwise.} \end{cases}
</span></p>
<div id="fig-epsilongreedy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-epsilongreedy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/bandit-estimates-epsilongreedy.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;2.13: \epsilon-greedy action selection. The greedy action is selected most of the time, but the other actions might be selected from time to time."><img src="img/bandit-estimates-epsilongreedy.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-epsilongreedy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.13: <span class="math inline">\epsilon</span>-greedy action selection. The greedy action is selected most of the time, but the other actions might be selected from time to time.
</figcaption>
</figure>
</div>
<p>The parameter <span class="math inline">\epsilon</span> controls the level of exploration: the higher <span class="math inline">\epsilon</span>, the more exploration. One can set <span class="math inline">\epsilon</span> high at the beginning of learning and progressively reduce it to exploit more. However, it chooses equally among all actions: the worst action is as likely to be selected as the next-to-best action.</p>
<p><a href="img/bandit-egreedy.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img src="img/bandit-egreedy.gif" class="img-fluid"></a></p>
</section>
<section id="softmax-action-selection" class="level3">
<h3 class="anchored" data-anchor-id="softmax-action-selection">Softmax action selection</h3>
<p><strong>Softmax action selection</strong> defines the probability of choosing an action using all estimated value. It represents the policy using a Gibbs (or Boltzmann) distribution:</p>
<p><span class="math display">
    \pi(a) = \dfrac{\exp \dfrac{Q_t(a)}{\tau}}{ \displaystyle\sum_{a'} \exp \dfrac{Q_t(a')}{\tau}}
</span></p>
<p>where <span class="math inline">\tau</span> is a positive parameter called the <strong>temperature</strong>.</p>
<div id="fig-softmax1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-softmax1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/bandit-estimates-softmax.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;2.14: Softmax action selection."><img src="img/bandit-estimates-softmax.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-softmax1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.14: Softmax action selection.
</figcaption>
</figure>
</div>
<p>Just as <span class="math inline">\epsilon</span>, the temperature <span class="math inline">\tau</span> controls the level of exploration:</p>
<ul>
<li>High temperature causes the actions to be nearly equiprobable (<strong>random agent</strong>).</li>
<li>Low temperature causes the greediest actions only to be selected (<strong>greedy agent</strong>).</li>
</ul>
<div id="fig-softmax2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-softmax2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/bandit-estimates-softmax2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Figure&nbsp;2.15: Influence of the temperature parameter."><img src="img/bandit-estimates-softmax2.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-softmax2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.15: Influence of the temperature parameter.
</figcaption>
</figure>
</div>
<p><a href="img/bandit-softmax.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img src="img/bandit-softmax.gif" class="img-fluid"></a></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Exploration schedule">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exploration schedule
</div>
</div>
<div class="callout-body-container callout-body">
<p>A useful technique to cope with the <strong>exploration-exploitation dilemma</strong> is to slowly decrease the value of <span class="math inline">\epsilon</span> or <span class="math inline">\tau</span> with the number of plays. This allows for more exploration at the beginning of learning and more exploitation towards the end. It is however hard to find the right <strong>decay rate</strong> for the exploration parameters.</p>
<p><a href="img/bandit-scheduling.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19"><img src="img/bandit-scheduling.png" class="img-fluid"></a></p>
</div>
</div>
</section>
<section id="optimistic-initial-values" class="level3">
<h3 class="anchored" data-anchor-id="optimistic-initial-values">Optimistic initial values</h3>
<p>The problem with online evaluation is that it depends a lot on the initial estimates <span class="math inline">Q_0</span>. If the initial estimates are already quite good (e.g.&nbsp;using expert knowledge), the Q-values will converge very fast. If the initial estimates are very wrong, we will need a lot of updates to correctly estimate the true values. This problem is called <strong>bootstrapping</strong>: the better your initial estimates, the better (and faster) the results.</p>
<p><span class="math display">
\begin{aligned}
    &amp;Q_{t+1}(a) = (1 - \alpha) \, Q_t(a) + \alpha \, r_{t+1}  \\
    &amp;\\
    &amp; \rightarrow Q_1(a) = (1 - \alpha) \, Q_0(a) + \alpha \, r_1 \\
    &amp; \rightarrow Q_2(a) = (1 - \alpha) \, Q_1(a) + \alpha \, r_2 = (1- \alpha)^2 \, Q_0(a) + (1-\alpha)\alpha \, r_1 + \alpha r_2 \\
\end{aligned}
</span></p>
<p>The influence of <span class="math inline">Q_0</span> on <span class="math inline">Q_t</span> <strong>fades</strong> quickly with <span class="math inline">(1-\alpha)^t</span>, but that can be lost time or lead to a suboptimal policy. However, we can use this at our advantage with <strong>optimistic initialization</strong>. By choosing very high initial values for the estimates (they can only decrease), one can ensure that all possible actions will be selected during learning by the greedy method, solving the <strong>exploration problem</strong>. This leads however to an <strong>overestimation</strong> of the value of other actions.</p>
<p><a href="img/bandit-optimistic.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-20"><img src="img/bandit-optimistic.gif" class="img-fluid"></a></p>
</section>
<section id="reinforcement-comparison" class="level3">
<h3 class="anchored" data-anchor-id="reinforcement-comparison">Reinforcement comparison</h3>
<p>Actions followed by large rewards should be made more likely to reoccur, whereas actions followed by small rewards should be made less likely to reoccur. But what is a large/small reward? Is a reward of 5 large or small? <strong>Reinforcement comparison</strong> methods only maintain a <strong>preference</strong> <span class="math inline">p_t(a)</span> for each action, which is not exactly its Q-value. The preference for an action is updated after each play, according to the update rule:</p>
<p><span class="math display">
    p_{t+1}(a_t) =    p_{t}(a_t) + \beta \, (r_t - \tilde{r}_t)
</span></p>
<p>where <span class="math inline">\tilde{r}_t</span> is the moving average of the recently received rewards (regardless the action):</p>
<p><span class="math display">
    \tilde{r}_{t+1} =  \tilde{r}_t + \alpha \, (r_t - \tilde{r}_t)
</span></p>
<p>If an action brings more reward than usual (<strong>good surprise</strong>), we increase the preference for that action. If an action brings less reward than usual (<strong>bad surprise</strong>), we decrease the preference for that action. <span class="math inline">\beta &gt; 0</span> and <span class="math inline">0 &lt; \alpha &lt; 1</span> are two constant parameters.</p>
<p>Preferences are updated by replacing the action-dependent Q-values by a baseline <span class="math inline">\tilde{r}_t</span>:</p>
<p><span class="math display">
    p_{t+1}(a_t) =    p_{t}(a_t) + \beta \, (r_t - \tilde{r}_t)
</span></p>
<p>The preferences can be used to select the action using the softmax method just as the Q-values (without temperature):</p>
<p><span class="math display">
    \pi_t (a) = \dfrac{\exp p_t(a)}{ \displaystyle\sum_{a'} \exp p_t(a')}
</span></p>
<p><a href="img/bandit-reinforcementcomparison.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-21"><img src="img/bandit-reinforcementcomparison.gif" class="img-fluid"></a></p>
<p>Reinforcement comparison can be very effective, as it does not rely only on the rewards received, but also on their comparison with a <strong>baseline</strong>, the average reward. This idea is at the core of <strong>actor-critic</strong> architectures which we will see later. The initial average reward <span class="math inline">\tilde{r}_{0}</span> can be set optimistically to encourage exploration.</p>
</section>
<section id="gradient-bandit-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="gradient-bandit-algorithm">Gradient bandit algorithm</h3>
<p>Instead of only increasing the preference for the executed action if it brings more reward than usual, we could also decrease the preference for the other actions. The preferences are used to select an action <span class="math inline">a_t</span> <em>via</em> softmax:</p>
<p><span class="math display">
    \pi_t (a) = \dfrac{\exp p_t(a)}{ \displaystyle\sum_{a'} \exp p_t(a')}
</span></p>
<p>Update rule for the <strong>action taken</strong> <span class="math inline">a_t</span>:</p>
<p><span class="math display">
    p_{t+1}(a_t) =    p_{t}(a_t) + \beta \, (r_t - \tilde{r}_t) \, (1 - \pi_t(a_t))
</span></p>
<p>Update rule for the <strong>other actions</strong> <span class="math inline">a \neq a_t</span>:</p>
<p><span class="math display">
    p_{t+1}(a) =    p_{t}(a) - \beta \, (r_t - \tilde{r}_t) \, \pi_t(a)
</span></p>
<p>Update of the reward <strong>baseline</strong>:</p>
<p><span class="math display">
    \tilde{r}_{t+1} =  \tilde{r}_t + \alpha \, (r_t - \tilde{r}_t)
</span></p>
<p>The preference can increase become quite high, making the policy greedy towards the end. No need for a temperature parameter!</p>
<p><a href="img/bandit-gradientbandit.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-22"><img src="img/bandit-gradientbandit.gif" class="img-fluid"></a></p>
</section>
<section id="upper-confidence-bound-action-selection" class="level3">
<h3 class="anchored" data-anchor-id="upper-confidence-bound-action-selection">Upper-Confidence-Bound action selection</h3>
<p>In the previous methods, <strong>exploration</strong> is controlled by an external parameter (<span class="math inline">\epsilon</span> or <span class="math inline">\tau</span>) which is <strong>global</strong> to each action an must be scheduled. A much better approach would be to decide whether to explore an action based on the <strong>uncertainty</strong> about its Q-value: If we are certain about the value of an action, there is no need to explore it further, we only have to exploit it if it is good.</p>
<p>The <strong>central limit theorem</strong> tells us that the variance of a sampling estimator decreases with the number of samples:</p>
<blockquote class="blockquote">
<p>The distribution of sample averages is normally distributed with mean <span class="math inline">\mu</span> and variance <span class="math inline">\frac{\sigma^2}{N}</span>.</p>
</blockquote>
<p><span class="math display">S_N \sim \mathcal{N}(\mu, \frac{\sigma}{\sqrt{N}})</span></p>
<p>The more you explore an action <span class="math inline">a</span>, the smaller the variance of <span class="math inline">Q_t(a)</span>, the more certain you are about the estimation, the less you need to explore it.</p>
<p><strong>Upper-Confidence-Bound</strong> (UCB) action selection is a <strong>greedy</strong> action selection method that uses an <strong>exploration</strong> bonus:</p>
<p><span class="math display">
    a^*_t = \text{argmax}_{a} \left[ Q_t(a) + c \, \sqrt{\frac{\ln t}{N_t(a)}} \right]
</span></p>
<p><span class="math inline">Q_t(a)</span> is the current estimated value of <span class="math inline">a</span> and <span class="math inline">N_t(a)</span> is the number of times the action <span class="math inline">a</span> has already been selected.</p>
<p>It realizes a balance between trusting the estimates <span class="math inline">Q_t(a)</span> and exploring uncertain actions which have not been explored much yet. The term <span class="math inline">\sqrt{\frac{\ln t}{N_t(a)}}</span> is an estimate of the variance of <span class="math inline">Q_t(a)</span>. The sum of both terms is an <strong>upper-bound</strong> of the true value <span class="math inline">\mu + \sigma</span>. When an action has not been explored much yet, the uncertainty term will dominate and the action be explored, although its estimated value might be low. When an action has been sufficiently explored, the uncertainty term goes to 0 and we greedily follow <span class="math inline">Q_t(a)</span>.</p>
<p>The <strong>exploration-exploitation</strong> trade-off is automatically adjusted by counting visits to an action.</p>
<p><span class="math display">
    a^*_t = \text{argmax}_{a} \left[ Q_t(a) + c \, \sqrt{\frac{\ln t}{N_t(a)}} \right]
</span></p>
<p><a href="img/bandit-ucb.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-23"><img src="img/bandit-ucb.gif" class="img-fluid"></a></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../src/0-Introduction.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Introduction</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/1.2-MDP.html" class="pagination-link" aria-label="Markov Decision Process">
        <span class="nav-page-text"><span class="chapter-title">Markov Decision Process</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0">Creative Commons BY-NC-SA 4.0</a>. Author <a href="mailto:julien.vitay@gmail.com">Julien Vitay</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"openEffect":"zoom","loop":false,"closeEffect":"zoom","descPosition":"bottom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>