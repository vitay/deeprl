<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>References – Deep Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/3.8-OtherPolicyGradient.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title">References</h1>
        </a>     
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Reinforcement Learning</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/vitay/deeprl" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/0-Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>Basic RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/1.2-MDP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Markov Decision Process</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/1.3-DP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/1.4-MC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/1.5-TD.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Temporal Difference algorithm</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>Value-based deep RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.1-FunctionApproximation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function approximation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.2-DeepNetworks.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.3-DQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">DQN</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>Policy-gradient methods</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.1-PolicyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy Gradient methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.2-ActorCritic.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Advantage Actor-Critic methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.3-ImportanceSampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Off-policy Actor-Critic</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.4-DPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deterministic Policy Gradient (DPG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.5-NaturalGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural Gradients</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.6-EntropyRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Entropy RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.7-DistributionalRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Distributional learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.8-OtherPolicyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Miscellaneous model-free algorithm</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/references.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">References</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Amari1998" class="csl-entry" role="listitem">
Amari, S.-I. (1998). Natural gradient works efficiently in learning.
<em>Neural Computation</em> 10, 251–276.
</div>
<div id="ref-Anschel2016" class="csl-entry" role="listitem">
Anschel, O., Baram, N., and Shimkin, N. (2016).
Averaged-<span>DQN</span>: <span>Variance Reduction</span> and
<span>Stabilization</span> for <span>Deep Reinforcement Learning</span>.
Available at: <a href="http://arxiv.org/abs/1611.01929">http://arxiv.org/abs/1611.01929</a>.
</div>
<div id="ref-Arjovsky2017" class="csl-entry" role="listitem">
Arjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein
<span>GAN</span>. Available at: <a href="http://arxiv.org/abs/1701.07875">http://arxiv.org/abs/1701.07875</a>.
</div>
<div id="ref-Baird1993" class="csl-entry" role="listitem">
Baird, L. C. (1993). Advantage updating. Wright-Patterson Air Force Base
Available at: <a href="http://leemon.com/papers/1993b.pdf">http://leemon.com/papers/1993b.pdf</a>.
</div>
<div id="ref-Bakker2001" class="csl-entry" role="listitem">
Bakker, B. (2001). Reinforcement <span>Learning</span> with <span>Long
Short-Term Memory</span>. in <em>Advances in <span>Neural Information
Processing Systems</span> 14 (<span>NIPS</span> 2001)</em>, 1475–1482.
Available at: <a href="https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory">https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory</a>.
</div>
<div id="ref-Barth-Maron2018" class="csl-entry" role="listitem">
Barth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., TB,
D., et al. (2018). Distributed <span>Distributional Deterministic Policy
Gradients</span>. Available at: <a href="http://arxiv.org/abs/1804.08617">http://arxiv.org/abs/1804.08617</a>.
</div>
<div id="ref-Bellemare2017" class="csl-entry" role="listitem">
Bellemare, M. G., Dabney, W., and Munos, R. (2017). A
<span>Distributional Perspective</span> on <span>Reinforcement
Learning</span>. Available at: <a href="http://arxiv.org/abs/1707.06887">http://arxiv.org/abs/1707.06887</a>.
</div>
<div id="ref-Cho2014" class="csl-entry" role="listitem">
Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,
Schwenk, H., et al. (2014). Learning <span>Phrase Representations</span>
using <span>RNN Encoder-Decoder</span> for <span>Statistical Machine
Translation</span>. Available at: <a href="http://arxiv.org/abs/1406.1078">http://arxiv.org/abs/1406.1078</a>.
</div>
<div id="ref-Chou2017" class="csl-entry" role="listitem">
Chou, P.-W., Maturana, D., and Scherer, S. (2017). Improving
<span>Stochastic Policy Gradients</span> in <span>Continuous
Control</span> with <span>Deep Reinforcement Learning</span> using the
<span>Beta Distribution</span>. in <em>International
<span>Conference</span> on <span>Machine Learning</span></em> Available
at: <a href="http://proceedings.mlr.press/v70/chou17a/chou17a.pdf">http://proceedings.mlr.press/v70/chou17a/chou17a.pdf</a>.
</div>
<div id="ref-Dabney2017" class="csl-entry" role="listitem">
Dabney, W., Rowland, M., Bellemare, M. G., and Munos, R. (2017).
Distributional <span>Reinforcement Learning</span> with <span>Quantile
Regression</span>. Available at: <a href="http://arxiv.org/abs/1710.10044">http://arxiv.org/abs/1710.10044</a>
[Accessed June 28, 2019].
</div>
<div id="ref-Degrave2022" class="csl-entry" role="listitem">
Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese,
F., et al. (2022). Magnetic control of tokamak plasmas through deep
reinforcement learning. <em>Nature</em> 602, 414–419. doi:<a href="https://doi.org/10.1038/s41586-021-04301-9">10.1038/s41586-021-04301-9</a>.
</div>
<div id="ref-Degris2012" class="csl-entry" role="listitem">
Degris, T., White, M., and Sutton, R. S. (2012). Linear <span>Off-Policy
Actor-Critic</span>. in <em>Proceedings of the 2012 <span>International
Conference</span> on <span>Machine Learning</span></em> Available at: <a href="http://arxiv.org/abs/1205.4839">http://arxiv.org/abs/1205.4839</a>.
</div>
<div id="ref-Duan2016" class="csl-entry" role="listitem">
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016).
Benchmarking <span>Deep Reinforcement Learning</span> for
<span>Continuous Control</span>. Available at: <a href="http://arxiv.org/abs/1604.06778">http://arxiv.org/abs/1604.06778</a>.
</div>
<div id="ref-Espeholt2018" class="csl-entry" role="listitem">
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., et
al. (2018). <span>IMPALA</span>: <span>Scalable Distributed
Deep-RL</span> with <span>Importance Weighted Actor-Learner
Architectures</span>. doi:<a href="https://doi.org/10.48550/arXiv.1802.01561">10.48550/arXiv.1802.01561</a>.
</div>
<div id="ref-Fortunato2017" class="csl-entry" role="listitem">
Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves,
A., et al. (2017). Noisy <span>Networks</span> for
<span>Exploration</span>. Available at: <a href="http://arxiv.org/abs/1706.10295">http://arxiv.org/abs/1706.10295</a>
[Accessed March 2, 2020].
</div>
<div id="ref-Gers2001" class="csl-entry" role="listitem">
Gers, F. (2001). Long <span>Short-Term Memory</span> in <span>Recurrent
Neural Networks</span>. Available at: <a href="http://www.felixgers.de/papers/phd.pdf">http://www.felixgers.de/papers/phd.pdf</a>.
</div>
<div id="ref-Goodfellow2014" class="csl-entry" role="listitem">
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,
D., Ozair, S., et al. (2014). Generative <span>Adversarial
Networks</span>. Available at: <a href="http://arxiv.org/abs/1406.2661">http://arxiv.org/abs/1406.2661</a>.
</div>
<div id="ref-Goodfellow2016" class="csl-entry" role="listitem">
Goodfellow, I., Bengio, Y., and Courville, A. (2016). <em>Deep
<span>Learning</span></em>. MIT Press Available at: <a href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a>.
</div>
<div id="ref-Gruslys2017" class="csl-entry" role="listitem">
Gruslys, A., Dabney, W., Azar, M. G., Piot, B., Bellemare, M., and
Munos, R. (2017). The <span>Reactor</span>: <span>A</span> fast and
sample-efficient <span>Actor-Critic</span> agent for <span>Reinforcement
Learning</span>. Available at: <a href="http://arxiv.org/abs/1704.04651">http://arxiv.org/abs/1704.04651</a>.
</div>
<div id="ref-Gu2017" class="csl-entry" role="listitem">
Gu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep
<span>Reinforcement Learning</span> for <span>Robotic
Manipulation</span> with <span>Asynchronous Off-Policy Updates</span>.
in <em>Proc. <span>ICRA</span></em> Available at: <a href="http://arxiv.org/abs/1610.00633">http://arxiv.org/abs/1610.00633</a>.
</div>
<div id="ref-Gu2016a" class="csl-entry" role="listitem">
Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and Levine, S.
(2016a). Q-<span>Prop</span>: <span>Sample-Efficient Policy
Gradient</span> with <span>An Off-Policy Critic</span>. Available at: <a href="http://arxiv.org/abs/1611.02247">http://arxiv.org/abs/1611.02247</a>.
</div>
<div id="ref-Gu2016" class="csl-entry" role="listitem">
Gu, S., Lillicrap, T., Sutskever, I., and Levine, S. (2016b). Continuous
<span>Deep Q-Learning</span> with <span class="nocase">Model-based
Acceleration</span>. Available at: <a href="http://arxiv.org/abs/1603.00748">http://arxiv.org/abs/1603.00748</a>.
</div>
<div id="ref-Haarnoja2017" class="csl-entry" role="listitem">
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement
<span>Learning</span> with <span>Deep Energy-Based Policies</span>.
Available at: <a href="http://arxiv.org/abs/1702.08165">http://arxiv.org/abs/1702.08165</a>
[Accessed February 13, 2019].
</div>
<div id="ref-Haarnoja2018a" class="csl-entry" role="listitem">
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., et
al. (2018). Soft <span>Actor-Critic Algorithms</span> and
<span>Applications</span>. Available at: <a href="http://arxiv.org/abs/1812.05905">http://arxiv.org/abs/1812.05905</a>
[Accessed February 5, 2019].
</div>
<div id="ref-Hafner2011" class="csl-entry" role="listitem">
Hafner, R., and Riedmiller, M. (2011). Reinforcement learning in
feedback control. <em>Machine Learning</em> 84, 137–169. doi:<a href="https://doi.org/10.1007/s10994-011-5235-x">10.1007/s10994-011-5235-x</a>.
</div>
<div id="ref-Harutyunyan2016" class="csl-entry" role="listitem">
Harutyunyan, A., Bellemare, M. G., Stepleton, T., and Munos, R. (2016).
Q(λ) with off-policy corrections. Available at: <a href="http://arxiv.org/abs/1602.04951">http://arxiv.org/abs/1602.04951</a>.
</div>
<div id="ref-Hausknecht2015" class="csl-entry" role="listitem">
Hausknecht, M., and Stone, P. (2015). Deep <span>Recurrent
Q-Learning</span> for <span>Partially Observable MDPs</span>. Available
at: <a href="http://arxiv.org/abs/1507.06527">http://arxiv.org/abs/1507.06527</a>.
</div>
<div id="ref-He2016" class="csl-entry" role="listitem">
He, F. S., Liu, Y., Schwing, A. G., and Peng, J. (2016). Learning to
<span>Play</span> in a <span>Day</span>: <span>Faster Deep Reinforcement
Learning</span> by <span>Optimality Tightening</span>. Available at: <a href="http://arxiv.org/abs/1611.01606">http://arxiv.org/abs/1611.01606</a>.
</div>
<div id="ref-He2015" class="csl-entry" role="listitem">
He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep <span>Residual
Learning</span> for <span>Image Recognition</span>. Available at: <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.
</div>
<div id="ref-Heess2015" class="csl-entry" role="listitem">
Heess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T.
(2015). Learning continuous control policies by stochastic value
gradients. <em>Proc. International Conference on Neural Information
Processing Systems</em>, 2944–2952. Available at: <a href="http://dl.acm.org/citation.cfm?id=2969569">http://dl.acm.org/citation.cfm?id=2969569</a>.
</div>
<div id="ref-Heinrich2015" class="csl-entry" role="listitem">
Heinrich, J., Lanctot, M., and Silver, D. (2015). Fictitious
<span>Self-Play</span> in <span>Extensive-Form Games</span>. 805–813.
Available at: <a href="http://proceedings.mlr.press/v37/heinrich15.html">http://proceedings.mlr.press/v37/heinrich15.html</a>.
</div>
<div id="ref-Heinrich2016" class="csl-entry" role="listitem">
Heinrich, J., and Silver, D. (2016). Deep <span>Reinforcement
Learning</span> from <span>Self-Play</span> in
<span>Imperfect-Information Games</span>. Available at: <a href="http://arxiv.org/abs/1603.01121">http://arxiv.org/abs/1603.01121</a>.
</div>
<div id="ref-Hessel2017" class="csl-entry" role="listitem">
Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G.,
Dabney, W., et al. (2017). Rainbow: <span>Combining Improvements</span>
in <span>Deep Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1710.02298">http://arxiv.org/abs/1710.02298</a>.
</div>
<div id="ref-Hochreiter1991" class="csl-entry" role="listitem">
Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen
<span>Netzen</span>. Available at: <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf</a>.
</div>
<div id="ref-Hochreiter1997" class="csl-entry" role="listitem">
Hochreiter, S., and Schmidhuber, J. (1997). Long <span>Short-Term
Memory</span>. <em>Neural Computation</em> 9, 1735–1780. doi:<a href="https://doi.org/10.1162/neco.1997.9.8.1735">10.1162/neco.1997.9.8.1735</a>.
</div>
<div id="ref-Horgan2018" class="csl-entry" role="listitem">
Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van
Hasselt, H., et al. (2018). Distributed <span>Prioritized Experience
Replay</span>. Available at: <a href="http://arxiv.org/abs/1803.00933">http://arxiv.org/abs/1803.00933</a>
[Accessed December 14, 2019].
</div>
<div id="ref-Ioffe2015" class="csl-entry" role="listitem">
Ioffe, S., and Szegedy, C. (2015). Batch <span>Normalization</span>:
<span>Accelerating Deep Network Training</span> by <span>Reducing
Internal Covariate Shift</span>. Available at: <a href="http://arxiv.org/abs/1502.03167">http://arxiv.org/abs/1502.03167</a>.
</div>
<div id="ref-Kakade2001" class="csl-entry" role="listitem">
Kakade, S. (2001). A <span>Natural Policy Gradient</span>. in
<em>Advances in <span>Neural Information Processing Systems</span>
14</em> Available at: <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a>.
</div>
<div id="ref-Kakade2002" class="csl-entry" role="listitem">
Kakade, S., and Langford, J. (2002). Approximately <span>Optimal
Approximate Reinforcement Learning</span>. <em>Proc. 19th International
Conference on Machine Learning</em>, 267–274. Available at: <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601</a>.
</div>
<div id="ref-Kapturowski2019" class="csl-entry" role="listitem">
Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W.
(2019). Recurrent experience replay in distributed reinforcement
learning. in, 19. Available at: <a href="https://openreview.net/pdf?id=r1lyTjAqYX">https://openreview.net/pdf?id=r1lyTjAqYX</a>.
</div>
<div id="ref-Kaufmann2023" class="csl-entry" role="listitem">
Kaufmann, E., Bauersfeld, L., Loquercio, A., Müller, M., Koltun, V., and
Scaramuzza, D. (2023). Champion-level drone racing using deep
reinforcement learning. <em>Nature</em> 620, 982–987. doi:<a href="https://doi.org/10.1038/s41586-023-06419-4">10.1038/s41586-023-06419-4</a>.
</div>
<div id="ref-Kendall2018" class="csl-entry" role="listitem">
Kendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen, J.-M., et
al. (2018). Learning to <span>Drive</span> in a <span>Day</span>.
Available at: <a href="http://arxiv.org/abs/1807.00412">http://arxiv.org/abs/1807.00412</a>
[Accessed December 19, 2018].
</div>
<div id="ref-Kingma2013" class="csl-entry" role="listitem">
Kingma, D. P., and Welling, M. (2013). Auto-<span>Encoding Variational
Bayes</span>. Available at: <a href="http://arxiv.org/abs/1312.6114">http://arxiv.org/abs/1312.6114</a>.
</div>
<div id="ref-Knight2018" class="csl-entry" role="listitem">
Knight, E., and Lerner, O. (2018). Natural <span class="nocase">Gradient
Deep Q-learning</span>. Available at: <a href="http://arxiv.org/abs/1803.07482">http://arxiv.org/abs/1803.07482</a>.
</div>
<div id="ref-Krizhevsky2012" class="csl-entry" role="listitem">
Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). <span>ImageNet
Classification</span> with <span>Deep Convolutional Neural
Networks</span>. in <em>Advances in <span>Neural Information Processing
Systems</span> (<span>NIPS</span>)</em> Available at: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>.
</div>
<div id="ref-Levine2013" class="csl-entry" role="listitem">
Levine, S., and Koltun, V. (2013). Guided <span>Policy Search</span>. in
<em>Proceedings of <span>Machine Learning Research</span></em>, 1–9.
Available at: <a href="http://proceedings.mlr.press/v28/levine13.html">http://proceedings.mlr.press/v28/levine13.html</a>.
</div>
<div id="ref-Li2022a" class="csl-entry" role="listitem">
Li, W., Zhu, Y., and Zhao, D. (2022). Missile guidance with assisted
deep reinforcement learning for head-on interception of maneuvering
target. <em>Complex Intell. Syst.</em> 8, 1205–1216. doi:<a href="https://doi.org/10.1007/s40747-021-00577-6">10.1007/s40747-021-00577-6</a>.
</div>
<div id="ref-Lillicrap2015" class="csl-entry" role="listitem">
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa,
Y., et al. (2015). Continuous control with deep reinforcement learning.
<em>CoRR</em>. Available at: <a href="http://arxiv.org/abs/1509.02971">http://arxiv.org/abs/1509.02971</a>.
</div>
<div id="ref-Lotzsch2017" class="csl-entry" role="listitem">
Lötzsch, W., Vitay, J., and Hamker, F. H. (2017). Training a deep policy
gradient-based neural network with asynchronous learners on a simulated
robotic problem. in <em><span>INFORMATIK</span> 2017.
<span>Gesellschaft</span> für <span>Informatik</span></em>, eds. M. Eibl
and M. Gaedke (Gesellschaft für Informatik, Bonn), 2143–2154. Available
at: <a href="https://dl.gi.de/handle/20.500.12116/3986">https://dl.gi.de/handle/20.500.12116/3986</a>.
</div>
<div id="ref-Luo2022" class="csl-entry" role="listitem">
Luo, J., Paduraru, C., Voicu, O., Chervonyi, Y., Munns, S., Li, J., et
al. (2022). Controlling <span>Commercial Cooling Systems Using
Reinforcement Learning</span>. doi:<a href="https://doi.org/10.48550/arXiv.2211.07357">10.48550/arXiv.2211.07357</a>.
</div>
<div id="ref-Machado2018" class="csl-entry" role="listitem">
Machado, M. C., Bellemare, M. G., and Bowling, M. (2018).
Count-<span>Based Exploration</span> with the <span>Successor
Representation</span>. Available at: <a href="http://arxiv.org/abs/1807.11622">http://arxiv.org/abs/1807.11622</a>
[Accessed February 23, 2019].
</div>
<div id="ref-Madeka2022" class="csl-entry" role="listitem">
Madeka, D., Torkkola, K., Eisenach, C., Luo, A., Foster, D. P., and
Kakade, S. M. (2022). Deep <span>Inventory Management</span>. doi:<a href="https://doi.org/10.48550/arXiv.2210.03137">10.48550/arXiv.2210.03137</a>.
</div>
<div id="ref-Malibari2023" class="csl-entry" role="listitem">
Malibari, N., Katib, I., and Mehmood, R. (2023). Systematic
<span>Review</span> on <span>Reinforcement Learning</span> in the
<span>Field</span> of <span>Fintech</span>. doi:<a href="https://doi.org/10.48550/arXiv.2305.07466">10.48550/arXiv.2305.07466</a>.
</div>
<div id="ref-Meuleau2000" class="csl-entry" role="listitem">
Meuleau, N., Peshkin, L., Kaelbling, L. P., and Kim, K. (2000).
Off-<span>Policy Policy Search</span>. Available at: <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894</a>.
</div>
<div id="ref-Mirowski2016" class="csl-entry" role="listitem">
Mirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A. J., Banino,
A., et al. (2016). Learning to <span>Navigate</span> in <span>Complex
Environments</span>. Available at: <a href="http://arxiv.org/abs/1611.03673">http://arxiv.org/abs/1611.03673</a>.
</div>
<div id="ref-Mnih2016" class="csl-entry" role="listitem">
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley,
T., et al. (2016). Asynchronous <span>Methods</span> for <span>Deep
Reinforcement Learning</span>. in <em>Proc. <span>ICML</span></em>
Available at: <a href="http://arxiv.org/abs/1602.01783">http://arxiv.org/abs/1602.01783</a>.
</div>
<div id="ref-Mnih2013" class="csl-entry" role="listitem">
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I.,
Wierstra, D., et al. (2013). Playing <span>Atari</span> with <span>Deep
Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1312.5602">http://arxiv.org/abs/1312.5602</a>.
</div>
<div id="ref-Mnih2015" class="csl-entry" role="listitem">
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J.,
Bellemare, M. G., et al. (2015). Human-level control through deep
reinforcement learning. <em>Nature</em> 518, 529–533. doi:<a href="https://doi.org/10.1038/nature14236">10.1038/nature14236</a>.
</div>
<div id="ref-Munos2016" class="csl-entry" role="listitem">
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016).
Safe and <span>Efficient Off-Policy Reinforcement Learning</span>.
Available at: <a href="http://arxiv.org/abs/1606.02647">http://arxiv.org/abs/1606.02647</a>.
</div>
<div id="ref-Nachum2017" class="csl-entry" role="listitem">
Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017). Bridging the
<span>Gap Between Value</span> and <span>Policy Based Reinforcement
Learning</span>. Available at: <a href="http://arxiv.org/abs/1702.08892">http://arxiv.org/abs/1702.08892</a>
[Accessed June 12, 2019].
</div>
<div id="ref-Nair2015" class="csl-entry" role="listitem">
Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De
Maria, A., et al. (2015). Massively <span>Parallel Methods</span> for
<span>Deep Reinforcement Learning</span>. Available at: <a href="https://arxiv.org/pdf/1507.04296.pdf">https://arxiv.org/pdf/1507.04296.pdf</a>.
</div>
<div id="ref-Nielsen2015" class="csl-entry" role="listitem">
Nielsen, M. A. (2015). <em>Neural <span>Networks</span> and <span>Deep
Learning</span></em>. Determination Press Available at: <a href="http://neuralnetworksanddeeplearning.com/">http://neuralnetworksanddeeplearning.com/</a>.
</div>
<div id="ref-Niu2011" class="csl-entry" role="listitem">
Niu, F., Recht, B., Re, C., and Wright, S. J. (2011).
<span>HOGWILD</span>!: <span>A Lock-Free Approach</span> to
<span>Parallelizing Stochastic Gradient Descent</span>. in <em>Proc.
<span>Advances</span> in <span>Neural Information Processing
Systems</span></em>, 21–21. Available at: <a href="http://arxiv.org/abs/1106.5730">http://arxiv.org/abs/1106.5730</a>.
</div>
<div id="ref-ODonoghue2016" class="csl-entry" role="listitem">
O’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2016).
Combining policy gradient and <span class="nocase">Q-learning</span>.
Available at: <a href="http://arxiv.org/abs/1611.01626">http://arxiv.org/abs/1611.01626</a>
[Accessed February 13, 2019].
</div>
<div id="ref-Oh2018" class="csl-entry" role="listitem">
Oh, J., Guo, Y., Singh, S., and Lee, H. (2018). Self-<span>Imitation
Learning</span>. Available at: <a href="http://arxiv.org/abs/1806.05635">http://arxiv.org/abs/1806.05635</a>.
</div>
<div id="ref-Peshkin2002" class="csl-entry" role="listitem">
Peshkin, L., and Shelton, C. R. (2002). Learning from <span>Scarce
Experience</span>. Available at: <a href="http://arxiv.org/abs/cs/0204043">http://arxiv.org/abs/cs/0204043</a>.
</div>
<div id="ref-Peters2008" class="csl-entry" role="listitem">
Peters, J., and Schaal, S. (2008). Reinforcement learning of motor
skills with policy gradients. <em>Neural Networks</em> 21, 682–697.
doi:<a href="https://doi.org/10.1016/j.neunet.2008.02.003">10.1016/j.neunet.2008.02.003</a>.
</div>
<div id="ref-Popov2017" class="csl-entry" role="listitem">
Popov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G.,
Vecerik, M., et al. (2017). Data-efficient <span>Deep Reinforcement
Learning</span> for <span>Dexterous Manipulation</span>. Available at:
<a href="http://arxiv.org/abs/1704.03073">http://arxiv.org/abs/1704.03073</a>.
</div>
<div id="ref-Precup2000" class="csl-entry" role="listitem">
Precup, D., Sutton, R. S., and Singh, S. (2000). Eligibility traces for
off-policy policy evaluation. in <em>Proceedings of the
<span>Seventeenth International Conference</span> on <span>Machine
Learning</span>.</em>
</div>
<div id="ref-Roy2022" class="csl-entry" role="listitem">
Roy, R., Raiman, J., Kant, N., Elkin, I., Kirby, R., Siu, M., et al.
(2022). <span>PrefixRL</span>: <span>Optimization</span> of
<span>Parallel Prefix Circuits</span> using <span>Deep Reinforcement
Learning</span>. doi:<a href="https://doi.org/10.1109/DAC18074.2021.9586094">10.1109/DAC18074.2021.9586094</a>.
</div>
<div id="ref-Ruder2016" class="csl-entry" role="listitem">
Ruder, S. (2016). An overview of gradient descent optimization
algorithms. Available at: <a href="http://arxiv.org/abs/1609.04747">http://arxiv.org/abs/1609.04747</a>.
</div>
<div id="ref-Salimans2017" class="csl-entry" role="listitem">
Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017).
Evolution <span>Strategies</span> as a <span>Scalable Alternative</span>
to <span>Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1703.03864">http://arxiv.org/abs/1703.03864</a>.
</div>
<div id="ref-Schaul2015" class="csl-entry" role="listitem">
Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized
<span>Experience Replay</span>. Available at: <a href="http://arxiv.org/abs/1511.05952">http://arxiv.org/abs/1511.05952</a>.
</div>
<div id="ref-Schulman2017" class="csl-entry" role="listitem">
Schulman, J., Chen, X., and Abbeel, P. (2017a). Equivalence
<span>Between Policy Gradients</span> and <span>Soft Q-Learning</span>.
Available at: <a href="http://arxiv.org/abs/1704.06440">http://arxiv.org/abs/1704.06440</a>
[Accessed June 12, 2019].
</div>
<div id="ref-Schulman2015a" class="csl-entry" role="listitem">
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
(2015a). Trust <span>Region Policy Optimization</span>. in
<em>Proceedings of the 31 st <span>International Conference</span> on
<span>Machine Learning</span></em>, 1889–1897. Available at: <a href="http://proceedings.mlr.press/v37/schulman15.html">http://proceedings.mlr.press/v37/schulman15.html</a>.
</div>
<div id="ref-Schulman2015" class="csl-entry" role="listitem">
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
(2015b). High-<span>Dimensional Continuous Control Using Generalized
Advantage Estimation</span>. Available at: <a href="http://arxiv.org/abs/1506.02438">http://arxiv.org/abs/1506.02438</a>.
</div>
<div id="ref-Schulman2017a" class="csl-entry" role="listitem">
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
(2017b). Proximal <span>Policy Optimization Algorithms</span>. Available
at: <a href="http://arxiv.org/abs/1707.06347">http://arxiv.org/abs/1707.06347</a>.
</div>
<div id="ref-Silver2016" class="csl-entry" role="listitem">
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den
Driessche, G., et al. (2016). Mastering the game of <span>Go</span> with
deep neural networks and tree search. <em>Nature</em> 529, 484–489.
doi:<a href="https://doi.org/10.1038/nature16961">10.1038/nature16961</a>.
</div>
<div id="ref-Silver2014" class="csl-entry" role="listitem">
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and
Riedmiller, M. (2014). Deterministic <span>Policy Gradient
Algorithms</span>. in <em>Proc. <span>ICML</span></em> Proceedings of
<span>Machine Learning Research</span>., eds. E. P. Xing and T. Jebara
(PMLR), 387–395. Available at: <a href="http://proceedings.mlr.press/v32/silver14.html">http://proceedings.mlr.press/v32/silver14.html</a>.
</div>
<div id="ref-Simonyan2015" class="csl-entry" role="listitem">
Simonyan, K., and Zisserman, A. (2015). Very <span>Deep Convolutional
Networks</span> for <span>Large-Scale Image Recognition</span>.
<em>International Conference on Learning Representations (ICRL)</em>,
1–14. doi:<a href="https://doi.org/10.1016/j.infsof.2008.09.005">10.1016/j.infsof.2008.09.005</a>.
</div>
<div id="ref-Sutton1998" class="csl-entry" role="listitem">
Sutton, R. S., and Barto, A. G. (1998). <em>Reinforcement
<span>Learning</span>: <span>An</span> introduction</em>. Cambridge, MA:
MIT press.
</div>
<div id="ref-Sutton2017" class="csl-entry" role="listitem">
Sutton, R. S., and Barto, A. G. (2017). <em>Reinforcement
<span>Learning</span>: <span>An Introduction</span></em>. 2nd ed.
Cambridge, MA: MIT Press Available at: <a href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a>.
</div>
<div id="ref-Sutton1999" class="csl-entry" role="listitem">
Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. (1999). Policy
gradient methods for reinforcement learning with function approximation.
in <em>Proceedings of the 12th <span>International Conference</span> on
<span>Neural Information Processing Systems</span></em> (MIT Press),
1057–1063. Available at: <a href="https://dl.acm.org/citation.cfm?id=3009806">https://dl.acm.org/citation.cfm?id=3009806</a>.
</div>
<div id="ref-Szita2006" class="csl-entry" role="listitem">
Szita, I., and Lörincz, A. (2006). Learning <span>Tetris Using</span>
the <span>Noisy Cross-Entropy Method</span>. <em>Neural Computation</em>
18, 2936–2941. doi:<a href="https://doi.org/10.1162/neco.2006.18.12.2936">10.1162/neco.2006.18.12.2936</a>.
</div>
<div id="ref-Tang2010" class="csl-entry" role="listitem">
Tang, J., and Abbeel, P. (2010). On a <span>Connection</span> between
<span>Importance Sampling</span> and the <span>Likelihood Ratio Policy
Gradient</span>. in <em>Adv. <span>Neural</span> inf.
<span>Process</span>. <span>Syst</span>.</em> Available at: <a href="http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf">http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf</a>.
</div>
<div id="ref-Todorov2008" class="csl-entry" role="listitem">
Todorov, E. (2008). General duality between optimal control and
estimation. in <em>2008 47th <span>IEEE Conference</span> on
<span>Decision</span> and <span>Control</span></em>, 4286–4292. doi:<a href="https://doi.org/10.1109/CDC.2008.4739438">10.1109/CDC.2008.4739438</a>.
</div>
<div id="ref-Toussaint2009" class="csl-entry" role="listitem">
Toussaint, M. (2009). Robot <span>Trajectory Optimization Using
Approximate Inference</span>. in <em>Proceedings of the 26th
<span>Annual International Conference</span> on <span>Machine
Learning</span></em> <span>ICML</span> ’09. (New York, NY, USA: ACM),
1049–1056. doi:<a href="https://doi.org/10.1145/1553374.1553508">10.1145/1553374.1553508</a>.
</div>
<div id="ref-Uhlenbeck1930" class="csl-entry" role="listitem">
Uhlenbeck, G. E., and Ornstein, L. S. (1930). On the <span>Theory</span>
of the <span>Brownian Motion</span>. <em>Physical Review</em> 36. doi:<a href="https://doi.org/10.1103/PhysRev.36.823">10.1103/PhysRev.36.823</a>.
</div>
<div id="ref-vanHasselt2010" class="csl-entry" role="listitem">
van Hasselt, H. (2010). Double <span class="nocase">Q-learning</span>.
in <em>Proceedings of the 23rd <span>International Conference</span> on
<span>Neural Information Processing Systems</span> - <span>Volume</span>
2</em> (Curran Associates Inc.), 2613–2621. Available at: <a href="https://dl.acm.org/citation.cfm?id=2997187">https://dl.acm.org/citation.cfm?id=2997187</a>.
</div>
<div id="ref-vanHasselt2015" class="csl-entry" role="listitem">
van Hasselt, H., Guez, A., and Silver, D. (2015). Deep
<span>Reinforcement Learning</span> with <span class="nocase">Double
Q-learning</span>. Available at: <a href="http://arxiv.org/abs/1509.06461">http://arxiv.org/abs/1509.06461</a>.
</div>
<div id="ref-Wang2017" class="csl-entry" role="listitem">
Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z.,
Munos, R., et al. (2017). Learning to reinforcement learn. Available at:
<a href="http://arxiv.org/abs/1611.05763">http://arxiv.org/abs/1611.05763</a>
[Accessed February 5, 2021].
</div>
<div id="ref-Wang2016" class="csl-entry" role="listitem">
Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de
Freitas, N. (2016). Dueling <span>Network Architectures</span> for
<span>Deep Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1511.06581">http://arxiv.org/abs/1511.06581</a>
[Accessed November 21, 2019].
</div>
<div id="ref-Watkins1989" class="csl-entry" role="listitem">
Watkins, C. J. (1989). Learning from delayed rewards.
</div>
<div id="ref-Wierstra2007" class="csl-entry" role="listitem">
Wierstra, D., Foerster, A., Peters, J., and Schmidhuber, J. (2007).
<span>“Solving <span>Deep Memory POMDPs</span> with <span>Recurrent
Policy Gradients</span>,”</span> in (Springer, Berlin, Heidelberg),
697–706. doi:<a href="https://doi.org/10.1007/978-3-540-74690-4_71">10.1007/978-3-540-74690-4_71</a>.
</div>
<div id="ref-Williams1992" class="csl-entry" role="listitem">
Williams, R. J. (1992). Simple statistical gradient-following algorithms
for connectionist reinforcement learning. <em>Machine Learning</em> 8,
229–256.
</div>
<div id="ref-Williams1991" class="csl-entry" role="listitem">
Williams, R. J., and Peng, J. (1991). Function optimization using
connectionist reinforcement learning algorithms. <em>Connection
Science</em> 3, 241–268.
</div>
<div id="ref-Yu2020b" class="csl-entry" role="listitem">
Yu, C., Liu, J., and Nemati, S. (2020). Reinforcement
<span>Learning</span> in <span>Healthcare</span>: <span>A Survey</span>.
doi:<a href="https://doi.org/10.48550/arXiv.1908.08796">10.48550/arXiv.1908.08796</a>.
</div>
<div id="ref-Ziebart2008" class="csl-entry" role="listitem">
Ziebart, B. D., Maas, A., Bagnell, J. A., and Dey, A. K. (2008). Maximum
<span>Entropy Inverse Reinforcement Learning</span>. in, 6.
</div>
</div>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../src/3.8-OtherPolicyGradient.html" class="pagination-link" aria-label="Miscellaneous model-free algorithm">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Miscellaneous model-free algorithm</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0">Creative Commons BY-NC-SA 4.0</a>. Author <a href="mailto:julien.vitay@gmail.com">Julien Vitay</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>