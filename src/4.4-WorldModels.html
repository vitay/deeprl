<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>World models, Dreamer – Deep Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/4.5-AlphaGo.html" rel="next">
<link href="../src/4.3-Planning.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="../site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title"><span class="chapter-title">World models, Dreamer</span></h1>
        </a>     
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Reinforcement Learning</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/vitay/deeprl" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/0-Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>Tabular RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/1.1-Bandits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Sampling and Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/1.2-MDP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Markov Decision Process</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/1.4-MC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte Carlo methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/1.5-TD.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Temporal Difference learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>Value-based deep RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.1-FunctionApproximation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function approximation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.2-DeepNetworks.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.3-DQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Q-network (DQN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.4-DQNvariants.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">DQN variants (Rainbow)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.5-DistributedLearning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Distributed learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/2.6-Misc.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Misc.</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>Policy-gradient methods</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.1-PolicyGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy Gradient methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.2-ActorCritic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advantage Actor-Critic (A3C)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.3-ImportanceSampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Off-policy Actor-Critic</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.4-DPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Deterministic Policy Gradient (DDPG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.5-NaturalGradient.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural gradients</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.6-PPO.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy optimization (TRPO, PPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.7-ACER.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Actor-Critic with Experience Replay (ACER)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.8-EntropyRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Entropy RL (SAC)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/3.9-Misc.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Misc.</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>Model-based deep RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/4.1-ModelBased.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/4.2-MBMF.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model-based-augmented model-free RL (Dyna-Q, I2A)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/4.3-Planning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Planning (MPC, TDM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/4.4-WorldModels.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">World models, Dreamer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/4.5-AlphaGo.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AlphaGo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/4.6-Misc.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Misc.</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>Advanced topics</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/5.1-Intrinsic.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Intrinsic motivation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/5.2-Inverse.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Inverse Reinforcement Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/5.3-OfflineRL.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Offline RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/5.4-Meta.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Meta learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/5.5-Hierarchical.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Hierarchical Reinforcement Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#world-models" id="toc-world-models" class="nav-link active" data-scroll-target="#world-models">World models</a>
  <ul class="collapse">
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture">Architecture</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  </ul></li>
  <li><a href="#deep-planning-network---planet" id="toc-deep-planning-network---planet" class="nav-link" data-scroll-target="#deep-planning-network---planet">Deep Planning Network - PlaNet</a></li>
  <li><a href="#dreamer" id="toc-dreamer" class="nav-link" data-scroll-target="#dreamer">Dreamer</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">World models, Dreamer</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="world-models" class="level2">
<h2 class="anchored" data-anchor-id="world-models">World models</h2>
<p>The core idea of <strong>world models</strong> <span class="citation" data-cites="Ha2018">(<a href="references.html#ref-Ha2018" role="doc-biblioref">Ha and Schmidhuber, 2018</a>)</span> is to explicitly separate the <strong>world model</strong> (what will happen next) from the <strong>controller</strong> (how to act). The neural networks used in deep RL are usually small, as rewards do not contain enough information to train huge networks. However, unsupervised data (without any label nor reward) is plenty and could be leveraged to learn useful representations. A huge <strong>world model</strong> can be efficiently trained by self-supervised / unsupervised methods, while a small <strong>controller</strong> should not need too many trials if its input representations are good.</p>
<p><span class="citation" data-cites="Ha2018">Ha and Schmidhuber (<a href="references.html#ref-Ha2018" role="doc-biblioref">2018</a>)</span> used the Vizdoom Take Cover environment (<a href="http://vizdoom.cs.put.edu.pl/" class="uri">http://vizdoom.cs.put.edu.pl/</a>) to demonstrate the power of world models, as well as a car racing environment.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>For a detailed explanation of world models, refer to:</p>
<p><a href="https://worldmodels.github.io/" class="uri">https://worldmodels.github.io/</a></p>
<p>The videos embedded here come from this page.</p>
</div>
</div>
<section id="architecture" class="level3">
<h3 class="anchored" data-anchor-id="architecture">Architecture</h3>
<p>The architecture of World Models is composed of three modules trained in succession:</p>
<ol type="1">
<li>The Vision module <span class="math inline">V</span>,</li>
<li>The Memory module <span class="math inline">M</span>,</li>
<li>The Controller module <span class="math inline">C</span>.</li>
</ol>
<div id="fig-wmarchitecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wmarchitecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/wm-overview.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;23.1: Architecture of world models. Source: https://worldmodels.github.io/"><img src="img/wm-overview.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wmarchitecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.1: Architecture of world models. Source: <a href="https://worldmodels.github.io/" class="uri">https://worldmodels.github.io/</a>
</figcaption>
</figure>
</div>
<p><strong>Vision module</strong></p>
<p>The vision module <span class="math inline">V</span> is the encoder of a <strong>variational autoencoder</strong> (VAE), trained on single frames of the game (obtained using a random policy). The resulting latent vector <span class="math inline">\mathbf{z}_t</span> contains a compressed representation of the frame <span class="math inline">\mathbf{o}_t</span>.</p>
<div id="fig-wmvision" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wmvision-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/wm-vae.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;23.2: Vision module. Source: https://worldmodels.github.io/"><img src="img/wm-vae.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wmvision-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.2: Vision module. Source: <a href="https://worldmodels.github.io/" class="uri">https://worldmodels.github.io/</a>
</figcaption>
</figure>
</div>
<p><strong>Memory module</strong></p>
<p>The sequence of latent representations <span class="math inline">\mathbf{z}_0, \ldots \mathbf{z}_t</span> in a game is fed to a LSTM layer (RNN) together with the actions <span class="math inline">a_t</span> to compress what happens over time.</p>
<p>A <strong>Mixture Density Network</strong> (MDN, <span class="citation" data-cites="Bishop1994">Bishop (<a href="references.html#ref-Bishop1994" role="doc-biblioref">1994</a>)</span>) is used to predict the <strong>distribution</strong> of the next latent representations <span class="math inline">P(\mathbf{z}_{t+1} | a_t, \mathbf{h}_t, \ldots \mathbf{z}_t)</span>. In short, MDN allows to perform probabilistic regression, but predicting both the mean and the variance of the data, instead of just its mean as in vanilla least sqaures regression. Most MDN methods use a mixture of Gaussian distributions to model the target distribution.</p>
<div id="fig-wmmemory" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wmmemory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/wm-mdn_rnn.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;23.3: Memory module. Source: https://worldmodels.github.io/"><img src="img/wm-mdn_rnn.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wmmemory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.3: Memory module. Source: <a href="https://worldmodels.github.io/" class="uri">https://worldmodels.github.io/</a>
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
RNN-MDN
</div>
</div>
<div class="callout-body-container callout-body">
<p>The RNN-MDN architecture has been used successfully in the past for sequence generation problems such as generating handwriting and sketches (Sketch-RNN, <span class="citation" data-cites="Ha2017">Ha and Eck (<a href="references.html#ref-Ha2017" role="doc-biblioref">2017</a>)</span>).</p>
<p>Check a demo here: <a href="https://magenta.tensorflow.org/sketch-rnn-demo" class="uri">https://magenta.tensorflow.org/sketch-rnn-demo</a></p>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="https://worldmodels.github.io/assets/mp4/sketch_rnn_insect.mp4"></video></div>
</div>
</div>
<p><strong>Controller module</strong></p>
<p>The last step is the <strong>controller</strong>. It takes a latent representation <span class="math inline">\mathbf{z}_t</span> and the current hidden state of the LSTM <span class="math inline">\mathbf{h}_t</span> as inputs and selects an action <strong>linearly</strong>:</p>
<p><span class="math display">a_t = \text{tanh}(W \, [\mathbf{z}_t, \mathbf{h}_t ] + b)</span></p>
<p>A RL actor cannot get simpler as that…</p>
<div id="fig-wmcontroller" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wmcontroller-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/wm-schematic.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;23.4: Controller module. Source: https://worldmodels.github.io/"><img src="img/wm-schematic.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wmcontroller-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.4: Controller module. Source: <a href="https://worldmodels.github.io/" class="uri">https://worldmodels.github.io/</a>
</figcaption>
</figure>
</div>
<p>The controller is not even trained with RL: it uses a genetic algorithm, the Covariance-Matrix Adaptation Evolution Strategy (CMA-ES, <span class="citation" data-cites="Hansen2001">Hansen and Ostermeier (<a href="references.html#ref-Hansen2001" role="doc-biblioref">2001</a>)</span>), to find the output weights that maximize the returns. The world model is trained by classical self-supervised learning using a random agent before learning, while the controller is simply evolved using a black-box optimizer.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
World models
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Algorithm:</strong></p>
<ol type="1">
<li><p>Collect 10,000 rollouts from a random policy.</p></li>
<li><p>Train VAE (V) to encode each frame into a latent vector <span class="math inline">\mathbf{z} \in \mathcal{R}^{32}</span>.</p></li>
<li><p>Train MDN-RNN (M) to model <span class="math inline">P(\mathbf{z}_{t+1} | a_t, \mathbf{h}_t, \ldots \mathbf{z}_t)</span>.</p></li>
<li><p>Evolve Controller (C) to maximize the expected cumulative reward of a rollout.</p></li>
</ol>
</div>
</div>
<p>For the car racing environment, the repartition of the number of weights clearly shows that the complexity of the model lies in the world model, not the controller:</p>
<p><strong>Parameters for car racing:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Parameter Count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">VAE</td>
<td style="text-align: left;">4,348,547</td>
</tr>
<tr class="even">
<td style="text-align: left;">MDN-RNN</td>
<td style="text-align: left;">422,368</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Controller</td>
<td style="text-align: left;">867</td>
</tr>
</tbody>
</table>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<p>Performance in car racing:</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video2" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title="World-models - Performance"><source src="https://worldmodels.github.io/assets/mp4/carracing_mistake_short.mp4"></video></div>
<p>Below is the input of the VAE and the reconstruction. The reconstruction does not have to be perfect as long as the latent space is informative.</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video3" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title="World-models - VAE"><source src="https://worldmodels.github.io/assets/mp4/carracing_vae_compare.mp4"></video></div>
<p>Having access to a full rollout of the future leads to more stable driving:</p>
<div id="fig-wmresults" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wmresults-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/wm-results.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;23.5: Performance of World models on the car racing environemnt. Source: https://worldmodels.github.io/"><img src="img/wm-results.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wmresults-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.5: Performance of World models on the car racing environemnt. Source: <a href="https://worldmodels.github.io/" class="uri">https://worldmodels.github.io/</a>
</figcaption>
</figure>
</div>
<p>In summary, the <strong>world model</strong> V+M is learned <strong>offline</strong> with a random agent, using self-supervised learning, while the <strong>controller</strong> C has few weights (1000) and can be trained by evolutionary algorithms, not even RL. The network can even learn by playing entirely in its <strong>own imagination</strong>, as the world model can be applied on itself and predict all future frames. It just needs to additionally predict the reward. After that, the learned policy can be transferred to the real environment.</p>
</section>
</section>
<section id="deep-planning-network---planet" class="level2">
<h2 class="anchored" data-anchor-id="deep-planning-network---planet">Deep Planning Network - PlaNet</h2>
<p>PlaNet <span class="citation" data-cites="Hafner2019">(<a href="references.html#ref-Hafner2019" role="doc-biblioref">Hafner et al., 2019</a>)</span> extends the idea of World models by learning the model together with the policy (<strong>end-to-end</strong>). It learns a <strong>latent dynamics model</strong> that takes the past observations <span class="math inline">o_t</span> into account (needed for POMDPs):</p>
<p><span class="math display">s_{t}, r_{t+1}, \hat{o}_t = f(o_t, a_t, s_{t-1})</span></p>
<p>and plans in the latent space using multiple rollouts:</p>
<p><span class="math display">a_t = \text{arg}\max_a \mathbb{E}[R(s_t, a, s_{t+1}, \ldots)]</span></p>
<p><strong>Training</strong></p>
<div id="fig-planet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-planet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/planet-model.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;23.6: Latent dynamics model of PlaNet. Source: https://planetrl.github.io/"><img src="img/planet-model.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-planet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.6: Latent dynamics model of PlaNet. Source: <a href="https://planetrl.github.io/" class="uri">https://planetrl.github.io/</a>
</figcaption>
</figure>
</div>
<p>The latent dynamics model is a sequential variational autoencoder learning concurrently:</p>
<ol type="1">
<li>An <strong>encoder</strong> from the observation <span class="math inline">o_t</span> to the latent space <span class="math inline">s_t</span>.</li>
</ol>
<p><span class="math display">q(s_t | o_t)</span></p>
<ol start="2" type="1">
<li>A <strong>decoder</strong> from the latent space to the reconstructed observation <span class="math inline">\hat{o}_t</span>.</li>
</ol>
<p><span class="math display">p(\hat{o}_t | s_t)</span></p>
<ol start="3" type="1">
<li>A <strong>transition model</strong> to predict the next latent representation given an action.</li>
</ol>
<p><span class="math display">p(s_{t+1} | s_t, a_t)</span></p>
<ol start="4" type="1">
<li>A <strong>reward model</strong> predicting the immediate reward.</li>
</ol>
<p><span class="math display">p(r_t | s_t)</span></p>
<p>Training sequences <span class="math inline">(o_1, a_1, o_2, \ldots, o_T)</span> can be generated <strong>off-policy</strong> (e.g.&nbsp;from demonstrations) or on-policy. The loss function to train this <strong>recurrent state-space model</strong> (RSSM), which has a stochastic component in the encoder (VAE), and has to compensate for latent overshooting (i.e.&nbsp;to enforce consistency between one-step and multi-step predictions in the latent space), is slightly complicated and is not explained here.</p>
<div id="fig-planet-training" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-planet-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/dreamer-model.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;23.7: Training the latent dynamics model of PlaNet. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html"><img src="img/dreamer-model.gif" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-planet-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.7: Training the latent dynamics model of PlaNet. Source: <a href="https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html" class="uri">https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html</a>
</figcaption>
</figure>
</div>
<p><strong>Inference</strong></p>
<p>From a single observation <span class="math inline">o_t</span> encoded into <span class="math inline">s_t</span>, we can generate 10000 rollouts using <strong>random sampling</strong>. In these rollouts, the action sequences are varied randomly, generating as many random sequences as needed. The return of each rollout can be estimated using the reward model. A belief over the action sequences is updated using the <strong>cross-entropy method</strong> (CEM, <span class="citation" data-cites="Szita2006">Szita and Lörincz (<a href="references.html#ref-Szita2006" role="doc-biblioref">2006</a>)</span>) in order to restrict the search.</p>
<p>After the 10000 rollouts are executed (in imagination), the sequence with the highest return is selected and its first action is executed. At the next time step, planning starts from scratch: this is the key idea of Model Predictive Control. There is no actor in PlaNet, only a transition model used for planning. The reason PlaNet works is that planning is done in the latent space, hich has a much lower dimensionality than the observations (e.g.&nbsp;images).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/planet-planning.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Source: https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html"><img src="img/planet-planning.png" class="img-fluid figure-img" alt="Source: https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html"></a></p>
<figcaption>Source: <a href="https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html" class="uri">https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html</a></figcaption>
</figure>
</div>
<p><strong>Results</strong></p>
<p>Planet learns continuous Mujoco image-based control problems in 2000 episodes, where D4PG needs 50 times more.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/tZk1eof_VNA" title="PlaNet" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>The latent dynamics model can learn 6 control tasks <strong>at the same time</strong>. As there is no actor, but only a planner, the same network can control all agents!</p>
<div id="fig-planetresults" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-planetresults-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/planet-results.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;23.8: Top row: agent behavior on the 6 Mujoco tasks. Bottom row: predcited frames by the agent. Source: https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html"><img src="img/planet-results.gif" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-planetresults-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.8: Top row: agent behavior on the 6 Mujoco tasks. Bottom row: predcited frames by the agent. Source: <a href="https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html" class="uri">https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html</a>
</figcaption>
</figure>
</div>
</section>
<section id="dreamer" class="level2">
<h2 class="anchored" data-anchor-id="dreamer">Dreamer</h2>
<p>Dreamer <span class="citation" data-cites="Hafner2020">(<a href="references.html#ref-Hafner2020" role="doc-biblioref">Hafner et al., 2020</a>)</span> extends the idea of PlaNet by additionally <strong>training an actor</strong> instead of using a MPC planner. The latent dynamics model is the same RSSM architecture. Training a “model-free” actor on imaginary rollouts instead of MPC planning should reduce the computational cost at inference time.</p>
<div id="fig-dreamer-principle" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dreamer-principle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/dreamer-principle.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;23.9: Dreamer first learns the World model (RSSM), then trains a model-free agent in its imagination to maximize the rewards, before being used to interact with the environment. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html"><img src="img/dreamer-principle.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dreamer-principle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.9: Dreamer first learns the World model (RSSM), then trains a model-free agent in its imagination to maximize the rewards, before being used to interact with the environment. Source: <a href="https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html" class="uri">https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html</a>
</figcaption>
</figure>
</div>
<p>The latent dynamics model is the same as in PlaNet, learning from past experiences.</p>
<div id="fig-dreamer-rssm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dreamer-rssm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/dreamer-model.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;23.10: Latent dynamics model of Dreamer, exactly the same as PlaNet. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html"><img src="img/dreamer-model.gif" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dreamer-rssm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.10: Latent dynamics model of Dreamer, exactly the same as PlaNet. Source: <a href="https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html" class="uri">https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html</a>
</figcaption>
</figure>
</div>
<p>The behavior module learns to predict the value of a state <span class="math inline">V_\varphi(s)</span> and the policy <span class="math inline">\pi_\theta(s)</span> (actor-critic). It is trained <strong>in imagination</strong> in the latent space using the reward model for the immediate rewards (to compute returns) and the transition model for the next states.</p>
<div id="fig-dreamer-mf" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dreamer-mf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/dreamer-actor.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;23.11: The actor-critic agent is trained in imagination using rollouts generated by the RSSM model. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html"><img src="img/dreamer-actor.gif" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dreamer-mf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.11: The actor-critic agent is trained in imagination using rollouts generated by the RSSM model. Source: <a href="https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html" class="uri">https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html</a>
</figcaption>
</figure>
</div>
<p>The current observation <span class="math inline">o_t</span> is encoded into a state <span class="math inline">s_t</span>, the actor selects an action <span class="math inline">a_t</span>, the transition model predicts <span class="math inline">s_{t+1}</span>, the reward model predicts <span class="math inline">r_{t+1}</span>, the critic predicts <span class="math inline">V_\varphi(s_t)</span>. At the end of the sequence, we apply <strong>backpropagation-through-time</strong> to train the actor and the critic.</p>
<p>The <strong>critic</strong> <span class="math inline">V_\varphi(s_t)</span> is trained on the imaginary sequence <span class="math inline">(s_t, a_t, r_{t+1}, s_{t+1}, \ldots, s_T)</span> to minimize the prediction error with the <span class="math inline">\lambda</span>-return:</p>
<p><span class="math display">R^\lambda_t = (1  - \lambda) \, \sum_{n=1}^{T-t-1} \lambda^{n-1} \, R^n_t + \lambda^{T-t-1} \, R_t</span></p>
<p>The <strong>actor</strong> <span class="math inline">\pi_\theta(s_t, a_t)</span> is trained on the sequence to maximize the sum of the value of the future states:</p>
<p><span class="math display">\mathcal{J}(\theta) = \mathbb{E}_{s_t, a_t \sim \pi_\theta} [\sum_{t'=t}^T V_\varphi(s_{t'})]</span></p>
<p>The main advantage of training an actor is that we need only one rollout when training it: backpropagation maximizes the expected returns. When acting, we just need to encode the history of the episode in the latent space, and the actor becomes model-free!</p>
<div id="fig-dreamer-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dreamer-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/dreamer-architecture.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;23.12: Complete Dreamer architecture. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html"><img src="img/dreamer-architecture.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dreamer-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.12: Complete Dreamer architecture. Source: <a href="https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html" class="uri">https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html</a>
</figcaption>
</figure>
</div>
<p>Dreamer beats model-free and model-based methods on 20 continuous control tasks.</p>
<p><a href="img/dreamer-results.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="img/dreamer-results.gif" class="img-fluid"></a></p>
<div id="fig-dreamer-results" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dreamer-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/dreamer-results.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;23.13: Results of Dreamer on various Mujoco tasks, compared to SotA control methods. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html"><img src="img/dreamer-results.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dreamer-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.13: Results of Dreamer on various Mujoco tasks, compared to SotA control methods. Source: <a href="https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html" class="uri">https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html</a>
</figcaption>
</figure>
</div>
<p>It also learns Atari and Deepmind lab video games, sometimes on par with Rainbow or IMPALA!</p>
<p><a href="img/dreamer-resultsatari.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img src="img/dreamer-resultsatari.gif" class="img-fluid"></a></p>
<div id="fig-dreamer-atari" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dreamer-atari-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/dreamer-resultsatari.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Figure&nbsp;23.14: Results of Dreamer on Atari gameSource: https://dreamrl.github.io/"><img src="img/dreamer-resultsatari.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dreamer-atari-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23.14: Results of Dreamer on Atari gameSource: <a href="https://dreamrl.github.io/" class="uri">https://dreamrl.github.io/</a>
</figcaption>
</figure>
</div>
<p>A recent extension of Dreamer, DayDreamer <span class="citation" data-cites="Wu2022">(<a href="references.html#ref-Wu2022" role="doc-biblioref">Wu et al., 2022</a>)</span>, allows physical robots to learn complex tasks in a few hours.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/daydreamer.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="DayDreamer. Source: @Wu2022 and https://danijar.com/daydreamer"><img src="img/daydreamer.png" class="img-fluid figure-img" alt="DayDreamer. Source: Wu et al. (2022) and https://danijar.com/daydreamer"></a></p>
<figcaption>DayDreamer. Source: <span class="citation" data-cites="Wu2022">Wu et al. (<a href="references.html#ref-Wu2022" role="doc-biblioref">2022</a>)</span> and <a href="https://danijar.com/daydreamer" class="uri">https://danijar.com/daydreamer</a></figcaption>
</figure>
</div>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/xAXvfVTgqr0" title="DayDreamer" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-Bishop1994" class="csl-entry" role="listitem">
Bishop, C. M. (1994). Mixture <span>Density Networks</span>. Birmingham, UK: Neural Computing Research Group, Aston University Available at: <a href="https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf">https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf</a> [Accessed November 12, 2024].
</div>
<div id="ref-Ha2017" class="csl-entry" role="listitem">
Ha, D., and Eck, D. (2017). A <span>Neural Representation</span> of <span>Sketch Drawings</span>. Available at: <a href="http://arxiv.org/abs/1704.03477">http://arxiv.org/abs/1704.03477</a> [Accessed January 17, 2021].
</div>
<div id="ref-Ha2018" class="csl-entry" role="listitem">
Ha, D., and Schmidhuber, J. (2018). World <span>Models</span>. doi:<a href="https://doi.org/10.5281/zenodo.1207631">10.5281/zenodo.1207631</a>.
</div>
<div id="ref-Hafner2020" class="csl-entry" role="listitem">
Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2020). Dream to <span>Control</span>: <span>Learning Behaviors</span> by <span>Latent Imagination</span>. Available at: <a href="http://arxiv.org/abs/1912.01603">http://arxiv.org/abs/1912.01603</a> [Accessed March 24, 2020].
</div>
<div id="ref-Hafner2019" class="csl-entry" role="listitem">
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., et al. (2019). Learning <span>Latent Dynamics</span> for <span>Planning</span> from <span>Pixels</span>. Available at: <a href="http://arxiv.org/abs/1811.04551">http://arxiv.org/abs/1811.04551</a> [Accessed January 24, 2020].
</div>
<div id="ref-Hansen2001" class="csl-entry" role="listitem">
Hansen, N., and Ostermeier, A. (2001). Completely <span>Derandomized Self-Adaptation</span> in <span>Evolution Strategies</span>. <em>Evolutionary Computation</em> 9, 159–195. doi:<a href="https://doi.org/10.1162/106365601750190398">10.1162/106365601750190398</a>.
</div>
<div id="ref-Szita2006" class="csl-entry" role="listitem">
Szita, I., and Lörincz, A. (2006). Learning <span>Tetris Using</span> the <span>Noisy Cross-Entropy Method</span>. <em>Neural Computation</em> 18, 2936–2941. doi:<a href="https://doi.org/10.1162/neco.2006.18.12.2936">10.1162/neco.2006.18.12.2936</a>.
</div>
<div id="ref-Wu2022" class="csl-entry" role="listitem">
Wu, P., Escontrela, A., Hafner, D., Goldberg, K., and Abbeel, P. (2022). <span>DayDreamer</span>: <span>World Models</span> for <span>Physical Robot Learning</span>. doi:<a href="https://doi.org/10.48550/arXiv.2206.14176">10.48550/arXiv.2206.14176</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../src/4.3-Planning.html" class="pagination-link" aria-label="Planning (MPC, TDM)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Planning (MPC, TDM)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/4.5-AlphaGo.html" class="pagination-link" aria-label="AlphaGo">
        <span class="nav-page-text"><span class="chapter-title">AlphaGo</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0">Creative Commons BY-NC-SA 4.0</a>. Author <a href="mailto:julien.vitay@gmail.com">Julien Vitay</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>videojs(video_shortcode_videojs_video1);</script>
<script>videojs(video_shortcode_videojs_video2);</script>
<script>videojs(video_shortcode_videojs_video3);</script>
<script>var lightboxQuarto = GLightbox({"selector":".lightbox","descPosition":"bottom","loop":false,"openEffect":"zoom","closeEffect":"zoom"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>