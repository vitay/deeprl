\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\newlabel{overview}{{}{3}{Overview}{chapter*.2}{}}
\@writefile{toc}{\contentsline {chapter}{Overview}{3}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Basics}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{basics}{{1}{5}{Basics}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Reinforcement learning and Markov Decision Process}{5}{section.1.1}\protected@file@percent }
\newlabel{reinforcement-learning-and-markov-decision-process}{{1.1}{5}{Reinforcement learning and Markov Decision Process}{section.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Interaction between an agent and its environment. Taken from Sutton and Barto (1998).\relax }}{5}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig-agentenv}{{1.1}{5}{Interaction between an agent and its environment. Taken from Sutton and Barto (1998).\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Policy and value functions}{6}{subsection.1.1.1}\protected@file@percent }
\newlabel{policy-and-value-functions}{{1.1.1}{6}{Policy and value functions}{subsection.1.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Bellman equations}{7}{subsection.1.1.2}\protected@file@percent }
\newlabel{bellman-equations}{{1.1.2}{7}{Bellman equations}{subsection.1.1.2}{}}
\newlabel{eq-v-value}{{1.1}{7}{Bellman equations}{equation.1.1.1}{}}
\newlabel{eq-return}{{1.2}{8}{Bellman equations}{equation.1.1.2}{}}
\newlabel{eq-q-value}{{1.3}{8}{Bellman equations}{equation.1.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Backup diagrams corresponding to the Bellman equations. Taken from Sutton and Barto (1998).\relax }}{8}{figure.caption.4}\protected@file@percent }
\newlabel{fig-backup}{{1.2}{8}{Backup diagrams corresponding to the Bellman equations. Taken from Sutton and Barto (1998).\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Dynamic programming}{9}{subsection.1.1.3}\protected@file@percent }
\newlabel{dynamic-programming}{{1.1.3}{9}{Dynamic programming}{subsection.1.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Dynamic programming alternates between policy evaluation and policy improvement. Taken from Sutton and Barto (1998).\relax }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig-dynamicprogramming}{{1.3}{9}{Dynamic programming alternates between policy evaluation and policy improvement. Taken from Sutton and Barto (1998).\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Monte-Carlo sampling}{10}{subsection.1.1.4}\protected@file@percent }
\newlabel{monte-carlo-sampling}{{1.1.4}{10}{Monte-Carlo sampling}{subsection.1.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Monte-Carlo methods accumulate rewards over a complete episode. Taken from Sutton and Barto (1998).\relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{fig-mc}{{1.4}{10}{Monte-Carlo methods accumulate rewards over a complete episode. Taken from Sutton and Barto (1998).\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}Temporal Difference}{12}{subsection.1.1.5}\protected@file@percent }
\newlabel{temporal-difference}{{1.1.5}{12}{Temporal Difference}{subsection.1.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Temporal difference algorithms update values after a single transition. Taken from Sutton and Barto (1998).\relax }}{13}{figure.caption.7}\protected@file@percent }
\newlabel{fig-td}{{1.5}{13}{Temporal difference algorithms update values after a single transition. Taken from Sutton and Barto (1998).\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.6}Eligibility traces}{14}{subsection.1.1.6}\protected@file@percent }
\newlabel{eligibility-traces}{{1.1.6}{14}{Eligibility traces}{subsection.1.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Principle of eligibility traces applied to the Gridworld problem using SARSA(\(\mitlambda \)). Taken from Sutton and Barto (1998).\relax }}{15}{figure.caption.8}\protected@file@percent }
\newlabel{fig-eligibilitytraces}{{1.6}{15}{Principle of eligibility traces applied to the Gridworld problem using SARSA(\(\lambda \)). Taken from Sutton and Barto (1998).\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.7}Actor-critic architectures}{16}{subsection.1.1.7}\protected@file@percent }
\newlabel{actor-critic-architectures}{{1.1.7}{16}{Actor-critic architectures}{subsection.1.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Actor-critic architecture (Sutton and Barto, 1998).\relax }}{16}{figure.caption.9}\protected@file@percent }
\newlabel{fig-actorcritic}{{1.7}{16}{Actor-critic architecture (Sutton and Barto, 1998).\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.8}Function approximation}{17}{subsection.1.1.8}\protected@file@percent }
\newlabel{function-approximation}{{1.1.8}{17}{Function approximation}{subsection.1.1.8}{}}
\newlabel{value-based-function-approximation}{{1.1.8}{18}{Value-based function approximation}{section*.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Value-based function approximation}{18}{section*.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Function approximators can either take a state-action pair as input and output the Q-value, or simply take a state as input and output the Q-values of all possible actions.\relax }}{18}{figure.caption.11}\protected@file@percent }
\newlabel{fig-functionapprox}{{1.8}{18}{Function approximators can either take a state-action pair as input and output the Q-value, or simply take a state as input and output the Q-values of all possible actions.\relax }{figure.caption.11}{}}
\newlabel{policy-based-function-approximation}{{1.1.8}{19}{Policy-based function approximation}{section*.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Policy-based function approximation}{19}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Deep learning}{20}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{deep-learning}{{2}{20}{Deep learning}{chapter.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.0.1}Deep neural networks}{20}{subsection.2.0.1}\protected@file@percent }
\newlabel{deep-neural-networks}{{2.0.1}{20}{Deep neural networks}{subsection.2.0.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Architecture of a deep neural network. Figure taken from Nielsen (2015), CC-BY-NC.\relax }}{20}{figure.caption.13}\protected@file@percent }
\newlabel{fig-dnn}{{2.1}{20}{Architecture of a deep neural network. Figure taken from Nielsen (2015), CC-BY-NC.\relax }{figure.caption.13}{}}
\newlabel{eq-fullyconnected}{{2.1}{20}{Deep neural networks}{equation.2.0.1}{}}
